[{"title":"【环境配置】Redis与MySQL多实例配置","date":"2020-02-11T15:08:46.000Z","path":"article/20200211.html","text":"最近由于工作的需要，需要在同一台服务器上搭建两个Redis与MySQL的实例。多实例：就是在一台机器上面开启多个不同的端口(如Redis用6379/6380，MySQL用3306/3307等)，运行多个服务进程；公用一套安装程序，使用不同的配置文件，数据文件。 1. Redis多实例配置1.1 查看主机Redis信息 用ps命令查看Redis进程 123[root@localhost ~] ps -ef |grep redisroot 1706 1 0 2019 ? 04:12:09 /usr/local/bin/redis-server *:6379 root 18174 2560 0 15:35 pts/0 00:00:00 grep redis 查找配置文件位置 12[root@localhost ~] locate redis.conf/etc/redis.conf 1.2 拷贝配置文件并修改 拷贝redis.conf并命名为redis6380.conf，并修改参数 12345678910111213141516[root@localhost ~] cp /etc/redis.conf /etc/redis6380.conf[root@localhost ~] vim /etc/redis6380.conf# 查找 /pidfile 找到pid位置# pidfile /var/run/redis.pid #修改pid，每个实例需要运行在不同的pidpidfile /var/run/redis6380.pid# # 查找 /port 6379 找到端口位置# port 6379 #修改端口port 6380# # 查找 /dir 找到数据目录位置# dir /mnt/newdatadrive/data/redis #修改数据存放目录dir /mnt/newdatadrive/data/redis6380# # 已开启Redis持久化appendonly yes 准备上面配置的文件 12[root@localhost ~] mkdir –p /mnt/newdatadrive/data/redis6380[root@localhost ~] cp /var/run/redis.pid /var/run/redis6380.pid 1.3 启动测试 启动6380端口Redis服务，并查看Redis进程 12345[root@localhost ~] /usr/local/bin/redis-server /etc/redis6380.conf[root@localhost ~] ps -ef |grep redisroot 1706 1 0 2019 ? 04:12:00 /usr/local/bin/redis-server *:6379 root 15967 1 0 12:16 ? 00:00:00 /usr/local/bin/redis-server *:6380 root 15994 8014 0 12:16 pts/2 00:00:00 grep redis 测试登录Redis客户端 12[root@localhost ~] redis-cli -p 6380127.0.0.1:6380&gt; QUIT #退出 停止6380端口的Redis服务 1redis-cli -p 6380 shutdown 1.4 Redis数据迁移 登录原Redis客户端(6379) 123456[root@localhost ~] redis-cli -p 6379127.0.0.1:6379&gt; SAVE #数据备份127.0.0.1:6379&gt; CONFIG GET dir #查看Redis数据目录1) &quot;dir&quot;2) &quot;/mnt/newdatadrive/data/redis&quot;127.0.0.1:6379&gt; QUIT #退出 拷贝数据文件appendonly.aof和dump.rdb到6380 12345678# 查看6379的数据文件[root@localhost ~] cd /mnt/newdatadrive/data/redis &amp;&amp; lltotal 55176-rw-r--r-- 1 root root 55411226 Feb 11 09:25 appendonly.aof-rw-r--r-- 1 root root 1017181 Feb 11 12:28 dump.rdb# 拷贝到6380[root@localhost ~] \\cp /mnt/newdatadrive/data/redis/appendonly.aof /mnt/newdatadrive/data/redis6380/appendonly.aof[root@localhost ~] \\cp /mnt/newdatadrive/data/redis/dump.rdb /mnt/newdatadrive/data/redis6380/dump.rdb 启动6380端口Redis服务，导入AOF数据文件 12[root@localhost ~] /usr/local/bin/redis-server /etc/redis6380.conf[root@localhost ~] redis-cli -p 6380 --pipe &lt; /mnt/newdatadrive/data/redis6380/appendonly.aof 登录Redis查看数据 12[root@localhost ~] redis-cli -p 6380127.0.0.1:6380&gt; #输入具体命令查看数据 1.5 配置远程可访问 修改配置文件redis6380.conf 1234567[root@localhost ~] vim /etc/redis6380.conf# 查找 /bind 找到：bind 127.0.0.1并注释，其它ip地址也可访问# bind 127.0.0.1# # 查找 /requirepass 去掉注释#，并把foobared 替换为密码，例如：password123456# requirepass foobaredrequirepass password123456 开启防火墙的端口号规则（安全组），将6380端口号开通 1[root@localhost ~] /sbin/iptables -I INPUT -p tcp --dport 6380 -j ACCEPT 修改完成后，要在服务里重启Redis服务才能使设置生效 1/usr/local/bin/redis-server /etc/redis6380.conf 测试远程访问 12C:\\Users\\zc&gt; redis-cli -h 192.168.111.226 -p 6380 -a password123456192.168.111.226:6380&gt; 停止6380的Redis服务也需要密码 1[root@localhost ~] redis-cli -p 6380 -a password123456 shutdown 2. MySQL多实例配置2.1 查看主机MySQL信息 查看现有MySQL数据库实例占用端口 123[root@localhost ~] netstat -anp | grep mysqldtcp6 0 0 :::3306 :::* LISTEN 1089/mysqld unix 2 [ ACC ] STREAM LISTENING 20497 1089/mysqld /var/lib/mysql/mysql.sock 须先关闭单实例，跟多实例会有冲突 备份数据：[root@localhost ~] mysqldump -P 3306 -u root -p --all-databases &gt; /home/backup/data3306.bak 停止单实例服务：[root@localhost ~] service mysqld stop 查找配置文件位置 123[root@localhost ~] locate my.cnf/etc/my.cnf/etc/my.cnf.d 2.2 添加一个3307端口的实例 拷贝my.cnf并命名为my3307.cnf，并修改参数，主要修改port,sockt,datadir 123456789101112131415161718192021222324252627282930[root@localhost ~] cp /etc/my.cnf /etc/my3307.cnf[root@localhost ~] vi /etc/my3307.cnf[mysqld]# server端字符集character-set-server=utf8collation-server=utf8_general_ciuser=root# 修改端口port=3307# 修改数据存放目录datadir=/var/lib/mysql3307# 客户端连接socketsocket=/var/lib/mysql/mysql3307.sock# 修改日志文件log-error=/var/log/mysqld3307.log# 修改pid，每个实例需要运行在不同的pidpid-file=/var/run/mysqld/mysqld3307.pid# 解决问题：TIMESTAMP with implicit DEFAULT value is deprecatedexplicit_defaults_for_timestamp=true# skip_grant_tables[mysql]socket=/var/lib/mysql/mysql3307.sockdefault-character-set=utf8[mysql.server]default-character-set=utf8[mysql_safe]default-character-set=utf8[client]socket=/var/lib/mysql/mysql3307.sockdefault-character-set=utf8 初始化数据库 123# 写入host避免反解析报错[root@localhost ~] echo &quot;127.0.0.1 `hostname`&quot; &gt;&gt; /etc/hosts &amp;&amp; cat /etc/hosts[root@localhost ~] mysqld --defaults-file=/etc/my3307.cnf --initialize-insecure 启动3307端口MySQL服务，并查看MySQL进程 1[root@localhost ~] mysqld --defaults-file=/etc/my3307.cnf --user=root &amp; 登录MySQL 1234# 多实例为root增加密码[root@localhost ~] mysqladmin -u root -S /var/lib/mysql/mysql3307.sock password &#x27;123qwe&#x27;# 登录[root@localhost ~] mysql -S /var/lib/mysql/mysql3307.sock -p 停止本实例MySQL服务 1[root@localhost ~] mysqladmin -u root -S /var/lib/mysql/mysql3307.sock shutdown 2.3 再添加一个3308端口的实例 拷贝my.cnf并命名为my3308.cnf，并修改参数，主要修改port,sockt,datadir 123456789101112131415161718192021222324252627282930[root@localhost ~] cp /etc/my.cnf /etc/my3308.cnf[root@localhost ~] vi /etc/my3308.cnf[mysqld]# server端字符集character-set-server=utf8collation-server=utf8_general_ciuser=root# 修改端口port=3308# 修改数据存放目录datadir=/var/lib/mysql3308# 客户端连接socketsocket=/var/lib/mysql/mysql3308.sock# 修改日志文件log-error=/var/log/mysqld3308.log# 修改pid，每个实例需要运行在不同的pidpid-file=/var/run/mysqld/mysqld3308.pid# 解决问题：TIMESTAMP with implicit DEFAULT value is deprecatedexplicit_defaults_for_timestamp=true# skip_grant_tables[mysql]socket=/var/lib/mysql/mysql3308.sockdefault-character-set=utf8[mysql.server]default-character-set=utf8[mysql_safe]default-character-set=utf8[client]socket=/var/lib/mysql/mysql3308.sockdefault-character-set=utf8 初始化数据库 1[root@localhost ~] mysqld --defaults-file=/etc/my3308.cnf --initialize-insecure 启动3308端口MySQL服务 1[root@localhost ~] mysqld --defaults-file=/etc/my3308.cnf --user=root &amp; 登录MySQL 1234# 多实例为root增加密码[root@localhost ~] mysqladmin -u root -S /var/lib/mysql/mysql3308.sock password &#x27;123qwe&#x27;# 登录[root@localhost ~] mysql -S /var/lib/mysql/mysql3308.sock -p 停止本实例MySQL服务 1[root@localhost ~] mysqladmin -u root -S /var/lib/mysql/mysql3308.sock shutdown 2.4 实例3307开启远程访问 开启3307端口防火墙 1[root@localhost ~] /sbin/iptables -I INPUT -p tcp --dport 3307 -j ACCEPT 测试远程访问 12C:\\Users\\zc&gt;mysql -h 192.168.111.227 -P 3307 -u root -pEnter password: ******","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"}]},{"title":"【安全认证】基于Shiro前后端分离的认证与授权(三.前端篇)","date":"2020-02-06T18:10:46.000Z","path":"article/20200207.html","text":"前两篇我们整合了SpringBoot+Shiro+JWT+Redis实现了登录认证，接口权限控制，接下来将要实现前端Vue的动态路由控制。 1. 前端权限控制思路（Vue）前端的权限控制，不同的权限对应着不同的路由，同时菜单也需根据不同的权限，异步生成。先回顾下整体流程： 登录: 提交账号和密码到服务端签发token，拿到token之后存入浏览器，再携带token(一般放在请求头中)再去获取用户的详细信息(包括用户权限等信息)。 权限验证：通过用户权限信息 构建 对应权限的路由，通过router.addRoutes动态挂载这些路由。 接下来将基于Vue开源后台模板vue-admin-template来演示具体流程，这里只演示重要代码，完整项目移步文章末尾获取源码。 2. 登录 先准备基础的静态路由(src/router/index.js) 12345678910111213141516171819202122232425export const constantRoutes = [ // 登陆页面 &#123; path: &#x27;/login&#x27;, component: () =&gt; import(&#x27;@/views/login/index&#x27;), hidden: true &#125;, // 首页 &#123; path: &#x27;/&#x27;, component: Layout, redirect: &#x27;/dashboard&#x27;, children: [&#123; path: &#x27;dashboard&#x27;, name: &#x27;Dashboard&#x27;, component: () =&gt; import(&#x27;@/views/dashboard/index&#x27;), meta: &#123; title: &#x27;首页&#x27;, icon: &#x27;dashboard&#x27; &#125; &#125;] &#125;, &#123; path: &#x27;/404&#x27;, component: () =&gt; import(&#x27;@/views/404&#x27;), hidden: true &#125;] 登录页面(src/views/login/index.vue)click事件触发登录操作 12345this.$store.dispatch(&#x27;user/login&#x27;, this.loginForm).then(() =&gt; &#123; this.$router.push(&#123; path: &#x27;/&#x27; &#125;); //登录成功之后重定向到首页&#125;).catch(err =&gt; &#123; this.$message.error(err); //登录失败提示错误&#125;); 登录逻辑(src/store/modules/user.js)action: 12345678910111213141516171819202122232425262728293031323334353637383940const actions = &#123; // user login login(&#123; commit &#125;, userInfo) &#123; const &#123; account, password &#125; = userInfo return new Promise((resolve, reject) =&gt; &#123; // 这里的login调用api接口请求数据 login(&#123; account: account.trim(), password: password &#125;).then(response =&gt; &#123; const &#123; data &#125; = response commit(&#x27;SET_TOKEN&#x27;, data) setToken(data) resolve() &#125;).catch(error =&gt; &#123; reject(error) &#125;) &#125;) &#125;, // get user info getInfo(&#123; commit, state &#125;) &#123; return new Promise((resolve, reject) =&gt; &#123; // 这里的getInfo调用api接口请求数据 getInfo(state.token).then(response =&gt; &#123; const &#123; data &#125; = response if (!data) &#123; reject(&#x27;Verification failed, please Login again.&#x27;) &#125; const &#123; nickname, avatar, roles, permissions &#125; = data // 全局储存用户信息 commit(&#x27;SET_NAME&#x27;, nickname) commit(&#x27;SET_AVATAR&#x27;, avatar) // 角色信息 commit(&#x27;SET_ROLES&#x27;, roles) // 指令权限信息 commit(&#x27;SET_PERMISSIONS&#x27;, permissions) resolve(data) &#125;).catch(error =&gt; &#123; reject(error) &#125;) &#125;) &#125;,&#125; /login与/getInfo接口与返回的数据 1234567891011121314151617181920POST (localhost:8282/login)请求参数: &#123;&quot;username&quot;:&quot;admin&quot;,&quot;password&quot;:&quot;123456&quot;&#125;响应: &#123; &quot;code&quot;:0, &quot;msg&quot;:&quot;登录成功&quot;, &quot;data&quot;:&quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJhZG1pbiIsInVpZCI6MSwiZXhwIjoxNTgwOTk4MTIzfQ.6jgqt_opjnosASlJ2oSIYZn1Sb2BQO-eUo_6OVTHv50&quot;&#125;// ------------getInfo--------------GET (localhost:8282/user/info)Headers: &#123;&quot;X-Token&quot;:&quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJhZG1pbiIsInVpZCI6MSwiZXhwIjoxNTgwOTk4MTIzfQ.6jgqt_opjnosASlJ2oSIYZn1Sb2BQO-eUo_6OVTHv50&quot;&#125;响应: &#123; &quot;code&quot;: 0, &quot;msg&quot;: &quot;获取成功&quot;, &quot;data&quot;: &#123; &quot;account&quot;: &quot;admin&quot;, &quot;nickname&quot;: &quot;超级管理员&quot;, &quot;roles&quot;: [&quot;admin&quot;], &quot;permissions&quot;: [&quot;user:list&quot;,&quot;user:add&quot;,&quot;user:delete&quot;,&quot;user:update&quot;] &#125;&#125; 获取用户信息(src/permission.js) 用户登录成功之后，我们会在全局钩子router.beforeEach中拦截路由，判断是否已获得token，在获得token之后我们就要去获取用户的基本信息 并且根据用户角色动态挂载路由。 123456789101112131415161718192021222324252627282930313233343536373839const whiteList = [&#x27;/login&#x27;] // 白名单router.beforeEach(async(to, from, next) =&gt; &#123; // 判断是否已获得token const hasToken = getToken() if (hasToken) &#123; if (to.path === &#x27;/login&#x27;) &#123; next(&#123; path: &#x27;/&#x27; &#125;) &#125; else &#123; const hasRole = store.getters.role if (hasRole) &#123; next() &#125; else &#123; try &#123; // 获取用户角色 [&#x27;admin&#x27;] 或,[&#x27;developer&#x27;,&#x27;editor&#x27;] const &#123; roles &#125; = await store.dispatch(&#x27;user/getInfo&#x27;) // 动态根据 角色 算出其对应有权限的路由 const accessRoutes = await store.dispatch(&#x27;permission/generateRoutes&#x27;, roles) // 动态挂载路由 router.addRoutes(accessRoutes) // addRouter是让挂载的路由生效，但是挂载后&#x27;router.options.routes&#x27;并未刷新(应该是个bug) // 所以还需要手动将路由加入&#x27;router.options.routes&#x27; router.options.routes = constantRoutes.concat(accessRoutes) next() &#125; catch (error) &#123; await store.dispatch(&#x27;user/resetToken&#x27;) Message.error(error || &#x27;Has Error&#x27;) next(`/login?redirect=$&#123;to.path&#125;`) &#125; &#125; &#125; &#125; else &#123; /* has no token*/ if (whiteList.indexOf(to.path) !== -1) &#123; next() &#125; else &#123; next(`/login?redirect=$&#123;to.path&#125;`) &#125; &#125;&#125;) 3. 动态挂载路由主要思路，前端会有一份包含所有路由的路由表。创建Vue实例时会先挂载登录等公共路由；当用户登录之后，通过getInfo(token)获取用户的角色(roles)，动态根据用户的roles算出其对应有权限的路由，再通过router.addRoutes动态挂载路由；使用vuex管理路由表，根据vuex中可访问的路由渲染菜单。但这些控制都只是页面级的，后端接口也需要做权限验证。 改造一下路由表，添加异步路由列表，将角色添加到元数据meta中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// 动态路由export const asyncRoutes = [ &#123; path: &#x27;/user&#x27;, component: Layout, redirect: &#x27;/user/list&#x27;, name: &#x27;User&#x27;, meta: &#123; title: &#x27;用户管理&#x27;, icon: &#x27;example&#x27; &#125;, children: [ &#123; path: &#x27;list&#x27;, name: &#x27;UserList&#x27;, component: () =&gt; import(&#x27;@/views/user/list&#x27;), meta: &#123; title: &#x27;用户列表&#x27;, icon: &#x27;nested&#x27; &#125; &#125;, &#123; path: &#x27;edit&#x27;, name: &#x27;UserEdit&#x27;, component: () =&gt; import(&#x27;@/views/user/form&#x27;), meta: &#123; title: &#x27;添加用户&#x27;, icon: &#x27;form&#x27; &#125; &#125; ] &#125;, &#123; path: &#x27;/admin&#x27;, component: Layout, children: [ &#123; path: &#x27;index&#x27;, name: &#x27;Form1&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), meta: &#123; title: &#x27;管理员角色测试&#x27;, icon: &#x27;form&#x27;, roles: [&#x27;admin&#x27;] &#125; &#125; ] &#125;, &#123; path: &#x27;/editor&#x27;, component: Layout, children: [ &#123; path: &#x27;index&#x27;, name: &#x27;Form2&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), meta: &#123; title: &#x27;编辑角色测试&#x27;, icon: &#x27;form&#x27;, roles: [&#x27;editor&#x27;] &#125; &#125; ] &#125;, &#123; path: &#x27;/form&#x27;, component: Layout, children: [ &#123; path: &#x27;index&#x27;, name: &#x27;Form3&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), meta: &#123; title: &#x27;用户角色测试&#x27;, icon: &#x27;form&#x27;, roles: [&#x27;user&#x27;] &#125; &#125; ] &#125;, &#123; path: &#x27;/nested&#x27;, component: Layout, redirect: &#x27;/nested/menu3&#x27;, name: &#x27;Nested&#x27;, meta: &#123; title: &#x27;子菜单权限测试&#x27;, icon: &#x27;form&#x27; &#125;, children: [ &#123; path: &#x27;menu1&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), name: &#x27;Menu1&#x27;, meta: &#123; title: &#x27;管理员可见&#x27;, roles: [&#x27;admin&#x27;] &#125; &#125;, &#123; path: &#x27;menu2&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), name: &#x27;Menu1&#x27;, meta: &#123; title: &#x27;编辑者可见&#x27;, roles: [&#x27;editor&#x27;] &#125; &#125;, &#123; path: &#x27;menu3&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), name: &#x27;Menu1&#x27;, meta: &#123; title: &#x27;普通用户可见&#x27;, roles: [&#x27;user&#x27;] &#125; &#125; ] &#125;, 根据前面获取用户信息的代码可发现，通过store.dispatch(&#39;permission/generateRoutes&#39;,roles)来获得有权限的路由，新建src/store/modules/permission.js如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import &#123; asyncRoutes, constantRoutes &#125; from &#x27;@/router&#x27;/** 判断用户是否拥有此路由的权限 */function hasPermission(roles, route) &#123; if (route.meta &amp;&amp; route.meta.roles) &#123; return roles.some(role =&gt; route.meta.roles.includes(role)) &#125; else &#123; return true &#125;&#125;/** 递归组装路由表，返回符合用户角色权限的路由列表 */export function filterAsyncRoutes(routes, roles) &#123; const res = [] routes.forEach(route =&gt; &#123; const tmp = &#123; ...route &#125; if (hasPermission(roles, tmp)) &#123; if (tmp.children) &#123; // 递归调用 tmp.children = filterAsyncRoutes(tmp.children, roles) &#125; res.push(tmp) &#125; &#125;) return res&#125;const state = &#123; routes: [], // 所有路由,包括静态路由和动态路由 addRoutes: [] // 动态路由&#125;const mutations = &#123; SET_ROUTES: (state, routes) =&gt; &#123; state.addRoutes = routes // 合并路由 state.routes = constantRoutes.concat(routes) &#125;&#125;const actions = &#123; // 生成动态路由 generateRoutes(&#123; commit &#125;, roles) &#123; return new Promise(resolve =&gt; &#123; let accessedRoutes if (roles.includes(&#x27;admin&#x27;)) &#123; // &#x27;超级管理员&#x27;拥有所有的路由，这样判断节省加载时间 accessedRoutes = asyncRoutes || [] &#125; else &#123; // 筛选出该角色有权限的路由 accessedRoutes = filterAsyncRoutes(asyncRoutes, roles) &#125; commit(&#x27;SET_ROUTES&#x27;, accessedRoutes) resolve(accessedRoutes) &#125;) &#125;&#125;export default &#123; namespaced: true, state, mutations, actions&#125; 4. axios拦截器通过request拦截器在每个请求头里面塞入token，好让后端对请求进行权限验证；代码位置:src/utils/request.js。 12345678910111213141516171819202122import axios from &#x27;axios&#x27;import store from &#x27;@/store&#x27;import &#123; getToken &#125; from &#x27;@/utils/auth&#x27;// create an axios instanceconst service = axios.create(&#123; baseURL: process.env.VUE_APP_BASE_API, // url = base url + request url withCredentials: false, // send cookies when cross-domain requests timeout: 5000 // request timeout&#125;)service.interceptors.request.use( config =&gt; &#123; if (store.getters.token) &#123; // 登陆后将token放入headers[&#x27;X-Token&#x27;]中 config.headers[&#x27;X-Token&#x27;] = getToken() &#125; return config &#125;, error =&gt; &#123; return Promise.reject(error) &#125;)export default service 5. 指令权限可以使用全局权限判断函数，实现按钮级别的权限判断。 12345678910111213141516&lt;template&gt; &lt;el-button v-if=&quot;checkPermission(&#x27;user:add&#x27;)&quot;&gt;添加&lt;/el-button&gt; &lt;el-button v-if=&quot;checkPermission(&#x27;user:delete&#x27;)&quot;&gt;删除&lt;/el-button&gt; &lt;el-button v-if=&quot;checkPermission(&#x27;user:update&#x27;)&quot;&gt;修改&lt;/el-button&gt; &lt;el-button v-if=&quot;checkPermission(&#x27;user:list&#x27;)&quot;&gt;查看&lt;/el-button&gt;&lt;/template&gt;&lt;script&gt;import checkPermission from &#x27;@/utils/permission&#x27; // 权限判断函数export default&#123; methods: &#123; checkPermission(value) &#123; return checkPermission(value) &#125; &#125;&#125;&lt;/script&gt; src/utils/permission.js: 12345import store from &#x27;@/store&#x27;export default function checkPermission(value) &#123; const permissions = store.getters.permissions return permissions.indexOf(value) &gt; -1&#125; 6. 效果演示： 注：搭建到这里的代码在github源码tag的V3.0中。源码地址: https://github.com/chaooo/springboot-vue-shiro.git","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"ssafe","permalink":"http://chaooo.github.io/tags/ssafe/"}]},{"title":"【安全认证】基于Shiro前后端分离的认证与授权(二.授权篇)","date":"2020-01-21T10:10:55.000Z","path":"article/20200121.html","text":"前面我们整合了SpringBoot+Shiro+JWT实现了登录认证，但还没有实现权限控制，这是接下来的工作。 1. JWT的Token续签1.1 续签思路 业务逻辑： 登录成功后，用户在未过期时间内继续操作，续签token。 登录成功后，空闲超过过期时间，返回token已失效，重新登录。 实现逻辑： 登录成功后将token存储到redis里面(这时候k、v值一样都为token)，并设置过期时间为token过期时间 当用户请求时token值还未过期，则重新设置redis里token的过期时间。 当用户请求时token值已过期，但redis中还在，则JWT重新生成token并覆盖v值(这时候k、v值不一样了)，然后设置redis过期时间。 当用户请求时token值已过期，并且redis中也不存在，则用户空闲超时，返回token已失效，重新登录。 1.2 编码实现 pom.xml引入Redis 12345678910&lt;!-- Redis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.8.0&lt;/version&gt;&lt;/dependency&gt; 编写Redis工具类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Componentpublic class RedisUtil &#123; @Resource private RedisTemplate&lt;String, Object&gt; redisTemplate; /** * 指定缓存失效时间 * @param key 键 * @param time 时间(秒) */ public boolean expire(String key, long time) &#123; try &#123; if (time &gt; 0) &#123; redisTemplate.expire(key, time, TimeUnit.SECONDS); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 根据key 获取过期时间 * @param key 键 不能为null * @return 时间(秒) 返回0代表为永久有效 */ public long getExpire(String key) &#123; return redisTemplate.getExpire(key, TimeUnit.SECONDS); &#125; /** * 普通缓存放入并设置时间 * @param key 键 * @param value 值 * @param time 时间(秒) time要大于0 如果time小于等于0 将设置无限期 * @return true成功 false 失败 */ public boolean set(String key, Object value, long time) &#123; try &#123; if (time &gt; 0) &#123; redisTemplate.opsForValue().set(key, value, time, TimeUnit.SECONDS); &#125; else &#123; set(key, value); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125;&#125; JwtUtil中增加返回过期秒数的方法 123456789101112public class JwtUtil &#123; /** 设置过期时间: 30分钟 */ private static final long EXPIRE_TIME = 30 * 60 * 1000; //... 其他代码省略 /** * 返回设置的过期秒数 * @return long 秒数 */ public static long getExpireTime()&#123; return EXPIRE_TIME/1000; &#125;&#125; 改写登录逻辑，生成token后存入Redis 1234567891011121314151617181920212223242526272829303132@Servicepublic class SysServiceImpl implements SysService &#123; private String getToken(User user)&#123; // 生成token String token = JwtUtil.createToken(user); // 为了过期续签，将token存入redis，并设置超时时间 redisUtil.set(token, token, JwtUtil.getExpireTime()); return token; &#125; /** * 用户登录(用户名，密码) * @param account 用户名 * @param password 密码 * @return token */ @Override public ResponseVo&lt;String&gt; login(String account, String password) &#123; //处理比对密码 User user = sysDao.selectByAccount(account); if(user!=null) &#123; String salt = user.getSalt(); String md5Password = Md5Util.md5(password+salt); String dbPassword = user.getPassword(); if(md5Password.equals(dbPassword)) &#123; //生成token给用户，并存入redis String token = getToken(user); return new ResponseVo&lt;&gt;(0,&quot;登录成功&quot;, token); &#125; &#125; return new ResponseVo&lt;&gt;( -1, &quot;登录失败&quot;); &#125;&#125; 改写MyRealm，加入token续签逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Slf4j@Component(&quot;MyRealm&quot;)public class MyRealm extends AuthorizingRealm &#123; /** * JWT Token续签： * 业务逻辑：登录成功后，用户在未过期时间内继续操作，续签token。 * 登录成功后，空闲超过过期时间，返回token已失效，重新登录。 * 实现逻辑： * 1.登录成功后将token存储到redis里面(这时候k、v值一样都为token)，并设置过期时间为token过期时间 * 2.当用户请求时token值还未过期，则重新设置redis里token的过期时间。 * 3.当用户请求时token值已过期，但redis中还在，则JWT重新生成token并覆盖v值(这时候k、v值不一样了)，然后设置redis过期时间。 * 4.当用户请求时token值已过期，并且redis中也不存在，则用户空闲超时，返回token已失效，重新登录。 */ public boolean tokenRefresh(String token, User user) &#123; String cacheToken = String.valueOf(redisUtil.get(token)); // 过期后会得到&quot;null&quot;值，所以需判断字符串&quot;null&quot; if (cacheToken != null &amp;&amp; cacheToken.length() != 0 &amp;&amp; !&quot;null&quot;.equals(cacheToken)) &#123; // 校验token有效性 if (!JwtUtil.isVerify(cacheToken)) &#123; // 生成token String newToken = JwtUtil.createToken(user); // 将token存入redis,并设置超时时间 redisUtil.set(token, newToken, JwtUtil.getExpireTime()); &#125; else &#123; // 重新设置超时时间 redisUtil.expire(token, JwtUtil.getExpireTime()); &#125; log.info(&quot;打印存入redis的过期时间：&quot;+redisUtil.getExpire(token)); return true; &#125; return false; &#125; /** * 重写认证逻辑 */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken auth) throws AuthenticationException &#123; log.info(&quot;————————身份认证——————————&quot;); String token = (String) auth.getCredentials(); if (null == token) &#123; throw new AuthenticationException(&quot;token为空!&quot;); &#125; // 解密获得username，用于和数据库进行对比 String account = JwtUtil.parseTokenAud(token); User user = sysService.selectByAccount(account); if (null == user) &#123; throw new AuthenticationException(&quot;用户不存在!&quot;); &#125; // 校验token是否过期 if (!tokenRefresh(token, user)) &#123; throw new AuthenticationException(&quot;Token已过期!&quot;); &#125; return new SimpleAuthenticationInfo(user, token,&quot;MyRealm&quot;); &#125;&#125; 到此，JWT的Token续签的功能已经全部实现了。 2. 权限管理2.1 首先增加三张数据表123456789101112131415161718192021222324252627282930313233343536/** 角色表 */DROP TABLE IF EXISTS `sys_role`;CREATE TABLE `sys_role` ( `id` INT(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;主键id&#x27;, `role_name` VARCHAR(100) DEFAULT NULL COMMENT &#x27;角色名称&#x27;, `description` VARCHAR(100) DEFAULT NULL COMMENT &#x27;描述&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=INNODB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT=&#x27;角色表&#x27;;INSERT INTO `sys_role`(`id`,`role_name`,`description`) VALUES (1,&#x27;admin&#x27;,&#x27;管理角色&#x27;),(2,&#x27;user&#x27;,&#x27;用户角色&#x27;);/** 权限表 */DROP TABLE IF EXISTS `sys_permission`;CREATE TABLE `sys_permission` ( `id` VARCHAR(32) NOT NULL COMMENT &#x27;主键id&#x27;, `name` VARCHAR(100) DEFAULT NULL COMMENT &#x27;菜单标题&#x27;, `url` VARCHAR(255) DEFAULT NULL COMMENT &#x27;路径&#x27;, `menu_type` INT(11) DEFAULT NULL COMMENT &#x27;菜单类型(0:一级菜单; 1:子菜单:2:按钮权限)&#x27;, `perms` VARCHAR(255) DEFAULT NULL COMMENT &#x27;菜单权限编码&#x27;, `sort_no` INT(10) DEFAULT NULL COMMENT &#x27;菜单排序&#x27;, `del_flag` INT(1) DEFAULT &#x27;0&#x27; COMMENT &#x27;删除状态 0正常 1已删除&#x27;, PRIMARY KEY (`id`) USING BTREE, KEY `index_prem_sort_no` (`sort_no`) USING BTREE, KEY `index_prem_del_flag` (`del_flag`) USING BTREE) ENGINE=INNODB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT=&#x27;菜单权限表&#x27;;INSERT INTO `sys_permission`(`id`,`name`,`url`,`menu_type`,`perms`,`sort_no`,`del_flag`) VALUES (&#x27;1&#x27;,&#x27;新增用户&#x27;,&#x27;/user/add&#x27;,2,&#x27;user:add&#x27;,1,0),(&#x27;2&#x27;,&#x27;删除用户&#x27;,&#x27;/user/delete&#x27;,2,&#x27;user:delete&#x27;,2,0),(&#x27;3&#x27;,&#x27;修改用户&#x27;,&#x27;/user/update&#x27;,2,&#x27;user:update&#x27;,3,0),(&#x27;4&#x27;,&#x27;查询用户&#x27;,&#x27;/user/list&#x27;,2,&#x27;user:list&#x27;,4,0);/** 角色与权限关联表 */DROP TABLE IF EXISTS `sys_role_permission`;CREATE TABLE `sys_role_permission` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `role_id` INT(11) DEFAULT NULL COMMENT &#x27;角色id&#x27;, `permission_id` INT(11) DEFAULT NULL COMMENT &#x27;权限id&#x27;, PRIMARY KEY (`id`) USING BTREE, KEY `index_group_role_per_id` (`role_id`,`permission_id`) USING BTREE, KEY `index_group_role_id` (`role_id`) USING BTREE, KEY `index_group_per_id` (`permission_id`) USING BTREE) ENGINE=INNODB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT=&#x27;角色权限表&#x27;;INSERT INTO `sys_role_permission`(`id`,`role_id`,`permission_id`) VALUES (1,1,1),(2,1,2),(3,1,3),(4,1,4),(5,2,4); 2.2 编码实现 补全MyRealm中授权验证逻辑 1234567891011121314151617181920212223242526272829@Slf4j@Component(&quot;MyRealm&quot;)public class MyRealm extends AuthorizingRealm &#123; //...其他代码省略/** * 获取用户权限信息，包括角色以及权限。 * 只有当触发检测用户权限时才会调用此方法，例如checkRole,checkPermissionJwtToken */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; log.info(&quot;————权限认证 [ roles、permissions]————&quot;); User user = null; if (principals != null) &#123; user = (User) principals.getPrimaryPrincipal(); &#125; SimpleAuthorizationInfo simpleAuthorizationInfo = new SimpleAuthorizationInfo(); if (user != null) &#123; // 用户拥有的角色，比如“admin/user” String role = sysService.getRoleByRoleid(user.getRoleid()); simpleAuthorizationInfo.addRole(role); log.info(&quot;角色为：&quot;+role); // 用户拥有的权限集合，比如“role:add,user:add” Set&lt;String&gt; permissions = sysService.getPermissionsByRoleid(user.getRoleid()); simpleAuthorizationInfo.addStringPermissions(permissions); log.info(&quot;权限有：&quot;+permissions.toString()); &#125; return simpleAuthorizationInfo; &#125;&#125; Service中添加获取角色与权限的方法，DAO与Mapper请移步源码。 1234567891011121314151617181920212223242526272829public interface SysService &#123; /** * 根据roleid查找用户角色名，自定义Realm中调用 * @param roleid * @return roles */ String getRoleByRoleid(Integer roleid); /** * 根据roleid查找用户权限，自定义Realm中调用 * @param roleid * @return Set&lt;permissions&gt; */ Set&lt;String&gt; getPermissionsByRoleid(Integer roleid);&#125;/** * 实现类 */@Servicepublic class SysServiceImpl implements SysService &#123; @Override public String getRoleByRoleid(Integer roleid) &#123; return sysDao.getRoleByRoleid(roleid); &#125; @Override public Set&lt;String&gt; getPermissionsByRoleid(Integer roleid) &#123; return sysDao.getPermissionsByRoleid(roleid); &#125;&#125; Controller中使用@RequiresPermissions来控制权限 1234567891011121314151617181920212223@RestControllerpublic class UserApi &#123; /** * 获取所有用户信息 * @return */ @RequiresPermissions(&quot;user:list&quot;) @GetMapping(&quot;/user/list&quot;) public ResponseVo list() &#123; return userService.loadUser(); &#125; /** * 用户更新资料 * @param user * @return */ @RequiresPermissions(&quot;user:update&quot;) @PostMapping(&quot;/user/update&quot;) public ResponseVo update(User user, HttpServletRequest request) &#123; String token = request.getHeader(&quot;X-Token&quot;); return userService.modifyUser(token, user); &#125;&#125; 注：这里的登录认证+授权控制 在github源码tag的V2.0中，后续版本再加入前端动态路由控制等。源码地址: https://github.com/chaooo/springboot-vue-shiro.git仅下载后端认证+授权控制源码:git clone --branch V2.0 https://github.com/chaooo/springboot-vue-shiro.git","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"ssafe","permalink":"http://chaooo.github.io/tags/ssafe/"}]},{"title":"【安全认证】基于Shiro前后端分离的认证与授权(一.认证篇)","date":"2020-01-18T15:26:02.000Z","path":"article/20200118.html","text":"1. 开始之前1.1 技术选型选用SpringBoot+Shiro+JWT实现登录认证，结合Redis服务实现token的续签，前端选用Vue动态构造路由及更细粒度的操作权限控制。 前后端分离项目中，我们一般采用的是无状态登录：服务端不保存任何客户端请求者信息，客户端需要自己携带着信息去访问服务端，并且携带的信息可以被服务端辨认。 而Shiro默认的拦截跳转都是跳转url页面，拦截校验机制恰恰使用的session；而前后端分离后，后端并无权干涉页面跳转。 因此前后端分离项目中使用Shiro就需要对其进行改造，我们可以在整合Shiro的基础上自定义登录校验，继续整合JWT(或者oauth2.0等)，使其成为支持服务端无状态登录，即token登录。 在Vue项目中，只需要根据登录用户的权限信息动态的加载路由列表就可以动态的构造出访问菜单。 1.2 整体流程 首次通过post请求将用户名与密码到login进行登入，登录成功后返回token； 每次请求，客户端需通过header将token带回服务器做JWT Token的校验； 服务端负责token生命周期的刷新，用户权限的校验； 2. SpringBoot整合Shiro+JWT这里贴出主要逻辑，源码请移步文章末尾获取。 数据表 12345678910111213141516/** 系统用户表 */DROP TABLE IF EXISTS sys_user;CREATE TABLE sys_user( id INT AUTO_INCREMENT COMMENT &#x27;用户ID&#x27;, account VARCHAR(30) NOT NULL COMMENT &#x27;用户名&#x27;, PASSWORD VARCHAR(50) COMMENT &#x27;用户密码&#x27;, salt VARCHAR(8) COMMENT &#x27;随机盐&#x27;, nickname VARCHAR(30) COMMENT &#x27;用户昵称&#x27;, roleId INT COMMENT &#x27;角色ID&#x27;, createTime DATE COMMENT &#x27;创建时间&#x27;, updateTime DATE COMMENT &#x27;更新时间&#x27;, deleteStatus VARCHAR(2) DEFAULT &#x27;1&#x27; COMMENT &#x27;是否有效：1有效，2无效&#x27;, CONSTRAINT sys_user_id_pk PRIMARY KEY(id), CONSTRAINT sys_user_account_uk UNIQUE(account));COMMIT; pom.xml 123456789101112&lt;!-- JWT --&gt;&lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;/groupId&gt; &lt;artifactId&gt;java-jwt&lt;/artifactId&gt; &lt;version&gt;3.8.3&lt;/version&gt;&lt;/dependency&gt;&lt;!-- shiro --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.2&lt;/version&gt;&lt;/dependency&gt; shiro配置类：构建securityManager环境，及配置shiroFilter并将jwtFilter添加进shiro的拦截器链中，放行登录注册请求。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566@Configurationpublic class ShiroConfig &#123; @Bean(&quot;securityManager&quot;) public DefaultWebSecurityManager getManager(MyRealm myRealm) &#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 使用自己的realm securityManager.setRealm(myRealm); /* * 关闭shiro自带的session，详情见文档 * http://shiro.apache.org/session-management.html#SessionManagement-StatelessApplications%28Sessionless%29 */ DefaultSubjectDAO subjectDAO = new DefaultSubjectDAO(); DefaultSessionStorageEvaluator defaultSessionStorageEvaluator = new DefaultSessionStorageEvaluator(); defaultSessionStorageEvaluator.setSessionStorageEnabled(false); subjectDAO.setSessionStorageEvaluator(defaultSessionStorageEvaluator); securityManager.setSubjectDAO(subjectDAO); return securityManager; &#125; @Bean(&quot;shiroFilter&quot;) public ShiroFilterFactoryBean factory(DefaultWebSecurityManager securityManager) &#123; ShiroFilterFactoryBean factoryBean = new ShiroFilterFactoryBean(); factoryBean.setSecurityManager(securityManager); // 拦截器 Map&lt;String, String&gt; filterChainDefinitionMap = new LinkedHashMap&lt;String, String&gt;(); // 配置不会被拦截的链接 顺序判断，规则：http://shiro.apache.org/web.html#urls- filterChainDefinitionMap.put(&quot;/register&quot;, &quot;anon&quot;); filterChainDefinitionMap.put(&quot;/login&quot;, &quot;anon&quot;); filterChainDefinitionMap.put(&quot;/unauthorized&quot;, &quot;anon&quot;); // 添加自己的过滤器并且取名为jwt Map&lt;String, Filter&gt; filterMap = new HashMap&lt;&gt;(1); filterMap.put(&quot;jwt&quot;, new JwtFilter()); factoryBean.setFilters(filterMap); // 过滤链定义，从上向下顺序执行，一般将/**放在最为下边 filterChainDefinitionMap.put(&quot;/**&quot;, &quot;jwt&quot;); // 未授权返回 factoryBean.setUnauthorizedUrl(&quot;/unauthorized&quot;); factoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return factoryBean; &#125; /** * 添加注解支持 */ @Bean @DependsOn(&quot;lifecycleBeanPostProcessor&quot;) public DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator() &#123; DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator = new DefaultAdvisorAutoProxyCreator(); // 强制使用cglib，防止重复代理和可能引起代理出错的问题 // https://zhuanlan.zhihu.com/p/29161098 defaultAdvisorAutoProxyCreator.setProxyTargetClass(true); return defaultAdvisorAutoProxyCreator; &#125; @Bean public LifecycleBeanPostProcessor lifecycleBeanPostProcessor() &#123; return new LifecycleBeanPostProcessor(); &#125; @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(DefaultWebSecurityManager securityManager) &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); advisor.setSecurityManager(securityManager); return advisor; &#125;&#125; 自定义Realm：继承AuthorizingRealm类，在其中实现登陆验证及权限获取的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Slf4j@Component(&quot;MyRealm&quot;)public class MyRealm extends AuthorizingRealm &#123; /** 注入SysService */ private SysService sysService; @Autowired public void setSysService(SysService sysService) &#123; this.sysService = sysService; &#125; /** * 必须重写此方法，不然Shiro会报错 */ @Override public boolean supports(AuthenticationToken token) &#123; return token instanceof JwtToken; &#125; /** * 用来进行身份认证，也就是说验证用户输入的账号和密码是否正确， * 获取身份验证信息，错误抛出异常 */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken auth) throws AuthenticationException &#123; log.info(&quot;————————身份认证——————————&quot;); String token = (String) auth.getCredentials(); if (null == token || !JwtUtil.isVerify(token)) &#123; throw new AuthenticationException(&quot;token无效!&quot;); &#125; // 解密获得username，用于和数据库进行对比 String account = JwtUtil.parseTokenAud(token); User user = sysService.selectByAccount(account); if (null == user) &#123; throw new AuthenticationException(&quot;用户不存在!&quot;); &#125; return new SimpleAuthenticationInfo(user, token,&quot;MyRealm&quot;); &#125; /** * 获取用户权限信息，包括角色以及权限。 * 只有当触发检测用户权限时才会调用此方法，例如checkRole,checkPermissionJwtToken */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; log.info(&quot;————权限认证 [ roles、permissions]————&quot;); SimpleAuthorizationInfo simpleAuthorizationInfo = new SimpleAuthorizationInfo(); /* 暂不编写，此处编写后，controller中可以使用@RequiresPermissions来对用户权限进行拦截 */ return simpleAuthorizationInfo; &#125;&#125; 鉴权登录过滤器：继承BasicHttpAuthenticationFilter类,该拦截器需要拦截所有请求除(除登陆、注册等请求)，用于判断请求是否带有token，并获取token的值传递给shiro的登陆认证方法作为参数，用于获取token； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Slf4jpublic class JwtFilter extends BasicHttpAuthenticationFilter &#123; @Override protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) &#123; try &#123; executeLogin(request, response); return true; &#125; catch (Exception e) &#123; unauthorized(response); return false; &#125; &#125; /** * 认证 */ @Override protected boolean executeLogin(ServletRequest request, ServletResponse response) &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; String authorization = httpServletRequest.getHeader(&quot;X-Token&quot;); JwtToken token = new JwtToken(authorization); // 提交给realm进行登入，如果错误他会抛出异常并被捕获 getSubject(request, response).login(token); return true; &#125; /** * 认证失败 跳转到 /unauthorized */ private void unauthorized(ServletResponse resp) &#123; try &#123; HttpServletResponse httpServletResponse = (HttpServletResponse) resp; httpServletResponse.sendRedirect(&quot;/unauthorized&quot;); &#125; catch (IOException e) &#123; log.error(e.getMessage()); &#125; &#125; /** * 对跨域提供支持 */ @Override protected boolean preHandle(ServletRequest request, ServletResponse response) throws Exception &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = (HttpServletResponse) response; httpServletResponse.setHeader(&quot;Access-control-Allow-Origin&quot;, httpServletRequest.getHeader(&quot;Origin&quot;)); httpServletResponse.setHeader(&quot;Access-Control-Allow-Methods&quot;, &quot;GET,POST,OPTIONS,PUT,DELETE&quot;); httpServletResponse.setHeader(&quot;Access-Control-Allow-Headers&quot;, httpServletRequest.getHeader(&quot;Access-Control-Request-Headers&quot;)); // 跨域时会首先发送一个option请求，给option请求直接返回正常状态 if (httpServletRequest.getMethod().equals(RequestMethod.OPTIONS.name())) &#123; httpServletResponse.setStatus(HttpStatus.OK.value()); return false; &#125; return super.preHandle(request, response); &#125;&#125; JwtToken 1234567891011121314public class JwtToken implements AuthenticationToken &#123; private String token; JwtToken(String token) &#123; this.token = token; &#125; @Override public Object getPrincipal() &#123; return token; &#125; @Override public Object getCredentials() &#123; return token; &#125;&#125; JWT工具类：利用登陆信息生成token，根据token获取username，token验证等方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class JwtUtil &#123; /** 设置过期时间: 30分钟 */ private static final long EXPIRE_TIME = 30 * 60 * 1000; /** 服务端的私钥secret,在任何场景都不应该流露出去 */ private static final String TOKEN_SECRET = &quot;zhengchao&quot;; /** * 生成签名，30分钟过期 */ public static String createToken(User user) &#123; try &#123; // 设置过期时间 Date date = new Date(System.currentTimeMillis() + EXPIRE_TIME); // 私钥和加密算法 Algorithm algorithm = Algorithm.HMAC256(TOKEN_SECRET); // 设置头部信息 Map&lt;String, Object&gt; header = new HashMap&lt;&gt;(2); header.put(&quot;typ&quot;, &quot;JWT&quot;); header.put(&quot;alg&quot;, &quot;HS256&quot;); // 返回token字符串 return JWT.create() .withHeader(header) .withClaim(&quot;aud&quot;, user.getAccount()) .withClaim(&quot;uid&quot;, user.getId()) .withExpiresAt(date) .sign(algorithm); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 检验token是否正确 */ public static boolean isVerify(String token)&#123; try &#123; Algorithm algorithm = Algorithm.HMAC256(TOKEN_SECRET); JWTVerifier verifier = JWT.require(algorithm).build(); verifier.verify(token); return true; &#125; catch (Exception e)&#123; return false; &#125; &#125; /** *从token解析出uid信息,用户ID */ public static int parseTokenUid(String token) &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(&quot;uid&quot;).asInt(); &#125; /** *从token解析出aud信息,用户名 */ public static String parseTokenAud(String token) &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(&quot;aud&quot;).asString(); &#125; /** *从token解析出过期时间 */ public static Date paraseExpiresTime(String token)&#123; DecodedJWT jwt = JWT.decode(token); return jwt.getExpiresAt(); &#125;&#125; MD5加密工具类 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Md5Util &#123; /** * md5加密 * @param s：待加密字符串 * @return 加密后16进制字符串 */ public static String md5(String s) &#123; try &#123; //实例化MessageDigest的MD5算法对象 MessageDigest md = MessageDigest.getInstance(&quot;MD5&quot;); //通过digest方法返回哈希计算后的字节数组 byte[] bytes = md.digest(s.getBytes(&quot;utf-8&quot;)); //将字节数组转换为16进制字符串并返回 return toHex(bytes); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * 获取随即盐 */ public static String salt()&#123; //利用UUID生成随机盐 UUID uuid = UUID.randomUUID(); //返回a2c64597-232f-4782-ab2d-9dfeb9d76932 String[] arr = uuid.toString().split(&quot;-&quot;); return arr[0]; &#125; /** * 字节数组转换为16进制字符串 * @param bytes数组 * @return 16进制字符串 */ private static String toHex(byte[] bytes) &#123; final char[] HEX_DIGITS = &quot;0123456789ABCDEF&quot;.toCharArray(); StringBuilder ret = new StringBuilder(bytes.length * 2); for (int i=0; i&lt;bytes.length; i++) &#123; ret.append(HEX_DIGITS[(bytes[i] &gt;&gt; 4) &amp; 0x0f]); ret.append(HEX_DIGITS[bytes[i] &amp; 0x0f]); &#125; return ret.toString(); &#125;&#125; 3. 注册与登录主要逻辑这里只贴出主要逻辑，DAO和Mapper映射可查看源码，源码请移步文章末尾获取。 登录Controller 1234567891011121314151617181920212223242526272829303132333435363738@RestControllerpublic class SysApi &#123; /** * 注入服务类 */ private SysService sysService; @Autowired public void setSysService(SysService sysService) &#123; this.sysService = sysService; &#125; /** * 注册(用户名，密码) * @param account * @param password * @return */ @PostMapping(&quot;/register&quot;) public ResponseVo&lt;String&gt; register(String account, String password) &#123; return sysService.register(account, password); &#125; /** * 登录(用户名，密码) * @param account * @param password * @return */ @PostMapping(&quot;/login&quot;) public ResponseVo&lt;String&gt; login(String account, String password) &#123; return sysService.login(account, password); &#125; /** * 处理非法请求 */ @GetMapping(&quot;/unauthorized&quot;) public ResponseVo unauthorized(HttpServletRequest request) &#123; return new ResponseVo(-1, &quot;Token失效请重新登录!&quot;); &#125;&#125; Service 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798public interface SysService &#123; /** * 注册(用户名，密码) */ ResponseVo&lt;String&gt; register(String account, String password); /** * 登录(用户名，密码) */ ResponseVo&lt;String&gt; login(String account, String password); /** * 根据account查找用户，自定义Realm中调用 */ User selectByAccount(String account);&#125;/** * 实现类 */@Servicepublic class SysServiceImpl implements SysService &#123; private SysDao sysDao; /** * 注入DAO */ @Autowired public void setSysDao(SysDao sysDao) &#123; this.sysDao = sysDao; &#125; /** * 用户注册(用户名，密码) * * @param account 用户名 * @param password 密码 * @return token */ @Override public ResponseVo&lt;String&gt; register(String account, String password) &#123; //检查用户名是否被占用 User user = sysDao.selectByAccount(account); if(user!=null) &#123; return new ResponseVo&lt;&gt;( -1, &quot;用户名被占用&quot;); &#125; //添加用户信息 user = new User(); //设置用户名 user.setAccount(account); //密码加密后再保存 String salt = Md5Util.salt(); String md5Password = Md5Util.md5(password+salt); user.setPassword(md5Password); user.setSalt(salt); //设置注册时间 user.setCreatetime(new Date()); //添加到数据库 int row = sysDao.insertSelective(user); //返回信息 if(row&gt;0) &#123; //生成token给用户 String token = JwtUtil.createToken(user); return new ResponseVo&lt;&gt;(0,&quot;注册成功&quot;, token); &#125;else &#123; return new ResponseVo&lt;&gt;( -1, &quot;注册失败&quot;); &#125; &#125; /** * 用户登录(用户名，密码) * * @param account 用户名 * @param password 密码 * @return token */ @Override public ResponseVo&lt;String&gt; login(String account, String password) &#123; //处理比对密码 User user = sysDao.selectByAccount(account); if(user!=null) &#123; String salt = user.getSalt(); String md5Password = Md5Util.md5(password+salt); String dbPassword = user.getPassword(); if(md5Password.equals(dbPassword)) &#123; //生成token给用户 String token = JwtUtil.createToken(user); return new ResponseVo&lt;&gt;(0,&quot;登录成功&quot;, token); &#125; &#125; return new ResponseVo&lt;&gt;( -1, &quot;登录失败&quot;); &#125; /** * 根据account查找用户，自定义Realm中调用 * * @param account * @return User */ @Override public User selectByAccount(String account) &#123; return sysDao.selectByAccount(account); &#125;&#125; 统一接口返回格式 123456789101112131415161718192021222324252627282930313233343536public class ResponseVo&lt;T&gt; &#123; /** 状态码 */ private int code; /** 提示信息 */ private String msg; /** 返回的数据 */ private T data; public ResponseVo() &#123;&#125; public ResponseVo(Integer code, String msg) &#123; this.code = code; this.msg = msg; &#125; public ResponseVo(Integer code, String msg, T data) &#123; this.code = code; this.msg = msg; this.data = data; &#125; public int getCode() &#123; return code; &#125; public void setCode(int code) &#123; this.code = code; &#125; public String getMsg() &#123; return msg; &#125; public void setMsg(String msg) &#123; this.msg = msg; &#125; public T getData() &#123; return data; &#125; public void setData(T data) &#123; this.data = data; &#125;&#125; 注：这里的登录认证逻辑在github源码tag的V1.0中，后续版本再加入Token续签和shiro前后端权限管理等。源码地址: https://github.com/chaooo/springboot-vue-shiro.git仅下载认证逻辑源码:git clone --branch V1.0 https://github.com/chaooo/springboot-vue-shiro.git","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"ssafe","permalink":"http://chaooo.github.io/tags/ssafe/"}]},{"title":"【安全认证】Shiro安全框架入门","date":"2019-12-22T12:44:40.000Z","path":"article/20191222.html","text":"1. 初识ShiroApache Shiro是一个强大易用的Java安全框架，提供了认证、授权、加密、会话管理、与Web集成、缓存等。 具体来说，满足对如下元素的支持： 用户，角色，权限(仅仅是操作权限，数据权限必须与业务需求紧密结合)，资源(url)。 用户分配角色，角色定义权限。 访问授权时支持角色或者权限，并且支持多级的权限定义。 Shiro作为一个完善的权限框架，可以应用在多种需要进行身份认证和访问授权的场景，例如：独立应用、web应用、spring框架中集成等。 2. Shiro整体架构在shiro架构中，有3个最主要的组件：Subject，SecurityManager，Realm。 Subject(如图上层部分)：”操作用户(主体)”，本质上就是当前访问用户的抽象描述。 SecurityManager(如图中层部分)：是Shiro架构中最核心的组件(控制器)，通过它可以协调其他组件完成用户认证和授权。 Authenticator：认证器，协调一个或者多个Realm，从Realm指定的数据源取得数据之后进行执行具体的认证。 Authorizer：授权器，用户访问控制授权，决定用户是否拥有执行指定操作的权限。 Session Manager：Session管理器，Shiro自己实现了一套Session管理机制。 Session DAO：实现了Session的操作，主要有增删改查。 CacheManager：缓存管理器，缓存角色数据和权限数据等。 Pluggable Realms：数据库与数据源之间的一个桥梁。Shiro获取认证信息、权限数据、角色数据 通过Realms来获取。 Cryptography：是用来做加解密，能非常快捷的实现数据加密。 Realm(如图下层部分)：定义了访问数据的方式，用来连接不同的数据源，如：LDAP，关系数据库，配置文件等等。 3. Shiro认证与授权3.1 Shiro认证【创建SecurityManager】&gt;【主体提交请求】&gt;【SecurityManager调用Authenticator去认证】&gt;【Realm验证】 操作用户（主体）提交请求到Security Manager调用Authenticator去认证,Authenticator通过Pluggable Realms去获取认证信息，Pluggable Realms是从下面的数据源（数据库）中去获取的认证信息，然后用通过Pluggable Realms从数据库中获取的认证信息和主体提交过来的认证数据做比对。 12345678910111213141516171819202122232425262728293031323334353637/** * Shiro认证 测试 */public class AuthenticationTest &#123; // 构建一个简单的数据源 SimpleAccountRealm simpleAccountRealm = new SimpleAccountRealm(); @Before public void addUser()&#123; // 参数分别为：用户名，密码，权限... simpleAccountRealm.addAccount(&quot;chaooo&quot;, &quot;123456&quot;, &quot;admin&quot;,&quot;user&quot;); &#125; /** * 认证测试方法 */ @Test public void testAuthentication()&#123; // 1.构建SecurityManager环境 DefaultSecurityManager defaultSecurityManager = new DefaultSecurityManager(); defaultSecurityManager.setRealm(simpleAccountRealm); // 2. 主体提交认证请求 SecurityUtils.setSecurityManager(defaultSecurityManager); Subject subject = SecurityUtils.getSubject(); // 3. 调用Subject.login(token)方法开始用户认证流程 UsernamePasswordToken token = new UsernamePasswordToken(&quot;chaooo&quot;, &quot;123456&quot;); try &#123; subject.login(token); &#125; catch (UnknownAccountException e) &#123; logger.error(String.format(&quot;用户不存在: %s&quot;, username), e); &#125; catch (IncorrectCredentialsException e) &#123; logger.error(String.format(&quot;密码不正确: %s&quot;, username), e); &#125; catch (ConcurrentAccessException e) &#123; logger.error(String.format(&quot;用户重复登录: %s&quot;, username), e); &#125; catch (AccountException e) &#123; logger.error(String.format(&quot;其他账户异常: %s&quot;, username), e); &#125; &#125;&#125; 3.2 Shiro授权shiro访问授权有3种实现方式：**api调用，java注解，jsp标签**。 通过api调用实现:【创建SecurityManager】&gt;【主体授权】&gt;【SecurityManager调用Authorizer授权】&gt;【Realm获取角色权限数据】 大体上和认证操作一样，也是通过Pluggable Realms从下面的数据源（数据库）中去获取权限数据,角色数据。 12345678910111213141516// 在执行访问授权验证之前，必须执行用户认证// 角色验证Subject subject = SecurityUtils.getSubject();if(subject.hasRole(&quot;admin&quot;)) &#123; //用户属于角色admin&#125;else&#123; //用户不属于角色admin&#125;// subject.checkRoles(&quot;admin&quot;,&quot;user&quot;);同时check多个角色// 权限验证String perm = &quot;log:manage:*&quot;;if(subject.isPermitted(perm)) &#123; logger.info(String.format(&quot;用户： %s 拥有权限：%s&quot;, name, perm));&#125;else &#123; logger.error(String.format(&quot;用户：%s 没有权限：%s&quot;, name, perm));&#125; 在spring框架中可以通过java注解 12345@RequiresPermissions(value=&#123;&quot;log:manage:*&quot;&#125;)public ModelAndView home(HttpServletRequest req) &#123; ModelAndView mv = new ModelAndView(&quot;home&quot;); return mv;&#125; 在JSP页面中还可以直接使用jsp标签 1234&lt;!-- 使用shiro标签 --&gt;&lt;shiro:hasPermission name=&quot;log:manage:*&quot;&gt; &lt;a href=&quot;&lt;%=request.getContextPath()%&gt;/user/home&quot;&gt;操作日志审计&lt;/a&gt;&lt;br/&gt;&lt;/shiro:hasPermission&gt; 3.3 Quickstart 新建一个Maven项目，pom导入jar包:shiro-all、slf4j-api、slf4j-log4j12、log4j; classpath下新建shiro.ini配置文件: shiro.ini123456789101112131415161718192021222324252627# -----------------------------------------------------------------------------# Users and their assigned roles## Each line conforms to the format defined in the# org.apache.shiro.realm.text.TextConfigurationRealm#setUserDefinitions JavaDoc# -----------------------------------------------------------------------------[users]# user &#x27;root&#x27; with password &#x27;secret&#x27; and the &#x27;admin&#x27; roleroot = secret, admin# user &#x27;guest&#x27; with the password &#x27;guest&#x27; and the &#x27;guest&#x27; roleguest = guest, guest# user &#x27;chaooo&#x27; with password &#x27;123456&#x27; and roles &#x27;user&#x27; and &#x27;guest&#x27;chaooo = 123456, user, guest# -----------------------------------------------------------------------------# Roles with assigned permissions# # Each line conforms to the format defined in the# org.apache.shiro.realm.text.TextConfigurationRealm#setRoleDefinitions JavaDoc# -----------------------------------------------------------------------------[roles]# &#x27;admin&#x27; role has all permissions, indicated by the wildcard &#x27;*&#x27;admin = *# The &#x27;schwartz&#x27; role can do anything (*) with any lightsaber:user = user:*# The &#x27;goodguy&#x27; role is allowed to &#x27;query&#x27; (action) the user (type) with license plate &#x27;zhangsan&#x27; (instance specific id)guest = user:query:zhangsan 启动运行Quickstart Quickstart.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596public class Quickstart &#123; private static final transient Logger log = LoggerFactory.getLogger(Quickstart.class); public static void main(String[] args) &#123; // 构建SecurityManager环境 DefaultSecurityManager securityManager = new DefaultSecurityManager(); IniRealm iniRealm = new IniRealm(&quot;classpath:shiro.ini&quot;); securityManager.setRealm(iniRealm); SecurityUtils.setSecurityManager(securityManager); // get the currently executing user: // 获取当前的 Subject Subject currentUser = SecurityUtils.getSubject(); // Do some stuff with a Session (no need for a web or EJB container!!!) // 测试使用 shiro的Session Session session = currentUser.getSession(); session.setAttribute(&quot;someKey&quot;, &quot;aValue&quot;); String value = (String) session.getAttribute(&quot;someKey&quot;); if (value.equals(&quot;aValue&quot;)) &#123; log.info(&quot;---&gt; Retrieved the correct value! [&quot; + value + &quot;]&quot;); &#125; // let&#x27;s login the current user so we can check against roles and permissions: // 测试当前的用户是否已经被认证. 即是否已经登录. // 调动 Subject 的 isAuthenticated() if (!currentUser.isAuthenticated()) &#123; // 把用户名和密码封装为 UsernamePasswordToken 对象 UsernamePasswordToken token = new UsernamePasswordToken(&quot;chaooo&quot;, &quot;123456&quot;); // rememberme token.setRememberMe(true); try &#123; // 执行登录. currentUser.login(token); &#125; // 若没有指定的账户, 则 shiro 将会抛出 UnknownAccountException 异常. catch (UnknownAccountException uae) &#123; log.info(&quot;----&gt; There is no user with username of &quot; + token.getPrincipal()); return; &#125; // 若账户存在, 但密码不匹配, 则 shiro 会抛出 IncorrectCredentialsException 异常。 catch (IncorrectCredentialsException ice) &#123; log.info(&quot;----&gt; Password for account &quot; + token.getPrincipal() + &quot; was incorrect!&quot;); return; &#125; // 用户被锁定的异常 LockedAccountException catch (LockedAccountException lae) &#123; log.info(&quot;The account for username &quot; + token.getPrincipal() + &quot; is locked. &quot; + &quot;Please contact your administrator to unlock it.&quot;); &#125; // ... catch more exceptions here (maybe custom ones specific to your application? // 所有认证时异常的父类. catch (AuthenticationException ae) &#123; //unexpected condition? error? &#125; &#125; //say who they are: //print their identifying principal (in this case, a username): log.info(&quot;----&gt; User [&quot; + currentUser.getPrincipal() + &quot;] logged in successfully.&quot;); //test a role: // 测试是否有某一个角色. 调用 Subject 的 hasRole 方法. if (currentUser.hasRole(&quot;admin&quot;)) &#123; log.info(&quot;----&gt; May the Admin be with you!&quot;); &#125; else &#123; log.info(&quot;----&gt; Hello, mere mortal.&quot;); return; &#125; //test a typed permission (not instance-level) // 测试用户是否具备某一个行为. 调用 Subject 的 isPermitted() 方法。 if (currentUser.isPermitted(&quot;user:query, edit&quot;)) &#123; log.info(&quot;----&gt; You are permitted to &#x27;query&#x27; and &#x27;edit&#x27; &#x27;user&#x27;&quot;); &#125; else &#123; log.info(&quot;Sorry, you don&#x27;t have permission&quot;); &#125; //a (very powerful) Instance Level permission: // 测试用户是否具备某一个行为. 资源标识符:操作:对象实例ID if (currentUser.isPermitted(&quot;user:query:zhangsan&quot;)) &#123; log.info(&quot;----&gt; You are permitted to &#x27;delete&#x27; &#x27;user&#x27; &#x27;zhangsan&#x27;&quot;); &#125; else &#123; log.info(&quot;Sorry, you don&#x27;t have permission!&quot;); &#125; //all done - log out! // 执行登出. 调用 Subject 的 Logout() 方法. System.out.println(&quot;----&gt;&quot; + currentUser.isAuthenticated()); currentUser.logout(); System.out.println(&quot;----&gt;&quot; + currentUser.isAuthenticated()); System.exit(0); &#125;&#125; 4. 在SpringMVC框架中集成Shiro4.1 配置Maven依赖123456789101112131415161718192021222324252627282930&lt;!-- shiro配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Enables support for web-based applications. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-web&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Enables AspectJ support for Shiro AOP and Annotations. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-aspectj&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Enables Ehcache-based famework caching. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-ehcache&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Enables Spring Framework integration. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt; Shiro使用了日志框架slf4j，因此需要对应配置指定的日志实现组件，如：log4j，logback等。 在此，以使用log4j为日志实现为例： 12345678910111213141516171819&lt;!--shiro使用slf4j作为日志框架，所以必需配置slf4j。同时，使用log4j作为底层的日志实现框架。--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 4.2 集成Shiro在Spring框架中集成Shiro，本质上是与Spring IoC容器和Spring MVC框架集成。 4.2.1 Shiro与Spring IoC容器集成 Spring IoC容器提供了一个非常重要的功能，就是依赖注入，将Bean的定义以及Bean之间关系的耦合通过容器来处理。 也就是说，在Spring中集成Shiro时，Shiro中的相应Bean的定义以及他们的关系也需要通过Spring IoC容器实现。 Shiro提供了与Web集成的支持，其通过一个ShiroFilter入口来拦截需要安全控制的URL，然后进行相应的控制。 ShiroFilter类是安全控制的入口点，其负责读取配置（如ini配置文件），然后判断URL 是否需要登录/权限等工作。 [urls] 部分的配置，其格式是：url = 拦截器[参数], 拦截器[参数] shiro中默认的过滤器： 默认拦截器名 拦截器类与说明（括号里的表示默认值） 身份验证相关 authc org.apache.shiro.web.filter.authc.FormAuthenticationFilter基于表单的拦截器；如”/**=authc”，如果没有登录会跳到相应的登录页面登录；主要属性：usernameParam：表单提交的用户名参数名（ username）； passwordParam：表单提交的密码参数名（password）； rememberMeParam：表单提交的密码参数名（rememberMe）； loginUrl：登录页面地址（/login.jsp）；successUrl：登录成功后的默认重定向地址； failureKeyAttribute：登录失败后错误信息存储key（shiroLoginFailure）； authcBasic org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilterBasic HTTP身份验证拦截器，主要属性：applicationName：弹出登录框显示的信息（application）； logout org.apache.shiro.web.filter.authc.LogoutFilter退出拦截器，主要属性：redirectUrl：退出成功后重定向的地址（/）;示例”/logout=logout” user org.apache.shiro.web.filter.authc.UserFilter用户拦截器，用户已经身份验证/记住我登录的都可；示例”/**=user” anon org.apache.shiro.web.filter.authc.AnonymousFilter匿名拦截器，即不需要登录即可访问；一般用于静态资源过滤；示例”/static/**=anon” 授权相关 roles org.apache.shiro.web.filter.authz.RolesAuthorizationFilter角色授权拦截器，验证用户是否拥有所有角色；主要属性：loginUrl：登录页面地址（/login.jsp）；unauthorizedUrl：未授权后重定向的地址；示例”/admin/**=roles[admin]” perms org.apache.shiro.web.filter.authz.PermissionsAuthorizationFilter权限授权拦截器，验证用户是否拥有所有权限；属性和roles一样；示例”/user/**=perms[“user:create”]” port org.apache.shiro.web.filter.authz.PortFilter端口拦截器，主要属性：port（80）：可以通过的端口；示例”/test= port[80]”，如果用户访问该页面是非80，将自动将请求端口改为80并重定向到该80端口，其他路径/参数等都一样 rest org.apache.shiro.web.filter.authz.HttpMethodPermissionFilterrest风格拦截器，自动根据请求方法构建权限字符串（GET=read, POST=create,PUT=update,DELETE=delete,HEAD=read,TRACE=read,OPTIONS=read, MKCOL=create）构建权限字符串；示例”/users=rest[user]”，会自动拼出”user:read,user:create,user:update,user:delete”权限字符串进行权限匹配（所有都得匹配，isPermittedAll）； ssl org.apache.shiro.web.filter.authz.SslFilterSSL拦截器，只有请求协议是https才能通过；否则自动跳转会https端口（443）；其他和port拦截器一样； 其他 noSessionCreation org.apache.shiro.web.filter.session.NoSessionCreationFilter不创建会话拦截器，调用 subject.getSession(false)不会有什么问题，但是如果 subject.getSession(true)将抛出 DisabledSessionException异常； URL匹配模式：url模式使用Ant 风格模式 Ant 路径通配符支持?、*、**，注意通配符匹配不包括目录分隔符“/”： ?：匹配一个字符，如/admin? 将匹配/admin1，但不匹配/admin 或/admin/； *：匹配零个或多个字符串，如/admin 将匹配/admin、/admin123，但不匹配/admin/1； **：匹配路径中的零个或多个路径，如/admin/** 将匹配/admin/a 或/admin/a/b URL匹配顺序：URL 权限采取第一次匹配优先的方式，即从头开始使用第一个匹配的url模式对应的拦截器链。如： /bb/**=filter1 /bb/aa=filter2 /**=filter3 如果请求的url是“/bb/aa”，因为按照声明顺序进行匹配，那么将使用filter1 进行拦截，所以通配符一般写在靠后。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt; &lt;property name=&quot;loginUrl&quot; value=&quot;/index&quot;/&gt; &lt;property name=&quot;successUrl&quot; value=&quot;/home&quot;/&gt; &lt;property name=&quot;unauthorizedUrl&quot; value=&quot;/unauthorized.jsp&quot;/&gt; &lt;!-- The &#x27;filters&#x27; property is not necessary since any declared javax.servlet.Filter bean --&gt; &lt;!-- defined will be automatically acquired and available via its beanName in chain --&gt; &lt;!-- definitions, but you can perform instance overrides or name aliases here if you like: --&gt; &lt;!-- &lt;property name=&quot;filters&quot;&gt; &lt;util:map&gt; &lt;entry key=&quot;logout&quot; value-ref=&quot;logoutFilter&quot; /&gt; &lt;/util:map&gt; &lt;/property&gt; --&gt; &lt;property name=&quot;filterChainDefinitions&quot;&gt; &lt;value&gt; # some example chain definitions: # /admin/** = authc, roles[admin] # /docs/** = authc, perms[document:read] /login = anon /logout = anon /error = anon /** = user # more URL-to-FilterChain definitions here &lt;/value&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt; &lt;!-- Single realm app. If you have multiple realms, use the &#x27;realms&#x27; property instead. --&gt; &lt;property name=&quot;realm&quot; ref=&quot;myRealm&quot; /&gt; &lt;!-- By default the servlet container sessions will be used. Uncomment this line to use shiro&#x27;s native sessions (see the JavaDoc for more): --&gt; &lt;!-- &lt;property name=&quot;sessionMode&quot; value=&quot;native&quot;/&gt; --&gt;&lt;/bean&gt;&lt;bean id=&quot;lifecycleBeanPostProcessor&quot; class=&quot;org.apache.shiro.spring.LifecycleBeanPostProcessor&quot;/&gt;&lt;!-- Define the Shiro Realm implementation you want to use to connect to your back-end --&gt;&lt;!-- security datasource: --&gt;&lt;bean id=&quot;myRealm&quot; class=&quot;org.apache.shiro.realm.jdbc.JdbcRealm&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;property name=&quot;permissionsLookupEnabled&quot; value=&quot;true&quot;/&gt;&lt;/bean&gt;&lt;!-- Enable Shiro Annotations for Spring-configured beans. Only run after --&gt;&lt;!-- the lifecycleBeanProcessor has run: --&gt;&lt;bean class=&quot;org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator&quot; depends-on=&quot;lifecycleBeanPostProcessor&quot;/&gt;&lt;bean class=&quot;org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt;&lt;/bean&gt; 4.2.2 与Spring MVC集成 跟在普通Java Web应用中使用Shiro一样，集成Shiro到Spring MVC时，实际上就是通过在web.xml中添加指定Filter实现。配置如下： 1234567891011121314151617&lt;!-- The filter-name matches name of a &#x27;shiroFilter&#x27; bean inside applicationContext.xml --&gt;&lt;!-- DelegatingFilterProxy作用是自动到Spring 容器查找名字为shiroFilter（filter-name）的bean并把所有Filter 的操作委托给它。 --&gt;&lt;filter&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;targetFilterLifecycle&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;!-- Make sure any request you want accessible to Shiro is filtered. /* catches all --&gt;&lt;!-- requests. Usually this filter mapping is defined first (before all others) to --&gt;&lt;!-- ensure that Shiro works in subsequent filters in the filter chain: --&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; Spring中集成Shiro的原理就是：通过在web.xml中配置的Shiro Filter与Spring IoC中定义的相应的Shiro Bean定义建立关系，从而实现在Spring框架集成Shiro。 4.3 数据源配置在Shiro中，Realm定义了访问数据的方式，用来连接不同的数据源，如：LDAP，关系数据库，配置文件等。 以org.apache.shiro.realm.jdbc.JdbcRealm为例，将用户信息存放在关系型数据库中。 在使用JdbcRealm时，必须要在关系型数据库中存在3张表，分别是 users表，存放认证用户基本信息，在该表中必须存在2个字段：username，password。 roles_permissions表，存放角色和权限定义，在该表中必须存在2个字段：role_name，permission。 user_roles表，存放用户角色对应关系，在该表中必须存在2个字段：username，role_name。 实际上，在更加复杂的应用场景下，通常需要扩展JdbcRealm。 4.4 认证在Shiro中，认证即执行用户登录，读取指定Realm连接的数据源，以验证用户身份的有效性与合法性。 在shiro中，用户需要提供principals （身份）和credentials（证明）给shiro，从而应用能验证用户身份： principals：身份，即主体的标识属性，可以是任何属性，如用户名、邮箱等，唯一即可。一个主体可以有多个principals，但只有一个Primary principals，一般是用户名/邮箱/手机号。 credentials：证明/凭证，即只有主体知道的安全值，如密码/数字证书等。 最常见的principals 和credentials 组合就是用户名/密码了 身份认证流程： 首先调用Subject.login(token) 进行登录，其会自动委托给SecurityManager SecurityManager负责真正的身份验证逻辑；它会委托给Authenticator 进行身份验证； Authenticator 才是真正的身份验证者，ShiroAPI 中核心的身份认证入口点，此处可以自定义插入自己的实现； Authenticator 可能会委托给相应的AuthenticationStrategy进行多Realm 身份验证，默认ModularRealmAuthenticator会调用AuthenticationStrategy进行多Realm 身份验证； Authenticator 会把相应的token 传入Realm，从Realm 获取身份验证信息，如果没有返回/抛出异常表示身份验证失败了。此处可以配置多个Realm，将按照相应的顺序及策略进行访问。 Realm：一般继承AuthorizingRealm（授权）即可；其继承了AuthenticatingRealm（即身份验证），而且也间接继承了CachingRealm（带有缓存实现） 123456789101112131415Subject subject = SecurityUtils.getSubject();if(!subject.isAuthenticated()) &#123; UsernamePasswordToken token = new UsernamePasswordToken(name, password); try &#123; subject.login(token); &#125; catch (UnknownAccountException e) &#123; logger.error(String.format(&quot;用户不存在: %s&quot;, token.getPrincipal()), e); &#125; catch (IncorrectCredentialsException e) &#123; logger.error(String.format(&quot;密码不正确: %s&quot;, token.getPrincipal()), e); &#125; catch (ConcurrentAccessException e) &#123; logger.error(String.format(&quot;用户重复登录: %s&quot;, token.getPrincipal()), e); &#125; catch (AccountException e) &#123; logger.error(String.format(&quot;其他账户异常: %s&quot;, token.getPrincipal()), e); &#125;&#125; 4.5 授权Shiro作为权限框架，仅仅只能控制对资源的操作权限，并不能完成对数据权限的业务需求。 而对于Java Web环境下Shiro授权，包含两个方面的含义。 其一，对于前端来说，用户只能看到他对应访问权限的元素。 其二，当用户执行指定操作（即：访问某个uri资源）时，需要验证用户是否具备对应权限。 对于第一点，在Java Web环境下，通过Shiro提供的JSP标签实现。 对于第二点，与在非Java Web环境下一样，需要在后端调用API进行权限（或者角色）检验。 在Spring框架中集成Shiro，还可以直接通过Java注解方式实现 Permissions： 规则：资源标识符：操作：对象实例ID,即对哪个资源的哪个实例可以进行什么操作.其默认支持通配符权限字符串，: 表示资源/操作/实例的分割；, 表示操作的分割，* 表示任意资源/操作/实例。如：user:edit:manager 也可以使用通配符来定义，如：user:edit:*、user:*:*、user:*:manager 部分省略通配符：缺少的部件意味着用户可以访问所有与之匹配的值，比如：user:edit等价于user:edit:*、user等价于user:*:* 注意：通配符只能从字符串的结尾处省略部件，也就是说user:edit并不等价于user:*:edit 授权流程: 首先调用Subject.isPermitted*/hasRole* 接口，其会委托给SecurityManager，而SecurityManager接着会委托给Authorizer； Authorizer是真正的授权者，如果调用如isPermitted(“user:view”)，其首先会通过PermissionResolver把字符串转换成相应的Permission 实例； 在进行授权之前，其会调用相应的Realm 获取Subject 相应的角色/权限用于匹配传入的角色/权限； Authorizer 会判断Realm 的角色/权限是否和传入的匹配，如果有多个Realm，会委托给ModularRealmAuthorizer进行循环判断，如果匹配如isPermitted*/hasRole* 会返回true，否则返回false表示授权失败。 ModularRealmAuthorizer进行多Realm 匹配流程： 首先检查相应的Realm 是否实现了实现了Authorizer； 如果实现了Authorizer，那么接着调用其相应的isPermitted*/hasRole*接口进行匹配； 如果有一个Realm匹配那么将返回true，否则返回false。 4.5.1 Shiro标签 &lt;shiro:guest&gt;&lt;/shiro:guest&gt;:用户没有身份验证时显示相应信息，即游客访问信息 &lt;shiro:user&gt;&lt;/shiro:user&gt;:用户已经经过认证/记住我登录后显示相应的信息。 &lt;shiro:authenticated&gt;&lt;/shiro:authenticated&gt;:用户已经身份验证通过，即Subject.login登录成功，不是记住我登录的 &lt;shiro:notAuthenticated&gt;&lt;/shiro:notAuthenticated&gt;标签：用户未进行身份验证，即没有调用Subject.login进行登录，包括记住我自动登录的也属于未进行身份验证。 &lt;shiro:pincipal&gt;&lt;/shiro:pincipal&gt;：显示用户身份信息，默认调用Subject.getPrincipal()获取，即Primary Principal。 **&lt;shiro:hasRole&gt;&lt;/shiro:hasRole&gt;**标签：如果当前Subject 有角色将显示body 体内容 &lt;shiro:hasAnyRoles&gt;&lt;/shiro:hasAnyRoles&gt;标签：如果当前Subject有任意一个角色（或的关系）将显示body体内容 &lt;shiro:lacksRole&gt;&lt;/shiro:lacksRole&gt;：如果当前Subject 没有角色将显示body 体内容 **&lt;shiro:hasPermission&gt;&lt;/shiro:hasPermission&gt;**：如果当前Subject 有权限将显示body体内容 &lt;shiro:lacksPermission&gt;&lt;/shiro:lacksPermission&gt;：如果当前Subject没有权限将显示body体内容 123456789&lt;!-- 在jsp页面中引入shiro标签库 --&gt;&lt;%@ taglib prefix=&quot;shiro&quot; uri=&quot;http://shiro.apache.org/tags&quot; %&gt;&lt;!-- 权限控制 --&gt;&lt;shiro:hasRole name=&quot;admin&quot;&gt; &lt;a&gt;用户管理&lt;/a&gt;&lt;/shiro:hasRole&gt;&lt;shiro:hasPermission name=&quot;winnebago:drive:eagle5&quot;&gt; &lt;a&gt;操作审计&lt;/a&gt;&lt;/shiro:hasPermission&gt; 4.5.2 调用API进行权限（或者角色）检验12345String roleAdmin = &quot;admin&quot;;Subject currentUser = SecurityUtils.getSubject();if(!currentUser.hasRole(roleAdmin)) &#123; //todo something&#125; 4.5.3 Shiro权限注解 @RequiresAuthentication：表示当前Subject已经通过login 进行了身份验证；即Subject. isAuthenticated() 返回true @RequiresUser：表示当前Subject 已经身份验证或者通过记住我登录的。 @RequiresGuest：表示当前Subject没有身份验证或通过记住我登录过，即是游客身份。 @RequiresRoles(value=&#123;“admin”, “user”&#125;, logical= Logical.AND)：表示当前Subject 需要角色admin 和user @RequiresPermissions(value=&#123;“user:a”, “user:b”&#125;, logical= Logical.OR)：表示当前Subject 需要权限user:a或user:b。 通过自定义拦截器可以扩展功能，例如：动态url-角色/权限访问控制的实现、根据Subject 身份信息获取用户信息绑定到Request（即设置通用数据）、验证码验证、在线用户信息的保存等 123456789@Controllerpublic class HomeController &#123; @RequestMapping(&quot;/home&quot;) @RequiresPermissions(value=&#123;&quot;log:manage:*&quot;&#125;) public ModelAndView home(HttpServletRequest req) &#123; ModelAndView mv = new ModelAndView(&quot;home&quot;); return mv; &#125;&#125; 4.6 Spring集成Shiro注意事项 Spring 4.2.0 RELEASE+ 与 Spring 4.1.9 RELEASE**-**版本，配置方式有所不同。 虽然shiro的注解定义是在Class级别的，但是实际验证只能支持方法级别：@RequiresAuthentication、@RequiresPermissions、@RequiresRoles。 5. Shiro会话管理Shiro提供了完整的企业级会话管理功能，不依赖于底层容器（如web容器tomcat），不管JavaSE还是JavaEE环境都可以使用，提供了会话管理、会话事件监听、会话存储/持久化、容器无关的集群、失效/过期支持、对Web 的透明支持、SSO 单点登录的支持等特性。 5.1 会话相关的API Subject.getSession()：即可获取会话；其等价于Subject.getSession(true)，即如果当前没有创建Session 对象会创建一个；Subject.getSession(false)，如果当前没有创建Session 则返回null session.getId()：获取当前会话的唯一标识 session.getHost()：获取当前Subject的主机地址 session.getTimeout() &amp; session.setTimeout(毫秒)：获取/设置当前Session的过期时间 session.getStartTimestamp() &amp; session.getLastAccessTime()：获取会话的启动时间及最后访问时间；如果是 JavaSE 应用需要自己定期调用 session.touch() 去更新最后访问时间；如果是 Web 应用，每次进入 ShiroFilter 都会自动调用 session.touch() 来更新最后访问时间。 session.touch() &amp; session.stop()：更新会话最后访问时间及销毁会话；当Subject.logout()时会自动调用 stop 方法来销毁会话。如果在web中，调用 HttpSession. invalidate()也会自动调用Shiro Session.stop 方法进行销毁Shiro 的会话 session.setAttribute(key, val) &amp; session.getAttribute(key) &amp; session.removeAttribute(key)：设置/获取/删除会话属性；在整个会话范围内都可以对这些属性进行操作 5.2 会话监听器会话监听器(SessionListiner):会话监听器用于监听会话创建、过期及停止事件 5.3 SessionDao AbstractSessionDAO 提供了 SessionDAO 的基础实现，如生成会话ID等 CachingSessionDAO 提供了对开发者透明的会话缓存的功能，需要设置相应的 CacheManager MemorySessionDAO 直接在内存中进行会话维护 EnterpriseCacheSessionDAO 提供了缓存功能的会话维护，默认情况下使用 MapCache 实现，内部使用ConcurrentHashMap 保存缓存的会话。 5.4 数据表12345create table sessions ( id varchar(200), session varchar(2000), constraint pk_sessions primary key(id)) charset=utf8 ENGINE=InnoDB; 5.5 会话验证 Shiro 提供了会话验证调度器，用于定期的验证会话是否已过期，如果过期将停止会话 出于性能考虑，一般情况下都是获取会话时来验证会话是否过期并停止会话的；但是如在 web 环境中，如果用户不主动退出是不知道会话是否过期的，因此需要定期的检测会话是否过期，Shiro 提供了会话验证调度器SessionValidationScheduler Shiro 也提供了使用Quartz会话验证调度器：QuartzSessionValidationScheduler 6. Shiro缓存 CacheManagerAware 接口 Shiro 内部相应的组件（DefaultSecurityManager）会自动检测相应的对象（如Realm）是否实现了CacheManagerAware 并自动注入相应的CacheManager。 Realm 缓存 Shiro 提供了 CachingRealm，其实现了CacheManagerAware 接口，提供了缓存的一些基础实现； AuthenticatingRealm 及 AuthorizingRealm 也分别提供了对AuthenticationInfo 和 AuthorizationInfo 信息的缓存。 Session 缓存 如 SecurityManager 实现了 SessionSecurityManager，其会判断 SessionManager 是否实现了acheManagerAware 接口，如果实现了会把CacheManager 设置给它。 SessionManager 也会判断相应的 SessionDAO（如继承自CachingSessionDAO）是否实现了CacheManagerAware，如果实现了会把 CacheManager设置给它 设置了缓存的 SessionManager，查询时会先查缓存，如果找不到才查数据库。 RememberMe Shiro 提供了记住我（RememberMe）的功能，比如访问如淘宝等一些网站时，关闭了浏览器，下次再打开时还是能记住你是谁，下次访问时无需再登录即可访问，基本流程如下： 首先在登录页面选中 RememberMe 然后登录成功；如果是浏览器登录，一般会把 RememberMe 的Cookie 写到客户端并保存下来； 关闭浏览器再重新打开；会发现浏览器还是记住你的； 访问一般的网页服务器端还是知道你是谁，且能正常访问； 但是比如我们访问淘宝时，如果要查看我的订单或进行支付时，此时还是需要再进行身份认证的，以确保当前用户还是你。 认证和记住我 subject.isAuthenticated() 表示用户进行了身份验证登录的，即使有 Subject.login 进行了登录； subject.isRemembered()：表示用户是通过记住我登录的，此时可能并不是真正的你（如你的朋友使用你的电脑，或者你的cookie 被窃取）在访问的 两者二选一，即 subject.isAuthenticated()==true，则subject.isRemembered()==false；反之一样。 建议 访问一般网页：如个人在主页之类的，我们使用user 拦截器即可，user 拦截器只要用户登录(isRemembered() || isAuthenticated())过即可访问成功； 访问特殊网页：如我的订单，提交订单页面，我们使用authc 拦截器即可，authc 拦截器会判断用户是否是通过Subject.login（isAuthenticated()==true）登录的，如果是才放行，否则会跳转到登录页面叫你重新登录。 实现 如果要自己做RememeberMe，需要在登录之前这样创建Token：UsernamePasswordToken(用户名，密码，是否记住我)，且调用UsernamePasswordToken 的：token.setRememberMe(true); 方法 参考文章： 细说shiro之一：shiro简介 细说shiro之五：在spring框架中集成shiro","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"ssafe","permalink":"http://chaooo.github.io/tags/ssafe/"}]},{"title":"【Redis】基于Redis的分布式锁实现","date":"2019-12-08T08:04:23.000Z","path":"article/20191208.html","text":"SETNX命令简介 SETNX key value返回(1:key的值被设置，0:key的值没被设置)，将key的值设为value，并且仅当key不存在。 锁的key为目标数据的唯一键，value为锁的期望超时时间点； 基于Redis实现的分布式锁，主要基于redis的setnx（set if not exist）命令； 1. jedis实现分布式锁12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt; 1.1 实现示例:1234567public static boolean correctGetLock(String lockKey, String requestId, int expireTime) &#123; String result = jedis.set(lockKey, requestId, &quot;NX&quot;, &quot;PX&quot;, expireTime); if (&quot;OK&quot;.equals(result)) &#123; return true; &#125; return false;&#125; jedis.set(String key, String value, String nxxx, String expx, int time) - **key**：保证唯一，用来当锁（redis记录的key） - **value**：redis记录的value，目的是为了标志锁的所有者（竞争锁的客户端），保证解锁时只能解自己加的锁。requestId可以使用UUID.randomUUID().toString()方法生成 - **nxxx**：&quot;NX&quot;意思是SET IF NOT EXIST，即当key不存在时，我们进行set操作，若key已经存在，则不做任何操作 - **expx**：&quot;PX&quot;意思是要给这个key加一个过期的设置（单位毫秒），过期时间由第五个参数决定 - **time**：expx设置为&quot;PX&quot;时，redis key的过期时间 1.2 解锁示例:12345678public boolean correctReleaseLock(String lockKey, String requestId) &#123; String script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;; Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId)); if (RELEASE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; eval命令执行Lua代码的时候，Lua代码将被当成一个命令去执行，并且直到eval命令执行完成，Redis才会执行其他命令，所以保证了检查和删除操作都是原子的。 1.3 这类琐最大的缺点加锁时只作用在一个Redis节点上，即使Redis通过sentinel保证高可用，如果这个master节点由于某些原因发生了主从切换，那么就会出现锁丢失的情况： 在Redis的master节点上拿到了锁； 但是这个加锁的key还没有同步到slave节点； master故障，发生故障转移，slave节点升级为master节点； 导致锁丢失。 因此，Redis作者antirez基于分布式环境下提出了一种更高级的分布式锁的实现方式：Redlock。基于Redis的Redisson实现了Redlock。 2. Redisson实现普通分布式锁普通分布式实现非常简单，无论是那种架构，向Redis通过EVAL命令执行LUA脚本即可。 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.3.2&lt;/version&gt;&lt;/dependency&gt; 单机模式: 123456789101112131415161718192021// 构造redisson实现分布式锁必要的ConfigConfig config = new Config();config.useSingleServer().setAddress(&quot;redis://172.29.1.180:5379&quot;) .setPassword(&quot;a123456&quot;).setDatabase(0);// 构造RedissonClientRedissonClient redissonClient = Redisson.create(config);// 设置锁定资源名称, 还可以getFairLock(), getReadWriteLock()RLock lock = redissonClient.getLock(&quot;DISLOCK&quot;);boolean isLock;try &#123; // 尝试获取分布式锁 // 500ms拿不到锁, 就认为获取锁失败。10000ms即10s是锁失效时间。 isLock = lock.tryLock(500, 10000, TimeUnit.MILLISECONDS); if (isLock) &#123; //TODO if get lock success, do something; &#125;&#125; catch (Exception e) &#123;&#125; finally &#123; // 无论如何, 最后都要解锁 lock.unlock();&#125; 哨兵模式:即Sentinel模式，实现代码和单机模式几乎一样，唯一的不同就是Config的构造： 1234Config config = new Config();config.useSentinelServers().addSentinelAddress( &quot;redis://172.29.3.245:26378&quot;,&quot;redis://172.29.3.245:26379&quot;, &quot;redis://172.29.3.245:26380&quot;) .setMasterName(&quot;mymaster&quot;).setPassword(&quot;a123456&quot;).setDatabase(0); 集群模式:即Cluster模式，集群模式构造Config如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556Config config = new Config();config.useClusterServers().addNodeAddress( &quot;redis://172.29.3.245:6375&quot;,&quot;redis://172.29.3.245:6376&quot;, &quot;redis://172.29.3.245:6377&quot;, &quot;redis://172.29.3.245:6378&quot;,&quot;redis://172.29.3.245:6379&quot;, &quot;redis://172.29.3.245:6380&quot;) .setPassword(&quot;a123456&quot;).setScanInterval(5000); ```### 3. Redisson实现Redlock分布式锁#### 3.1 Redlock算法大概原理：- 在`Redis`的分布式环境中，我们假设有`N`个`Redis master`。这些节点**完全互相独立，不存在主从复制或者其他集群协调机制**。我们确保将在`N`个实例上使用与在`Redis`单实例下相同方法获取和释放锁。- 为了取到锁，客户端应该执行以下操作: - 获取当前`Unix`时间，以毫秒为单位。 - 依次尝试从`N`个实例，使用相同的`key`和具有唯一性的`value`（例如UUID）获取锁。 - 客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。 - **当且仅当(N/2+1)的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功**，例如3个节点至少需要`3/2+1=2`2个。 - 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。 - 若获取锁失败，客户端应该在**所有的Redis实例上进行解锁**（即便某些Redis实例根本就没有加锁成功）。#### 3.2 使用`Redlock`单机模式`Redis`为例:``` javaConfig config = new Config();config.useClusterServers().addNodeAddress( &quot;redis://127.0.0.1:6379&quot;,&quot;redis://127.0.0.1:6369&quot;, &quot;redis://127.0.0.1:6359&quot;, &quot;redis://127.0.0.1:6349&quot;,&quot;redis://127.0.0.1:6339&quot;) .setPassword(&quot;******&quot;);// 节点1Config config1 = new Config();config1.useSingleServer().setAddress(&quot;redis://127.0.0.1:6379&quot;);RedissonClient redissonClient1 = Redisson.create(config1);// 节点2Config config2 = new Config();config2.useSingleServer().setAddress(&quot;redis://127.0.0.1:6378&quot;);RedissonClient redissonClient2 = Redisson.create(config2);// 节点3Config config3 = new Config();config3.useSingleServer().setAddress(&quot;redis://127.0.0.1:6377&quot;);RedissonClient redissonClient3 = Redisson.create(config3);// 设置锁定资源名称String resourceName = &quot;REDLOCK&quot;;RLock lock1 = redissonClient1.getLock(resourceName);RLock lock2 = redissonClient2.getLock(resourceName);RLock lock3 = redissonClient3.getLock(resourceName);// 实例化RedissonRedLockRedissonRedLock redLock = new RedissonRedLock(lock1, lock2, lock3);try &#123; boolean isLock = redLock.tryLock(500, 30000, TimeUnit.MILLISECONDS); if (isLock) &#123; //TODO if get lock success, do something; Thread.sleep(30000); &#125;&#125; catch (Exception e) &#123;&#125; finally &#123; //解锁 redLock.unlock();&#125; 最核心的变化就是 RedissonRedLock redLock=**new RedissonRedLock(lock1,lock2,lock3);**，因为我这里是以三个节点为例。 如果是主从Redis架构、哨兵Redis架构、集群Redis架构实现Redlock，只需要改变上述config1、config2、config3为主从模式、哨兵模式、集群模式配置即可，但相应需要3个独立的Redis主从集群、3个Redis独立的哨兵集群、3个独立的Cluster集群。 以sentinel模式架构为例，3个sentinel模式集群，如果要获取分布式锁，那么需要向这3个sentinel集群通过EVAL命令执行LUA脚本，需要3/2+1=2，即至少2个sentinel集群响应成功，才算成功的以Redlock算法获取到分布式锁。 4. Redlock问题合集4.1 N个节点的理解假设我们用N(&gt;=3)个节点实现Redlock算法的分布式锁。不是一个有N个主节点的cluster集群；而是要么是N个redis单实例，要么是N个sentinel集群，要么是N个cluster集群。 4.2 失效时间如何设置这个问题的场景是，假设设置失效时间10秒，如果由于某些原因导致10秒还没执行完任务，这时候锁自动失效，导致其他线程也会拿到分布式锁。这确实是Redis分布式最大的问题，不管是普通分布式锁，还是Redlock算法分布式锁，都没有解决这个问题。也有一些文章提出了对失效时间续租，即延长失效时间，很明显这又提升了分布式锁的复杂度（没有现成的框架有实现）。 4.3 redis分布式锁的高可用关于Redis分布式锁的安全性问题，在分布式系统专家Martin Kleppmann和Redis的作者Antirez之间已经发生过一场争论。有兴趣的同学，搜索”基于Redis的分布式锁到底安全吗”就能得到你想要的答案，需要注意的是，有上下两篇（这应该就是传说中的神仙打架吧）。 4.4 使用Zookeeper还是Redis实现分布式锁没有绝对的好坏，只有更适合自己的业务。就性能而言，Redis很明显优于Zookeeper；就分布式锁实现的健壮性(高可用)而言，Zookeeper很明显优于Redis。至于如何选择，还要看具体业务场景。 参考：https://mp.weixin.qq.com/s/8uhYult2h_YUHT7q7YCKYQ","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"}]},{"title":"【ElasticStack】Beats+Logstash+Elasticsearch+Kibana基础整合","date":"2019-11-23T14:29:59.000Z","path":"article/20191123.html","text":"1. ElasticStack的组成 **Beats**：数据采集 LogStash: 数据处理 ElasticSearch(核心引擎): 数据存储、查询和分析 Kibana: 数据探索与可视化分析 2. FilebeatFilebeat是本地文件的轻量型日志数据采集器。Beats可以直接（或者通过Logstash）将数据发送到Elasticsearch，在那里你可以进一步处理和增强数据，然后在Kibana中将其可视化。 2.1 Filebeat工作原理Filebeat由两个主要组件组成：prospector和harvester。这些组件一起工作来读取文件（tail file）并将事件数据发送到您指定的输出 **Harvester**： 负责读取单个文件的内容 如果文件在读取时被删除或重命名，Filebeat将继续读取文件 **prospector**： prospector负责**管理harvester**并找到所有要读取的文件来源 如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester Filebeat目前支持两种prospector类型：log和stdin Filebeat如何保持文件的状态 Filebeat 保存每个文件的状态并经常将状态刷新到磁盘上的注册文件中 该状态用于记住harvester正在读取的最后偏移量，并确保发送所有日志行 如果输出（例如Elasticsearch或Logstash）无法访问，Filebeat会跟踪最后发送的行，并在输出再次可用时继续读取文件。 在Filebeat运行时，每个prospector内存中也会保存的文件状态信息，当重新启动Filebeat时，将使用注册文件的数据来重建文件状态，Filebeat将每个harvester在从保存的最后偏移量继续读取 Filebeat存储唯一标识符以检测文件是否先前已采集过 Filebeat如何确保至少一次交付 Filebeat保证事件至少会被传送到配置的输出一次，并且不会丢失数据。 Filebeat能够实现此行为，因为它将每个事件的传递状态存储在注册文件中。 在输出阻塞或未确认所有事件的情况下，Filebeat将继续尝试发送事件，直到接收端确认已收到。 如果Filebeat在发送事件的过程中关闭，它不会等待输出确认所有收到事件。 发送到输出但在Filebeat关闭前未确认的任何事件在重新启动Filebeat时会再次发送。 这可以确保每个事件至少发送一次，但最终会将重复事件发送到输出。 也可以通过设置shutdown_timeout选项来配置Filebeat以在关闭之前等待特定时间 2.2 Filebeat安装与配置安装Filebeat，创建配置文件itcast.yml，控制台运行测试 12345678910#创建如下配置文件 itcast.ymlfilebeat.inputs:- type: stdin # 标准输入 enabled: trueoutput.console: # 输出到控制台 pretty: true enable: true #启动filebeat./filebeat -e -c itcast.yml 输入hello运行结果如下： 1234567891011121314151617181920212223242526hello&#123; &quot;@timestamp&quot;: &quot;2019-11-23T09:21:19.213Z&quot;, &quot;@metadata&quot;: &#123; #元数据信息 &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;version&quot;: &quot;7.4.2&quot; # beat版本 &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;chaooo&quot; &#125;, &quot;agent&quot;: &#123; &#125;, &quot;log&quot;: &#123; &quot;offset&quot;: 0, &quot;file&quot;: &#123; &quot;path&quot;: &quot;&quot; &#125; &#125;, &quot;message&quot;: &quot;hello&quot;, # 输入的内容 &quot;input&quot;: &#123; # 控制台标准输入 &quot;type&quot;: &quot;stdin&quot; &#125;, &quot;ecs&quot;: &#123; &quot;version&quot;: &quot;1.1.0&quot; &#125;&#125; 2.3 读取文件创建配置文件itcast-log.yml 12345678910filebeat.inputs:- type: log enabled: true paths: - /test/*.log # 可以使用单个路径output.console: # 输出到控制台 pretty: true enable: true#启动filebeat./filebeat -e -c itcast-log.yml 在/test/下创建a.log文件，并输入如下内容hello world,观察filebeat输出: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; &quot;@timestamp&quot;: &quot;2019-11-23T09:45:56.379Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;version&quot;: &quot;7.4.2&quot; &#125;, &quot;log&quot;: &#123; &quot;offset&quot;: 0, &quot;file&quot;: &#123; &quot;path&quot;: &quot;/test/a.log&quot; &#125; &#125;, &quot;message&quot;: &quot;hello&quot;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;ecs&quot;: &#123; &quot;version&quot;: &quot;1.1.0&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;chaooo&quot; &#125;, &quot;agent&quot;: &#123; &#125;&#125;&#123; &quot;@timestamp&quot;: &quot;2019-11-23T09:45:56.379Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;version&quot;: &quot;7.4.2&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;chaooo&quot; &#125;, &quot;agent&quot;: &#123; &#125;, &quot;log&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;/test/a.log&quot; &#125;, &quot;offset&quot;: 7 &#125;, &quot;message&quot;: &quot;world&quot;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;ecs&quot;: &#123; &quot;version&quot;: &quot;1.1.0&quot; &#125;&#125; 2.4 自定义字段123456789101112filebeat.inputs:- type: log enabled: true paths: - /test/*.log # 可以使用单个路径 tags: [&quot;web&quot;] #添加自定义tag，便于后续的处理 fields: #添加自定义字段 from: itcast-im fields_under_root: true #true为添加到根节点，false为添加到子节点中output.console: # 输出到控制台 pretty: true enable: true 2.5 输出到Elasticsearch12345678910111213filebeat.inputs:- type: log enabled: true paths: - /test/*.log # 可以使用单个路径 tags: [&quot;web&quot;] #添加自定义tag，便于后续的处理 fields: #添加自定义字段 from: itcast-im fields_under_root: falsesetup.template.settings: index.number_of_shards: 3 #指定索引的分区数output.elasticsearch: #指定ES的配置 hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] 2.6 读取Nginx日志文件12345678910filebeat.inputs:- type: log enabled: true paths: - /usr/local/nginx/logs/*.log tags: [&quot;nginx&quot;] setup.template.settings: index.number_of_shards: 3 #指定索引的分区数output.elasticsearch: #指定ES的配置 hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] 2.7 Filebeat的Module日志数据的读取与处理可以不用手动配置的，在Filebeat中，有大量的Module，可以直接使用简化配置。 12345678910111213141516171819202122232425262728293031323334353637./filebeat modules listEnabled: Disabled:apacheauditdawscefciscocorednselasticsearchenvoyproxygooglecloudhaproxyibmmqicingaiisiptableskafkakibanalogstashmongodbmssqlmysqlnatsnetflownginxosquerypanwpostgresqlrabbitmqredissantasuricatasystemtraefikzeek 可以看到，内置了很多的module，但都没有启用，如果需要启用需要进行enable操作： 12/filebeat modules enable nginx #启动./filebeat modules disable nginx #禁用 2.8 nginx module与filebeat配置1234567- module: nginx access: # Access logs enabled: true var.paths: [&quot;/usr/local/nginx/logs/access.log*&quot;] error: # Error logs enabled: true var.paths: [&quot;/usr/local/nginx/logs/error.log*&quot;] 1234567891011121314#vim itcast-nginx.ymlfilebeat.inputs:#- type: log# enabled: true# paths:# - /usr/local/nginx/logs/*.log# tags: [&quot;nginx&quot;]setup.template.settings: index.number_of_shards: 3output.elasticsearch: hosts: [&quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;]filebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: false 若启动报错，需要在Elasticsearch中安装ingest-user-agent、ingest-geoip插件 3. Metricbeat用于从系统和服务收集指标。Metricbeat和Filebeat一样，是一个轻量级的采集器，Metricbeat由模块(Module)和度量集(Metricset)组成。Metricbeat模块定义了从特定服务（如Redis，MySQL等）收集数据的基本逻辑。该模块指定有关服务的详细信息，包括如何连接，收集指标的频率以及要收集的指标。 Metricbeat有2部分组成，一部分是Module，另一部分为Metricset。 Module 收集的对象，如：mysql、redis、nginx、操作系统等； Metricset 收集指标的集合，如：cpu、memory、network等； 以Redis Module为例： 3.1 安装配置安装Metricbeat，根据实际情况配置文件metricbeat.yml 123456789metricbeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: falsesetup.kibana: host: &quot;192.168.56.13:5601&quot;output.elasticsearch: hosts: [&quot;192.168.56.13:9200&quot;] username: &quot;elastic&quot; password: &quot;qiuyuetao&quot; 启动：./metricbeat -e查看module列表：./metricbeat modules list 12345678910Enabled:system #默认启用 Disabled:apacheelasticsearchnginxmysqlredis... system module默认启用的，其配置: 123456789101112131415161718192021222324252627282930cat system.yml# Module: system# Docs: https://www.elastic.co/guide/en/beats/metricbeat/6.5/metricbeat-modulesystem.html- module: system period: 10s metricsets: - cpu - load - memory - network - process - process_summary #- core #- diskio #- socket process.include_top_n: by_cpu: 5 # include top 5 processes by CPU by_memory: 5 # include top 5 processes by memory- module: system period: 1m metricsets: - filesystem - fsstat processors: - drop_event.when.regexp: system.filesystem.mount_point: &#x27;^/(sys|cgroup|proc|dev|etc|host|lib)($|/)&#x27;- module: system period: 15m metricsets: - uptime 3.2 Nginx Module在nginx中，需要开启状态查询，才能查询到指标数据。 12345# 配置nginxlocation /nginx-status &#123; stub_status on; access_log off;&#125; 通过192.168.56.13/nginx-status查看nginx-status Active connections：正在处理的活动连接数 server accepts handled requests（连接数，握手数，处理请求总数） Reading: 0 Writing: 1 Waiting: 1（ 读取到客户端的Header信息数，返回给客户端Header信息数，已经处理完正在等候下一次请求指令的驻留链接） 配置Nginx Module（metricbeat/modules.d/nginx.yml） 1234567- module: nginx metricsets: [&quot;stubstatus&quot;] period: 10s # Nginx hosts hosts: [&quot;http://192.168.56.11&quot;] # Path to server status. Default server-status server_status_path: &quot;nginx_status&quot; 启动：./metricbeat -e 4. LogstashLogstash是ElasticStack中的实时数据采集引擎，可以采集来自不同数据源的数据，并对数据进行处理后输出到多种输出源，是Elastic Stack的重要组成部分。 4.1 Logstash的数据处理过程 Inputs(Codecs)--&gt;Filters--&gt;Outputs(Codecs) 用户通过定义pipeline配置文件，设置需要使用的input，filter，output, codec插件，以实现特定的数据采集，数据处理，数据输出等功能 Inputs：用于从数据源获取数据，常见的插件如file, syslog, redis, beats等 Filters：用于处理数据如格式转换，数据派生等，常见的插件如grok, mutate, drop, clone, geoip等 Outputs：用于数据输出，常见的插件如elastcisearch，file, graphite, statsd等 Codecs：Codecs不是一个单独的流程，而是在输入和输出等插件中用于数据转换的模块，用于对数据进行编码处理，常见的插件如json，multiline 4.2 执行模型 每个Input启动一个线程，从对应数据源获取数据 Input会将数据写入一个队列：默认为内存中的有界队列（意外停止会导致数据丢失）。为了防止数丢失Logstash提供了两个特性： Persistent Queues：通过磁盘上的queue来防止数据丢失 Dead Letter Queues：保存无法处理的event（仅支持Elasticsearch作为输出源） Logstash会有多个pipeline worker, 每一个pipeline worker会从队列中取一批数据，然后执行filter和output（worker数目及每次处理的数据量均由配置确定） 4.3 安装配置下载Logstash并解压，配置有三部分，如下： 123456789input &#123; #输入stdin &#123; ... &#125; #标准输入&#125;filter &#123; #过滤，对数据进行分割、截取等处理...&#125;output &#123; #输出stdout &#123; ... &#125; #标准输出&#125; 4.4 读取自定义日志 日志结构：2019-11-23 21:21:21|ERROR|读取数据出错|参数：id=1002，日志中的内容是使用“|”进行分割的，使用，我们在处理的时候，也需要对数据做分割处理。 编写配置文件 123456789101112131415#vim itcast-pipeline.confinput &#123; file &#123; path =&gt; &quot;/itcast/logstash/logs/app.log&quot; start_position =&gt; &quot;beginning&quot; &#125;&#125;filter &#123; mutate &#123; split =&gt; &#123;&quot;message&quot;=&gt;&quot;|&quot;&#125; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; 启动测试 1234567891011121314151617#启动./bin/logstash -f ./itcast-pipeline.conf#写日志到文件echo &quot;2019-11-23 21:21:21|ERROR|读取数据出错|参数：id=1002&quot; &gt;&gt; app.log#输出的结果&#123; &quot;@timestamp&quot; =&gt; 2019-03-15T08:44:04.749Z, &quot;path&quot; =&gt; &quot;/itcast/logstash/logs/app.log&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;node01&quot;, &quot;message&quot; =&gt; [ [0] &quot;2019-11-23 21:21:21&quot;, [1] &quot;ERROR&quot;, [2] &quot;读取数据出错&quot;, [3] &quot;参数：id=1002&quot; ]&#125; 输出到Elasticsearch配置 12345output &#123; elasticsearch &#123; hosts =&gt; [ &quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;] &#125;&#125; 5. Elasticsearch + Logstash + Beats + Kibana基础整合123 (读取) (发送) (写入) (读取)【日志文件】&lt;----【FileBeat】----&gt;【Logstash】----&gt;【Elasticsearch】&lt;----【Kibana】 Filebeat配置与启动： 12345678910111213#vim itcast-dashboard.ymlfilebeat.inputs:- type: log enabled: true paths: - /itcast/logs/*.logsetup.template.settings: index.number_of_shards: 3output.logstash: hosts: [&quot;192.168.40.133:5044&quot;] #Logstash端口号 #启动./filebeat -e -c itcast-dashboard.yml Logstash配置与启动： 123456789101112131415161718192021222324252627282930313233#vim itcast-dashboard.confinput &#123; beats &#123; port =&gt; &quot;5044&quot; &#125;&#125;filter &#123; mutate &#123; split =&gt; &#123;&quot;message&quot;=&gt;&quot;|&quot;&#125; &#125; mutate &#123; add_field =&gt; &#123; &quot;userId&quot; =&gt; &quot;%&#123;message[1]&#125;&quot; &quot;visit&quot; =&gt; &quot;%&#123;message[2]&#125;&quot; &quot;date&quot; =&gt; &quot;%&#123;message[3]&#125;&quot; &#125; &#125; mutate &#123; convert =&gt; &#123; &quot;userId&quot; =&gt; &quot;integer&quot; &quot;visit&quot; =&gt; &quot;string&quot; &quot;date&quot; =&gt; &quot;string&quot; &#125; &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; [ &quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;] &#125;&#125; #启动./bin/logstash -f itcast-dashboard.conf ElasticSearch启动与Kibana启动： 123456# ElasticSearch默认端口:9200bin/elasticsearch # kibana默认端口:5601bin/kibana#通过浏览器进行访问,添加Logstash索引到Kibana中http://192.168.40.133:5601","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://chaooo.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://chaooo.github.io/tags/Kibana/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"http://chaooo.github.io/tags/ElasticStack/"},{"name":"LogStash","slug":"LogStash","permalink":"http://chaooo.github.io/tags/LogStash/"}]},{"title":"【ElasticStack】ElasticSearch聚合分析与数据建模","date":"2019-11-21T14:30:47.000Z","path":"article/20191121.html","text":"1. ElasticSearch中的聚合分析聚合分析，英文Aggregation，是ES除了搜索功能之外提供的针对ES数据进行统计分析的功能。 特点： ①功能丰富，可满足大部分分析需求； ②实时性高，所有计算结果实时返回。 基于分析规则的不同，ES将聚合分析主要划分为以下4种： Metric: 指标分析类型，如：计算最值，平均值等； Bucket: 分桶类型，类似于group by语法，根据一定规则划分为若干个桶分类； Pipeline: 管道分析类型，基于上一级的聚合分析结果进行再分析； Matrix: 矩阵分析类型。 12345678910111213# 聚合分析格式：GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; # 关键词 &quot;&lt;aggregation_name&gt;&quot;:&#123; # 自定义聚合分析名称，一般起的有意义 &quot;&lt;aggregation_type&gt;&quot;:&#123; # 聚合分析类型 &quot;&lt;aggregation_body&gt;&quot; # 聚合分析主体 &#125; &#125; [,&quot;aggs&quot;:&#123;[&lt;svb_aggregation&gt;]+&#125;] # 可包含多个子聚合分析 &#125;&#125; 1.1 Metric聚合分析主要分为两类：单值分析（输出单个结果）和多值分析（输出多个结果）。 1.1.1 单值分析 min：返回数值类型字段的最小值 max：返回数值类型字段的最大值 avg：返回数值类型字段的平均值 sum：返回数值类型字段值的总和 cardinality：返回字段的基数 使用多个单值分析关键词，返回多个结果 123456789101112131415161718192021222324252627282930313233343536373839GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;min_age&quot;:&#123; &quot;min&quot;:&#123; # 关键字min/max/avg/sum/cardinality &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125;&#125;## 使用多个单值分析关键词，返回多个分析结果GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;min_age&quot;:&#123; &quot;min&quot;:&#123; # 求最小年龄 &quot;field&quot;:&quot;age&quot; &#125; &#125;, &quot;max_age&quot;:&#123; &quot;max&quot;:&#123; # 求最大年龄 &quot;field&quot;:&quot;age&quot; &#125; &#125;, &quot;avg_age&quot;:&#123; &quot;avg&quot;:&#123; # 求平均年龄 &quot;field&quot;:&quot;age&quot; &#125; &#125;, &quot;sum_age&quot;:&#123; &quot;sum&quot;:&#123; # 求年龄总和 &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125;&#125; 1.1.2 多值分析 stats：返回所有单值结果 extended_stats：对stats进行扩展，包含更多，如：方差，标准差，标准差范围等 Percentile：百分位数统计 Top hits：一般用于分桶之后获取该桶内最匹配的定不稳当列表，即详情数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;stats_age&quot;:&#123; &quot;stats&quot;:&#123; # 关键字stats/extended_stats/percentiles &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125;&#125;## 使用percentiles关键词进行百分位数预测。GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;per_age&quot;:&#123; &quot;percentiles&quot;:&#123; # 关键字 &quot;field&quot;:&quot;age&quot;, &quot;values&quot;:[20, 25] # 判断20和25分别在之前的年轻区间的什么位置，以百分数显示 &#125; &#125; &#125;&#125;## 使用top_hits关键词GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; &quot;match&quot;:&#123; &quot;field&quot;:&quot;job.keyword&quot;, # 按job.keyword进行分桶聚合 &quot;size&quot;:10 &#125;, &quot;aggs&quot;:&#123; &quot;top_employee&quot;:&#123; &quot;top_hits&quot;:&#123; &quot;size&quot;:10, # 返回文档数量 &quot;sort&quot;:[ &#123; &quot;age&quot;:&#123; &quot;order&quot;:&quot;desc&quot; # 按年龄倒叙排列 &#125; &#125; ] &#125; &#125; &#125; &#125; &#125; &#125;&#125; 1.2 Bucket聚合分析Bucket，意为桶。即：按照一定规则，将文档分配到不同的桶中，达分类的目的。常见的有以下五类： Terms: 直接按term进行分桶，如果是text类型，按分词后的结果分桶 Range: 按指定数值范围进行分桶 Date Range: 按指定日期范围进行分桶 Histogram: 直方图，按固定数值间隔策略进行数据分割 Date Histogram: 日期直方图，按固定时间间隔进行数据分割 1.2.1 TermsTerms: 直接按term进行分桶，如果是text类型，按分词后的结果分桶 12345678910111213# 使用terms关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;terms_job&quot;:&#123; &quot;terms&quot;:&#123; # 关键字 &quot;field&quot;:&quot;job.keyword&quot;, # 按job.keyword进行分桶 &quot;size&quot;:5 # 返回五个文档 &#125; &#125; &#125;&#125; 1.2.2 RangeRange: 按指定数值范围进行分桶： 123456789101112131415161718192021222324252627# 使用range关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;number_ranges&quot;:&#123; &quot;range&quot;:&#123; # 关键字 &quot;field&quot;:&quot;age&quot;, # 按age进行分桶 &quot;ranges&quot;:[ &#123; &quot;key&quot;:&quot;&gt;=19 &amp;&amp; &lt; 25&quot;, # 第一个桶： 19&lt;=年龄&lt;25 &quot;from&quot;:19, &quot;to&quot;:25 &#125;, &#123; &quot;key&quot;:&quot;&lt; 19&quot;, # 第二个桶： 年龄&lt;19 &quot;to&quot;:19 &#125;, &#123; &quot;key&quot;:&quot;&gt;= 25&quot;, # 第三个桶： 年龄&gt;=25 &quot;from&quot;:25 &#125; ] &#125; &#125; &#125;&#125; 1.2.3 Date RangeDate Range: 按指定日期范围进行分桶 12345678910111213141516171819202122232425262728# 使用date_range关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;date_ranges&quot;:&#123; &quot;date_range&quot;:&#123; # 关键字 &quot;field&quot;:&quot;birth&quot;, # 按age进行分桶 &quot;format&quot;:&quot;yyyy&quot;, &quot;ranges&quot;:[ &#123; &quot;key&quot;:&quot;&gt;=1980 &amp;&amp; &lt; 1990&quot;, # 第一个桶： 1980&lt;=出生日期&lt;1990 &quot;from&quot;:&quot;1980&quot;, &quot;to&quot;:&quot;1990&quot; &#125;, &#123; &quot;key&quot;:&quot;&lt; 1980&quot;, # 第二个桶： 出生日期&lt;1980 &quot;to&quot;:1980 &#125;, &#123; &quot;key&quot;:&quot;&gt;= 1990&quot;, # 第三个桶： 出生日期&gt;=1990 &quot;from&quot;:1990 &#125; ] &#125; &#125; &#125;&#125; 1.2.4 HistogramHistogram: 直方图，按固定数值间隔策略进行数据分割 1234567891011121314151617# 使用histogram关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;age_hist&quot;:&#123; &quot;histogram&quot;:&#123; # 关键词 &quot;field&quot;:&quot;age&quot;, &quot;interval&quot;:3, # 设定间隔大小为2 &quot;extended_bounds&quot;:&#123; # 设定数据范围 &quot;min&quot;:0, &quot;max&quot;:30 &#125; &#125; &#125; &#125;&#125; 1.2.5 Date HistogramDate Histogram: 日期直方图，按固定时间间隔进行数据分割 123456789101112131415161718# 使用date_histogram关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;birth_hist&quot;:&#123; &quot;date_histogram&quot;:&#123; # 关键词 &quot;field&quot;:&quot;birth&quot;, &quot;interval&quot;:&quot;year&quot;, # 设定间隔大小为年year &quot;format&quot;:&quot;yyyy&quot;, &quot;extended_bounds&quot;:&#123; # 设定数据范围 &quot;min&quot;:&quot;1980&quot;, &quot;max&quot;:&quot;1990&quot; &#125; &#125; &#125; &#125;&#125; 1.3 Bucket+Metric聚合分析Bucket聚合分析允许通过添加子分析来进一步进行分析，该子分析可以是Bucket，也可以是Metric。 分桶之后再分桶（Bucket+Bucket），在数据可视化中一般使用千层饼图进行显示。 分桶之后再数据分析（Bucket+Metric） 123456789101112131415161718192021222324252627# 分桶之后再分桶——Bucket+BucketGET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 第一层Bucket &quot;match&quot;:&#123; &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10 &#125;, &quot;aggs&quot;:&#123; &quot;age_range&quot;:&#123; &quot;range&quot;:&#123; # 第二层Bucket &quot;field&quot;:&quot;age&quot;, &quot;ranges&quot;:[ &#123;&quot;to&quot;:20&#125;, &#123;&quot;from&quot;:20,&quot;to&quot;:30&#125;, &#123;&quot;from&quot;:30&#125; ] &#125; &#125; &#125; &#125; &#125; &#125;&#125; 12345678910111213141516171819202122# 分桶之后再数据分析——Bucket+MetricGET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 第一层Bucket &quot;match&quot;:&#123; &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10 &#125;, &quot;aggs&quot;:&#123; &quot;stats_age&quot;:&#123; &quot;stats&quot;:&#123; # 第二层Metric &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 1.4 Pipeline聚合分析针对聚合分析的结果进行再分析，且支持链式调用： 12345678910111213141516171819202122232425# 使用pipeline聚合分析,计算订单月平均销售额。GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;sales_per_month&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;date&quot;, &quot;interval&quot;:&quot;month&quot; &#125;, &quot;aggs&quot;:&#123; &quot;sales&quot;:&#123; &quot;sum&quot;:&#123; &quot;field&quot;:&quot;price&quot; &#125; &#125; &#125; &#125;, &quot;avg_monthly_sales&quot;:&#123; &quot;avg_bucket&quot;:&#123; # bucket类型 &quot;buckets_path&quot;:&quot;sales_per_month&gt;sales&quot; # 使用buckets_path参数，表明是pipeline &#125; &#125; &#125;&#125; pipeline的分析结果会输出到原结果中，由输出位置不同，分为两类：Parent和Sibling。 Sibling。结果与现有聚合分析结果同级，如：Max/Min/Sum/Avg Bucket、Stats/Extended Stats Bucket、Percentiles Bucket Parent。结果内嵌到现有聚合分析结果中，如：Derivate、Moving Average、Cumulative Sum 123456789101112131415161718192021222324# Sibling聚合分析(min_bucket)GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 根据job.keyword进行分桶 &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10 &#125;, &quot;aggs&quot;:&#123; &quot;avg_salary&quot;:&#123; &quot;avg&quot;:&#123; # 之后Metric中求工资的平均数 &quot;field&quot;:&quot;salary&quot; &#125; &#125; &#125; &#125;, &quot;min_salary_by_job&quot;:&#123; &quot;min_bucket&quot;:&#123; # 关键词 &quot;buckets_path&quot;:&quot;jobs&gt;avg_salary&quot; # 按工资平均数，排列每个桶中的job &#125; &#125;&#125; 1234567891011121314151617181920212223242526# Parent聚合分析(Derivate)GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;bitrh&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;birth&quot;, &quot;interval&quot;:&quot;year&quot;, &quot;min_doc_count&quot;:0 &#125;, &quot;aggs&quot;:&#123; &quot;avg_salary&quot;:&#123; &quot;avg&quot;:&#123; &quot;field&quot;:&quot;salary&quot; &#125; &#125;, &quot;derivative_avg_salary&quot;:&#123; &quot;derivative&quot;:&#123; # 关键词 &quot;buckets_path&quot;:&quot;avg_salary&quot; &#125; &#125; &#125; &#125; &#125;&#125; 1.5 聚合分析的作用范围ES聚合分析默认作用范围是query的结果集 1234567891011121314151617181920# ES中聚合分析的默认作用范围是query的结果集GET my_index/_search&#123; &quot;size&quot;:0, &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; &quot;match&quot;:&#123; # 此时，只在username字段中包含alfred的文档中进行分桶 &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10 &#125; &#125; &#125; &#125;&#125; 可通过以下方式修改：filter、post_filter、global filter: 为某个结合分析设定过滤条件，从而在不改变整体query语句的情况下修改范围 post_filter，作用于文档过滤，但在聚合分析之后才生效 global，无视query条件，基于所有文档进行分析 1234567891011121314151617181920212223# 使用filter进行过滤GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs_salary_small&quot;:&#123; &quot;filter&quot;:&#123; &quot;range&quot;:&#123; &quot;salary&quot;:&#123; &quot;to&quot;:10000 &#125; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 在salary小于10000的文档中对工作进行分桶 &quot;field&quot;:&quot;job.keyword&quot; &#125; &#125; &#125; &#125; &#125;&#125; 1234567891011121314151617# 使用post_filter进行过滤GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 在salary小于10000的文档中对工作进行分桶 &quot;field&quot;:&quot;job.keyword&quot; &#125; &#125; &#125;, &quot;post_filter&quot;:&#123; # 在集合分析之后才生效 &quot;match&quot;:&#123; &quot;job.keyword&quot;:&quot;java engineer&quot; &#125; &#125;&#125; 123456789101112131415161718192021222324252627# 使用global进行过滤GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;job.keyword&quot;:&quot;java engineer&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;java_avg_salary&quot;:&#123; &quot;avg&quot;:&#123; &quot;field&quot;:&quot;salary&quot; &#125; &#125;, &quot;all&quot;:&#123; &quot;global&quot;:&#123; # 关键词 &quot;aggs&quot;:&#123; &quot;avg_salary&quot;:&#123; &quot;avg&quot;:&#123; &quot;field&quot;:&quot;salary&quot; # 依然是对所有的文档进行查询，而不会去管query &#125; &#125; &#125; &#125; &#125; &#125;&#125; 1.6 聚合分析中的排序 可使用自带的关键数据排序，如：_count文档数、_key按key值 也可使用聚合结果进行排序 123456789101112131415161718192021# 使用自带的数据进行排序GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10, &quot;order&quot;:[ &#123; &quot;_count&quot;:&quot;asc&quot; # 默认按_count倒叙排列 &#125;, &#123; &quot;_key&quot;:&quot;desc&quot; 使用多个排序值，从上往下的顺序进行排列 &#125; ] &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627# 使用聚合结果进行排序GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;salary_hist&quot;:&#123; &quot;histogram&quot;:&#123; &#125;, &quot;aggs&quot;:&#123; &quot;age&quot;:&#123; &quot;filter&quot;:&#123; &quot;range&quot;:&#123; &quot;age&quot;:&#123; &quot;gte&quot;:10 &#125; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;avg_age&quot;:&#123; &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 1.7 计算精准度问题ES聚合的执行流程：每个Shard上分别计算，由coordinating Node做聚合。 Terms计算不准确原因：数据分散在多个Shard上，coordinating Node无法得悉数据全貌，那么在取数据的时候，造成精准度不准确。 如下图：正确结果应该为a,b,c,而返回的是a,b,d 解决办法有两种： 直接设置shard数量为1；消除数据分散问题，但无法承载大数据量。 设置shard_size大小，即每次从shard上额外多获取数据，从而提升精准度 terms聚合返回结果中有两个统计值： doc_count_error_upper_bound：被遗漏的term可能的最大值； sum_other_doc_count：返回结果bucket的term外其他term的文档总数。 设定show_term_doc_count_error可以查看每个bucket误算的最大值(doc_count_error_upper_bound,为0表示计算准确) Shard_Size默认大小：(size*1.5)+10 通过调整Shard_Size的大小降低doc_count_error_upper_bound来提升准确度 增大了整体的计算量，从而降低了响应时间 权衡 海量数据、精准度、实时性 三者只能取其二。 Elasticsearch目前支持两种近似算法：cardinality(度量) 和 percentiles(百分位数度量) 结果近似准确，但不一定精准 可通过参数的调整使其结果精准，但同时消耗更多时间和性能 2. ElasticSearch的数据建模数据建模(Data Modeling)大致分为三个阶段：概念建模、逻辑建模、物理建模 概念模型：时间占比10% 基础。确定系统的核心需求和范围边界，实际实体与实体之间的关系。 逻辑模型：时间占比60-70% 核心。确定系统的核心需求和范围边界，实际实体与实体之间的关系。 物理模型：时间占比20-30% 落地实现。结合具体的数据库产品，在满足业务读写性能等需求的前提下确定最终的定义。 2.1 ES中的数据建模ES是基于Luence以倒排索引为基础实现的存储体系，不遵循关系型数据库中的范式约定。 2.2 Mapping字段相关设置 enabled:true/false。false表示 仅存储，不做搜索或聚合分析。 **index:true/false。是否构建倒排索引。不需进行字段的检索的时候设为false。 index_options:docs/freqs/positions/offsets。确定存储倒排索引的哪些信息。 norms:true/false。是否存储归一化相关系数，若字段仅用于过滤和聚合分析，则可关闭。 doc_values:true/false。是否启用doc_values，用于排序和聚类分析。默认开启。 field_data:true/false。是否设text类型为fielddata，实现排序和聚合分析。默认关闭。 store:true/false。是否存储该字段。 coerce:true/false。 是否开启数值类型转换功能，如：字符串转数字等。 multifields:多字段。灵活使用多字段特性来解决多样业务需求。 dynamic:true/false/strict。控制mapping自动更新。 date_detection:true/false。是否启用自定识别日期类型，一般设为false，避免不必要的识别字符串中的日期。 2.3 Mapping字段属性设定流程判断类型—&gt;是否需要检索—&gt;是否需要排序和聚合分析—&gt;是否需要另行存储 判断类型 字符串类型：需要分词，则设为text，否则设为keyword。 枚举类型：基于性能考虑，设为keyword，即便该数据为整型。 数值类型：尽量选择贴近的类型，如byte即可表示所有数值时，即用byte，而不是所有都用long。 其他类型：布尔型，日期类型，地理位置类型等。 是否需要检索 完全不需要检索、排序、聚合分析的字段enabled设为false。 不需检索的字段index设为false。 需检索的字段，可通过如下配置设定需要的存储粒度: index_options 结合需要设定。 norms 不需归一化数据时可关闭。 是否需要排序和聚合分析 当不需要排序和聚合分析功能时： doc_values设为false。 field_data设为false。 是否需要另行存储 store设为true即可存储该字段的原始内容(且与_source无关)，一般结合_source的enabled设为false时使用。 2.4 ES建模实例 针对博客文章设定索引blog_index，包含字段： 标题：title 发布日期：publish_data 作者：author 摘要：abstract 网址：url 简易的数据模型： 123456789101112131415161718192021222324# 简易模型blog_indexPUT blog_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;title&quot;:&#123; #title设为text，包含自字段keyword。支持检索、排序、聚合分析 &quot;type&quot;:&quot;text&quot;, &quot;fields&quot;:&#123; &quot;keyword&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125; &#125; &#125;,#publish_data设为date，支持检索、排序、聚合分析 &quot;publish_data&quot;:&#123;&quot;type&quot;:&quot;date&quot;&#125;, # author设为keyword，支持检索、排序、聚合分析 &quot;author&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;, # abstract设为text，支持检索、排序、聚合分析 &quot;abstract&quot;:&#123;&quot;type&quot;:&quot;text&quot;&#125;, # url设为date，不需进行检索 &quot;url&quot;:&#123;&quot;enabled&quot;:false&#125; &#125; &#125; &#125;&#125; 如果在blog_index中加入一个内容字段content 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 为blog_index增加content字段PUT blog_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; #关闭，不存原始内容到_source &quot;_source&quot;:&#123;&quot;enabled&quot;:false&#125;, &quot;properties&quot;:&#123; #title设为text，包含自字段keyword。支持检索、排序、聚合分析 &quot;title&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;fields&quot;:&#123; &quot;keyword&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125;, &quot;store&quot;:true #对数据进行存储 &#125;,#publish_data设为date，支持检索、排序、聚合分析 &quot;publish_data&quot;:&#123; &quot;type&quot;:&quot;date&quot;, &quot;store&quot;:true # 对数据进行存储 &#125;, &quot;author&quot;:&#123;# author设为keyword，支持检索、排序、聚合分析 &quot;type&quot;:&quot;keyword&quot;, &quot;store&quot;:true # 对数据进行存储 &#125;, &quot;abstract&quot;:&#123;# abstract设为text，支持检索、排序、聚合分析 &quot;type&quot;:&quot;text&quot;, &quot;store&quot;:true # 对数据进行存储 &#125;, &quot;content&quot;:&#123;# content设为text，支持检索、排序、聚合分析 &quot;type&quot;:&quot;text&quot;, &quot;store&quot;:true # 对数据进行存储 &#125;, &quot;url&quot;:&#123; &quot;type&quot;:&quot;keyword&quot;, # url设为keyword &quot;doc_values&quot;:false, # url不支持排序和聚合分析 &quot;norms&quot;:false, # url也不需要归一化数据 &quot;ignore_above&quot;:100, # 预设内容长度为100 &quot;store&quot;:true # 对数据进行存储 &#125; &#125; &#125; &#125;&#125; 在搜索时增加高亮: 在此时，content里面的数据会存储大量的内容数据，数据量可能达到上千、上万，甚至几十万。那么在搜索的时候，根据search机制，如果还是像之前一样进行_search搜索，并只显示其他字段的话，其实依然还是每次获取了content字段的内容，影响性能，所以，使用stored_fields参数，控制返回的字段。节省了大量资源： 123456789101112131415# 使用stored_fields返回指定的存储后的字段GET blog_index/_search&#123; &quot;stored_fields&quot;:[&quot;title&quot;,&quot;publish_data&quot;,&quot;author&quot;,&quot;Abstract&quot;,&quot;url&quot;], &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;content&quot;:&quot;world&quot;#依然进行content搜索，但是不返回所有的content字段 &#125; &#125;, &quot;highlight&quot;:&#123; #针对content字段进行高亮显示 &quot;fields&quot;:&#123; &quot;content&quot;:&#123;&#125; &#125; &#125;&#125; 注意：GET blog_index/_search?_source=title 虽然只显示了title，但是search机制决定了，会把所有_source内容获取到，但只是显示title。 2.5 ES中关联关系处理ES不擅长处理关系型数据库中的关联关系，因为底层使用的倒排索引，如：文章表blog和评论表comment之间通过blog_id关联。目前ES主要有以下4种常用的方法来处理关联关系： Nested Object:嵌套文档 Parent/Child:父子文档 Data denormalization:数据的非规范化 Application-side joins:服务端Join或客户端Join 2.5.1 Application-side joins（服务端Join或客户端Join）索引之间完全独立（利于对数据进行标准化处理，如便于上述两种增量同步的实现），由应用端的多次查询来实现近似关联关系查询。 适用于第一个实体只有少量的文档记录的情况（使用ES的terms查询具有上限，默认1024，具体可在elasticsearch.yml中修改），并且最好它们很少改变。这将允许应用程序对结果进行缓存，并避免经常运行第一次查询。 2.5.2 Data denormalization（数据的非规范化）通俗点就是通过字段冗余，以一张大宽表来实现粗粒度的index，这样可以充分发挥扁平化的优势。但是这是以牺牲索引性能及灵活度为代价的。 使用的前提：冗余的字段应该是很少改变的；比较适合与一对少量关系的处理。当业务数据库并非采用非规范化设计时，这时要将数据同步到作为二级索引库的ES中，就很难使用上述增量同步方案，必须进行定制化开发，基于特定业务进行应用开发来处理join关联和实体拼接。 宽表处理在处理一对多、多对多关系时，会有字段冗余问题，适合“一对少量”且这个“一”更新不频繁的应用场景。 2.5.3 Nested objects（嵌套文档）索引性能和查询性能二者不可兼得，必须进行取舍。嵌套文档将实体关系嵌套组合在单文档内部（类似与json的一对多层级结构），这种方式牺牲索引性能（文档内任一属性变化都需要重新索引该文档）来换取查询性能，可以同时返回关系实体，比较适合于一对少量的关系处理。 当使用嵌套文档时，使用通用的查询方式是无法访问到的，必须使用合适的查询方式（nested query、nested filter、nested facet等），很多场景下，使用嵌套文档的复杂度在于索引阶段对关联关系的组织拼装。 2.5.4 Parent/Child（父子文档）父子文档牺牲了一定的查询性能来换取索引性能，适用于一对多的关系处理。其通过两种type的文档来表示父子实体，父子文档的索引是独立的。父-子文档ID映射存储在 Doc Values 中。当映射完全在内存中时， Doc Values 提供对映射的快速处理能力，另一方面当映射非常大时，可以通过溢出到磁盘提供足够的扩展能力。 在查询parent-child替代方案时，发现了一种filter-terms的语法，要求某一字段里有关联实体的ID列表。基本的原理是在terms的时候，对于多项取值，如果在另外的index或者type里已知主键id的情况下，某一字段有这些值，可以直接嵌套查询。具体可参考官方文档的示例：通过用户里的粉丝关系，微博和用户的关系，来查询某个用户的粉丝发表的微博列表。 父子文档相比嵌套文档较灵活，但只适用于“一对大量”且这个“一”不是海量的应用场景，该方式比较耗内存和CPU，这种方式查询比嵌套方式慢5~10倍，且需要使用特定的has_parent和has_child过滤器查询语法，查询结果不能同时返回父子文档（一次join查询只能返回一种类型的文档）。 而受限于父子文档必须在同一分片上，ES父子文档在滚动索引、多索引场景下对父子关系存储和联合查询支持得不好，而且子文档type删除比较麻烦（子文档删除必须提供父文档ID）。 如果业务端对查询性能要求很高的话，还是建议使用宽表化处理的方式，这样也可以比较好地应对聚合的需求。在索引阶段需要做join处理，查询阶段可能需要做去重处理，分页方式可能也得权衡考虑下。 2.6 ES中的reindexreindex：指重建所有数据的过程，一般发生在一下情况： mapping设置变更，如：字段类型变化，分词器字典更新等； index设置变更，如：分片数变化； 迁移数据。 ES提供了线程的api用于完成数据重建： _update_by_query：在现有索引上重建； _reindex：在其他索引上重建。 1234# 将blog_index中所有文档重建一遍：# 如果遇到版本冲突，依然执行。POST blog_index/_update_by_query?conflicts=proceed # 此时如果blog_index中没有store的数据，则会报错 2.6.1 使用_update_by_query，更新文档的字段值和部分文档：12345678910111213# 更新文档的字段值及部分文档POST blog_index/_update_by_query&#123; &quot;script&quot;:&#123; # 更新文档的字段值 &quot;source&quot;:&quot;ctx._source.likes++&quot;, # 代码 &quot;lang&quot;:&quot;painless&quot; # ES自带script语法 &#125;, &quot;query&quot;:&#123; # 更新部分文档 &quot;term&quot;:&#123; &quot;user&quot;:&quot;tom&quot; &#125; &#125;&#125; 在reindex发起后进入的文档，不会参与重建，类似于快照的机制。因此：一般在文档不再发生变更时，进行文档的reindex。 2.6.2 使用_reindex，重建数据：12345678910# 使用_reindex：POST _reindex&#123; &quot;source&quot;:&#123; # 被重建索引 &quot;index&quot;:&quot;blog_index&quot; &#125;, &quot;dest&quot;:&#123; # 目标索引 &quot;index&quot;:&quot;blog_new_index&quot; &#125;&#125; 数据重建时间，受到索引文档规模的影响，此时设定url参数wait_for_completion为false，来异步执行。 ES通过task来描述此类执行任务，并提供了task api来查看任务的执行进度和相关数据： 1234# 使用task apiPOST blog_index/_update_by_query?comflicts=proceed&amp;wait_for_completion=false# 使用返回的taskid，查看任务的执行进度和相关数据GET _tasks/&lt;返回的task id&gt; 2.7 其他建议： 对mapping进行版本管理： 要么写文件/注释，加入到Git仓库，一眼可见； 要么增加metadata字段，维护版本，并在每次更新mapping设置的时候加1。 123&quot;metadata&quot;:&#123; &quot;version&quot;:1&#125; 防止字段过多： index.mapping.total_fields_limit，默认1000个。一般是因为没有高质量的数据建模导致，如：dynamic设为true。此时考虑查分多个索引来解决问题。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://chaooo.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://chaooo.github.io/tags/Kibana/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"http://chaooo.github.io/tags/ElasticStack/"},{"name":"LogStash","slug":"LogStash","permalink":"http://chaooo.github.io/tags/LogStash/"}]},{"title":"【ElasticStack】ElasticSearch分布式特性 与 Search机制","date":"2019-11-20T12:44:13.000Z","path":"article/20191120.html","text":"1. ElasticSearch的分布式特性1.1 分布式介绍 ES支持集群模式，即一个分布式系统。其好处主要有以下2个: 可增大系统容量。比如：内存、磁盘的增加使得ES能够支持PB级别的数据； 提高了系统可用性。即使一部分节点停止服务，集群依然可以正常对外服务。 ES集群由多个ES实例构成。 不同集群通过集群名字来区分，通过配置文件elasticsearch.yml中的cluster.name可以修改，默认为elasticsearch 每个ES实例的本质，其实是一个JVM进程，且有自己的名字，通过配置文件中的node.name可以修改。 1.2 构建ES集群1234# 创建一个本地化集群my_clusterbin/elasticsearch -Epath.data=node1 -Ecluster.name=my_cluster -Enode.name=node1 -dbin/elasticsearch -Ehttp.port=8200 -Epath.data=node2 -Ecluster.name=my_cluster -Enode.name=node2 -dbin/elasticsearch -Ehttp.port=7200 -Epath.data=node3 -Ecluster.name=my_cluster -Enode.name=node3 -d 可以通过cerebro插件可以看到，集群my_cluster中存在三个节点，分别为：node1、node2、node3 **Cluster State**：ES集群相关的数据，主要记录如下信息： 节点信息：如节点名称、连接地址等 索引信息：如索引名称、配置等 Master Node**：主节点，可修改cluster state的节点。一个集群只能有一个**。 cluster state存储于每个节点上，master维护最新版本并向其他从节点同步。 master节点是通过集群中所有节点选举产生的，可被选举的节点称为**master-eligible节点** 通过配置node.master:true设置节点为可被选举节点(默认为true) **Cordinating Node**：处理请求的节点。是所有节点的默认角色，且不能取消。 路由请求到正确的节点处理，如：创建索引的请求到master节点。 **Data Node**：存储数据的节点，默认节点都是data类型。配置node.data:true。 1.3 副本与分片 提高系统可用性： 服务可用性：集群 数据可用性：副本(Replication) 增大系统容量：分片(Shard) 分片是ES能支持PB级别数据的基石：可在创建索引时指定 分片存储部分数据，可以分布于任意节点； 分片数在索引创建时指定，且后续不能更改，默认为5个； 有主分片和副本分片之分，以实现数据的高可用； 副本分片由主分片同步数据，可以有多个，从而提高数据吞吐量。 分片数的设定很重要，需要提前规划好 过小会导致后续无法通过增加节点实现水平扩容 过大会导致一个节点分片过多，造成资源浪费，同时会影响查询性能 例如：在3个节点的集群中配置索引指定3个分片和1个副本（index.number_of_shards:3,index.number_of_replicas:1），分布如下： 怎样增加节点或副本提高索引的吞吐量 同时增加新的节点和加新的副本，这样把新的副本放在新的节点上，进行索引数据读取的时候，并且读取，就会提升索引数据读取的吞吐量。 1.4 ES集群状态 与 故障转移 ES的**健康状态(Cluster Health)**分为三种： Greed，绿色。表示所有主分片和副本分片都正常分配； Yellow，黄色。表示所有主分片都正常分配，但有副本分片未分配； Red，红色。表示有主分片未分配。 可通过GET _cluster/health查看集群状态 返回集群名称，集群状态，节点数，活跃分片数等信息。 如果此时磁盘空间不够，name在创建新的索引的时候，主副分片都不会再分配，此时的集群状态会直接飙红，但此时依然可以访问集群和索引，也可以正常进行搜索。 所以：ES的集群状态为红色，不一定就不能正常服务。 故障转移 Failover 当其余节点发现定时ping主节点master无响应的时候，集群状态转为Red。此时会发起master选举。 新master节点发现若有主分片未分配，会将副本分片提升为主分片，此时集群状态转为Yellow。 新master节点会将提升后的主分片生成新的副本，此时集群状态转为Green。整个故障转移过程结束。 1.5 文档分布式存储通过文档到分片的映射算法，使文档均匀分布到所有分片上，以充分利用资源。 文档对应分片计算公式：shard = hash(routing)%number_of_primary_shards hash保证数据均匀分布在分片中 routing作为关键参数，默认为文档ID，也可自行指定 number_of_primary_shards为主分片数 主分片数一旦设定，不能更改：为了保证文档对应的分片不会发生改变。 文档创建流程: 文档读取流程 文档批量创建流程 文档批量读取流程 1.6 脑裂问题 在分布式系统中一个经典的网络问题 当一个集群在运行时，作为master节点的node1的网络突然出现问题，无法和其他节点通信，出现网络隔离情况。那么node1自己会组成一个单节点集群，并更新cluster state；同时作为data节点的node2和node3因为无法和node1通信，则通过选举产生了一个新的master节点node2，也更新了cluster state。那么当node1的网络通信恢复之后，集群无法选择正确的master。 解决方案也很简单： 仅在可选举的master-eligible节点数&gt;=quorum的时候才进行master选举。 quorum(至少为2)=master-eligible数量/2 + 1。 通过discovery.zen.minimum_master_nodes为quorum即可避免脑裂。 1.7 Shards分片详解 倒排索引一旦生成，不能更改。 优点： 不用考虑并发写文件的问题，杜绝了锁机制带来的性能问题 文件不在更改，则可以利用文件系统缓存，只需载入一次，只要内存足够，直接从内存中读取该文件，性能高； 利于生成缓存数据(且不需更改)； 利于对文件进行压缩存储，节省磁盘和内存存储空间。 缺点：在写入新的文档时，必须重构倒排索引文件，然后替换掉老倒排索引文件后，新文档才能被检索到，导致实时性差。 解决文档搜索的实时性问题的方案： 新文档直接生成新待排索引文件，查询时同时查询所有倒排索引文件，然后做结果的汇总即可，从而提升了实时性。 Segment Lucene就采用了上述方案，构建的单个倒排索引称为Segment，多个Segment合在一起称为Index(Lucene中的Index)。在ES中的一个shard分片，对应一个Lucene中的Index。且Lucene有一个专门记录所有Segment信息的文件叫做Commit Point。 Segment写入磁盘的过程依然很耗时，可以借助文件系统缓存的特性。【先将Segment在内存中创建并开放查询，来进一步提升实时性】，这个过程在ES中被称为：refresh。 在refresh之前，文档会先存储到一个缓冲队列buffer中，refresh发生时，将buffer中的所有文档清空，并生成Segment。 ES默认每1s执行一次refresh操作，因此实时性提升到了1s。这也是ES被称为近实时的原因（Near Real Time）。 translog文件 translog机制：当文档写入buffer时，同时会将该操作写入到translog中，这个文件会即时将数据写入磁盘，在6.0版本之后默认每个要求都必须落盘，这个操作叫做fsync操作。这个时间也是可以通过配置：index.translog.*进行修改的。比如每五秒进行一次fdync操作，那么风险就是丢失这5s内的数据。 文档搜索实时性——flush(十分重要) flush的功能，就是：将内存中的Segment写入磁盘，主要做如下工作： 将translog写入磁盘； 将index bufffer清空，其中的文档生成一个新的Segment，相当于触发一次refresh； 更新Commit Point文件并写入磁盘； 执行fsync落盘操作，将内存中的Segment写入磁盘； 删除旧的translog文件。 refresh与flush的发生时机 refresh：发生时机主要有以下几种情况： 间隔时间达到。 通过index.settings.refresh_interval设置，默认为1s。 index.buffer占满时。 通过indices.memory.index_buffer_size设置，默认JVM heap的10%，且所有shard共享。 flush发生时。会触发一次refresh。 flush：发生时机主要有以下几种情况： 间隔时间达到。 5.x版本之前，通过index.translog.flush_threshold_period设置，默认30min。 5.x版本之后，ES强制每30min执行一次flush，不能再进行更改。 translog占满时。 通过index.translog.flush_threshold_size设置，默认512m。且每个Index有自己的translog。 删除和更新文档： 删除： Segment一旦生成，就不能更改，删除的时候，Lucene专门维护一个.del文件，记录所有已删除的文档。 .del文件上记录的是文档在Lucene中的ID，在查询结果返回之前，会过滤掉.del文件中的所有文档。 更新： 先删除老文档，再创建新文档，两个文档的ID在Lucene中的ID不同，但是在ElasticSearch中ID相同。 Segment Merging(合并) 随着Segment的增多，由于每次查询的Segment数量也增多，导致查询速度变慢； ES会定时在后台进行Segment merge的操作，减少Segment数量； 通过force_merge api可以手动强制做Segment的合并操作。 2. ElasticSearch的集群优化2.1 生产环境部署 遵照官方建议设置所有系统参数。 在ES的配置文件中elasticsearch.yml中，尽量只写必备的参数，其他可通过api进行动态设置，随着ES版本的不断升级，很多网上流传的参数，现在已经不再适用，所以不要胡乱复制。 建议设置的基本参数有： cluster.name node.name node.master/node.data/node.ingest network.host: 建议显示指定为服务器的内网ip，切勿直接指定0.0.0.0，很容易直接从外部被修改ES数据。 discovery.zen.ping.unicast.hosts: 设置集群其他节点地址，一般设置选举节点即可 discovery.zen.minimum_master_nodes: 一般设置为2，有3个即可。 path.data/path.log 除上述参数外，再根据需要增加其他的静态配置参数，如：refresh优化参数，indices.memory.index_buffer_size。 动态设定的参数有transient(短暂的)和persistent(持续的)两种，前者在集群重启后会丢失，后者在集群重启后依然# 生效。二者都覆盖了yml中的配置，举例： 12345678910# 使用transient和persistent动态设置ES集群参数PUT /_cluster/Settings&#123; &quot;persistent&quot;:&#123; # 永久 &quot;discovery.zen.minimum_master_nodes:2 &#125;, &quot;transient&quot;:&#123; # 临时 &quot;indices.store.throttle.max_bytes_per_sec&quot;:&quot;50mb&quot; &#125;&#125; 关于JVM内存设定 每个节点尽量不要超多31GB。 预留一半内存给操作系统，用来做文件缓存。ES的具体内存大小根据node要存储的数据量来估算，为了保证性能 搜索类项目中：内存：数据量 ===&gt; 1：16； 日志类项目中：内存：数据量 ===&gt; 1：48/96。 123456假设现有数据1TB，3个node，1个副本，那么：每个node存储(1+1)*1024 / 3 = 666GB,即700GB左右，做20%预留空间，每个node约存850GB数据。此时：如果是搜索类项目，每个node内存约为850/16=53GB，已经超过31GB最大限制；而：31*16 = 496，意味着每个node最大只能存496GB的数据，则：2024/496=4.08...即至少需要5个节点。如果是日志类项目，每个node最大能存:31*48=1488GB,则：2024/1488=1.36...，则三个节点已经够了。 2.2 写性能优化在写上面的优化，主要是增大写的吞吐量——EPS(Event Per Second) 优化方案： Client：多线程写，批量写bulk； ES：在高质量数据建模的前提下，主要在refresh、translig和flush之间做文章。 降低refresh写入内存的频率： 增大refresh_interval，降低实时性，增大每次refresh处理的文件数，默认1s。可以设为-1s，禁止自动refresh。 增大index buffer大小，参数为：indices.memory.index_buffer_size。此为静态参数，需设定在elasticsea.yml中，默认10% 降低translog写入磁盘频率，同时会降低容灾能力： index.translog.durability：设为async； index.translog.sync_interval。设置需要的大小如：120s =&gt; 每120s才写一次磁盘。 index.translog.flush_threshold_size。默认512m。即当translog大小超过此值，会触发一次flush，可以调大避免flush过早触发。 在flush方面，从6.x开始，ES固定每30min执行一次，所以优化点不多，一般都是ES自动完成。 其他： 将副本数设置为0，在文档全部写完之后再加副本； 合理设计shard数，保证shard均匀地分布在所有node上，充分利用node资源： index.routing.allocation.total_shards_per_node：限定每个索引在每个node上可分配的主副分片数， 如：有5个node，某索引有10个主分片，1个副本(10个副分片)，则：20/5=45,但是实际要设置为5，预防某个node下线后分片迁移失败。 写性能优化，主要还是index级别的设置优化。一般在refresh、translog、flush三个方面进行优化； 2.3 读性能优化 主要受以下几方面影响： 数据模型是否符合业务模型？ 数据规模是否过大？ 索引配置是否优化？ 查询运距是否优化？ 高质量的数据建模 将需通过cripte脚本动态计算的值，提前计算好作为字段存入文档中； 尽量使数据模型贴近业务模型 根据不同数据规模设定不同的SLA(服务等级协议)，万级数据和千万级数据和亿万级数据性能上肯定有差异； 索引配置优化 根据数据规模设置合理的分片数，可通过测试得到最适合的分片数； 分片数并不是越多越好 查询语句优化 尽量使用Filter上下文，减少算分场景(Filter有缓存机制，能极大地提升查询性能)； 尽量不用cript进行字段计算或算分排序等； 结合profile、explain API分析慢查询语句的症结所在，再去优化数据模型。 2.4 其他优化点 如何设定shard数？ ES的性能基本是线性扩展的，因此，只需测出一个shard的性能指标，然后根据实际的性能需求就可算出所需的shard数。 测试一个shard的流程如下： 搭建与生产环境相同配置的单节点集群； 设定一个单分片0副本的索引； 写入实际生产数据进行测试，获取（写性能指标）； 针对数据进行查询操作，获取（读性能指标）。 压力测试工具，可以采用ES自带的esrally，从经验上讲： 如果是搜索引擎场景，单shard大小不超过15GB； 如果是日志分析场景，单shard大小不超过50GB； 估算索引的总数据大小，除以上述单shard大小，也可得到经验上的分片数。 2.5 ES集群监控使用官方免费插件X-pack。 安装与启动： 123456# X-pack的安装cd ~/elasticsearch-6.1.1bin/elasticsearch-plugin install x-pack# cd ~/kibana-6.1.1bin/kibana-plugin indtall x-pack 之后重启ES集群即可。在kibana的界面可以看到新增了工具，使用Monitoring进行集群监控。 3. ElasticSearch中Search的运行机制 Search执行的时候，实际分为两个步骤执行： Query阶段：搜索 Fetch阶段：获取 3.1 Query—Then—Fetch：若集群my_cluster中存在三个节点node1、node2、node3，其中master为node1，其余的为data节点。 Query阶段: Fetch阶段: 3.2 相关性算分：相关性算分在shard和shard之间是相互独立的。也就意味着：同一个单词term在不同的shard上的TDF等值也可能是不同的。得分与shard有关。当文档数量不多时，会导致相关性算分严重不准的情况发生。 解决方案： 设置分片数为1个，从根本上排除问题。（此方案只适用于百万/少千万级的少量数据） 使用DFS Query-then-Fetch查询方式。 DFS Query-then-Fecth： 在拿到所有文档后，再重新进行完整的计算一次相关性得分，耗费更多的CPU和内存，执行性能也较低。所以也不推荐。 123456789# 使用DLS Query-then-Fetch进行查询：GET my_index/_search？search_type=dfs_query_then_fetch&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; ... &#125; &#125;&#125; 3.3 排序相关：默认采用相关性算分结果进行排序。可通过sort参数自定义排序规则，如： 1234567891011121314151617181920212223# 使用sort关键词进行排序GET my_index/_search&#123; &quot;sort&quot;:&#123; # 关键词 &quot;birth&quot;:&quot;desc&quot; &#125;&#125;# 或使用数组形式定义多字段排序规则GET my_index/_search&#123; &quot;sort&quot;:[ # 使用数组 &#123; &quot;birth&quot;:&#123; &quot;order&quot;:&quot;asc&quot; &#125; &#125;, &#123; &quot;age&quot;:&#123; &quot;order&quot;:&quot;desc&quot; &#125; &#125; ]&#125; 直接按数字/日期排序，如上例中birth 按字符串进行排序：字符串排序较特殊，因为在ES中有keyword和text两种： 123456789101112131415# 直接对text类型进行排序GET my_index/_search&#123; &quot;sort&quot;:&#123; &quot;username&quot;:&quot;desc&quot; # 针对username字段进行倒序排序 &#125;&#125;## 针对keyword进行排序GET my_index/_search&#123; &quot;sort&quot;:&#123; &quot;username.keyword&quot;:&quot;desc&quot; # 针对username的子类型keyword类型进行倒叙排序 &#125;&#125; 3.3.1 关于fielddata和docvalues:排序的实质是对字段的原始内容排序的过程，此过程中倒排索引无法发挥作用，需要用到正排索引。即：通过文档ID和字段得到原始内容。 ES提供2中实现方式： Fielddata。 默认禁用。 DocValues。 默认启用，除了text类型。 对比 Fielddata DocValues 创建时机 搜索时即时创建 创建索引时创建，和倒排索引创建时间一致 创建位置 JVM Heap 磁盘 优点 不占用额外磁盘空间 不占用Heap内存 缺点 文档较多时，同时创建会花费过多时间，占用过多Heap内存 减慢索引的速度，占用额外的磁盘空间 3.3.2 Fielddata的开启:Fielddata默认关闭，可通过如下api进行开启，且在后续使用时随时可以开启/关闭： 使用场景：一般在对分词做聚合分析的时候开启。 12345678910# 开启字段的fielddata设置PUT my_index/_mapping/doc&#123; &quot;properties&quot;:&#123; &quot;username&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;fielddata&quot;:true # 关键词 &#125; &#125;&#125; 3.3.3 Docvalues的关闭Docvalues默认开启，可在创建索引时关闭，且之后不能再打开，要打开只能做reindex操作。 使用场景：当明确知道，不会使用这个字段排序或者不做聚合分析的时候，可关闭doc_values，减少磁盘空间的占用。 1234567891011121314# 关闭字段的docvalues设置PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;username&quot;:&#123; &quot;type&quot;:&quot;keyword&quot;, &quot;doc_values&quot;:false # 关键词 &#125; &#125; &#125; &#125;&#125; 3.4 分页与遍历ES提供了三种方式来解决分页和遍历的问题： from/size，scroll，search_after。 3.4.1 from/size from：指明开始位置； size：指明获取总数 123456# 使用from——sizeGET my_index/_search&#123; &quot;from&quot;:1, # 从第2个开始搜索 &quot;size&quot;:2 # 获取2个长度&#125; 经典问题：深度分页。 问题：如何在数据分片存储的情况下， 获取前1000个文档？ 答案： 先从每个分片上获取前1000个文档， 然后由处理节点聚合所有分片的结果之后，再排序获取前1000个文档。 此时页数越深，处理的文档就越多，占用的内存就越大，耗时就越长。这就是深度分页问题。 为了尽量避免深度分页为题，ES通过设定index.max_result_window限定最多到10000条数据。 在设计分页系统时，有一个分页数十分重要： total_page=(total + page_size -1) / page_size 总分页数= (文档总数+认为设定的文档大小-1) / 人为设定的文档大小 但是在搜索引擎中的意义并不大，因为如果排在前面的结果都不能让用户满意，那么越往后，越不能让用户满意。 3.4.2 scroll 遍历文档集的API，以快照的方式来避免深度分页问题。 不能用来做实时搜索，因为数据不是实时的； 尽量不用复杂的sort条件，使用_doc最高效； 使用比较复杂。 步骤： 发起一个scroll search，会返回后续会用到的_scroll_id 调用scroll search的api，获取文档集合，不断迭代至返回hits数组为空时停止 之后不断返回新的_scroll_id，使用新的_scroll_id进行查询，直到返回数组为空。 当不断的进行迭代，会产生很多scroll，导致大量内存被占用，可以通过clear api进行删除 12345678910111213141516171819202122232425# 发起一个scroll searchGET my_index/_search?scroll=5m # 该快照的有效时间为5min&#123; &quot;size&quot;1 # 指明每次scroll返回的文档数&#125;## 调用scroll search 的api，获取文档集合POST _search/scroll&#123; &quot;scroll&quot;:&quot;5m&quot;, # 指明有效时间 &quot;scroll_id&quot;:&quot;xxxxxx&quot; # 上一步返回的_scroll_id&#125;## 使用clear api对scroll进行删除DELETE /_search/scroll&#123; &quot;scroll_id&quot;:[ &quot;xxxxxx&quot;, # _scroll_id &quot;xxxxxx&quot;, # _scroll_id ...... ]&#125;## 删除所有的scrollDELETE /_search/scroll/_all 3.4.3 search_after避免深度分页的性能问题，提供实时的下一页文档获取功能。 缺点：不能使用from参数，即：不能指定页数。且只能下一页，不能上一页。 使用步骤： 第一步：正常搜索，但是要指定sort值，并保证值唯一： 第二步：使用上一步最后一个文档的sort值进行查询： 1234567891011121314151617181920# 第一步，正常搜索GET my_index/_search&#123; &quot;size&quot;:1, &quot;sort&quot;:&#123; &quot;age&quot;:&quot;desc&quot;, &quot;_id&quot;:&quot;desc&quot; &#125;&#125;## 第二步，使用sort值进行查询GET my_index/_search&#123; &quot;size&quot;:1, &quot;search_after&quot;:[28,&quot;2&quot;],# 28,&quot;2&quot;，是上一次搜索返回的sort值 &quot;sort&quot;:&#123; &quot;age&quot;:&quot;desc&quot;, &quot;_id&quot;:&quot;desc&quot; &#125;&#125; 3.4.4 如何避免深度分页问题:这个问题目前连google都没能解决，所以只能最大程度避免，通过唯一排序值定位每次要处理的文档数都控制在size内： 应用场景： from/size:需实时获取顶部的部分文档，且需自由翻页（实时）； scroll:需全部文档，如：导出所有数据的功能（非实时）； search_after:需全部文档，不需自由翻页（实时）。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://chaooo.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://chaooo.github.io/tags/Kibana/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"http://chaooo.github.io/tags/ElasticStack/"},{"name":"LogStash","slug":"LogStash","permalink":"http://chaooo.github.io/tags/LogStash/"}]},{"title":"【ElasticStack】ElasticSearch入门","date":"2019-11-19T12:42:59.000Z","path":"article/20191119.html","text":"1. 概述1.1 ElasticStack特点 使用门槛低，开发周期短，上线快 性能好，查询快，实时展示结果 扩容方便，快速支撑增长迅猛的数据 1.2 ElasticStack各组件作用 **Beats**：数据采集 LogStash: 数据处理 ElasticSearch(核心引擎): 数据存储、查询和分析 Kibana: 数据探索与可视化分析 1.3 ElasticStack使用场景 搜索引擎、日志分析、指标分析 1.4 ElasticStack安装启动 ElasticSearch启动：解压到安装目录，启动bin/elasticsearch（默认端口:http:\\\\localhost:9200, 加参数-d后台启动） ElasticSearch集群： 123bin/elasticsearch -d bin/elasticsearch -Ehttp.port=8200 -Epath.data=node2 -dbin/elasticsearch -Ehttp.port=7200 -Epath.data=node3 -d Kibana启动：解压到安装目录，启动bin/kibana（默认端口:http:\\\\localhost:5601） 1.5 ElasticSearch常见术语 Document(文档)：用户存储在ES中的数据文档 Index(索引)：由具有相同字段的文档列表组成 field(字段)：包含具体数据 Node(节点)：一个ES的实例，构成clister的单元 Cluster(集群)：对外服务的一个/多个节点 1.6 Document介绍 常用数据类型：字符串、数值型、布尔型、日期型、二进制、范围类型 每个文档都有一个唯一ID标识。（可以自行指定，也可由ES自动生成） 元数据，用于标注文档的相关信息： _index：文档所在的索引名 _type：文档所在的类型名(后续的版本中type这个概念将会被移除，也不允许一个索引中有多个类型) _id：文档唯一标识 _source：文档的原始JSON数据，可从这获取每个字段的内容 _all：整合所有字段内容到该字段。（默认禁用） _version：文档字段版本号，标识被操作了几次 Index介绍： 索引中存储相同结构的文档，且每个index都有自己的Mapping定义，用于定义字段名和类型； 一个集群中可以有多个inex，类似于可以有多个table。 RESTful API两种交互方式： CURL命令行：curl -XPUT xxx Kibana DevTools————PUT xxx{ } Index API： 用户创建、删除、获取索引配置等。 创建索引： PUT /test_index #创建一个名为test_index的索引 查看索引： GET _cat/indices #查看所有的索引 删除索引： DELETE /test_index #删除名为test_index的索引 1.7 CRUD操作（交互基于Kibana DevTools） 创建文档 12345678910111213# 创建ID为1的DocumentPUT /test_index/doc/1&#123; &quot;username&quot;:&quot;alfred&quot;, &quot;age&quot;:&quot;24&quot;&#125;## 不指定ID创建Document(ID会自动生成)POST /test_index/doc&#123; &quot;username&quot;:&quot;buzhiding&quot;, &quot;age&quot;:&quot;1&quot;&#125; 查询文档： 1234567891011121314# 查看名为test_index的索引中id为1的文档GET /test_index/doc/1## 查询所有文档：# 查询名为test_index的索引中所有文档,用到endpoint：_search，默认返回符合的前10条# term和match的区别：term完全匹配，不进行分词器分析；match模糊匹配，进行分词器分析，包含即返回GET /test_index/doc/_search&#123; &quot;query&quot;:&#123; &quot;term&quot;:&#123; &quot;_id&quot;:&quot;1&quot; &#125; &#125;&#125; 批量操作文档： 12345678910111213141516171819202122232425# 批量创建文档，用到endpoint：_bulk# index和create的区别，如果文档存在时，使用create会报错，而index会覆盖POST _bulk&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;doc&quot;,&quot;_id&quot;:&quot;3&quot;&#125;&#125;&#123;&quot;username&quot;:&quot;alfred&quot;,&quot;age&quot;:&quot;20&quot;&#125;&#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;doc&quot;,&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;update&quot;:&#123;&quot;_id&quot;:&quot;2&quot;,&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;doc&quot;&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;age&quot;:&quot;30&quot;&#125;&#125;## 批量查询文档，使用endpoint:_mgetGET _mget&#123; &quot;doc&quot;:[ &#123; &quot;_index&quot;:&quot;test_index&quot;, &quot;_type&quot;:&quot;doc&quot;, &quot;_id&quot;:&quot;1&quot; &#125;, &#123; &quot;_index&quot;:&quot;test_index&quot;, &quot;_type&quot;:&quot;doc&quot;, &quot;_id&quot;:&quot;2&quot; &#125; ]&#125; 删除文档： 1234567891011121314151617# 根据搜索内容删除文档,使用endpoint:_delete_by_queryPOST /test_index/doc/_delete_by_query&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;buzhiding&quot; &#125; &#125;&#125;# # 删除整个test_index的索引中的文档,依然使用endpoint:_delete_by_queryPOST /test_index/doc/_delete_by_query&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125; 2. ElasticSearch倒排索引与分词2.1 倒排索引 正排索引和倒排索引 正排索引：文档ID —&gt; 文档内容 倒排索引：单词—&gt; 文档ID列表 倒排索引组成：（单词词典，倒排列表） 单词词典（Term Dictionary） 记录所有文档的单词，记录了单词到倒排列表的关联信息，一般使用B+Tree实现。 倒排列表（Posting List） 记录单词对应的文档集合，由倒排索引项Posting List组成。 倒排索引项： 文档ID：用于获取原始信息。 词频TF：记录该单词在该文档中的出现次数，用于计算相关性得分。 位置Position：记录单词在文档中的分词位置(多个)，用于词语搜索。 偏移Offset：记录单词在文档的开始和结束位置，用于高亮显示。 2.2 分词Analysis分词：将文本转换成一系列单词Term/Token的过程，也可称作文本分析，ES中叫作：Analysis。 一些概念： Token(词元)：全文搜索引擎会用某种算法对要建索引的文档进行分析， 从文档中提取出若干Token(词元)。 Tokenizer(分词器)：这些算法叫做Tokenizer(分词器) Token Filter(词元处理器)：这些Token会被进一步处理， 比如转成小写等， 这些处理算法被称为TokenFilter(词元处理器) Term(词)：被处理后的结果被称为Term(词) Character Filter(字符过滤器)：文本被Tokenizer处理前可能要做一些预处理， 比如去掉里面的HTML标记， 这些处理的算法被称为Character Filter(字符过滤器) Analyzer(分析器)：这整个的分析算法被称为Analyzer(分析器)，由Tokenizer(分词器)和Filter(过滤器)组成 ES有很多内置Analyzer,比如： standard：按单词边界划分、支持多语言、小写处理、移除大部分标点符号，支持停用词 whitespace：空格为分隔符 simple：按非字母划分、小写处理 stop：类似简单分词器，同时支持移除停用词(the、an、的、这等) keyword：不分词 pattern：通过正则表达式自定义分隔符，默认\\w+，即：非字词的符号作为分隔符 第三方analyzer插件：常用的中文分词器有： IK：实现中英文分词，支持多模式，可自定义词库，支持热更新分词词典。 jieba。python中流行，支持繁体分词、并行分词，可自定义词典、词性标记等。 ES提供了一个测试分词的API接口，使用endpoint：_analyze，不指定分词时，会使用默认的standard 123456789101112131415161718192021# 指定分词器进行分词测试POST _analyze&#123; &quot;analyzer&quot;:&quot;standard&quot;, &quot;text&quot;:&quot;hello world!&quot;&#125;# # 直接指定索引中字段：使用username字段的分词方式对text进行分词。POST test_index/_analyze&#123; &quot;field&quot;:&quot;username&quot;, &quot;text&quot;:&quot;hello world!&quot;&#125;# # 自定义分词器，自定义Tokenizer、filter、等进行分词：POST _analyze&#123; &quot;tokenizer&quot;:&quot;standard&quot;, &quot;filter&quot;:[&quot;lowercase&quot;], &quot;text&quot;:&quot;Hello World!&quot;&#125; 3. ElasticSearch的Mapping3.1 Mapping简介Mapping：类似于数据库中的表结构 主要作用如下： 定义Index下的Field Name； 定义Field的类型，如：数值型、字符串型、布尔型等； 定义倒排索引的相关配置，如：是否有索引，记录position等。 获取一个mapping，使用endpoint：_mapping，例如： GET /test_index/_mapping 3.2 自定义Mapping 使用mappings进行自定义mapping。 Mapping中的字段类型一旦设定之后，禁止直接修改。 因为Luence事先的倒排索引生成后不能修改。 如果一定要改，可以重新建立新的索引，然后对应修改mapping，之后将之前的数据进行reindex操作，导入新的文档。 自定义mapping时允许新增字段。通过dynamic参数进行控制字段的新增，dynamic有三种配置： true：默认配置，允许自动新增字段； false：不允许自动新增字段，文档可以正常写入，但不能进行查询等操作； strict：严格模式。文档不能写入，写入会报错。 123456789101112131415161718192021# 创建名为my_index的索引，并自定义mapping# 使用dynamic参数控制字段的新增PUT my_index&#123; &quot;mappings&quot;:&#123; # 关键字 &quot;doc&quot;:&#123; # 类型名 &quot;dynamic&quot;:false, # 设置为false，索引不允许新增字段 &quot;properties&quot;:&#123; # 字段名称及类型定义 &quot;title&quot;:&#123; &quot;type&quot;:&quot;text&quot; # 字段类型 &#125;, &quot;name&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125; &#125; &#125; &#125;&#125; 3.3 copy_to的使用将该字段的值复制到目标字段，类似于6.0版本之前的_all的作用。且不会出现在_source，一般只用来进行搜索。 1234567891011121314151617181920212223242526272829303132333435363738394041# copy_to的使用PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;first_name&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;copy_to&quot;:&quot;full_name&quot; &#125;, &quot;last_name&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;copy_to&quot;:&quot;full_name&quot; &#125;, &quot;full_name&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125; &#125; &#125; &#125;&#125;# # 向索引写入数据PUT my_index/doc/1&#123; &quot;first_name&quot;:&quot;John&quot;, &quot;last_name&quot;:&quot;Smith&quot;&#125;# # 查询索引my_index中full_name同时包含John 和 Smith的数据GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;full_name&quot;:&#123; &quot;query&quot;:&quot;John Smith&quot;, &quot;operator&quot;:&quot;and&quot; &#125; &#125; &#125;&#125; 3.4 index参数的使用控制当前字段是否为索引，默认true，当设置为false的时候，不进行记录，此时该字段不能被搜索 1234567891011121314# index参数的使用PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;cookie&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;index&quot;:false # 设置为false，该字段不能被搜索 &#125; &#125; &#125; &#125;&#125; 此时在进行数据写入和查询，不能进行该字段搜索。一般用来进行不想被查询的私密信息设置，如身份证号，电话号码等： 12345# 向使用了index参数的字段写入信息PUT my_index/doc/1&#123; &quot;cookie&quot;:&quot;name=alfred&quot;&#125; 3.5 index_options参数的使用：控制倒排索引记录的内容，有如下四种配置： docs：只记录文档ID freqs：记录文档ID和词频TF positions：记录文档ID、词频TF和分词位置 offsets：记录文档ID、词频TF、分词位置和偏移 其中：text类型默认的配置是positions，其他的比如integer等类型默认为docs，目的是为了节省空间。 1234567891011121314# index_options参数的使用PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;cookie&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;index_options&quot;:&quot;offsets&quot; # 记录文档ID、词频TF、分词位置和偏移 &#125; &#125; &#125; &#125;&#125; 3.6 null_value参数的使用：当字段遇到空值null时的处理策略。默认为null，即跳过。此时ES会忽略该值，可通过修改进行默认值的修改： 1234567891011121314# 使用null_value修改ES遇到null值时的默认返回值PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;cookie&quot;:&#123; &quot;type&quot;:&quot;keyword&quot;, &quot;null_value&quot;:&quot;NULL&quot; # 当遇到空值null的时候，返回一个字符串形式的NULL &#125; &#125; &#125; &#125;&#125; 3.7 Field字段的数据类型： 核心数据类型 字符串型：text(分词)，keyword(不分词) 数值型：long,integer,short,byte,double,float,half_float,scaled_float 日期类型：date 布尔类型：boolean 二进制类型：binary 范围类型：integer_range,float_range,long_range,double_range,date_range 复杂数据类型 数组类型：array 对象类型：object 嵌套类型：nested object 地理位置数据类型 点：geo-point 形状：geo-shape 专用类型 记录ip地址：ip 实现自动补全：completion 记录分词数：token_count 记录字符串hash值：murmur3 perclator join 多字段特性： ES允许对同一个字段采用不同的配置，如：分词。举例：对一个人名实现拼音搜索，只需要在人名字段中新增一个子字段pinyin即可。 3.8 ES的自动类型识别： Dynamic Mapping： ES可以自动识别文档字段类型，从而降低用户使用成本。 123456# ES的自动类型识别PUT my_index/doc/1&#123; &quot;username&quot;:&quot;alfred&quot;, # username字段自动识别为text类型 &quot;age&quot;:20 # age字段自动识别为long类型&#125; ES依靠JSON文档的字段类型实现自动识别字段类型： JSON类型 ElasticSearch类型 null 忽略 boolean boolean 浮点类型 float 整数类型 long object object array 由第一个非null的值的类型决定 String 匹配为日期，则为date类型(默认开启)匹配为数字，则为long类型/float类型(默认关闭)都未匹配，则设为text类型，并附带keyword子字段 验证ES的字段类型自动识别： 123456789101112# 验证ES的字段类型自动识别PUT my_index/doc/1&#123; &quot;username&quot;:&quot;alfred&quot;, # 字符串类型text &quot;age&quot;:20, # 整数long &quot;bitrh&quot;:&quot;1998-10-10&quot;, # 默认识别日期date &quot;married&quot;:false, # 布尔类型boolean &quot;year&quot;:&quot;18&quot; # 默认不识别数字text &quot;tags&quot;:[&quot;boy&quot;,&quot;fashion&quot;],# 数组中第一个不为null的元素为字符串类型，所以为text &quot;money&quot;:100.1 # 浮点类型float&#125;# 再对my_index进行mapping查询，就会获得每个字段的类型： 3.9 ES中日期类型和数字的自动识别：ES中可自行配置日期的格式，默认：[“strict_date_optional_time“,”yyyy/MM/dd HH:mm:ss Z|| yyyy/MM/dd z“] 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 1. 使用dynamic_date_formats自定义日期格式PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;dynamic_date_formats&quot;:[&quot;MM/dd/yyyy&quot;] &#125; &#125;&#125;# 写入符合自定义格式的日期数据，可识别为date类型PUT my_index/doc/1&#123; &quot;create_time&quot;:&quot;01/01/2019&quot; # create_time字段识别为date类型&#125;# # 2. 使用date_detection可以关闭自动识别日期格式：PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;date_detection&quot;:false &#125; &#125;&#125;# PUT my_index/doc/1&#123; &quot;create_time&quot;:&quot;01/01/2019&quot; # create_time字段是text类型&#125;# # ES中可配置数字是否识别，默认关闭：PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;numeric_detection&quot;:true # 开启数字自动识别 &#125; &#125;&#125;# 写入数字数据，ES可以自动识别其类型PUT mu_index/doc/1&#123; &quot;year&quot;:&quot;18&quot;, # year字段自动识别为long类型 &quot;money&quot;:&quot;100.1&quot; # money字段自动识别为float类型&#125; 3.10 ES中根据自动识别的数据类型，动态生成字符类型例: 字符串类型都设为keyword类型（不分词） 以message开头的字段都设为text类型（分词） 以long_开头的字段都设为long类型 自动匹配为double的类型都设为float类型。（为了节省空间） 12345678910111213141516171819# ES根据自动识别的数据类型、字段名等动态设定字符类型PUT test_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;dynamic_template&quot;:[ &#123; &quot;strings&quot;:&#123; # 匹配到所有的字符串类型，全部设为keyword类型 &quot;match_mapping_type&quot;:&quot;string&quot;, &quot;mapping&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; 匹配规则的参数： match_mapping_type：匹配ES自动识别的字段类型，如boolean、long、string等 match、unmatch：匹配字段名，比如”match”:”message*” ===&gt;以message开头的数据 path_match、path_unmatch：匹配路径 3.11 自定义mapping的操作步骤 写入一条文档到ES的临时索引中，获取(复制)ES自动生成的mapping 修改获得的mapping，并在其中自定义相关配置 使用修改后的mapping创建实际所需索引。 4. ElasticSearch的Search API在ES中，为了实现对存储的数据进行查询分析，使用endpoint：**_search**。 实现对所有索引的泛查询：GET /_search 实现对一个索引的单独查询：GET /my_index/_search 实现对多个索引的指定查询：GET /my_index1,my_index2/_search 实现对符合指定要求的索引进行查询：GET /my_*/_search 在进行查询的时候，主要有两种方式：(URI Search，Request Body Search) **URI Search**：操作简单，直接通过命令行方便测试，但仅包含部分查询语法； 如：GET /my_index/_search?q=username:alfred **Request Body Search**：ES提供的完备查询语法，使用Query DSL(Domain Specific Language)进行查询 123456789# 如：Request Body Search方式进行查询GET /my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;&#125; 4.1 URI Search 通过url query参数实现搜索，常用参数有： **q**：指定查询的语句，使用query string syntax语法 **df**：q中不指定字段时默认查询的字段（在不指定的时候默认查询所有字段） **sort**：排序 **timeout**：指定超时时间，默认不超时 **from,size**：用于分页 举例： GET my_index/_search?q=alfred&amp;df=username&amp;sort=age:asc&amp;from=4&amp;size=10&amp;timeout=1s 解释：查询索引my_index中username字段中包含alfred的文档，结果按age字段升序排列，返回第5-14个文档，若超过1s未结束，则以超时结束。 query string syntax语法 前置内容：term:单词，phrase:词语。 单词与词语语法： 单词：alfred way等价于alfred OR way 词语：&quot;alfred way&quot;语句查询，要求先后顺序 泛查询：不指定字段，会在所有字段中去匹配其单词 指定字段查询：指定字段，在指定字段中匹配单词 Group分组设定，使用括号指定匹配的规则 举例：GET my_index/_search?q=username:(alfred OR way)AND lee 4.1.1 URI Search API 泛查询： 1234GET my_index/_search?q=alfred&#123; &quot;profile&quot;:true #使用profile参数，可以明确地看到ES如何执行的查询条件&#125; 指定字段查询： 123456789101112# a.查询字段username中包含alfred的文档GET my_index/_search?q=username:alfred## b.查询字段username中包含alfred或way的文档GET my_index/_search?q=username:alfred way## c.查询字段username为&quot;alfred way&quot;的文档GET my_index/_search?q=username:&quot;alfred way&quot;## d.分组后，查询字段username中包含alfred，包含way的文档GET my_index/_search?q=username:(alfred way)# 这个和b的结果一样，但是区别在于使用分组之后，不进行泛查询。 布尔操作符AND(&amp;&amp;)、OR(||)、NOT(!)、+(must)、-(must_not) 1234567891011# 查询索引my_index中username包含alfred但是不包含way的文档GET my_index/_search?q=username:(alfred NOT way)## 查询索引my_index中一定包含lee，一定不含alfred，可能有way的文档GET my_index/_search?q=username:(way +lee -alfred)# 或写成GET my_index/_search?q=username:((lee &amp;&amp; !alfred) || (way &amp;&amp; lee &amp;&amp; !alfred))## 注意：url中，+(加号)会被解析成空格，所以要用 %2B ：# 查询索引my_index中一定包含lee，一定不包含alfred，可能包含way的文档GET my_index/_search?q=username:(way %2Blee -alfred) 范围查询（支持数值和日期） 区间写法：闭区间使用[]，开区间使用&#123;&#125; age:[1 TO 10] # 1&lt;= age &lt;=10 age:[1 TO 10&#125; # 1&lt;= age &lt;10 age:[1 TO ] # age &gt;=1 age:[* TO 10] # age &lt;=10 算数符号写法： age:&gt;=1 age:(&gt;=1 &amp;&amp; &lt;= 10) / age:(+ &gt;= 1 + &lt;= 10) 还可以对日期进行范围查询，注意：年/月是从1月1号/1号开始算的： 12345678# a.查询索引my_index中username字段包含alfred_或_年龄大于20的文档GET my_index/_search?q=username:alfred age&gt;20# # b.查询索引my_index中username字段包含alfred_且_年龄大于20的文档GET my_index/_search?q=username:alfred AND age&gt;20# # 查询索引my_index中birth字段在1985和1990之间的文档GET my_index/_search?q=birth:(&gt;1985 AND &lt; 1990) 通配符查询 ?代表一个字符，*代表0个或多个字符，如：name:a?lfred或name:a*d或name:alfred* 注意：通配符匹配的执行效率较低，且占用内存较多，不建议使用，如果没有特殊要求，也不要将?或者*放在最前面，因为意味着要匹配所有文档，可能会造成OOM。 正则表达式/模糊匹配/近似度查询 正则表达式：举例：/[a]?l.*/ 模糊匹配：fuzzy query 近似度查询：proximity search 12345# 模糊匹配。匹配与alfred差一个字符的词，比如：alfreds、alfret等GET my_index/_search?q=username:alfred~1## 近似度查询，查询字段username和&quot;alfred way&quot;差n个单词的文档GET my_index/_search?q=username:&quot;alfred way&quot; ~5 使用场景常见于用户输入词的纠错中。 4.2 Request Body SearchES自带的完备查询语句，将查询语句通过http request body发送到ES，主要参数有： query：符合Query DSL语法的查询条件 from，size timeout sort Query DSL语法： 基于JSON定义的查询语言，主要包含两个类型： 字段类查询————如：term，match，range等。只针对一个字段进行查询 复合查询————如：bool查询等。包含一个/多个字段类查询/符合查询语句 4.2.1 字段类查询-全文匹配针对text类型的字段进行全文检索，会对查询语句进行“先分词再查询”处理，如：match、match_phrase等 4.2.1.1 match query 对字段进行全文检索(最基本和最常用的查询类型)，举例： 12345678GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; # 关键词 &quot;username&quot;:&quot;alfred way&quot; # 字段名和查询语句 &#125; &#125;&#125; 从结果，可以返回匹配文件总数，返回文档列表，_score相关性得分等。一般的执行流程为： 1.对查询语句分词==&gt;2.根据字段的倒排索引列表，进行匹配算分==&gt;3.汇总得分==&gt;4.根据得分排序，返回匹配文档 使用operator参数，可以控制单词间关系，有and/or： 12345678910# 使用operator参数控制单词间关系GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred way&quot;, &quot;operator&quot;:&quot;and&quot; # and，同时包含alfred和way &#125; &#125;&#125; 使用minimum_should_match参数控制需匹配的单词数 12345678910# 使用minimum_should_match参数控制需匹配的单词数GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred way&quot;, &quot;minimum_should_match&quot;:&quot;2&quot; &#125; &#125;&#125; 4.2.1.2 相关性算分，其本质就是一个排序问题 计算文档与待查询语句之间的相关度，一般有四个重要概念： Term Frequency 词频(正相关) Document Frequency 文档频率(负相关) Inverse Term Frequency 逆文本频率(正相关) Field-length Norm 文档长度(负相关) 目前ES有两个相关性算分的模型： TF/IDF模型：经典模型。 BM25模型：5.x版本后的默认模型，是对TF/IDF的优化模型。 TF/IDF模型：在使用kibana进行查询时，使用explain参数，可以查看具体的计算方法。 12345678910# 使用explain参数，可以查看具体的相关性的得分是如何计算的GET my_index/_search&#123; &quot;explain&quot;:true, # 设置为true &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;&#125; 注意：ES计算相关性得分是根据shard进行的，即分片的分数计算相互独立，所以在使用的时候要注意分片数，可以通过设定分片数为1来避免这个问题，主要是为了观察，不代表之后所有的分片全都设为1。一般放在创建索引后，未加数据之前。 1234567# 设定shards数量为1PUT my_index&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot;:&quot;1&quot; &#125;&#125; BM25模型。5.x版本后的默认模型，是对TF/IDF的优化模型。 best match，25指：迭代了25次才计算。BM25的使用，降低了TF/IDF中因为TF过大导致的负面影响，在BM25中，一个单词的TF一直增长，到一定程度就趋于0变化。 4.2.1.3 match phrase query对字段做全文检索，有顺序要求。 使用match——phrase查询词语 12345678GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; # 关键词 &quot;job&quot;:&quot;java engineer&quot; &#125; &#125;&#125; 通过使用slop参数，可以控制单词间间隔： 1234567891011GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;job&quot;:&#123; &quot;query&quot;:&quot;java engineer&quot;, &quot;slop&quot;:&quot;1&quot; # 关键词，设定单词间隔 &#125; &#125; &#125;&#125; 4.2.1.4 query string query类似于URI Search中的q参数查询，举例： 使用query_string查询 1234567891011121314151617181920GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;query_string&quot;:&#123; &quot;default_field&quot;:&quot;username&quot;, &quot;query&quot;:&#123;alfred AND way&quot; &#125; &#125;&#125;##* 或 */GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;query_string&quot;:&#123; &quot;fileds&quot;:[&quot;username&quot;,&quot;job&quot;], &quot;query&quot;:&quot;alfred OR (java AND ruby)&quot; &#125; &#125;&#125; 4.2.1.5 simple query string query类似于query string，但会忽略错误的查询语法，且仅支持部分查询语句。使用+，|，-分别代替AND，OR，NOT。 使用simple query string query123456789GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;simple_query_string&quot;:&#123; &quot;fields&quot;:[username], &quot;query&quot;:&quot;alfred +way&quot; #等价于 &quot;query&quot;:&quot;alfred AND way&quot; &#125; &#125;&#125; 4.2.2 字段类查询-单词匹配4.2.2.1 term/terms query将待查询语句作为整个单词进行查询，不做分词处理，举例： 使用term进行单查询 12345678GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;term&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;&#125; 使用terms进行多查询 12345678GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;username&quot;:[&quot;alfred&quot;,&quot;way&quot;] &#125; &#125;&#125; 此时如果直接使用alfred way作为username查询条件，是不会返回任何文档的。因为在username的倒排索引列表中，存在&quot;alfred&quot;和&quot;way&quot;的索引，但是不存在&quot;alfred way&quot;的索引。 4.2.2.2 range query 范围查询，主要针对数值类型和日期类型。 gt: greater than 大于 gte: greate than or equal to 大于等于 lt: less than 小于 lte: less than or equal to 小于等于 对数值的查询 123456789101112# range query对数值的查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;range&quot;:&#123; &quot;age&quot;:&#123; &quot;gte&quot;:10, &quot;lte&quot;:20 &#125; &#125; &#125;&#125; 对日期的查询 123456789101112# range query对日期的查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;range&quot;:&#123; &quot;birth&quot;:&#123; &quot;lte&quot;:&quot;1988-01-01&quot; # 或者使用&quot;lte&quot;:&quot;now-30y&quot;,这种Date Math类型 &#125; &#125; &#125;&#125; Date Math类型：针对日期提供的一种更友好的计算方式。当前时间用now代替，具体时间的引用，需要使用||间隔。年、月、日、时、分、秒跟date一致：y、M、w、d、h、m、s。举例： 12345# 假设当前时间为2019-01-02 12:00:00now+1h =&gt; 2019-01-02 13:00:00now-1h =&gt; 2019-01-02 11:00:00now-1h/d =&gt; 2019-01-02 00:00:002019-01-01||+1M/d =&gt; 2019-02-01 00:00:00 4.2.3 复合查询包含一个/多个字段类查询/符合查询语句 4.2.3.1 constant_score query constant_score query: 将内部的查询结果文档得分全部设定为1或boost的值。返回的相关性得分全部为1或boost 1234567891011# 使用constant_score queryGET my_index/_Search&#123; &quot;query&quot;:&#123; &quot;constant_score&quot;:&#123; #关键词 &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125; &#125;&#125; 4.2.3.2 bool querybool query: 由一个/多个布尔子句组成，主要包含以下四个： filter: 只过滤符合条件的文档，不计算相关性得分，返回的相关性得分全部为0； ES会对filter进行智能缓存，因此执行效率较高，在做简单匹配查询且不考虑得分的时候没推荐使用filter代替query 12345678910111213# 使用filter查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; # 关键词 &quot;filter&quot;:[ &quot;term&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; ] &#125; &#125;&#125; must: 文档必须符合must中的所有条件，影响相关性得分； 1234567891011121314151617181920# 使用must进行查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;, &#123; &quot;match&quot;:&#123; &quot;job&quot;:&quot;specialist&quot; &#125; &#125; ] &#125; &#125;&#125; must_not: 文档必须排除must_not中的所有条件； 12345678910111213141516171819202122# 使用must_not进行查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match&quot;:&#123; &quot;job&quot;:&quot;java&quot; &#125; &#125; ], &quot;must_not&quot;:[ &#123; &quot;match&quot;:&#123; &quot;job&quot;:&quot;ruby&quot; &#125; &#125; ] &#125; &#125;&#125; should: 文档可以符合should中的条件，影响相关性得分，分为两种情况：同时配合minimum_should_match控制满足调价你的个数/百分比。 bool查询中只有should，不包含must的情况 bool查询中既有should，又包含must的情况，文档不必满足should中的条件，但是如果满足的话则会增加相关性得分。 123456789101112131415161718192021222324252627282930313233343536373839# bool查询中只有should的情况GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;should&quot;:[ &#123; &quot;term&quot;:&#123;&quot;job&quot;:&quot;java&quot;&#125; # 条件1 &#125;, &#123; &quot;term&quot;:&#123;&quot;job&quot;:&quot;ruby&quot;&#125; # 条件3 &#125; &#123; &quot;term&quot;:&#123;&quot;job&quot;:&quot;specialist&quot;&#125; # 条件3 &#125; ], &quot;minimum_should_match&quot;:2 # 至少需要满足两个条件 &#125; &#125;&#125;# # bool查询中同时包含should和mustGET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;should&quot;:[ # 同时包含should &#123; &quot;term&quot;:&#123;&quot;job&quot;:&quot;ruby&quot;&#125; &#125; ], &quot;must&quot;:[ # 同时包含must &#123; &quot;term&quot;:&#123;&quot;usernmae&quot;:&quot;alfred&quot;&#125; &#125; ] &#125; &#125;&#125; 当一个查询语句位于query或filter上下文的时候，ES的执行结果也不同。 - - - query 查找和查询语句最匹配的文档，并对所有文档计算相关性得分 querybool中的：must/should filter 查找和查询语句最匹配的文档 bool中的：filter/must_notconstant_score中的：filter 12345678910111213141516171819202122232425262728# query和filter上下文GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ # query上下文 &#123; &quot;term&quot;:&#123;&quot;title&quot;:&quot;Search&quot;&#125; &#125;, &#123; &quot;term&quot;:&#123;&quot;content&quot;:&quot;ElasticSearch&quot;&#125; &#125; ], &quot;filter&quot;:[ # filter上下文 &#123; &quot;term&quot;:&#123;&quot;status&quot;:&quot;published&quot;&#125; &#125;, &#123; &quot;range&quot;:&#123; &quot;publish_date&quot;:&#123; &quot;gte&quot;:&quot;2015-01-01&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; 4.2.3.3 count APIcount API: 获取符合条件的文档书，使用endpoint：_count。 123456789# 使用_count获取符合条件的文档数GET my_index/_count # 关键词&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;&#125; 4.2.3.4 Source FilteringSource Filtering: 过滤返回结果中的_source中的字段，主要由以下两种方式： GET my_index/_search?_source=username #url参数 使用Request Body Search： 123456789101112131415161718# 不返回_sourceGET my_index/_search&#123; &quot;_source&quot;:false&#125;# 返回_source部分字段GET my_index/_search&#123; &quot;_source&quot;:[&quot;username&quot;,&quot;age&quot;]&#125;# 通配符匹配返回_source部分字段GET my_index/_search&#123; &quot;_source&quot;:&#123; &quot;includes&quot;:&quot;*I*&quot;, &quot;encludes&quot;:&quot;birth&quot; &#125;&#125;","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://chaooo.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://chaooo.github.io/tags/Kibana/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"http://chaooo.github.io/tags/ElasticStack/"},{"name":"LogStash","slug":"LogStash","permalink":"http://chaooo.github.io/tags/LogStash/"}]},{"title":"【并发编程】NIO、Netty及websocket实现","date":"2019-10-20T15:34:31.000Z","path":"article/20191020.html","text":"1. BIO/NIO/AIO演变Java IO 方式有很多种，基于不同的IO抽象模型和交互方式，可以进行简单区分。 IO类型 模型 客户端:线程数 API使用难度 调试难度 可靠性 吞吐量 BIO 流，同步阻塞 1:1 简单 简单 很差 非常低 伪异步IO 同步阻塞 M:N 简单 简单 较差 中等 NIO 同步非阻塞 M:1 复杂 复杂 较高 高 AIO 异步非阻塞 M:0,被动回调 复杂 复杂 高 高 区分同步(synchronous)或异步(asynchronous) 同步是一种可靠的有序运行机制，当我们进行同步操作时，后续的任务是等待当前调用返回，才会进行下一步； 异步则相反，其他任务不需要等待当前调用返回，通常依靠事件、回调等机制来实现任务间次序关系。 区分阻塞(blocking)与非阻塞(non-blocking) 在进行阻塞操作时，当前线程会处于阻塞状态，无法从事其他任务，只有当条件就绪才能继续，比如 ServerSocket 新连接建立完毕，或数据读取、写入操作完成； 非阻塞则是不管 IO 操作是否结束，直接返回，相应操作在后台继续处理 传统的java.io包，它基于流模型实现，同步阻塞的交互方式，如File抽象、输入输出流等。好处是代码简单、直观，缺点是IO效率和扩展性局限性 很多时候，也把java.net下面提供的部分网络API，比如Socket、ServerSocket、HttpURLConnection也归类到同步阻塞IO类库，因为网络通信同样是IO行为。 伪异步IO：后端通过维护一个消息队列和N个活跃线程, 通过一个线程池来处理多个客户端的请求接入，通过线程池，可以灵活地调配线程资源，设置线程的最大值，防止由于海量并发接入而导致的线程耗尽和宕机。 JDK4引入了NIO框架(java.nio)，提供了Channel、Selector、Buffer等新的抽象，可以构建多路复用的、同步非阻塞IO程序，同时提供了更接近操作系统底层的高性能数据操作方式。 JDK7中，NIO有了进一步的改进，引入了异步非阻塞IO方式，也叫AIO(Asynchronous IO)。异步IO操作基于事件和回调机制，可以简单理解为，应用操作直接返回，而不会阻塞在那里，当后台处理完成，操作系统会通知相应线程进行后续工作。 1.1 NIO的主要组成部分： Buffer(缓冲区)，高效的数据容器，除了布尔类型，所有原始数据类型都有相应的Buffer实现。 Buffer最常见的类型是ByteBuffer，另外还有CharBuffer，ShortBuffer，IntBuffer，LongBuffer，FloatBuffer，DoubleBuffer。 Channel(通道)，是NIO中被用来支持批量式IO操作的一种抽象。 和流不同，通道是双向的。数据可以从Channel读到Buffer中，也可以从Buffer 写到Channel中。 Selector(多路复用器)，是NIO实现多路复用的基础，它允许单线程处理多个Channel。 Selector是基于底层操作系统机制，不同模式、不同版本都存在区别。 要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。 1.2 NIO多路复用的过程 通过Selector.open()创建一个Selector，作为类似调度员的角色。 创建一个ServerSocketChannel，并绑定监听端口，设置为非阻塞模式 将Channel向Selector注册，通过指定SelectionKey.OP_ACCEPT，告诉调度员，它关注的是新的连接请求。 Selector循环阻塞在select操作，当有Channel发生接入请求，就会被唤醒。 调用selectedKeys方法获取就绪channel集合 通过SocketChannel和Buffer进行数据操作。 1.3 AIO AIO也叫NIO2.0 是一种非阻塞异步的通信模式。在NIO的基础上引入了新的异步通道的概念，并提供了异步文件通道和异步套接字通道的实现。 没有采用NIO的多路复用器，而是使用异步通道的概念。 其read，write方法的返回类型都是Future对象。而Future模型是异步的，其核心思想是：去主函数等待时间。 AIO模型中通过AsynchronousSocketChannel和AsynchronousServerSocketChannel完成套接字通道的实现。非阻塞，异步。 2. Netty框架Netty是一个高性能事件驱动，异步非阻塞的IO开源框架，由Jboss提供，用于建立Tcp等底层的链接，基于Netty可以建立高性能的Http服务器，快速开发高性能、高可靠的网络服务器和客户端程序。支持Http、websocket，tcp，udp等协议。 Netty使用场景：高性能领域（游戏，大数据分布式计算等）、多线程并发领域（多路复用模型，多线程模型，主从多线程模型）、异步通信领域 Netty 是一个吸收了多种协议（包括FTP、SMTP、HTTP等各种二进制文本协议）的实现经验，在保证易于开发的同时还保证了其应用的性能，稳定性和伸缩性。 2.1 Netty的核心概念 ServerBootstrap，服务器端程序的入口，这是 Netty 为简化网络程序配置和关闭等生命周期管理，所引入的 Bootstrapping 机制。我们通常要做的创建 Channel、绑定端口、注册 Handler 等，都可以通过这个统一的入口，以Fluent API等形式完成，相对简化了 API 使用。与之相对应， Bootstrap则是 Client 端的通常入口。 Channel，作为一个基于 NIO 的扩展框架，Channel 和 Selector 等概念仍然是 Netty 的基础组件，但是针对应用开发具体需求，提供了相对易用的抽象。 EventLoop，这是 Netty 处理事件的核心机制。例子中使用了 EventLoopGroup。我们在 NIO 中通常要做的几件事情，如注册感兴趣的事件、调度相应的 Handler 等，都是 EventLoop 负责。 ChannelFuture，这是 Netty 实现异步 IO 的基础之一，保证了同一个 Channel 操作的调用顺序。Netty 扩展了 Java 标准的 Future，提供了针对自己场景的特有Future定义。 ChannelHandler，这是应用开发者放置业务逻辑的主要地方，也是我上面提到的“Separation Of Concerns”原则的体现。 ChannelPipeline，它是 ChannelHandler 链条的容器，每个 Channel 在创建后，自动被分配一个 ChannelPipeline。在上面的示例中，我们通过 ServerBootstrap 注册了 ChannelInitializer，并且实现了 initChannel 方法，而在该方法中则承担了向 ChannelPipleline 安装其他 Handler 的任务。 2.2 对比 Java 标准 NIO 类库，Netty是如何实现更高性能的？单独从性能角度，Netty 在基础的 NIO 等类库之上进行了很多改进，例如： 更加优雅的 Reactor 模式实现、灵活的线程模型、利用 EventLoop 等创新性的机制，可以非常高效地管理成百上千的 Channel。 充分利用了 Java 的 Zero-Copy 机制，并且从多种角度，“斤斤计较”般的降低内存分配和回收的开销。例如，使用池化的 Direct Buffer 等技术，在提高 IO 性能的同时，减少了对象的创建和销毁；利用反射等技术直接操纵 SelectionKey，使用数组而不是 Java 容器等。 使用更多本地代码。例如，直接利用 JNI 调用 Open SSL 等方式，获得比 Java 内建 SSL 引擎更好的性能。 在通信协议、序列化等其他角度的优化。 Netty 的设计强调了 “Separation Of Concerns”，通过精巧设计的事件机制，将业务逻辑和无关技术逻辑进行隔离，并通过各种方便的抽象，一定程度上填补了了基础平台和业务开发之间的鸿沟，更有利于在应用开发中普及业界的最佳实践。另外，Netty &gt; java.nio + java. net！ 除了核心的事件机制等，Netty 还额外提供了很多功能，例如： 从网络协议的角度，Netty 除了支持传输层的 UDP、TCP、SCTP协议，也支持 HTTP(s)、WebSocket 等多种应用层协议，它并不是单一协议的 API。 在应用中，需要将数据从 Java 对象转换成为各种应用协议的数据格式，或者进行反向的转换，Netty 为此提供了一系列扩展的编解码框架，与应用开发场景无缝衔接，并且性能良好。 它扩展了 Java NIO Buffer，提供了自己的 ByteBuf 实现，并且深度支持 Direct Buffer 等技术，甚至 hack 了 Java 内部对 Direct Buffer 的分配和销毁等。同时，Netty 也提供了更加完善的 Scatter/Gather 机制实现。 3. 基于Netty搭建简单的Http服务 环境准备：jdk1.8、Netty4.1.43.Final 代码编写：MyChannelInitializer.java、MyClientHandler.java、NettyServer.java MyChannelInitializer.java：添加了Http的处理协议 1234567891011public class MyChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel channel) &#123; // 数据解码操作 channel.pipeline().addLast(new HttpResponseEncoder()); // 数据编码操作 channel.pipeline().addLast(new HttpRequestDecoder()); // 在管道中添加我们自己的接收数据实现方法 channel.pipeline().addLast(new MyServerHandler()); &#125;&#125; MyServerHandler.java 123456789101112131415161718192021222324252627282930313233343536373839public class MyServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof HttpRequest) &#123; DefaultHttpRequest request = (DefaultHttpRequest) msg; System.out.println(&quot;URI:&quot; + request.getUri()); System.err.println(msg); &#125; if (msg instanceof HttpContent) &#123; LastHttpContent httpContent = (LastHttpContent) msg; ByteBuf byteData = httpContent.content(); if (!(byteData instanceof EmptyByteBuf)) &#123; //接收msg消息 byte[] msgByte = new byte[byteData.readableBytes()]; byteData.readBytes(msgByte); System.out.println(new String(msgByte, StandardCharsets.UTF_8)); &#125; &#125; String sendMsg = &quot;不平凡的岁月终究来自你每日不停歇的刻苦拼搏，每一次真正成长都因看清脚下路而抉择出的生活。&quot;; FullHttpResponse response = new DefaultFullHttpResponse( HttpVersion.HTTP_1_1, HttpResponseStatus.OK, Unpooled.wrappedBuffer(sendMsg.getBytes(StandardCharsets.UTF_8))); response.headers().set(HttpHeaderNames.CONTENT_TYPE, &quot;text/plain;charset=UTF-8&quot;); response.headers().set(HttpHeaderNames.CONTENT_LENGTH, response.content().readableBytes()); response.headers().set(HttpHeaderNames.CONNECTION, HttpHeaderValues.KEEP_ALIVE); ctx.write(response); ctx.flush(); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); cause.printStackTrace(); &#125;&#125; NettyServer.java 1234567891011121314151617181920212223242526public class NettyServer &#123; public static void main(String[] args) &#123; new NettyServer().bing(7397); &#125; private void bing(int port) &#123; //配置服务端NIO线程组 EventLoopGroup parentGroup = new NioEventLoopGroup(); EventLoopGroup childGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(parentGroup, childGroup) .channel(NioServerSocketChannel.class)//非阻塞模式 .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true) .childHandler(new MyChannelInitializer()); ChannelFuture f = b.bind(port).sync(); System.out.println(&quot;http-netty server start done. &quot;); f.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; childGroup.shutdownGracefully(); parentGroup.shutdownGracefully(); &#125; &#125;&#125; 启动NettyServer，Postman访问http://localhost:7397并设置参数 4. WebSocketWebSocket是一种H5协议规范，通过握手机制客户端与服务器之间就能够建立一个类似Tcp的连接，从而方便客户端与服务器之间的通信。 它是一种解决客户端与服务端实时通信而产生的技术：WebSocket本质是一种基于TCP协议，先通过Http/Https发一个特殊的Http请求进行握手，握手后会创建一个用于交换数据的TCP链接，之后客户端和服务端使用该TCP链接进行实时通信。当WebSocket的客户端和服务端握手后 建立通信后，就不再需要之前的http请求参与。 4.1 WebSocket的优点： 节省通信开销，之前WebServer实现通信，都使用轮询，需要不停的向服务器发送请求，而HttpRequest的handler很长，请求包含真正的数据可能很小，会占用很多额外的带宽和服务器资源。 建立连接后，服务器可主动传数据给客户端，客户端也可以随意向服务端传数据。交换数据时所携带的头信息很小。浏览器（客户端）和服务器只需要做一个握手的动作。 实时通信：WebSocket不仅限于Ajax方式通信。ajax方式需要浏览器发起请求。而WebSocket技术 服务端和客户端可以彼此相互推送信息，从而实现实时通信。 4.2 WebSocket建立连接过程：客户端发起握手请求 ---&gt; 服务端响应请求 ---&gt; 建立连接 详细流程：建立一个WebSocket连接，客户端或浏览器首先向服务器发送一个特殊的Http请求(携带一些附加头信息)Upgrade:websocket，服务端解析附加头信息，产生应答消息，然后响应给客户端，之后客户端就与服务端建立响应的链接。 4.3 WebSocket生命周期： 打开事件：端点上建立新链接时，该事件是先于其他任何事件发生之前。该事件发生会产生三部分信息。 创建WebSocket Session对象：用于表示已经建立好的链接 配置对象：包含配置端点的信息。 一组路径参数，用于打开节点握手时，WebSocket端入栈匹配的URI 消息事件：主要是接收WebSocket对话中，另一端发送的消息。链接上的消息将会有三种形式抵达客户端。 文本消息 用String处理 二进制消息 用byteBuffer或者byte[]处理 pong消息 用Java WebSocket API中的pong.message接口的实例来处理 错误事件：WebSocket链接或者端点发生错误时产生。可以处理入栈消息时发生的各种异常。入栈消息可能产生的三种异常。 WebSocket建立链接时发生错误：SessionException类型 WebSocket试图将入栈消息解码成开发人员使用的对象时 EncodeException类型 WebSocket端点的其他方法运行时产生的错误，WebSocket实现将记录端点操作过程中产生的任何异常 关闭事件：WebSocket链接端点关闭，做一些清理工作，可以由参与连接的任意一个端点发出。 4.4 WebSocket如何关闭链接：流程：当服务器被指示关闭WebSocket链接时，服务端会发起一个TCP Close操作， 客户端应该等待服务器的TCP Close 关闭WebSocket连接，端点需关闭底层TCP连接。 底层TCP连接，在大多数正常情况下，应该首先被服务器关闭，服务器持有TIME_WAIT状态（因为这会防止它在2个报文最大生存时间（2MLS）内重新打开连接，然而当一个新的带有更高的seq number的SYN时没有对应的服务器影响TIME_WAIT连接被立即重新打开）。 在异常情况下（例如在一个合理的时间量后没有接收到服务器的TCP Close）,客户端可以发起TCP Close。 5. 基于Netty搭建WebSocket多人聊天室 使用SpringBoot+Netty+WebSocket搭建功能。 使用Netty提供的HttpServerCodec、HttpObjectAggregator、ChunkedWriteHandler进行编码解码处理。 环境准备：jdk1.8、Netty4.1.43.Final、spring-boot-starter-web 目录结构 12345678910111213141516171819202122└── src.main ├── java │ └── top.chaooo.hellonetty │ ├── domain │ │ ├── ClientMsgProtocol.java │ │ └── ServerMsgProtocol.java │ ├── server │ │ ├── MyChannelInitializer.java │ │ ├── MyServerHandler.java │ │ └── NettyServer.java │ ├── util │ │ ├── ChannelHandler.java │ │ └── MsgUtil.java │ ├── controller │ │ └── NettyController.java │ └── NettyApplication.java └── resources ├── static(js,img) ├── templates │ └── index.html └── application.yml resources/application.yml：基础配置信息，包括了；应用端口、netty服务端端口等 12345678910111213server: port: 8080netty: host: 127.0.0.1 port: 7397spring: thymeleaf: mode: HTML5 encoding: UTF-8 content-type: text&#x2F;html cache: false server/MyChannelInitializer.java：websocket处理协议 12345678910public class MyChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel channel) &#123; channel.pipeline().addLast(&quot;http-codec&quot;, new HttpServerCodec()); channel.pipeline().addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)); channel.pipeline().addLast(&quot;http-chunked&quot;, new ChunkedWriteHandler()); // 在管道中添加我们自己的接收数据实现方法 channel.pipeline().addLast(new MyServerHandler()); &#125;&#125; server/MyServerHandler.java：处理websocket消息信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class MyServerHandler extends ChannelInboundHandlerAdapter &#123; private Logger logger = LoggerFactory.getLogger(MyServerHandler.class); private WebSocketServerHandshaker handshaker; /** * 当客户端主动链接服务端的链接后，这个通道就是活跃的了。 * 也就是客户端与服务端建立了通信通道并且可以传输数据 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; SocketChannel channel = (SocketChannel) ctx.channel(); logger.info(&quot;链接报告开始&quot;); logger.info(&quot;链接报告信息：有一客户端链接到本服务端&quot;); logger.info(&quot;链接报告IP:&#123;&#125;&quot;, channel.localAddress().getHostString()); logger.info(&quot;链接报告Port:&#123;&#125;&quot;, channel.localAddress().getPort()); logger.info(&quot;链接报告完毕&quot;); ChannelUtil.channelGroup.add(ctx.channel()); &#125; /** * 当客户端主动断开服务端的链接后，这个通道就是不活跃的。 * 也就是说客户端与服务端的关闭了通信通道并且不可以传输数据 */ @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; logger.info(&quot;客户端断开链接&#123;&#125;&quot;, ctx.channel().localAddress().toString()); ChannelUtil.channelGroup.remove(ctx.channel()); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; //http if (msg instanceof FullHttpRequest) &#123; FullHttpRequest httpRequest = (FullHttpRequest) msg; if (!httpRequest.decoderResult().isSuccess()) &#123; DefaultFullHttpResponse httpResponse = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.BAD_REQUEST); // 返回应答给客户端 if (httpResponse.status().code() != 200) &#123; ByteBuf buf = Unpooled.copiedBuffer(httpResponse.status().toString(), CharsetUtil.UTF_8); httpResponse.content().writeBytes(buf); buf.release(); &#125; // 如果是非Keep-Alive，关闭连接 ChannelFuture f = ctx.channel().writeAndFlush(httpResponse); if (httpResponse.status().code() != 200) &#123; f.addListener(ChannelFutureListener.CLOSE); &#125; return; &#125; WebSocketServerHandshakerFactory wsFactory = new WebSocketServerHandshakerFactory(&quot;ws:/&quot; + ctx.channel() + &quot;/websocket&quot;, null, false); handshaker = wsFactory.newHandshaker(httpRequest); if (null == handshaker) &#123; WebSocketServerHandshakerFactory.sendUnsupportedVersionResponse(ctx.channel()); &#125; else &#123; handshaker.handshake(ctx.channel(), httpRequest); &#125; return; &#125; //ws if (msg instanceof WebSocketFrame) &#123; WebSocketFrame webSocketFrame = (WebSocketFrame) msg; //关闭请求 if (webSocketFrame instanceof CloseWebSocketFrame) &#123; handshaker.close(ctx.channel(), (CloseWebSocketFrame) webSocketFrame.retain()); return; &#125; //ping请求 if (webSocketFrame instanceof PingWebSocketFrame) &#123; ctx.channel().write(new PongWebSocketFrame(webSocketFrame.content().retain())); return; &#125; //只支持文本格式，不支持二进制消息 if (!(webSocketFrame instanceof TextWebSocketFrame)) &#123; throw new Exception(&quot;仅支持文本格式&quot;); &#125; String request = ((TextWebSocketFrame) webSocketFrame).text(); System.out.println(&quot;服务端收到：&quot; + request); ClientMsgProtocol clientMsgProtocol = JSON.parseObject(request, ClientMsgProtocol.class); //1请求个人信息 if (1 == clientMsgProtocol.getType()) &#123; ctx.channel().writeAndFlush(MsgUtil.buildMsgOwner(ctx.channel().id().toString())); return; &#125; //群发消息 if (2 == clientMsgProtocol.getType()) &#123; TextWebSocketFrame textWebSocketFrame = MsgUtil.buildMsgAll(ctx.channel().id().toString(), clientMsgProtocol.getMsgInfo()); ChannelUtil.channelGroup.writeAndFlush(textWebSocketFrame); &#125; &#125; &#125; /** * 抓住异常，当发生异常的时候，可以做一些相应的处理，比如打印日志、关闭链接 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); logger.info(&quot;异常信息：\\r\\n&quot; + cause.getMessage()); &#125;&#125; server/NettyServer.java：主服务 1234567891011121314151617181920212223242526272829303132333435363738@Component(&quot;nettyServer&quot;)public class NettyServer &#123; private Logger logger = LoggerFactory.getLogger(NettyServer.class); //配置服务端NIO线程组 private final EventLoopGroup parentGroup = new NioEventLoopGroup(); private final EventLoopGroup childGroup = new NioEventLoopGroup(); private Channel channel; public ChannelFuture bing(InetSocketAddress address) &#123; ChannelFuture channelFuture = null; try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(parentGroup, childGroup) .channel(NioServerSocketChannel.class) //非阻塞模式 .option(ChannelOption.SO_BACKLOG, 128) .childHandler(new MyChannelInitializer()); channelFuture = b.bind(address).syncUninterruptibly(); channel = channelFuture.channel(); &#125; catch (Exception e) &#123; logger.error(e.getMessage()); &#125; finally &#123; if (null != channelFuture &amp;&amp; channelFuture.isSuccess()) &#123; logger.info(&quot;demo-netty server start done&quot;); &#125; else &#123; logger.error(&quot;demo-netty server start error&quot;); &#125; &#125; return channelFuture; &#125; public void destroy() &#123; if (null == channel) return; channel.close(); parentGroup.shutdownGracefully(); childGroup.shutdownGracefully(); &#125; public Channel getChannel() &#123; return channel; &#125;&#125; util/MsgUtil.java：消息构建工具类 123456789101112131415161718192021public class MsgUtil &#123; public static TextWebSocketFrame buildMsgAll(String channelId, String msgInfo) &#123; //模拟头像 int i = Math.abs(channelId.hashCode()) % 10; ServerMsgProtocol msg = new ServerMsgProtocol(); msg.setType(2); //链接信息;1自发信息、2群发消息 msg.setChannelId(channelId); msg.setUserHeadImg(&quot;head&quot; + i + &quot;.jpg&quot;); msg.setMsgInfo(msgInfo); return new TextWebSocketFrame(JSON.toJSONString(msg)); &#125; public static TextWebSocketFrame buildMsgOwner(String channelId) &#123; ServerMsgProtocol msg = new ServerMsgProtocol(); msg.setType(1); //链接信息;1链接信息、2消息信息 msg.setChannelId(channelId); return new TextWebSocketFrame(JSON.toJSONString(msg)); &#125;&#125; util/ChannelUtil.java：存储每一个客户端接入进来时的channel对象 1234public class ChannelUtil &#123; //用于存放用户Channel信息，也可以建立map结构模拟不同的消息群 public static ChannelGroup channelGroup = new DefaultChannelGroup(GlobalEventExecutor.INSTANCE);&#125; domain/*MsgProtocol.java：省略get/set 123456789101112public class ServerMsgProtocol &#123; private int type; //链接信息;1:自发信息、2:群发消息 private String channelId; //通信管道ID，实际使用中会映射成用户名 private String userHeadImg; //用户头像[模拟分配] private String msgInfo; //通信消息 // ...&#125;public class ClientMsgProtocol &#123; private int type; //1:请求个人信息，2:发送聊天信息 private String msgInfo; //消息 // ...&#125; controller/NettyController.java：路由控制层 12345678@Controllerpublic class NettyController &#123; @RequestMapping(value = &quot;/index&quot;) public String index(Model model) &#123; model.addAttribute(&quot;name&quot;, &quot;Dear&quot;); return &quot;index&quot;; &#125;&#125; js逻辑：依赖jquery.min.js、jquery.serialize-object.min.js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// JavaScript Documentvar socket;$(function()&#123; if(!window.WebSocket)&#123; window.WebSocket = window.MozWebSocket; &#125; if(!window.WebSocket)&#123; alert(&quot;您的浏览器不支持WebSocket协议！推荐使用谷歌浏览器进行测试。&quot;); return; &#125; socket = new WebSocket(&quot;ws://localhost:7397/websocket&quot;); socket.onmessage = function(event)&#123; var msg = JSON.parse(event.data); //链接信息;1自发信息、2群发消息 if(1 == msg.type)&#123; jQuery.data(document.body, &#x27;channelId&#x27;, msg.channelId); return; &#125; //链接信息;1自发信息、2群发消息 if(2 == msg.type)&#123; var channelId = msg.channelId; //自己 if(channelId == jQuery.data(document.body, &#x27;channelId&#x27;))&#123; var module = $(&quot;.msgBlockOwnerClone&quot;).clone(); module.removeClass(&quot;msgBlockOwnerClone&quot;).addClass(&quot;msgBlockOwner&quot;).css(&#123;display: &quot;block&quot;&#125;); module.find(&quot;.headPoint&quot;).attr(&quot;src&quot;, &quot;res/img/&quot;+msg.userHeadImg); module.find(&quot;.msgBlock_msgInfo .msgPoint&quot;).text(msg.msgInfo); $(&quot;#msgPoint&quot;).before(module); util.divScroll(); &#125; //好友 else&#123; var module = $(&quot;.msgBlockFriendClone&quot;).clone(); module.removeClass(&quot;msgBlockFriendClone&quot;).addClass(&quot;msgBlockFriend&quot;).css(&#123;display: &quot;block&quot;&#125;); module.find(&quot;.headPoint&quot;).attr(&quot;src&quot;, &quot;res/img/&quot;+msg.userHeadImg); module.find(&quot;.msgBlock_channelId&quot;).text(&quot;ID：&quot;+msg.channelId); module.find(&quot;.msgBlock_msgInfo .msgPoint&quot;).text(msg.msgInfo); $(&quot;#msgPoint&quot;).before(module); util.divScroll(); &#125; &#125; &#125;; socket.onopen = function(event)&#123; console.info(&quot;打开WebSoket 服务正常，浏览器支持WebSoket!&quot;); var clientMsgProtocol = &#123;&#125;; clientMsgProtocol.type = 1; clientMsgProtocol.msgInfo = &quot;请求个人信息&quot;; socket.send(JSON.stringify(clientMsgProtocol)); &#125;; socket.onclose = function(event)&#123; console.info(&quot;WebSocket 关闭&quot;); &#125;; document.onkeydown = function(e) &#123; if (13 == e.keyCode &amp;&amp; e.ctrlKey)&#123; util.send(); &#125; &#125;&#125;);util = &#123; send: function()&#123; if(!window.WebSocket)&#123;return;&#125; if(socket.readyState == WebSocket.OPEN)&#123; var clientMsgProtocol = &#123;&#125;; clientMsgProtocol.type = 2; clientMsgProtocol.msgInfo = $(&quot;#sendBox&quot;).val(); socket.send(JSON.stringify(clientMsgProtocol)); $(&quot;#sendBox&quot;).val(&quot;&quot;); &#125;else&#123; alert(&quot;WebSocket 连接没有建立成功！&quot;); &#125; &#125;, divScroll: function()&#123; var div = document.getElementById(&#x27;show&#x27;); div.scrollTop = div.scrollHeight; &#125; &#125;; 主要Html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot; xmlns:sec=&quot;http://www.thymeleaf.org/extras/spring-security&quot;&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/jquery.serialize-object.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/index.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;chatDiv&quot;&gt; &lt;div id=&quot;chat&quot; style=&quot;width:529px; height:667px; background-color:#F5F5F5; float:right;&quot;&gt; &lt;!-- 会话区域 begin --&gt; &lt;div id=&quot;show&quot; style=&quot;width:529px; height:450px; float:left;overflow-y:scroll;&quot;&gt; &lt;!-- 消息块；好友 --&gt; &lt;div class=&quot;msgBlockFriendClone&quot; style=&quot; display:none; margin-left:30px; margin-top:15px; width:340px; height:auto; margin-bottom:15px; float:left;&quot;&gt; &lt;div class=&quot;msgBlock_userHeadImg&quot; style=&quot;float:left; width:35px; height:35px;border-radius:3px;-moz-border-radius:3px; background-color:#FFFFFF;&quot;&gt; &lt;img class=&quot;headPoint&quot; src=&quot;/img/head5.jpg&quot; width=&quot;35px&quot; height=&quot;35px&quot; style=&quot;border-radius:3px;-moz-border-radius:3px;&quot;/&gt; &lt;/div&gt; &lt;div class=&quot;msgBlock_channelId&quot; style=&quot;float:left; width:100px; margin-top:-5px; margin-left:10px; padding-bottom:2px; font-size:10px;&quot;&gt; &lt;!-- 名称 --&gt; &lt;/div&gt; &lt;div class=&quot;msgBlock_msgInfo&quot; style=&quot;height:auto;width:280px;float:left;margin-left:12px; margin-top:4px;border-radius:3px;-moz-border-radius:3px; &quot;&gt; &lt;div style=&quot;width:4px; height:20px; background-color:#CC0000; float:left;border-radius:3px;-moz-border-radius:3px;&quot;&gt;&lt;/div&gt; &lt;div class=&quot;msgPoint&quot; style=&quot;float:left;width:260px; padding:7px; background-color:#FFFFFF; border-radius:3px;-moz-border-radius:3px; height:auto; font-size:12px;display:block;word-break: break-all;word-wrap: break-word;&quot;&gt; &lt;!-- 信息 --&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!-- 消息块；自己 --&gt; &lt;div class=&quot;msgBlockOwnerClone&quot; style=&quot; display:none; margin-right:30px; margin-top:15px; width:340px; height:auto; margin-bottom:15px; float:right;&quot;&gt; &lt;div style=&quot;float:right; width:35px; height:35px;border-radius:3px;-moz-border-radius:3px; background-color:#FFFFFF;&quot;&gt; &lt;img class=&quot;headPoint&quot; src=&quot;/img/head3.jpg&quot; width=&quot;35px&quot; height=&quot;35px&quot; style=&quot;border-radius:3px;-moz-border-radius:3px;&quot;/&gt; &lt;/div&gt; &lt;div class=&quot;msgBlock_msgInfo&quot; style=&quot;height:auto;width:280px;float:left;margin-left:12px; margin-top:4px;border-radius:3px;-moz-border-radius:3px; &quot;&gt; &lt;div class=&quot;msgPoint&quot; style=&quot;float:left;width:260px; padding:7px; background-color:#FFFFFF; border-radius:3px;-moz-border-radius:3px; height:auto; font-size:12px;display:block;word-break: break-all;word-wrap: break-word;&quot;&gt; &lt;!-- 信息 --&gt; &lt;/div&gt; &lt;div style=&quot;width:4px; height:20px; background-color:#CC0000; float:right;border-radius:3px;-moz-border-radius:3px;&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;span id=&quot;msgPoint&quot;&gt;&lt;/span&gt; &lt;/div&gt; &lt;!-- 会话区域 end --&gt; &lt;div style=&quot;width:100%; height:2px; float:left; background-color:#CCCCCC;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;margin:0 auto; width:100%; height:149px; margin-top:5px; background-color:#FFFFFF; float:left;&quot;&gt; &lt;textarea id=&quot;sendBox&quot; style=&quot;font-size:14px; border:0; width:499px; height:80px; outline:none; padding:15px;font-family:”微软雅黑”;resize: none;&quot;&gt;&lt;/textarea&gt; &lt;div style=&quot;margin-top:20px; float:right; margin-right:35px; padding:5px; padding-left:15px; padding-right:15px; font-size:12px; background-color:#F5F5F5;border-radius:3px;-moz-border-radius:3px; cursor:pointer;&quot; onclick=&quot;javascript:util.send();&quot;&gt;发送(S)&lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 启动SpringBoot，Netty会随着启动； 用不同浏览器访问 http://localhost:8080/index 测试多人实时聊天。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"并发编程","slug":"concurrent","permalink":"http://chaooo.github.io/tags/concurrent/"}]},{"title":"【并发编程】阻塞队列 与 线程池","date":"2019-10-14T15:04:54.000Z","path":"article/20191014.html","text":"池和队列的关系 线程池或者数据库连接池，都有最大限制。如果超出了限制数量，则新进来的申请连接都要放入额外的队列里，等到池空出来时，从队列中取出连接放进池里。 1. BlockingQueue（阻塞队列）1234567Queue接口 |———— BlockingQueue接口 |———— ArrayBlockingQueue类 |———— DelayQueue类 |———— LinkedBlockingQueue类 |———— PriorityBlockingQueue类 |———— SynchronousQueue类 BlockingQueue继承了Queue接口，提供了一些阻塞方法，主要作用如下： 当线程向队列中插入元素时，如果队列已满，则阻塞线程，直到队列有空闲位置（非满）； 当线程从队列中取元素（删除队列元素）时，如果队列为空，则阻塞线程，直到队列有元素； BlockingQueue在Queue方法基础上增加了两类和阻塞相关的方法：put(e)、take()；offer(e, time, unit)、poll(time, unit)。 操作类型 抛出异常 返回特殊值 阻塞线程 超时 插入 add(e) offer(e) put(e) offer(e, time, unit) 删除 remove() poll() take() poll(time, unit) 读取 element() peek() / / put(e)**和take()**方法会一直阻塞调用线程，直到线程被中断或队列状态可用； offer(e, time, unit)**和poll(time, unit)**方法会限时阻塞调用线程，直到超时或线程被中断或队列状态可用。 阻塞队列主要用在生产者/消费者的场景 1.1 ArrayBlockingQueueArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。 ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。 1.2 DelayQueueDelayQueue阻塞的是其内部元素，DelayQueue中的元素必须实现java.util.concurrent.Delayed接口，Delayed接口继承了Comparable接口，这是因为DelayedQueue中的元素需要进行排序，一般情况，我们都是按元素过期时间的优先级进行排序。 DelayQueue应用场景：定时关闭连接、缓存对象，超时处理等 1.3 LinkedBlockingQueueLinkedBlockingQueue阻塞队列大小的配置是可选的，如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量 。它的内部实现是一个链表。 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。 1.4 PriorityBlockingQueuePriorityBlockingQueue是一个没有边界的队列，它的排序规则和java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。 所有插入PriorityBlockingQueue的对象必须实现java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。 从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。 1.5 SynchronousQueueSynchronousQueue队列内部仅允许容纳一个元素。 当一个线程插入一个元素后会被阻塞，除非这个元素被另一个线程消费。 2. Callable &amp; FutureCallable与Runnable的功能大致相似，Callable功能强大一些，就是被线程执行后，可以返回值，并且能抛出异常。 Runnable接口只有一个run()方法，实现类重写run方法，把一些费时操作写在其中，然后使用某个线程去执行该Runnable实现类即可实现多线程。 Callable是一个泛型接口只有一个call()方法，返回的类型就是创建Callable传进来的V类型。 1234@FunctionalInterfacepublic interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; Callable一般是和ExecutorService配合来使用的，在ExecutorService接口中声明了若干个submit方法的重载版本 2.1 Future &amp; FutureTaskFuture就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果。必要时可以通过get方法获取执行结果，该方法会阻塞直到任务返回结果。 也就是说Future提供了三种功能： 判断任务是否完成； 能够中断任务； 能够获取任务执行结果。 在Future接口中声明了5个方法：**cancel、isCancelled、isDone、get** boolean cancel(boolean mayInterruptIfRunning);//用来取消任务，参数mayInterruptIfRunning表示是否允许取消正在执行却没有执行完毕的任务。 如果取消已经完成的任务会返回false；如果任务还没有执行会返回true； 如果任务正在执行，则返回mayInterruptIfRunning设置的值(true/false)； boolean isCancelled();//任务是否被取消成功，如果在任务正常完成前被取消成功，则返回 true。 boolean isDone();//任务是否已经完成，若任务完成，则返回true； V get();//获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回； V get(long timeout, TimeUnit unit);//获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null。 Future可以得到别的线程任务方法的返回值。Future是一个接口,引用对象指向的实际是FutureTask。 3. FutureTask**FutureTask**的父类是RunnableFuture，而RunnableFuture继承了Runnbale和Futrue这两个接口 从FutureTask构造方法可以了解到： FutureTask最终都是执行Callable类型的任务。 如果构造函数参数是Runnable，会被Executors.callable方法转换为Callable类型。 Executors.callable方法直接返回一个RunnableAdapter实例。 RunnableAdapter是FutureTask的一个静态内部类并且实现了Callable，也就是说RunnableAdapter是Callable子类。 RunnableAdapter的call方法实现代码是，执行Runnable的run方法，并返回构造FutureTask传入result参数。 FutureTask总结： FutureTask实现了两个接口，Runnable和Future，所以它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值，这个组合的好处：假设有一个很费时逻辑需要计算并且返回这个值，同时这个值不是马上需要，那么就可以使用这个组合，用另一个线程去计算返回值，而当前线程在使用这个返回值之前可以做其它的操作，等到需要这个返回值时，再通过Future得到！ 注意： 通过Executor执行线程任务都是以Callable形式，如果传入Runnable都会转化为Callable。 通过new Thread(runnable)，只能是Runnable子类形式。 4. Fork/Join从JDK1.7开始，Java提供Fork/Join框架用于并行执行任务，它的思想就是讲一个大任务分割成若干小任务，最终汇总每个小任务的结果得到这个大任务的结果。 主要有两步：任务切分 -&gt; 结果合并 第一步**分割任务**。首先我们需要有一个 fork 类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。 第二步执行任务并**合并结果。分割的子任务分别放在双端队列**里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。 工作窃取算法（work-stealing）是指某个线程从其他队列里窃取任务来执行。 Fork/Join 使用两个类来完成以上两个步骤： **ForkJoinTask**：我们要使用 ForkJoin 框架，必须首先创建一个 ForkJoin 任务。它提供在任务中执行 fork() 和 join() 操作的机制，通常情况下我们不需要直接继承 ForkJoinTask 类，而只需要继承它的子类，Fork/Join 框架提供了以下两个子类： RecursiveAction：用于没有返回结果的任务。 RecursiveTask：用于有返回结果的任务。 **ForkJoinPool**：ForkJoinTask 需要通过 ForkJoinPool 来执行，任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进入队列的头部。当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务。 5. 线程池线程池可以看作是一个资源集，任何池的作用都大同小异，主要是用来减少资源创建、初始化的系统开销。 一个线程池包括以下四个基本组成部分： 线程池管理器（ThreadPool）：用于创建并管理线程池，包括 创建线程池，销毁线程池，添加新任务； 工作线程（PoolWorker）：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务； 任务接口（Task）：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等； 任务队列（taskQueue）：用于存放没有处理的任务。提供一种缓冲机制。 12345678Executor接口 |———— ExecutorService接口 |———— AbstractExecutorService抽象类 |———— ForkJoinPool类 |———— ThreadPoolExecutor类 |———— ScheduledExecutorService接口 |———— ScheduledThreadPoolExecutor类Executors类 5.1 通过Executors工厂类中的六个静态方法创建线程池六大静态方法创建的ThreadPoolExecutor对象，返回的父接口的引用，即返回的ExecutorService的引用。六大静态方法内部都是直接或间接调用ThreadPoolExecutor类的构造方法创建线程池对象。 newCachedThreadPool(ThreadPoolExecutor)：创建一个可缓存的线程池 如果线程池的大小超过了处理任务所需要的线程,那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）,maximumPoolSize最大可以至(Integer.MAX_VALUE),若达到该上限,直接OOM。 newFixedThreadPool(ThreadPoolExecutor)：创建固定大小的线程池。 每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 newSingleThreadExecutor(ThreadPoolExecutor)：创建一个单线程的线程池。 这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务,保证按任务的提交顺序依次执行。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newScheduledThreadPool(ScheduledThreadPoolExecutor)：创建一个支持定时及周期性任务执行的线程池。 线程数最大至Integer.MAX_ VALUE,存在OOM风险,不回收工作线程. newSingleThreadScheduledExecutor(ScheduledThreadPoolExecutor)：创建一个单线程用于定时以及周期性执行任务的需求。 newWorkStealingPool(ForkJoinPool)：创建一个工作窃取 JDK8 引入,创建持有足够线程的线程池支持给定的并行度;并通过使用多个队列减少竞争; Executors返回的线程池对象的弊端： FixedThreadPool和SingleThreadExecutor： 允许的请求队列长度为Integer.MAX_VALUE，可能会堆积大量的请求，从而导致OOM。 CachedThreadPool： 允许的创建线程数量为Integer.MAX_VALUE，可能会创建大量的线程，从而导致OOM。 5.2 通过ThreadPoolExecutor构造方法创建线程池123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, //核心线程数，包括空闲线程 int maximumPoolSize,//最大线程数 long keepAliveTime, //线程空闲时间 TimeUnit unit, //时间单位 BlockingQueue&lt;Runnable&gt; workQueue,//缓存队列 ThreadFactory threadFactory, //线程工厂 RejectedExecutionHandler handler //拒绝策略) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 5.2.1 corePoolSize(核心线程数量) corePoolSize的设置非常关键： =0：则任务执行完之后,没有任何请求进入时销毁线程池的线程 &gt;0：即使本地任务执行完毕,核心线程也不会被销毁 设置过大会浪费资源; 设置过小会导致线程频繁地创建或销毁 若设置了allowCoreThreadTimeOut这个参数,当提交一个任务到线程池时,若线程数量(包括空闲线程)小于corePoolSize,线程池会创建一个新线程放入works(一个HashSet)中执行任务,等到需要执行的任务数大于线程池基本大小时就不再创建,会尝试放入等待队列workQueue；如果调用线程池的prestartAllCoreThreads(),线程池会提前创建并启动所有核心线程 5.2.2 maximumPoolSize（线程池最大线程数） maximumPoolSize表示线程池能够容纳同时执行的最大线程数,必须&gt;=1. 若队列满,并且已创建的线程数小于最大线程数,则线程池会再创建新的线程放入works中执行任务,CashedThreadPool的关键,固定线程数的线程池无效 如果maximumPoolSize = corePoolSize,即是固定大小线程池. 若使用了无界任务队列,这个参数就没什么效果 5.2.3 keepAliveTime（线程池中的线程空闲时间） 线程没有任务执行时最多保持多久时间终止（线程池的工作线程空闲后，保持存活的时间) 如果任务很多，并且每个任务执行的时间比较短，可以调大时间，提高线程的利用率 当空闲时间达到keepAliveTime时,线程会被销毁,直到只剩下corePoolSize个线程;避免浪费内存和句柄资源. 在默认情况下,当线程池的线程数大于corePoolSize时,keepAliveTime才起作用. 但是当ThreadPoolExecutor的allowCoreThreadTimeOut=true时,核心线程超时后也会被回收. 5.2.4 TimeUnit（时间单位） keepAliveTime的时间单位通常是TimeUnit.SECONDS 可选的单位：天(DAYS)、小时(HOURS)、分钟(MINUTES)、毫秒(MILLISECONDS)、微秒(MICROSECONDS，千分之一毫秒) 和 纳秒(NANOSECONDS，千分之一微秒) 5.2.5 workQueue（缓存队列） 存储待执行任务的阻塞队列，这些任务必须是Runnable的对象（如果是Callable对象，会在submit内部转换为Runnable对象） 当请求的线程数大于maximumPoolSize时,线程进入BlockingQueue. 可以选择以下几个阻塞队列: LinkedBlockingQueue:一个基于链表结构的阻塞队列,此队列按FIFO排序元素,吞吐量通常要高于ArrayBlockingQueue.静态工厂方法Executors.newFixedThreadPool()使用了这个队列 SynchronousQueue:一个不存储元素的阻塞队列.每个插入操作必须等到另一个线程调用移除操作,否则插入操作一直处于阻塞状态,吞吐量通常要高于LinkedBlockingQueue,静态工厂方法Executors.newCachedThreadPool使用了这个队列 5.2.6 threadFactory （线程工厂） 用于设置创建线程的工厂; 线程池的命名是通过增加组名前缀来实现的，可以通过线程工厂给每个创建出来的线程设置更有意义的名字 在虚拟机栈分析时,就可以知道线程任务是由哪个线程工厂产生的. 5.2.7 RejectedExecutionHandler（拒绝策略） 当队列和线程池都满,说明线程池饱和,必须采取一种策略处理提交的新任务；策略默认**AbortPolicy**,表无法处理新任务时抛出异常 当超过参数workQueue的任务缓存区上限的时候,就可以通过该策略处理请求,这是一种简单的限流保护. 友好的拒绝策略可以是如下三种: 保存到数据库进行削峰填谷;在空闲时再提取出来执行 转向某个提示页面 打印日志 AbortPolicy：丢弃任务，抛出RejectedExecutionException CallerRunsPolicy：只用调用者所在线程来运行任务,有反馈机制，使任务提交的速度变慢）。 DiscardOldestPolicy：若没有发生shutdown,尝试丢弃队列里最近的一个任务,并执行当前任务, 丢弃任务缓存队列中最老的任务，并且尝试重新提交新的任务 DiscardPolicy:不处理,丢弃掉, 拒绝执行，不抛异常 当然,也可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略.如记录日志或持久化存储不能处理的任务 5.3 自定义一个ThreadPoolExecutor线程池12345678910111213141516171819202122232425262728ThreadPoolExecutor pool = new ThreadPoolExecutor( 5, //核心线程数 Runtime.getRuntime().availableProcessors() * 2,//最大线程数 60,//线程空闲时间 TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(200), new ThreadFactory() &#123; @Override public Thread newThread(Runnable r) &#123; Thread t = new Thread(r); t.setName(&quot;order-thread&quot;);//设置有意义的线程名字 if(t.isDaemon()) &#123;//若是守护线程将其释放 t.setDaemon(false); &#125; if(Thread.NORM_PRIORITY != t.getPriority()) &#123; //恢复线程优先级 t.setPriority(Thread.NORM_PRIORITY); &#125; return t; &#125; &#125;, new RejectedExecutionHandler() &#123; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; System.err.println(&quot;拒绝策略:&quot; + r); &#125; &#125; ); 5.3.1 线程池执行流程 要求 线程池有上限，使用有限队列 当线程池核心线程数量用完，先扔进队列 队列也用完后，看最大线程数量 最大线程数量用完后，走拒绝策略 拒绝策略可以打印一些日志，做一些补偿 线程池用完一定要优雅的关闭 线程池要统一管理，不要用Executors工厂类，要用ThreadPoolExecutor自定义线程池 5.3.2 线程池配置-核心线程数量线程CPU时间所占比例越高，需要越少线程(CPU密集)。线程等待时间所占比例越高，需要越多线程(IO密集)。 CPU密集型：内存运算、不涉及IO操作等 设置线程数为：CPU核数+1 IO密集型：数据读取、存取、数据库操作、持久化操作等 最佳线程数目：CPU核数/(1-阻塞系数) 这个阻塞系数一般为0.8~0.9之间，也可以取0.8或者0.9。 java.lang.Runtime.availableProcessors() 方法返回到Java虚拟机的可用的处理器数量(CPU核数)。此值可能会改变在一个特定的虚拟机调用。应用程序可用处理器的数量是敏感的，因此偶尔查询该属性，并适当地调整自己的资源使用情况.","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"并发编程","slug":"concurrent","permalink":"http://chaooo.github.io/tags/concurrent/"}]},{"title":"【并发编程】AQS框架 与 锁框架（JUC.locks）","date":"2019-10-10T15:00:24.000Z","path":"article/20191010.html","text":"1. AQS（队列同步器）AbstractQueuedSynchronizer：队列同步器，简称AQS。 AQS维护了一个volatile int state(代表资源共享变量) 和一个**FIFO线程等待队列**(多线程争用资源被阻塞时会进入此队列)。 AQS定义了两种资源共享方式：Exclusive(独占)，Share(共享) isHeldExclusively方法：该线程是否正在独占资源 tryAcquire/tryRelease：独占的方式尝试获取和释放资源 tryAcquireShared/tryReleaseShared：共享的方式尝试获取和释放资源 整个框架的核心就是如何管理线程阻塞队列，该队列是严格的FIFO队列，因此不支持线程优先级的同步。 AQS只有一个同步队列，可以有多个条件队列。 同步队列的最佳选择是自身没有使用底层锁来构造的非阻塞数据结构，同步队列选择了**CLH**作为实现的基础。 条件队列：AQS框架提供了一个ConditionObject类，给维护独占同步的类以及实现Lock接口的类使用。 使用Node实现**FIFO双向队列**，可以用于构建锁 或 其他同步装置的基础框架 内部有一个int变量表示的**同步状态(同步状态通过getState、setState、compareAndSetState**来维护，同时这三个方法能够保证线程安全) AQS是个抽象类（但没有抽象方法），同步组件一般通过维护AQS的继承子类来实现。 AQS既支持独占地获取同步状态(排它锁)，又支持共享地获取同步状态(共享锁)，从而实现不同类型的组件。 AQS是基于模板方法，同步组件需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。 Synchronizer(同步器)：是一个对象，它根据本身的状态调节线程的控制流。常见类型的Synchronizer包括信号量、关卡和闭锁。 2. CountDownLatch（倒计时闭锁） 闭锁(latch)是一种Synchronizer，它可以延迟线程的进度直到线程达到终止状态。 CountDownLatch(倒计时闭锁)是一个灵活的闭锁实现。 CountDownLatch是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程执行完后再执行。 CountDownLatch原理：是通过一个计数器来实现的，计数器的初始化值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就相应得减1。当计数器到达0时，表示所有的线程都已完成任务，然后在闭锁上等待的线程就可以恢复执行任务。 await()，阻塞程序继续执行 countDown()，计数器的值减1，当计数器值减至零时，所有因调用await()方法而处于等待状态的线程就会继续往下执行。 计数器不能被重置，如果业务上需要一个可以重置计数次数的版本，可以考虑使用CycliBarrier CountDownLatch使用场景：应用初始化 3. Semaphore（信号量） Semaphore(信号量)：用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源。 Semaphore原理：线程需要通过acquire()方法获取许可，而release()释放许可。如果许可数达到最大活动数，那么调用acquire()之后，便进入等待队列，等待已获得许可的线程释放许可，从而使得多线程能够合理的运行。 acquire()：获取权限，其底层实现与CountDownLatch.countdown()类似; release()：释放权限，其底层实现与acquire()是一个互逆的过程。 Semaphore可以用于做流量控制，特别公用资源有限的应用场景，比如数据库连接。 4. CyclicBarrier（同步屏障） CyclicBarrier(同步屏障)：可以让一组线程达到一个屏障时被阻塞，直到最后一个线程达到屏障时，所有被阻塞的线程才能继续执行。 CyclicBarrier类似于CountDownLatch，它也是通过计数器来实现的。但是相比于CountDownLatch功能更加强大。 CyclicBarrier原理：当某个线程调用await方法时，该线程进入等待状态，且计数器加1，当计数器的值达到设置的初始值时，所有因调用await进入等待状态的线程被唤醒，继续执行后续操作。因为CycliBarrier在释放等待线程后可以重用，所以称为循环barrier。 4.1 CountDownLatch 和 CyclicBarrier 对比 CountDownLatch描述的是线程(1个或多个)等待其他线程的关系；CyclicBarrier描述的是多个线程相互等待的关系。 CountDownLatch的计数器只能使用一次。而CyclicBarrier的计数器可以使用reset()方法重置并复用。 CountDownLatch方法比较少，操作比较简单，而CyclicBarrier提供的方法更多，比如： getNumberWaiting()：获取阻塞的线程数量。 isBroken()：获取阻塞线程的状态，被中断返回true，否则返回false。 CyclicBarrier的构造方法可以传入barrierAction，指定当所有线程都到达时执行的业务功能； CyclicBarrier可以用于多线程计算数据，最后合并计算结果的应用场景 5. JUC.locks 锁框架123456789java.util.concurrent.locks |———— Lock接口 |———— ReentrantLock类 |———— ReentrantReadWriteLock.ReadLock内部类 |———— ReentrantReadWriteLock.WriteLock内部类 |———— Condition接口 |———— ReadWriteLock接口 |———— ReentrantReadWriteLock类 |———— LockSupport类 Lock接口核心方法：lock()，unlock()，lockInterruptibly()，newCondition()，tryClock() lock()方法类似于使用synchronized关键字加锁，如果锁不可用，出于线程调度目的，将禁用当前线程，并且在获得锁之前，该线程将一直处于休眠状态。 lockInterruptibly()方法顾名思义，就是如果锁不可用，那么当前正在等待的线程是可以被中断的，这比synchronized关键字更加灵活。 Condition接口核心方法：awit()，signal()，signalAll() 可以看做是Obejct类的wait()、notify()、notifyAll()方法的替代品，与Lock配合使用 ReadWriteLock接口核心方法：readLock()，writeLock() 获取读锁和写锁，注意除非使用Java8新锁，否则读读不互斥，读写是互斥的 6. ReentrantLock（可重入锁）ReentrantLock重入锁使用**AQS同步状态**来保存锁重复持有的次数 底层代码分析： **state**初始化为0，表示未锁定状态 A线程lock()时，会调用tryAcquire()独占该锁并将**state+1** 此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0(即释放锁)为止，其他线程才有机会获取该锁 当然，锁释放之前，A线程自己是可以重复获取此锁的(state会累加)，这就是可重入的概念 synchronized实现的锁的重入依赖于JVM，是一种重量级锁。ReentrantLock实现了在内存语义上的synchronized，使用**AQS同步状态**来保存锁重复持有的次数。当锁被一个线程获取时，ReentrantLock也会记录下当前获得锁的线程标识，以便检查是否是重复获取，以及当错误的线程试图进行解锁操作时检测是否存在非法状态异常。 公平锁和非公平锁 公平锁还是非公平锁取决于ReentrantLock的构造方法，默认无参为非公平锁(NonfairSync)；含参构造方法，入参true为FairSync，入参false为NonfairSync。 非公平锁中，抢到AQS的同步状态的未必是同步队列的首节点，只要线程通过CAS抢到了同步状态或者在acquire中抢到同步状态，就优先占有锁（插队），而相对同步队列这个严格的FIFO队列来说，所以会被认为是非公平锁。 公平锁的实现直接调用AQS的acquire方法，acquire中调用tryAcquire。和非公平锁相比，这里不会执行一次CAS，接下来在tryAcquire去抢占锁的时候，也会先调用hasQueuedPredecessors看看前面是否有节点已经在等待获取锁了，如果存在则同步队列的前驱节点优先（排队FIFO）。 虽然公平锁看起来在公平性上比非公平锁好，但是公平锁为此付出了大量线程切换的代价，而非公平锁在锁的获取上不能保证公平，就有可能出现锁饥饿，即有的线程多次获取锁而有的线程获取不到锁，没有大量的线程切换保证了非公平锁的吞吐量。 7. 读写锁RRW（ReentrantReadWriteLock）ReentrantLock是独占锁，ReentrantReadWriteLock是读写锁。 独占锁通过state变量的0和1两个状态来控制是否有线程占有锁，共享锁通过state变量0或者非0来控制多个线程访问。 读写锁定义为：一个资源能够被多个读线程访问，或者被一个写线程访问，但是不能同时存在读写线程。 ReentrantReadWriteLock的特殊之处其实就是用一个int值表示两种不同的状态（低16位表示写锁的重入次数，高16位表示读锁的使用次数），并通过两个内部类同时实现了AQS的两套API，核心部分与共享/独占锁并无什么区别。 ReentrantReadWriteLock也会发生写请求饥饿的情况，因为写请求一样会排队，不管是公平锁还是非公平锁，在有读锁的情况下，都不能保证写锁一定能获取到，这样只要读锁一直占用，就会发生写饥饿的情况。JDK8中新增的改进读写锁StampedLock可解决饥饿问题 8. LockSupport工具类归根结底，LockSupport调用的Unsafe中的native代码：park()，unpark()； park函数是将当前Thread阻塞，而unpark函数则是将另一个Thread唤醒。 与Object类的wait/notify机制相比，park/unpark有两个优点： 以thread为操作对象更符合阻塞线程的直观定义； 操作更精准，可以准确地唤醒某一个线程（Object类的notify随机唤醒一个线程，notifyAll唤醒所有等待的线程），增加了灵活性 park方法的调用一般要在方法一个循环判断体里面。之所以这样做，是为了防止线程被唤醒后，不进行判断而意外继续向下执行，这其实是一种的多线程设计模式-Guarded Suspension。 9. StampedLock（Java8新型锁）ReentrantReadWriteLock锁具有读写锁，问题在于ReentrantReadWriteLock使得多个读线程同时持有读锁（只要写锁未被占用），而写锁是独占的 ，很容易造成写锁获取不到资源(写请求饥饿)。 Java8引入了一个新的读写锁叫StampedLock. 不仅这个锁更快，而且它提供强大的乐观锁API。这种乐观策略的锁非常类似于无锁的操作，使得乐观锁完全不会阻塞写线程。 StampedLock的主要特点： 所有获取锁的方法，都返回一个邮戳（Stamp），Stamp为0表示获取失败，其余都表示成功； 所有释放锁的方法，都需要一个邮戳（Stamp），这个Stamp必须是和成功获取锁时得到的Stamp一致； StampedLock是不可重入的；（如果一个线程已经持有了写锁，再去获取写锁的话就会造成死锁） StampedLock有三种访问模式： Reading（读模式）：功能和ReentrantReadWriteLock的读锁类似 Writing（写模式）：功能和ReentrantReadWriteLock的写锁类似 Optimistic reading（乐观读模式）：这是一种优化的读模式。 StampedLock支持读锁和写锁的相互转换 RRW(ReentrantReadWriteLock)中，当线程获取到写锁后，可以降级为读锁，但是读锁是不能直接升级为写锁的；StampedLock提供了读锁和写锁相互转换的功能，使得该类支持更多的应用场景。 无论写锁还是读锁，都不支持Conditon等待","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"并发编程","slug":"concurrent","permalink":"http://chaooo.github.io/tags/concurrent/"}]},{"title":"【并发编程】JUC并发容器类","date":"2019-10-06T15:04:22.000Z","path":"article/20191006.html","text":"在java.util.concurrent包中，提供了两种类型的并发集合：一种是阻塞式，另一种是非阻塞式。 阻塞式集合：当集合已满或为空时，被调用的添加（满）、移除（空）方法就不能立即被执行，调用这个方法的线程将被阻塞，一直等到该方法可以被成功执行 非阻塞式集合：当集合已满或为空时，被调用的添加（满）、移除（空）方法就不能立即被执行，调用这个方法的线程不会被阻塞，而是直接则返回null或抛出异常。 1. 线程安全相关容器1.1 线程安全-同步容器： ArrayList –&gt; Vector,Stack HashMap –&gt; HashTable(key、value不能为null) Collections.synchronizedXXX(List/Set/Map) //本质是对相应的容器进行包装，通过在方法中加synchronized同步锁来实现 同步容器的同步原理就是在方法上用synchronized修饰。性能开销大。 在单独使用里面的方法的时候，可以保证线程安全，但是，复合操作需要额外加锁来保证线程安全。 1.2 线程安全-并发容器： ArrayList –&gt; CopyOnWriteArrayList**：保证最终一致性，写时复制，适用于读多写少**的并发场景 HashSet、TreeSet –&gt; CopyOnWriteArraySet、ConcurrentSkipListSet： HashMap、TreeMap –&gt; **ConcurrentHashMap**、ConcurrentSkipListMap： 1.3 安全共享对象策略 线程限制：一个被线程限制的对象，由线程独占，并且只能被占有者修改 共享只读：一个共享只读的对象，在没有额外同步的情况下，可以被多个线程并发访问，但不能修改 线程安全对象：一个线程安全的对象或者容器，在内部通过同步机制来保证线程安全，其他线程无需额外的同步就可以通过公共接口随意访问它 被守护对象：被守护对象只能通过获取特定的锁来访问 2. CopyOnWrite机制CopyOnWrite（简称COW），是计算机程序设计领域中的一种优化策略，也是一种思想–即写入时复制思想。 在CopyOnWrite中，对容器的修改操作加锁后，通过copy一个新的容器副本来进行修改，修改完毕后将容器替换为新的容器即可。 这种方式的好处显而易见：通过copy一个新的容器来进行修改，这样读操作就不需要加锁，可以并发读，因为在读的过程中是采用的旧的容器，即使新容器做了修改对旧容器也没有影响，同时也很好的解决了迭代过程中其他线程修改导致的并发问题。 从JDK1.5开始，java.util.concurrent包中提供了两个CopyOnWrite机制容器，分别为**CopyOnWriteArrayList和CopyOnWriteArraySet** CopyOnWriteArrayList通过使用**ReentrantLock锁**来实现线程安全： 在添加、获取元素时，使用getArray()获取底层数组对象，获取此时集合中的数组对象；使用setArray()设置底层数组，将原有数组对象指针指向新的数组对象—-实以此来实现CopyOnWrite副本概念 添加元素: 在添加元素之前进行加锁操作，保证数据的原子性。在添加过程中，进行数组复制，修改操作，再将新生成的数组复制给集合中的array属性。最后，释放锁； 由于array属性被volatile修饰，所以当添加完成后，其他线程就可以立刻查看到被修改的内容。 获取元素：在获取元素时，由于array属性被volatile修饰，所以每当获取线程执行时，都会拿到最新的数据。此外，添加线程在进行添加元素时，会将新的数组赋值给array属性，所以在获取线程中并不会因为元素的添加而导致本线程的执行异常。因为获取线程中的array和被添加后的array指向了不同的内存区域。 在执行add()时，为什么还要在加锁的同时又copy了一分新的数组对象? 因为，在add()时候加了锁，首先不会有多个线程同时进到add中去，这一点保证了数组的安全。当在一个线程执行add时，又进行了数组的复制操作，生成了一个新的数组对象，在add后又将新数组对象的指针指向了旧的数组对象指针，注意此时是指针的替换，原来旧的数组对象还存在。这样就实现了，添加方法无论如何操作数组对象，获取方法在获取到集合后，都不会受到其他线程添加元素的影响。 CopyOnWrite机制的优缺点 优点: CopyOnWriteArrayList保证了数据在多线程操作时的最终一致性。 缺点: 缺点也同样显著，那就是内存空间的浪费：因为在写操作时，进行数组复制，在内存中产生了两份相同的数组。如果数组对象比较大，那么就会造成频繁的GC操作，进而影响到系统的性能； 适用场景：读多写少的并发场景 3. ConcurrentHashMapConcurrentHashMap容器相较于CopyOnWrite容器在并发加锁粒度上有了更大一步的优化，它通过修改对单个hash桶元素加锁的达到了更细粒度的并发控制。 在底层数据结构上，ConcurrentHashMap和HashMap都使用了数组+链表+红黑树的方式，只是在HashMap的基础上添加了并发相关的一些控制。 JDK1.8中取消了segment分段锁，而采用CAS和synchronized来保证并发安全。synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。并且初始化操作大大简化，修改为lazy-load形式。 3.1 put方法过程put方法内部是一个 putVal 的调用： 判断键值是否为null，为null抛出异常 调用spread()方法计算key的hashCode()获得哈希地址 判断Node[]数组(table)是否为空，若空则进行初始化操作 需要注意的是这里并没有加synchronized，也就是允许多个线程去**尝试**初始化table，但是在初始化函数里面使用了CAS保证只有一个线程去执行初始化过程 使用(容量大小-1 &amp; 哈希地址)计算下标，如果没有碰撞，使用CAS原子性操作放入桶中；插入失败(被别的线程抢先插入了)则进入下次循环。 如果该下标上的节点(头节点)的哈希地址为-1，代表需要扩容，该线程执行helpTransfer()方法协助扩容。 如果碰撞了(bucket不为空)且又不需要扩容，则进入到bucket中，且锁住该bucket，其他bucket不影响。 进入到bucket里面，首先判断这个bucket存储的是红黑树(哈希地址小于0)还是链表。 如果是链表，则遍历链表，若节点已经存在(key相同)就覆盖旧值，没有找到相同的节点就将新增的节点插入到链表尾部。如果是红黑树，则将节点插入。到这里释放锁。 判断该bucket上的链表长度是否链表长度超过阀值（TREEIFY_THRESHOLD==8），大于则调用treeifyBin()方法将链表转成红黑树。 调用addCount()方法，作用是将ConcurrentHashMap的键值对数量+1，还有另一个作用是检查ConcurrentHashMap是否需要扩容。 总结： JDK8中的实现也是锁分离的思想，它把锁分的比segment（JDK1.5）更细一些，只要hash不冲突，就不会出现并发获得锁的情况。它首先使用无锁操作CAS插入头结点，如果插入失败，说明已经有别的线程插入头结点了，再次循环进行操作。如果头结点已经存在，则通过synchronized获得头结点锁，进行后续的操作。性能比segment分段锁又再次提升。 3.2 ConcurrentHashMap多线程环境下扩容 transfer()方法为ConcurrentHashMap扩容操作的核心方法。由于ConcurrentHashMap支持多线程扩容，而且也没有进行加锁，所以实现会变得有点儿复杂。整个扩容操作分为两步： 构建一个nextTable，其大小为原来大小的两倍，这个步骤是在单线程环境下完成的 将原来table里面的内容复制到nextTable中，这个步骤是允许多线程操作的，所以性能得到提升，减少了扩容的时间消耗。 扩容的时机： 如果新增节点之后，所在链表的元素个数达到了阈值 8，则会调用treeifyBin方法把链表转换成红黑树，不过在结构转换之前，会对数组长度进行判断： 如果数组长度n小于阈值MIN_TREEIFY_CAPACITY，默认是64，则会调用tryPresize方法把数组长度扩大到原来的两倍，并触发transfer方法，重新调整节点的位置。 新增节点之后，会调用addCount方法记录元素个数，并检查是否需要进行扩容，当数组元素个数达到阈值时，会触发transfer方法，重新调整节点的位置。 JDK8的源码里面就引入了一个**ForwardingNode**类，在一个线程发起扩容的时候，就会改变sizeCtl这个值，其含义如下： sizeCtl ：默认为0，用来控制table的初始化和扩容操作，具体应用在后续会体现出来。 -1 代表table正在初始化 -N 表示有N-1个线程正在进行扩容操作 其余情况： 如果table未初始化，表示table需要初始化的大小。 如果table初始化完成，表示table的容量，默认是table大小的0.75倍 扩容时候会判断sizeCtl的值，如果超过阈值就要扩容，首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素f，初始化一个forwardNode实例fwd，如果f == null，则在table中的i位置放入fwd，否则采用头插法的方式把当前旧table数组的指定任务范围的数据给迁移到新的数组中，然后给旧table原位置赋值fwd。直到遍历过所有的节点以后就完成了复制工作，把table指向nextTable，并更新sizeCtl为新数组大小的0.75倍 ，扩容完成。在此期间如果其他线程的有读写操作都会判断head节点是否为forwardNode节点，如果是就帮助扩容。 在扩容时读写操作如何进行 对于get读操作，如果当前节点有数据，还没迁移完成，此时不影响读，能够正常进行。如果当前链表已经迁移完成，那么头节点会被设置成fwd节点，此时get线程会帮助扩容。 对于put/remove写操作，如果当前链表已经迁移完成，那么头节点会被设置成fwd节点，此时写线程会帮助扩容，如果扩容没有完成，当前链表的头节点会被锁住，所以写线程会被阻塞，直到扩容完成。 总结: ConcurrentHashMap扩容的原理是新生成原来2倍的数组，然后拷贝旧数组数据到新的数组里面，在多线程情况下，这里面如果注意线程安全问题，在解决安全问题的同时，我们也要关注其效率，这才是并发容器类的最出色的地方。 3.3 size、mappingCount方法 size和mappingCount方法都是用来统计table的size的 这两者不同的地方在size返回的是一个int类型，即可以表示size的范围是[-2^31，2^31-1]，超过这个范围就返回int能表示的最大值 mappingCount返回的是一个long类型，即可以表示size的范围是[-2^63，2^63-1]。 这两个方法都是调用的sumCount()方法实现统计。 对于size和迭代器是弱一致性 volatile修饰的数组引用是强可见的，但是其元素却不一定，所以，这导致size的根据sumCount的方法并不准确。 同理Iteritor的迭代器也一样，并不能准确反映最新的实际情况 4. ConcurrentSkipListMapConcurrentSkipListMap内部使用跳表（SkipList）这种数据结构来实现，他的结构相对红黑树来说非常简单理解，实现起来也相对简单，而且在理论上它的查找、插入、删除时间复杂度都为log(n)。在并发上，ConcurrentSkipListMap采用无锁的**CAS+自旋**来控制。 跳表简单来说就是一个多层的链表，底层是一个普通的链表，然后逐层减少，通常通过一个简单的算法实现每一层元素是下一层的元素的二分之一，这样当搜索元素时从最顶层开始搜索，可以说是另一种形式的二分查找。 ConcurrentSkipListMap的**put**(插入)： 调用doPut()方法，可以分为3大步来理解： 第一步获取前继节点后通过CAS来插入节点； 第二步对level层数进行判断，如果大于最大层数，则插入一层； 第三步插入对应层的数据。整个插入过程全部通过CAS自旋的方式保证并发情况下的数据正确性。 5. volatile &amp; Atmoic &amp; UnSafe **volatile**作用：①多线程间的可见性、②阻止指令重排序 **Atmoic系列类**提供了原子性操作，保障多线程下的安全 **UnSafe类**的作用：①内存操作、②字段的定位与修改(底层)、③线程挂起与恢复、④CAS操作(乐观锁)","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"并发编程","slug":"concurrent","permalink":"http://chaooo.github.io/tags/concurrent/"}]},{"title":"【Java知识梳理】常见集合类 的 数据结构","date":"2019-10-03T14:53:06.000Z","path":"article/20191003.html","text":"集合(Collection/Map)1234567891011121314151617Collection接口 |———— List接口 |———— ArrayList类 |———— Vector类 |———— LinkedList类 |———— Stack类 |———— Set接口 |———— HashSet类 |———— TreeSet类 |———— LinkedHashSet类 |———— Queue接口 |———— LinkedList类Map接口 |———— HashMap类 |———— TreeMap类 |———— LinkedHashMap类 |———— Hashtable类 0.1 List Arraylist： 动态数组 Vector： 动态数组(线程安全) LinkedList： 双向链表(JDK1.6之前为循环链表，JDK1.7取消了循环) 0.2 Set HashSet（无序，唯一）：基于 HashMap 实现的，底层采用 HashMap 来保存元素 LinkedHashSet： LinkedHashSet 继承于 HashSet，并且其内部是通过 LinkedHashMap 来实现的。 TreeSet（有序，唯一）：红黑树(自平衡的排序二叉树) 0.3 Map HashMap： JDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间 LinkedHashMap： LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。 Hashtable： 数组+链表(线程安全)，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap： 红黑树（自平衡的排序二叉树） 0.4 如何选用集合主要根据集合的特点来选用： 键值对就选用Map接口下的集合，需要排序时选择TreeMap，不需要排序时就选择HashMap,需要保证线程安全就选用ConcurrentHashMap. 只需要存放元素值时，就选用Collection接口下的集合，需要保证元素唯一时选择实现Set接口的集合（TreeSet或HashSet），不需要就选择实现List接口的ArrayList或LinkedList 0.5 对数公式log 与 时空复杂度 若a^n = b (a&gt;0,a≠1) 则 n = log(a)b , 如log(2)8 = 3; Java数据结构中log默认以2为底(个人理解,有待考证) 常用O(1), O(n), O(logn)表示对应算法的时间复杂度, 也用于表示空间复杂度。 O(1): 最低的时空复杂度, 无论数据规模多大，都可以在一次计算后找到目标 O(n): 数据量增大n倍时，耗时增大n倍; 比如常见的遍历算法 O(n^2): 数据量增大n倍时，耗时增大n的平方倍; 比如冒泡排序，对n个数排序，需要扫描n×n次 o(logn): 当数据增大n倍时，耗时增大logn倍; 二分查找就是O(logn)的算法，每找一次排除一半的可能，256个数据中查找只要找8次就可以找到目标(2^8=256) O(nlogn): 同理，就是n乘以logn，当数据增大256倍时，耗时增大256*8=2048倍。这个复杂度高于线性低于平方。归并排序就是O(nlogn)的时间复杂度 0.6 移位运算符按照平移的方向和填充数字的规则分为三种:&lt;&lt;(左移)、&gt;&gt;(带符号右移) 和 &gt;&gt;&gt;(无符号右移) 左移 &lt;&lt; : 丢弃最高位,0补最低位；左移n位就相当于乘以2的n次方 右移 &gt;&gt; : 符号位不变,高位补上符号位(正数0, 负数1)；右移n位相当于除以2的n次方 无符号右移 &gt;&gt;&gt; : 忽略符号位，0补最高位(补码移位所得) 正数的左移与右移，负数的无符号右移，就是相应的补码移位所得，在高位补0即可。 负数的右移，就是补码高位补1,然后按位取反加1即可。 运算规则： 左移：高位移出(舍弃)，低位的空位补零；int类型时，每移动1位它的第31位就要被移出并且丢弃；long类型时，每移动1位它的第63位就要被移出并且丢弃；byte和short类型时，将自动把这些类型扩大为int型。 右移：低位移出(舍弃)，高位的空位补符号位，即正数补0，负数补1；当右移的运算数是byte 和short类型时，将自动把这些类型扩大为 int 型。 无符号右移：补码移位，高位补0；正数和右移表现一致，负数变成了很大的正数； 1. ArraylistArrayList的底层是数组队列，相当于动态数组。与数组相比，它的容量能动态增长。在添加大量元素前，应用程序使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。它继承于 AbstractList，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。 数组时间复杂度: 插入/删除:O(n)**，增加(末尾)/随机访问: O(1)** ArrayList 继承了AbstractList，实现了List。它是一个数组队列，提供了相关的添加、删除、修改、遍历等功能 ArrayList 实现了RandomAccess 接口， RandomAccess 是一个标志接口，表明实现这个这个接口的 List 集合是支持快速随机访问的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问 ArrayList 实现了Cloneable 接口，即覆盖了函数 clone()，能被克隆。 ArrayList 实现java.io.Serializable 接口，这意味着ArrayList支持序列化，能通过序列化去传输。 和 Vector 不同，ArrayList 中的操作不是线程安全的！所以，建议在单线程中才使用 ArrayList，而在多线程中可以选择 Vector 或者 CopyOnWriteArrayList。 1.1 ArrayList扩容机制*（重点） 以无参数构造方法创建ArrayList时，实际上初始化赋值的是一个空数组；当add第一个元素时，才真正分配容量(默认10) ArrayList在每次增加元素(1个或一组)时，都要调用ensureCapacityInternal()方法来确保足够的容量 当容量不足以容纳当前的元素个数时，进入grow()方法进行扩容，首先设置新的容量为旧容量的1.5倍 若设置后的新容量还不够，则设置新容量为minCapacity(所需最小容量) 比较新容量是否大于MAX_ARRAY_SIZE(Integer最大值减8)，若大于，再比较minCapacity是否大于MAX_ARRAY_SIZE，若大于，设置新的容量为Integer.MAX_VALUE(Integer最大值)，否则设置新的容量为MAX_ARRAY_SIZE(Integer最大值减8) 最后用Arrays.copyof()方法将元素拷贝到新的数组 (第Integer.MAX_VALUE+1次添加元素时，抛出OutOfMemoryError异常) System.arraycopy()和Arrays.copyOf()方法通过源码发现这两个实现数组复制的方法被广泛使用, 比如插入操作add(int index, E element)方法就很巧妙的用到了 System.arraycopy()方法让数组自己复制自己实现让index开始之后的所有成员后移一个位置 Arrays.copyOf()内部也是调用了System.arraycopy()方法 Arrays.copyOf()是系统自动在内部新建一个数组，并返回该数组 System.arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置 1.2 ensureCapacityArrayList对外提供了一个ensureCapacity(int n)方法 最好在add大量元素之前用ensureCapacity方法，以减少增量重新分配的次数 ensureCapacity一次性扩容到位，否则在添加大量元素的过程中，一点一点的进行扩容 1.3 内部类1234private class Itr implements Iterator&lt;E&gt;&#123;...&#125;private class ListItr extends Itr implements ListIterator&lt;E&gt;&#123;...&#125;private class SubList extends AbstractList&lt;E&gt; implements RandomAccess&#123;...&#125;static final class ArrayListSpliterator&lt;E&gt; implements Spliterator&lt;E&gt;&#123;...&#125; ArrayList有四个内部类 Itr 实现了Iterator接口，同时重写了里面的hasNext()， next()， remove() 等方法； ListItr 继承 Itr，实现了ListIterator接口，同时重写了hasPrevious()， nextIndex()， previousIndex()， previous()， set(E e)， add(E e) 等方法 Iterator和ListIterator的区别: ListIterator在Iterator的基础上增加了添加对象，修改对象，逆向遍历等方法，这些是Iterator不能实现的。 2. LinkedListLinkedList是基于双向链表实现的, 可以在任何位置进行高效地插入和移除操作的有序序列。 复杂度: 增加(末尾)/删除:O(1)**，插入/获取: O(n)** LinkedList 继承AbstractSequentialList的双向链表。它也可以被当作堆栈、队列或双端队列进行操作。 LinkedList 实现 List 接口，能对它进行队列操作。 LinkedList 实现 Deque 接口，即能将LinkedList当作双端队列使用。 LinkedList 实现了Cloneable接口，即覆盖了函数clone()，能克隆。 LinkedList 实现java.io.Serializable接口，这意味着LinkedList支持序列化，能通过序列化去传输。 LinkedList 不是线程安全的，如果想使LinkedList变成线程安全的，可以调用静态类Collections类中的synchronizedList方法 2.1 LinkedList底层分析:LinkedList的底层是一个双向链表，链表中挂载着一个个的Node元素；可以从LinkedList的Node内部类看出奥秘： 1234567891011121314transient Node&lt;E&gt; first; //头指针transient Node&lt;E&gt; last; //尾指针//内部类private static class Node&lt;E&gt; &#123; E item; // 数据域（当前节点的值） Node&lt;E&gt; next; // 后继（指向当前一个节点的后一个节点） Node&lt;E&gt; prev; // 前驱（指向当前节点的前一个节点） // 构造函数，赋值前驱后继 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; LinkedList 是基于链表结构实现，所以在类中包含了 first 和 last 两个指针(Node)。 Node 中包含了上一个节点和下一个节点的引用，这样就构成了双向的链表。 2.2 LinkedList增删改查 链表批量增加，是靠for循环遍历原数组，依次执行插入节点操作。增加一定会修改modCount。 通过下标获取某个node的时候(add select)，会根据index处于前半段还是后半段进行一个折半，以提升查询效率 删也一定会修改modCount。 按下标删，也是先根据index找到Node，然后去链表上unlink掉这个Node。 按元素删，会先去遍历链表寻找是否有该Node，如果有，去链表上unlink掉这个Node。 改也是先根据index找到Node，然后替换值。不修改modCount。 CRUD操作里，都涉及到根据index去找到Node的操作。 2.2 unlink原理 先判断该节点是否存在上一个节点，即是否有前驱节点。 无前驱节点则说明要删除的节点为链表的第一节点，那么只需要把该节点的下一个节点设置为链表的第一个节点。 有前驱节点则需要把前驱节点的尾部引用指向该节点的下一个节点。 再判断该节点是否存在下一个节点，即是否有后继节点。 无后继节点则说明该节点是链表的最后一个节点，那么只需要把该节点前驱节点设置成链表的最后一个节点即可。 有后继节点则需要把后继节点的头部引用指向该节点的上一个节点。 核心就是在于将要删除的节点的前驱节点尾部指向该节点的后继节点，将要删除的节点的后继节点的头部指向该节点的前驱节点。这样便完成了链表的删除操作。 删除和新增方法的实现基本是对该节点的上一个节点和下一个节点的引用设置，不需要操作其他节点，效率相对较高 2.3 offer与add的区别 offer属于 offer in interface Deque。 add 属于 add in interface Collection。 当队列为空时候，使用add方法会报错，而offer方法会返回false。 作为List使用时,一般采用add / get方法来 压入/获取对象。 作为Queue使用时,才会采用 offer/poll/take等方法作为链表对象时,offer等方法相对来说没有什么意义这些方法是用于支持队列应用的。 2.2 对比Vector、ArrayList、LinkedList有何区别这三者都是实现集合框架中的 List，也就是所谓的有序集合，因此具体功能也比较近似，比如都提供按照位置进行定位、添加或者删除的操作，都提供迭代器以遍历其内容等。但因为具体的设计区别，在行为、性能、线程安全等方面，表现又有很大不同。 Vector 是 Java 早期提供的线程安全的动态数组，如果不需要线程安全，并不建议选择，毕竟同步是有额外开销的。Vector 内部是使用对象数组来保存数据，可以根据需要自动的增加容量，当数组已满时，会创建新的数组，并拷贝原有数组数据扩容为旧容量的2倍。 ArrayList 是应用更加广泛的动态数组实现，它本身不是线程安全的，所以性能要好很多。ArrayList 也是可以根据需要调整容量，在扩容为旧容量的1.5倍。 LinkedList 顾名思义是 Java 提供的双向链表，不需要扩容，它也不是线程安全的。LinkedList不支持高效的随机元素访问。 3. HashMapHashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的, 用于存储Key-Value键值对的集合，每一个键值对也叫做一个Entry。这些Entry分散存储在一个数组当中，这个数组就是HashMap的主干。 HashMap继承了AbstractMap类，实现了Map，Cloneable，Serializable接口 继承 abstractMap，也就是用来减轻实现Map接口的编写负担。 实现 Cloneable：能够使用Clone()方法，在HashMap中，实现的是浅层次拷贝，即对拷贝对象的改变会影响被拷贝的对象。 实现 Serializable：能够使之序列化，即可以将HashMap对象保存至本地，之后可以恢复状态。 JDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以实现O(logn)时间复杂读查找。 HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。 HashMap的实例有两个参数影响其性能: 初始容量(默认16)：哈希表中桶的数量 加载因子(默认0.75)：哈希表在其容量自动增加之前可以达到多满的一种尺度 当哈希表中条目数超出了当前容量*加载因子(其实就是HashMap的实际容量)时，则对该哈希表进行rehash操作，将哈希表扩充至两倍的桶数。 3.1 HashMap的 put 方法过程*（重点）put方法内部是一个 putVal 的调用： 对 Key 求 Hash 值，然后再计算下标。 如果没有碰撞，直接放入桶中， 如果碰撞了，若是树节点，就putTreeVal添加元素，若不是就遍历链表插入。 如果链表长度超过阀值（TREEIFY_THRESHOLD==8），就把链表转成红黑树。 如果节点已经存在就替换旧值，若未找到则继续 如果桶满了（容量 * 加载因子），就需要 resize(扩容为原来2倍并重新散列,元素的下标要么不变，要么变为【原下标+原容量】)。 3.2 HashMap 桶下标计算 下标：hash(key) &amp; (table.length - 1) 扰动函数**hash(key)**：(key==null) ? 0 : (key.hashCode()^(key.hashCode() &gt;&gt;&gt; 16)) 低16位 和 高 16位 做了一个异或得到 hash值 与 (容器长度-1)进行**取模(%)**运算,得到下标。 利用位运算代替取模运算，提高程序的计算效率：（当 b=2^n 时，a%b = a &amp; (b-1) ），也是因此，HashMap 才将初始长度设置为 16，且扩容只能是以 2 的倍数（2^n）扩容。 有些数据计算出的哈希值差异主要在高位，而HashMap里的哈希寻址是忽略容量以上的高位的，那么这种处理就可以尽可能有效的避免哈希碰撞。 HashMap 的性能表现非常依赖于哈希码的有效性: equals相等，hashCode一定要相等。重写了 hashCode 也要重写 equals。hashCode 需要保持一致性，状态改变返回的哈希值仍然要一致。 3.3 HashMap 容量、负载因子和树化 容量和负载系数决定了可用的桶的数量，空桶太多会浪费空间，如果使用的太满则会严重影响操作的性能。 如果能够知道 HashMap 要存取的键值对数量，可以考虑预先设置合适的容量大小。 计算条件：*负载因子 * 容量 &gt; 元素数量*；所以，预先设置的容量需要满足，大于“预估元素数量/负载因子”，同时它是2的幂数 容量理论最大极限由 MAXIMUM_CAPACITY 指定，数值为 1&lt;&lt;30，也就是2的30次方 3.3.1 HashMap 负载因子loadFactor loadFactor加载因子是控制数组存放数据的疏密程度，越大越密，越小越稀疏。 loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给出的一个比较好的临界值。 给定的默认容量为16，负载因子为0.75。当数量达到了 16*0.75 = 12 就需要将当前16的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。 而对于负载因子，建议： 如果没有特别需求，不要轻易进行更改，因为 JDK 自身的默认负载因子是非常符合通用场景的需求的。 如果确实需要调整，建议不要设置超过 0.75 的数值，因为会显著增加冲突，降低 HashMap 的性能。 如果使用太小的负载因子，按照上面的公式，预设容量值也进行调整，否则可能会导致更加频繁的扩容，增加无谓的开销，本身访问性能也会受影响。 3.3.2 HashMap 门限值thresholdthreshold = capacity * loadFactor，当Size&gt;=threshold的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 衡量数组是否需要扩增的一个标准。 门限值等于(负载因子 x 容量)，如果构建 HashMap 的时候没有指定它们，那么就是依据相应的默认常量值。 门限通常是以倍数进行调整 （newThr = oldThr &lt;&lt; 1），根据 putVal 中的逻辑，当元素个数超过门限大小时，则调整 Map 大小。 扩容后，需要将老的数组中的元素重新放置到新的数组，这是扩容的一个主要开销来源。 3.3.2 HashMap 树化改造树化改造逻辑主要在 putVal 和 treeifyBin 中。 链表结构（这里叫 bin）的数量大于 TREEIFY_THRESHOLD(默认为8) 时： 如果容量小于 MIN_TREEIFY_CAPACITY(默认为64) ，只会进行简单的扩容。 如果容量大于 MIN_TREEIFY_CAPACITY(默认为64)，则会进行树化改造。 3.4 HashMap 扩容resize HashMap扩容条件： 元素个数超出了加载因子与当前容量的乘积，并且发生了Hash碰撞 HashMap扩容步骤： 创建一个新的Entry空数组，长度是原来的2倍。 遍历原Entry数组，把所有的Entry重新Hash到新数组里。 重新散列的元素下标要么【不变】，要么变为【原下标+原容量】，取决于位运算((n - 1) &amp; hash) 经过一次扩容处理后，元素会更加均匀的分布在各个桶中，会提升访问效率。但会遍历所有的元素，时间复杂度很高；遍历元素所带来的坏处大于元素在桶中均匀分布所带来的好处。尽量避免进行扩容处理。 3.5 常见的hash算法及冲突的解决hash函数，即散列函数。它可以将不定长的输入，通过散列算法转换成一个定长的输出，这个输出就是散列值(不保证唯一)。 常见Hash算法： 直接定址法：直接以关键字k或者k加上某个常数（k+c）作为哈希地址（H(k)=ak+b）。 数字分析法：提取关键字中取值比较均匀的数字作为哈希地址（如一组出生日期，相较于年-月，月-日的差别要大得多，可以降低冲突概率） 分段叠加法：按照哈希表地址位数将关键字分成位数相等的几部分，其中最后一部分可以比较短。然后将这几部分相加，舍弃最高进位后的结果就是该关键字的哈希地址。 平方取中法：如果关键字各个部分分布都不均匀的话，可以先求出它的平方值，然后按照需求取中间的几位作为哈希地址。 伪随机数法：选择一随机函数，取关键字的随机值作为散列地址，通常用于关键字长度不同的场合。 除留余数法：用关键字k除以某个不大于哈希表长度m的数p，将所得余数作为哈希表地址（H(k)=k%p, p&lt;=m; p一般取m或素数）。 常见解决hash冲突的方法 链地址法：将哈希表的每个单元作为链表的头结点，所有哈希地址为 i 的元素构成一个同义词链表。即发生冲突时就把该关键字链在以该单元为头结点的链表的尾部。 开放定址法：即发生冲突时，去寻找下一个空的哈希地址。只要哈希表足够大，总能找到空的哈希地址。 再哈希法：即发生冲突时，由其他的函数再计算一次哈希值。 建立公共溢出区：将哈希表分为基本表和溢出表，发生冲突时，将冲突的元素放入溢出表。 HashMap就是使用链地址法来解决冲突的（JDK1.8增加了红黑树） 3.6 对比Hashtable、HashMap、TreeMap有什么不同Hashtable、HashMap、TreeMap 都是最常见的一些 Map 实现，是以键值对的形式存储和操作数据的容器类型。 Hashtable 是早期 Java 类库提供的一个哈希表实现，本身是同步的，不支持 null 键和值，由于同步导致的性能开销，所以已经很少被推荐使用。 HashMap 是应用更加广泛的哈希表实现，行为上大致上与 HashTable 一致，主要区别在于 HashMap 不是同步的，支持 null 键和值等。通常情况下，HashMap 进行 put 或者 get 操作，可以达到常数时间的性能，所以它是绝大部分利用键值对存取场景的首选，比如，实现一个用户 ID 和用户信息对应的运行时存储结构。 TreeMap 则是基于红黑树的一种提供顺序访问的 Map，和 HashMap 不同，它的 get、put、remove 之类操作都是 O(log(n))的时间复杂度，具体顺序可以由指定的 Comparator 来决定，或者根据键的自然顺序来判断。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"}]},{"title":"【Redis】Redis穿透、击穿、雪崩和数据一致性","date":"2019-09-27T11:15:31.000Z","path":"article/20190927.html","text":"1. 缓存穿透访问一个不存在的key，缓存不起作用，请求会穿透到DB，流量大时DB会挂掉。 解决方案： 采用布隆过滤器（bloomfilter就类似于一个hash set），使用一个足够大的bitmap，用于存储可能访问的key，不存在的key直接被过滤； 访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。 接口限流与熔断、降级 使用互斥锁排队（分布式环境中要使用分布式锁，单机的话用普通的锁（synchronized、Lock）） 2. 缓存雪崩大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。 解决方案 可以给缓存设置过期时间时加上一个随机值时间，使得每个key的过期时间分布开来，不会集中在同一时刻失效。 建立备份缓存，缓存A和缓存B，A设置超时时间，B不设值超时时间，先从A读缓存，A没有读B，并且更新A缓存和B缓存; 加锁排队，实现同上; 3. 缓存击穿一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。 解决方案 在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。 4. 缓存并发竞争多个redis的client同时set key引起的并发问题（例如：多客户端同时并发写一个key，一个key的值是1，本来按顺序修改为2,3,4，最后是4，但是顺序变成了4,3,2，最后变成了2） 解决方案 如果对这个key操作，不要求顺序：准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可。 如果对这个key操作，要求顺序： 分布式锁+时间戳（假设系统B先抢到锁，将key1设置为{ValueB 7:05}。接下来系统A抢到锁，发现自己的key1的时间戳早于缓存中的时间戳（7:00&lt;7:05），那就不做set操作了） 利用消息队列（把Redis.set操作放在队列中使其串行化,必须的一个一个执行） 5. 缓存和数据库一致性解决方案5.1 并发量、一致性要求都不是很高的场景 写流程：先淘汰缓存，再写数据库，之后再异步将数据刷回缓存 读流程：先读缓存，如果缓存没读到，则去读DB，之后再异步将数据刷回缓存 优点：实现起来简单，异步刷新，补缺补漏 缺点：容灾不足，并发问题，一个比较大的缺陷在于刷新缓存有可能会失败，而失败之后缓存中数据就一直会处于错误状态，所以它并不能保证数据的最终一致性 5.2 业务简单，读写QPS比较低的场景（QPS每秒查询率(Query Per Second)） 写流程：先淘汰缓存，再写数据库，监听从库binlog，通过解析binlog来刷新缓存 读流程：第一步先读缓存，如果缓存没读到，则去读DB，之后再异步将数据刷回缓存 优点：容灾 缺点：只适合简单业务，复杂业务容易发生并发问题（例如：读/写的时候，缓存中的数据已失效，此时又发生了更新） 5.3 业务只需要达到“最终一致性”要求的场景 写流程：先淘汰缓存，再写数据库，监听从库binlog，通过分析binlog我们解析出需要需要刷新的数据标识，然后将数据标识写入MQ，接下来就消费MQ，解析MQ消息来读库获取相应的数据刷新缓存。 读流程：第一步先读缓存，如果缓存没读到，则去读DB，之后再异步将数据标识写入MQ（这里MQ与写流程的MQ是同一个），接下来就消费MQ，解析MQ消息来读库获取相应的数据刷新缓存。 优点：容灾完善，无并发问题 缺点：只能达到”最终一致性” 5.4 强一致性的场景 写流程：我们把修改的数据通过Cache_0标记“正在被修改”，如果标记成功，写数据库，删除缓存，监听从库binlog，通过分析binlog我们解析出需要需要刷新的数据标识，然后将数据标识写入MQ，接下来就消费MQ，解析MQ消息来读库获取相应的数据刷新缓存； 那如果标记失败，则要放弃这次修改。 读流程：先读Cache_0，看看要读的数据是否被标记，如果被标记，则直接读主库；如果没有被标记，读缓存，如果缓存没读到，则去读DB，之后再异步将数据标识写入MQ（这里MQ与写流程的MQ是同一个），接下来就消费MQ，解析MQ消息来读库获取相应的数据刷新缓存。 优点：容灾完善，无并发问题 缺点：增加Cache_0强依赖，复杂度是比较高的（涉及到Databus、MQ、定时任务等等组件）","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"}]},{"title":"【Redis】深入学习Redis及集群","date":"2019-09-20T08:19:26.000Z","path":"article/20190920.html","text":"Redis本质上是一个Key-Value类型的内存数据库，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过10万次读写操作，是已知性能最快的Key-Value DB。Redis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB。另外Redis也可以对存入的Key-Value设置expire时间。 Redis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 1. Redis数据结构及命令操作1.1 基本概念及操作 默认16个数据库，类似数组下表从零开始，初始默认使用零号库； 统一密码管理，16个库都是同样密码，要么都OK要么一个也连接不上，redis默认端口是6379； select命令切换数据库：select 0-15； dbsize：查看当前数据库的key的数量； flushdb：清空当前库； flushall；通杀全部库； 1.2 Redis数据结构redis存储的是：key-value格式的数据，其中key都是字符串，value有5种不同的数据结构:String、Hash、List、Set、Zset(Sorted Set) String：set, get, del, append, strlen Hash：hset, hget, hdel, hmset(批量设值), hmget, hgetall List：lpush, rpush, lrange, lpop(删除), rpop, lindex Set：sadd, smembers, srem(根据可以移除member), sismember(判断是否为key的成员) ZSet：zadd, zrange, zrem 1.3 Redis键(key)–常用命令介绍 keys *：查看所有 key ； exists key的名字：判断某个 key 是否存在； move key dbID（0-15）： 当前库就没有了，被移除了； expire key 秒钟： 为给定的 key 设置过期时间； ttl key： 查看还有多少秒过期，-1表示永不过期，-2表示已过期； type key： 查看你的 key 是什么类型； 2. Redis持久化Redis作为一个键值对内存数据库(NoSQL)，数据都存储在内存当中，在处理客户端请求时，所有操作都在内存当中进行，为了避免内存中数据丢失，Redis提供了RDB和AOF两种不同的数据持久化方式。 2.1 RDB（Redis DataBase）RDB是一种快照存储持久化方式，具体就是将Redis某一时刻的内存数据保存到硬盘的文件当中，默认保存的文件名为dump.rdb，而在Redis服务器启动时，会重新加载dump.rdb文件的数据到内存当中恢复数据。 开启RDB持久化方式一：save命令，或bgsave(异步) 开启方式二：在Redis配置文件redis.conf配置，配置完后启动时加载：redis-server redis.conf 123save 900 1 # 900s内至少达到一条写命令save 300 10 # 300s内至少达至10条写命令save 60 10000 # 60s内至少达到10000条写命令 RDB的几个优点 与AOF方式相比，通过rdb文件恢复数据比较快。 rdb文件非常紧凑，适合于数据备份。 通过RDB进行数据备，由于使用子进程生成，所以对Redis服务器性能影响较小。 RDB的几个缺点 如果服务器宕机的话，采用RDB的方式会造成某个时段内数据的丢失，比如我们设置10分钟同步一次或5分钟达到1000次写入就同步一次，那么如果还没达到触发条件服务器就死机了，那么这个时间段的数据会丢失。 使用save命令会造成服务器阻塞，直接数据同步完成才能接收后续请求。 使用bgsave命令在forks子进程时，如果数据量太大，forks的过程也会发生阻塞，另外，forks子进程会耗费内存。 2.2 AOF(Append-only file)与RDB存储某个时刻的快照不同，AOF持久化方式会记录客户端对服务器的每一次写操作命令（以日志的形式），并将这些写操作以Redis协议追加保存到以后缀为aof文件末尾，在Redis服务器重启时，会加载并运行aof文件的命令，以达到恢复数据的目的。 开启方式：在Redis配置文件redis.conf配置 123456appendonly yes # 开启aof机制appendfilename &quot;appendonly.aof&quot; # aof文件名# 写入策略,always表示每个写操作都保存到aof文件中,也可以是everysec(每秒写入一次)或no(操作系统处理)appendfsync alwaysno-appendfsync-on-rewrite no # 默认不重写aof文件dir ~/redis/ # 保存目录 aof文件太大，加载aof文件恢复数据时，就会非常慢，为了解决，Redis通过重写aof，可以生成一个恢复当前数据的最少命令集，两种方式：配置no-appendfsync-on-rewrite(默认no)，或者客户端向服务器发送bgrewriteaof命令 AOF的优点：AOF只是追加日志文件，因此对服务器性能影响较小，速度比RDB要快，消耗的内存较少。 AOF的缺点：AOF方式生成的日志文件太大，即使通过AFO重写，文件体积仍然很大。恢复数据的速度比RDB慢。 当RDB与AOF两种方式都开启时，Redis会优先使用AOF日志来恢复数据，因为AOF保存的文件比RDB文件更完整。 2.2.1 AOF文件修复 备份被写坏的AOF文件 运行redis-check-aof –fix进行修复 用diff -u来看下两个文件的差异，确认问题点 重启redis，加载修复后的AOF文件 3. Redis的高并发和快速原因 redis是基于内存的，内存的读写速度非常快； redis是单线程的，省去了很多上下文切换线程的时间； redis使用多路复用技术，可以处理并发的连接。非阻塞IO 内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间。 另外，数据结构也帮了不少忙，Redis全程使用hash结构，读取速度快，还有一些特殊的数据结构，对数据存储进行了优化，如压缩表，对短数据进行压缩存储，再如，跳表，使用有序的数据结构加快读取的速度。 还有一点，Redis采用自己实现的事件分离器，效率比较高，内部采用非阻塞的执行方式，吞吐能力比较大。 4. Redis利用哨兵(Sentinel)，复制(Replication)这两个功能来保证高可用 哨兵(Sentinel)：可以管理多个Redis服务器，它提供了监控，提醒以及自动的故障转移的功能。 集群监控：负责监控Redis master和slave进程是否正常工作 消息通知：如果某个Redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员 故障转移：如果master node挂掉了，会自动转移到slave node上 配置中心：如果故障转移发生了，通知client客户端新的master地址 复制(Replication)：则是负责让一个Redis服务器可以配备多个备份的服务器。 从数据库向主数据库发送sync(数据同步)命令。 主数据库接收同步命令后，会保存快照，创建一个RDB文件。 当主数据库执行完保持快照后，会向从数据库发送RDB文件，而从数据库会接收并载入该文件。 主数据库将缓冲区的所有写命令发给从服务器执行。 以上处理完之后，之后主数据库每执行一个写命令，都会将被执行的写命令发送给从数据库。 5. Redis 主从复制、哨兵和集群这三个有什么区别主从复制是为了数据备份，哨兵是为了高可用，Redis主服务器挂了哨兵可以切换，集群则是因为单实例能力有限，搞多个分散压力。 主从模式：读写分离，备份，一个Master可以有多个Slaves。 哨兵entinel：监控，自动转移，哨兵发现主服务器挂了后，就会从slave中重新选举一个主服务器。 集群Cluster：为了解决单机Redis容量有限的问题，将数据按一定的规则分配到多台机器，内存/QPS不受限于单机，可受益于分布式集群高扩展性。 6. Redis Cluster集群Redis Cluster，是Redis 3.0开始引入的分布式存储方案。集群由多个节点(Node)组成，Redis的数据分布在这些节点中。集群中的节点分为主节点和从节点：只有主节点负责读写请求和集群信息的维护；从节点只进行主节点数据和状态信息的复制。 集群的作用： 数据分区：数据分区(或称数据分片)是集群最核心的功能。 高可用：集群支持主从复制和主节点的自动故障转移（与哨兵类似）；当任一节点发生故障时，集群仍然可以对外提供服务。 6.1 Redis Cluster集群的搭建可以分为四步： 启动节点：将节点以集群模式启动，此时节点是独立的，并没有建立联系； 节点握手：让独立的节点连成一个网络； 分配槽：将16384个槽分配给主节点； 指定主从关系：为从节点指定主节点。 6.2 Redis Cluster工作原理 客户端与Redis节点直连,不需要中间Proxy层，直接连接任意一个Master节点 根据公式HASH_SLOT=CRC16(key) mod 16384，计算出映射到哪个分片上，然后Redis会去相应的节点进行操作 123456 CRC16(key) | 0~5460 | &lt;--Slot--|Redis(M)|&lt;---|Redis(S可多个从) mode 16384 |Client --------------&gt; | 5461~10922| &lt;--Slot--|Redis(M)|&lt;---|Redis(S可多个从) | |10923~10383| &lt;--Slot--|Redis(M)|&lt;---|Redis(S可多个从) 6.3 Redis Cluster优点: 无需Sentinel哨兵监控，如果Master挂了，Redis Cluster内部自动将Slave切换Master 可以进行水平扩容 支持自动化迁移，当出现某个Slave宕机了，那么就只有Master了，这时候的高可用性就无法很好的保证了，万一master也宕机了，咋办呢？ 针对这种情况，如果说其他Master有多余的Slave ，集群自动把多余的Slave迁移到没有Slave的Master 中。 6.4 Redis Cluster缺点: 批量操作是个坑（不同的key会划分到不同的slot中，因此直接使用mset或者mget等操作是行不通） 资源隔离性较差，容易出现相互影响的情况。 6.5 Redis Cluster总结： Redis Cluster集群架构，不同的key是有可能分配在不同的Redis节点上的，在这种情况下Redis的事务机制是不生效。 单机下的redis可以支持16个数据库（db0 ~ db15），在Redis Cluster集群架构下只有一个数据库空间，即db0。 不同的key会划分到不同的slot中，因此直接使用mset或者mget等操作是行不通。 如果Hash对象非常大，是不支持映射到不同节点的！只能映射到集群中的一个节点上。 Redis集群模式下进行批量操作：如果执行的key数量比较少，就用串行get操作； 如果需要执行的key很多，就使用Hashtag保证这些key映射到同一台redis节点上。 Redis Cluster的架构，是属于分片集群的架构，不做读写分离，因为redis本身在内存上操作，不会涉及IO吞吐，即使读写分离也不会提升太多性能，Redis在生产上的主要问题是考虑容量，单机最多10-20G，key太多降低redis性能.因此采用分片集群结构，已经能保证了我们的性能。其次，用上了读写分离后，还要考虑主从一致性，主从延迟等问题，徒增业务复杂度。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"}]},{"title":"【数据库优化】MySQL事务处理与并发控制","date":"2019-09-07T15:31:48.000Z","path":"article/20190907.html","text":"1. MySQL事务 事务: 数据库操作的最小工作单元，是作为单个逻辑工作单元执行的一系列操作；事务是一组不可再分割的操作集合(工作逻辑单元)； 事务的特性(ACID)： 原子性（Atomicity，或称不可分割性）：最小的工作单元，整个工作单元要么一起提交成功，要么全部失败回滚 一致性（Consistency）：事务中操作的数据及状态改变是一致的，即写入资料的结果必须完全符合预设的规则， 不会因为出现系统意外等原因导致状态的不一致 隔离性（Isolation，又称独立性）：一个事务所操作的数据在提交之前，对其他事务的可见性设定（一般设定为不可见） 持久性（Durability）：事务所做的修改就会永久保存，不会因为系统意外导致数据的丢失 事务的开启与提交模式 若参数autocommit=0，自动开启手动提交 若参数autocommit=1（系统默认值），又分为两种状态： 自动开启自动提交：用户的每一个操作都是一个完整的事务周期。 手动开启手动提交：从用户执行start transaction命令到用户执行commit命令之间的一系列操作为一个完整的事务周期。若不执行commit命令，系统则默认事务回滚。 begin 或者 start transaction – 开启事务 commit 或者 rollback – 事务提交或回滚 1.1 事务的隔离级别 查看/设置隔离级别 查看：SELECT @@tx_isolation 设置：set tx_isolation=&#39;xxx&#39; 读未提交（Read Uncommitted） 事务未提交对其他事务也是可见的，脏读（dirty read） 读提交（Read Committed）–解决脏读问题 一个事务开始之后，只能看到自己提交的事务所做的修改，不可重复读（nonrepeatable read） 可重复读（Repeatable Read）–解决不可重复读问题 在同一个事务中多次读取同样的数据结果是一样的，这种隔离级别未定义解决幻读的问题 串行化（Serializable）–解决所有问题 最高的隔离级别，通过强制事务的串行执行，但是会导致大量超时以及锁争用问题 Mysql默认采用REPEATABLE_READ隔离级别，Oracle默认采用READ_COMMITTED隔离级别。事务的隔离级别的实现：锁、MVCC（多版本并发控制 Multiversion Currency Control）。 1.2 事务的七大传播行为Spring在TransactionDefinition接口中规定了7种类型的事务传播行为。事务传播行为是Spring框架独有的事务增强特性，他不属于的事务实际提供方数据库行为。 事务传播行为用来描述由某一个事务传播行为修饰的方法被嵌套进另一个方法的时事务如何传播。 @Transactional(propagation = Propagation.REQUIRED) 第一类：运行在同一个事务 **REQUIRED**（required）：默认，支持当前事务，如果当前没有事务，就新建一个事务。 SUPPORTS（supports）：支持当前事务，如果当前没有事务，就不使用事务(以非事务方式执行) MANDATORY（mandatory）：支持当前事务，如果当前没有事务，就抛出异常 第二类：运行在不同事务 **REQUIRES_NEW**（requires new）：新建事务，如果当前存在事务，把当前事务挂起 NOT_SUPPORTED(not supported)：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起 NEVER（never）：以非事务方式执行，如果当前存在事务，则抛出异常 第三类：嵌套执行–即外层事务如果失败，内层事务要么回滚到保存点要么回滚到初始状态 **NESTED**（nested）：如果当前事务存在，则嵌套事务执行 2. 锁锁是用于管理不同事务对共享资源的并发访问，InnoDB存储引擎支持行锁和表锁（InnoDB表锁是另类的行锁） InnoDB行锁 共享锁（读锁）：Shared Locks 排它锁（写锁）：Exclusive Locks InnoDB表锁 意向锁共享锁（IS）：Intention Shared Locks 意向锁排它锁（IX）：Intention Exclusive Locks 自增锁：AUTO-INC Locks 行锁的算法 记录锁 Record Locks 间隙锁 Gap Locks 临键锁 Next-key Locks 2.1 共享锁(Shared) &amp; 排他锁(Exclusive)它们都是标准的行级锁。 共享锁（S锁）：读锁，读锁允许多个连接可以同一时刻并发的读取同一资源,互不干扰，但是只能读不能修改; 加锁： select * from users WHERE id=1 LOCK IN SHARE MODE; 解锁：**commit或rollback** 排他锁（X锁）：写锁，一个写锁会阻塞其他的写锁或读锁，保证同一时刻只有一个连接可以写入数据，同时防止其他用户对这个数据的读写。 加锁： select * from users WHERE id=1 FOR UPDATE; delete/update/insert 默认上 X 锁 解锁：**commit或rollback** 注意：所谓共享锁、排他锁其实均是锁机制本身的策略，通过这两种策略对锁做了区分。 InnoDB的行锁是通过给索引上的索引项加锁来实现的。 只有通过索引条件进行数据检索，InnoDB才使用行级锁，否则，InnoDB 将使用表锁（锁住索引的所有记录） 2.2 意向锁(Intention) &amp; 自增锁(AUTO-INC)它们都是标准的表级锁。 意向锁（Intention Locks）：表级别的锁。先提前声明一个意向，并获取表级别的意向锁（IS或IX），如果获取成功，才被允许对该表加行锁(S或X)。(即一个数据行加锁前必须先取得该表的意向锁) 意向锁(IS、IX)是InnoDB数据操作之前自动加的，不需要用户干预 意义：当事务想去进行锁表时，可以先判断意向锁是否存在，存在时则可快速返回该表不能启用表锁 自增锁（AUTO-INC Locks）：针对自增列自增长的一个特殊的表级别锁 show variables like &#39;innodb_autoinc_lock_mode&#39;; 默认取值1，代表连续，事务未提交ID永久丢失 2.3 记录锁(Record) &amp; 间隙锁(Gap) &amp; 临键锁(Next-key) 临键锁 Next-key locks： 锁住记录+区间（左开右闭） Innodb默认行锁算法 当sql执行按照索引进行数据的检索时,查询条件为范围查找（between and、&lt;、&gt;等）并有数 据命中则此时SQL语句加上的锁为Next-key locks，锁住索引的记录+区间（左开右闭） 间隙锁 Gap locks： 锁住数据不存在的区间（左开右开） 当记录不存在，临键锁退化为Gap锁 当sql执行按照索引进行数据的检索时，查询条件的数据不存在，这时SQL语句加上的锁即为 Gap locks，Gap只在RR事务隔离级别存在，锁住索引不存在的区间（左开右开） 记录锁 Record locks： 锁住具体的索引项 唯一性(主键/唯一)索引，条件为精准匹配，退化成Record锁 当sql执行按照唯一性（Primary key、Unique key）索引进行数据的检索时，查询条件等值匹 配且查询的数据是存在，这时SQL语句加上的锁即为记录锁Record locks，锁住具体的索引项 2.4 死锁的产生与避免 死锁 在InnoDB中，锁是逐步获得的，就造成了死锁的可能（2个或以上并发事务） 每个事务都持有锁（或者是已经在等待锁）; 每个事务都需要再继续持有锁；事务之间产生加锁的循环等待，形成死锁。 死锁的产生与避免 类似的业务逻辑以固定的顺序访问表和行。 大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概 率。 降低隔离级别，如果业务允许，将隔离级别调低也是较好的选择 为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添 加上锁（或者说是表锁） 3. MVCC(多版本并发控制)Multiversion concurrency control (多版本并发控制)： MVCC 就是 同一份数据临时保留多版本的一种方式，进而实现并发控制 是行级锁的变种，它在普通读情况下避免了加锁操作，因此开销更低。 MVCC 提供了时点（point in time）一致性视图。MVCC 并发控制下的读事务一般使用时间戳或者事务ID去标记当前读的数据库的状态（版本），读取这个版本的数据。读、写事务相互隔离，不需要加锁。读写并存的时候，写操作会根据目前数据库的状态，创建一个新版本，并发的读则依旧访问旧版本的数据。 3.1 MVCC逻辑流程 在MySQL中建表时，每个表都会有三列隐藏记录，其中和MVCC有关系的有两列 DB_TRX_ID：数据行的版本号 DB_ROLL_PT：删除版本号 MVCC逻辑流程-插入 在插入数据的时候，会把全局事务ID记录到列DB_TRX_ID中去 MVCC逻辑流程-删除 执行完删除SQL之后数据并没有被真正删除，而是对删除版本号(DB_ROLL_PT)做改变 MVCC逻辑流程-修改 修改数据的时候 会先复制一条当前记录行数据，同时标记这条数据的数据行版本号为当前事务ID，最后把旧数据的删除版本号标记为新数据行版本号的值(即当前事务ID)。 MVCC逻辑流程-查询 查找数据行版本号早于当前事务ID的数据行记录 也就是说，数据行的版本号要小于或等于 当前事务ID，这样也就确保了读取到的数据是当前事务开始前已经存在的数据，或者是自身事务改变过的数据 查找删除版本号要么为NULL，要么大于当前事务版本号的记录 这样确保查询出来的数据行记录在事务开启之前没有被删除 MySQL解决不可重复读和脏读并不是单纯利用 MVCC 机制来实现的。 4. MySQL事务日志(Undo Log和Redo Log)innodb事务日志包括redo log和undo log。redo log是重做日志，提供前滚操作，undo log是回滚日志，提供回滚操作。undo log不是redo log的逆向过程，其实它们都算是用来恢复的日志： redo log通常是物理日志，记录的是数据页的物理修改，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页(恢复数据页，且只能恢复到最后一次提交的位置)。 undo用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。 s 4.1 Undo Log Undo Log定义： undo意为取消，以撤销操作为目的，返回指定某个状态的操作 undo log指事务开始之前，在操作任何数据之前,首先将需操作的数据备份到一个地方 (Undo Log) UndoLog是为了实现事务的原子性而出现的产物 Undo Log实现事务原子性： 事务处理过程中如果出现了错误或者用户执行了 ROLLBACK语句,Mysql可以利用Undo Log中的备份 将数据恢复到事务开始之前的状态 UndoLog在Mysql innodb存储引擎中用来实现多版本并发控制 Undo log实现多版本并发控制： 事务未提交之前，Undo保存了未提交之前的版本数据，Undo中的数据可作为数据旧版本快照供 其他并发事务进行快照读 4.2 当前读 &amp; 快照读 快照读： SQL读取的数据是快照版本，也就是历史版本，普通的SELECT就是快照读 innodb快照读，数据的读取将由 cache(原本数据) + undo(事务修改过的数据) 两部分组成 当前读： SQL读取的数据是最新版本。通过锁机制来保证读取的数据无法通过其他事务进行修改 UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE都是当前读 4.3 Redo Log Undo Log定义： Redo，顾名思义就是重做。以恢复操作为目的，重现操作； Redo log指事务中操作的任何数据,将最新的数据备份到一个地方 (Redo Log) Redo log的持久： 不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入redo 中。具体 的落盘策略可以进行配置 RedoLog是为了实现事务的持久性而出现的产物 Redo Log实现事务持久性： 防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的未入磁盘数据进行持久化这一特性。 一旦事务成功提交且数据持久化落盘之后，此时Redo log中的对应事务数据记录就失去了意义，所 以Redo log的写入是日志文件循环写入的 附: 58同城数据库设计30条军规 军规适用场景：并发量大、数据量大的互联网业务 解读：讲解原因，解读比军规更重要 一、基础规范 必须使用InnoDB存储引擎 解读：支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 必须使用UTF8字符集 UTF-8MB4 解读：万国码，无需转码，无乱码风险，节省空间 数据表、数据字段必须加入中文注释 解读：N年后谁tm知道这个r1,r2,r3字段是干嘛的 禁止使用存储过程、视图、触发器、Event 解读：高并发大数据的互联网业务，架构设计思路是“解放数据库CPU，将计算转移到服务 层”，并发量大的情况下，这些功能很可能将数据库拖死，业务逻辑放到服务层具备更好的 扩展性，能够轻易实现“增机器就加性能”。数据库擅长存储与索引，CPU计算还是上移吧 禁止存储大文件或者大照片 解读：为何要让数据库做它不擅长的事情？大文件和照片存储在文件系统，数据库里存URI 多好 二、命名规范 只允许使用内网域名，而不是ip连接数据库 线上环境、开发环境、测试环境数据库内网域名遵循命名规范 业务名称：xxx，线上环境：xxx.db，开发环境：xxx.rdb，测试环境：xxx.tdb 从库在名称后加-s标识，备库在名称后加-ss标识 线上从库：xxx-s.db 线上备库：xxx-sss.db 库名、表名、字段名：小写，下划线风格，不超过32个字符，必须见名知意，禁止 拼音英文混用 表名t_xxx，非唯一索引名idx_xxx，唯一索引名uniq_xxx 三、表设计规范 单实例表数目必须小于500 单表列数目必须小于30 表必须有主键，例如自增主键 解读： 主键递增，数据行写入可以提高插入性能，可以避免page分裂，减少表碎片提升空间和 内存的使用 主键要选择较短的数据类型， Innodb引擎普通索引都会保存主键的值，较短的数据类 型可以有效的减少索引的磁盘空间，提高索引的缓存效率 无主键的表删除，在row模式的主从架构，会导致备库夯住 禁止使用外键，如果有外键完整性约束，需要应用程序控制 解读：外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响 sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能，大数据高并发业务场景 数据库使用以性能优先 四、字段设计规范 必须把字段定义为NOT NULL并且提供默认值 解读： null的列使索引/索引统计/值比较都更加复杂，对MySQL来说更难优化 null这种类型MySQL内部需要进行特殊处理，增加数据库处理记录的复杂性；同等条 件下，表中有较多空字段的时候，数据库的处理性能会降低很多 null值需要更多的存储空，无论是表还是索引中每行中的null的列都需要额外的空间来标 识 对null 的处理时候，只能采用is null或is not null，而不能采用=、in、&lt;、&lt;&gt;、!=、 not in这些操作符号。如：where name!=’shenjian’，如果存在name为null值的记 录，查询结果就不会包含name为null值的记录 禁止使用TEXT、BLOB类型 解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内 存命中率急剧降低，影响数据库性能 禁止使用小数存储货币 解读：使用整数吧，小数容易导致钱对不上 必须使用varchar(20)存储手机号 解读： 涉及到区号或者国家代号，可能出现+-() 手机号会去做数学运算么？ varchar可以支持模糊查询，例如：like“138%” 禁止使用ENUM，可使用TINYINT代替 解读： 增加新的ENUM值要做DDL操作 ENUM的内部实际存储就是整数，你以为自己定义的是字符串？ 五、索引设计规范 单表索引建议控制在5个以内 单索引字段数不允许超过5个 解读：字段超过5个时，实际已经起不到有效过滤数据的作用了 禁止在更新十分频繁、区分度不高的属性上建立索引 解读： 更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能 “性别”这种区分度不大的属性，建立索引是没有什么意义的，不能有效过滤数据，性 能与全表扫描类似 建立组合索引，必须把区分度高的字段放在前面 解读：能够更加有效的过滤数据 六、SQL使用规范 禁止使用SELECT *，只获取必要的字段，需要显示说明列属性 解读： 读取不需要的列会增加CPU、IO、NET消耗 不能有效的利用覆盖索引 禁止使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性 解读：容易在增加或者删除字段后出现程序BUG 禁止使用属性隐式转换 解读：SELECT uid FROM t_user WHERE phone=13812345678 会导致全表扫描，而不 能命中phone索引 禁止在WHERE条件的属性上使用函数或者表达式 解读：SELECT uid FROM t_user WHERE from_unixtime(day)&gt;=’2017-02-15’ 会导致全 表扫描 正确的写法是：SELECT uid FROM t_user WHERE day&gt;= unix_timestamp(‘2017-02-15 00:00:00’) 禁止负向查询，以及%开头的模糊查询 解读： 负向查询条件：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描 %开头的模糊查询，会导致全表扫描 禁止大表使用JOIN查询，禁止大表使用子查询 解读：会产生临时表，消耗较多内存与CPU，极大影响数据库性能 禁止使用OR条件，必须改为IN查询 解读：旧版本Mysql的OR查询是不能命中索引的，即使能命中索引，为何要让数据库耗费 更多的CPU帮助实施查询优化呢？ 应用程序必须捕获SQL异常，并有相应处理","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"【数据库优化】MySQL索引的使用及优化","date":"2019-09-05T09:04:12.000Z","path":"article/20190905.html","text":"1. 索引的基本概念索引是为了加速对表中数据行的检索而创建的一种分散存储的数据结构。 索引意义： 索引能极大的减少存储引擎需要扫描的数据量 索引可以把随机IO变成顺序IO 索引可以帮助我们在进行分组、排序等操作时，避免使用临时表 增加索引会有利于查询效率，但会降低insert，update，delete的效率，但实际上往往不是这样的，过多的索引会不但会影响使用效率，同时会影响查询效率，这是由于数据库进行查询分析时，首先要选择使用哪一个索引进行查询，如果索引过多，分析过程就会越慢，这样同样的减少查询的效率，因此我们要知道如何增加，有时候要知道维护和删除不需要的索引。 2. 索引的适用场景2.1 适合建索引的场景 表的主键自动建立唯一索引 表的字段唯一约束 直接条件查询的字段（在SQL中用于条件约束的字段） 查询中与其它表关联的字段 查询中排序的字段（排序的字段如果通过索引去访问那将大大提高排序速度） 查询中统计或分组统计的字段 表记录太少（如果一个表只有5条记录，采用索引去访问记录的话，那首先需访问索引表，再通过索引表访问数据表，一般索引表与数据表不在同一个数据块） 经常插入、删除、修改的表（对一些经常处理的业务表应在查询允许的情况下尽量减少索引） 数据重复且分布平均的表字段（假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率大约为50%，那么对这种表A字段建索引一般不会提高数据库的查询速度。） 经常和主字段一块查询但主字段索引值比较多的表字段 对千万级MySQL数据库建立索引的事项及提高性能的手段 2.2 不适合建索引的场景 表记录太少（300万左右性能开始逐渐下降，虽然官方文档说撑得住5-8百万以上，但是根本也不能等到这个时候再去优化，性能肯定会受到影响） 经常增删改的表（why：提高了查询速度，同事却会降低了更新表的速度，入队表进行INSERT,UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存下索引文件）。 数据重复切分布平均的表字段，因此应该只为最经常查询和最经常排序的数据建立索引。注意，如果某个数据列包括许多重复的内容，为他建立索引就没有太大的实际效果了。（加入一个表有10万行的记录，有一个字段A只有True和False两个值，且每个值的分布概率大约为50%，那么对这种表的A字段建立索引一般不会提高数据库的查询速度。再比如对银行卡建立索引，毕竟银行卡没有重复的。索引的选择性是指索引列中不同值的数据与表中的记录数的比，如果一个表中有2000条记录，表索引列就有1980个不同的值，那么这个索引的选择性就是1980/2000=0.99。一个索引的选择性越接近于1，这个索引的效率就越高。） 3. MySQl中索引的结构（B+树）3.1 基本概念： 二叉树：一个节点最多两个子节点，一个节点只存储一个关键字，等于则命中，小于走左节点，大于走右节点； B树：多路搜索树，每个节点存储M/2到M个关键字，所有关键字在整颗树中出现，且只出现一次，非叶子节点可以命中； B+树：在B树基础上，为叶子节点增加链表指针，所有关键字都在叶子节点中出现(有序)，叶子节点才命中； B*树：在B+树基础上，为非叶子节点也增加兄弟链表指针，将节点的最低利用率从1/2提高到2/3； 3.2 B+树的特性： 所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的； 不可能在非叶子结点命中； 非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层； 更适合文件索引系统； 3.3 B+树的三个特点： 关键字数和子树相同 在 B+ 树中，节点的关键字代表子树的最大值，因此关键字数等于子树数。 非叶子节点仅用作索引，它的关键字和子节点有重复元素 除叶子节点外的所有节点的关键字，都在它的下一级子树中同样存在，最后所有数据都存储在叶子节点中。 根节点的最大关键字其实就表示整个 B+ 树的最大元素。 叶子节点用指针连在一起 叶子节点包含了全部的数据，并且按顺序排列，B+ 树使用一个链表将它们排列起来，这样在查询时效率更快。 由于 B+ 树的中间节点不含有实际数据，只有子树的最大数据和子树指针，因此磁盘页中可以容纳更多节点元素，也就是说同样数据情况下，B+ 树会 B 树更加“矮胖”，因此查询效率更快。B+ 树的查找必会查到叶子节点，更加稳定。有时候需要查询某个范围内的数据，由于 B+ 树的叶子节点是一个有序链表，只需在叶子节点上遍历即可，不用像 B 树那样挨个中序遍历比较大小。 3.4 B+ 树的三个优点： 层级更低，IO 次数更少 每次都需要查询到叶子节点，查询性能稳定 叶子节点形成有序链表，范围查询方便 4. 索引的优化4.1 优化法则（口诀）： 全值匹配我最爱，最左前缀要遵守 带头大哥不能死，中间兄弟不能断 索引列上无计算，范围之后全失效 like百分写最右，覆盖索引不写星 不等控制还有or，索引失效要少用 var引号不能丢，SQL优化也不难 4.2 具体描述 全值匹配 怎么建索引就怎么用索引，where后面的条件越来越多精度越来越高，精度越来越高带来的就是长度和花费的代价也就越来越多 最佳左前缀法则 指的是查询从索引的最左前列开始并且不跳过索引中的列。 例如：复合索引A-&gt;B-&gt;C，如果把开头A去掉的话，B，C也就都失效了（带头大哥不能死）；如果把中间B去掉的话，则只会走索引A，而C就失效了（中间兄弟不能断）。 不在索引列上做任何操作(计算、函数、(自动or手动)类型转换)，会导致索引失效而转向全表扫描 存储引擎不能使用索引中范围条件右面的列 例如：select t from test where A=1 and B&gt;2 and C=3，那么B&gt;2后的查询条件失效。 尽量使用覆盖索引————只访问索引的查询（索引列和查询列一致），减少 SELECT *。 MySQL在使用不等于(!= 或&lt;&gt;)的时候无法使用索引会导致全表扫描 is null，is not null 也无法使用索引 like以通配符开头(&#39;%abc...&#39;)mysql索引失效回变成全表扫描的操作（使用覆盖索引可解决），只有通配符在右面(&#39;abc...%&#39;)的才能避免索引失效。 字符串不加单引号索引失效 少用or，用它来连接时会索引失效 5. in 和 exists区别及应用场景5.1 in 和 exists的区别: 如果子查询得出的结果集记录较少，主查询中的表较大且又有索引时应该用in, 反之如果外层的主查询记录较少，子查询中的表大，又有索引时使用exists。 其实我们区分in和exists主要是造成了驱动顺序的改变(这是性能变化的关键)，如果是exists，那么以外层表为驱动表，先被访问，如果是IN，那么先执行子查询，所以我们会以驱动表的快速返回为目标，那么就会考虑到索引及结果集的关系了 ，另外IN时不对NULL进行处理。 in 是把外表和内表作hash 连接，而exists是对外表作loop循环，每次loop循环再对内表进行查询。一直以来认为exists比in效率高的说法是不准确的。 5.2 not in 和not exists 如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引； 而not extsts 的子查询依然能用到表上的索引。 所以无论那个表大，用not exists都比not in要快 6. order by 和 group by 优化索引的主要作用就是查找和排序，ORDER BY 子句尽量使用Index方式排序，能避免使用FileSort方式排序，尽可能在索引列上外城排序操作，遵照索引键的最佳左前缀。 6.1 提高ORDER BY速度的技巧 ORDER BY时不要使用SELECT *，只查需要的字段。 增大sort_buffer_size参数大小（根据系统能力去提高，因为这个参数是针对每个进程的） 增大max_length_for_sort_data参数大小 6.2 GROUP BY的优化 GROUP BY实质上是先排序后进行分组，遵照索引的最佳左前缀。 当无法使用索引列，考虑增大max_length_for_sort_data和sort_buffer_size的参数设置。 WHERE 高于 HAVING，能写在WHERE解决的条件就不要去HAVING限定了。 注意：group by 表面上叫分组，但是分组之前比排序。所以说group by和order by两者排序的法则和索引优化的原则几乎是一致的。当然也有不一样的地方，group by 还有having的存在。如果group by错乱，会导致临时表的产生。(就是说group by的顺序不对，建好的索引我用不上，我内部使用了内排序产生了filesort，为了把这些数据挪出来内部建了一张临时表来进行分组) 一般性建议： 对于单值索引，尽量选择针对query过滤性更好的索引 在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段的顺序中，位置越靠左越好。 在选择组合索引的时候，尽量选择可能包含当前query中的where子句中更多字段的索引 尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的。 参考链接：https://www.zhihu.com/people/hen-six-49/activities","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"【数据库优化】MySQL性能优化基础","date":"2019-09-01T13:36:46.000Z","path":"article/20190901.html","text":"1. MySQL基础操作1.1 MySQL备份与恢复 备份：在mysql的安装目录的bin目录下有mysqldump命令，可以完成对数据库的备份。 语法：mysqldump -u 用户名 -p 数据库名&gt; 磁盘SQL文件路径 由于mysqldump命令不是sql命令，需要在dos窗口下使用。 仅仅只会备份数据库中的表和数据，恢复时需要先手动创建数据库。 恢复：先手动创建数据库：create database 数据库名 然后dos窗口：mysql -u 用户名-p 导入库名&lt; 磁盘SQL文件绝对路径 1.2 MySQL事务基础 特性(ACID)：原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。 事务隔离级别：读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 MYSQL事务处理主要有两种方法： 用 BEGIN, ROLLBACK, COMMIT来实现: BEGIN 开始一个事务 COMMIT 事务确认 ROLLBACK 事务回滚 直接用 SET 来改变 MySQL 的自动提交模式: SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交 事务并发操作出现几种问题: 丢失修改数据、读“脏”数据、数据不一致 1.3 查看设置MySQL编码 查看：mysql&gt; show variables like &#39;character%&#39; 设置: # vi /etc/my.cnf：123456789101112[mysqld] character‐set‐server=utf8collation‐server=utf8_general_cisql_mode=&#x27;NO_ENGINE_SUBSTITUTION&#x27;[mysql] default‐character‐set = utf8 [mysql.server] default‐character‐set = utf8 [mysqld_safe] default‐character‐set = utf8 [client] default‐character‐set = utf8 mysql的主配置文件: /etc/my.cnf 数据库文件存放位置: /var/lib/mysql 数据库的日志输出存放位置: /var/log/mysql 端口: Netstat –nltp 看是否能找到3306的端口 1.4 范式概念：范式就是符合某一规范级别的关系模式的集合。共有7种范式：1NF⊃2NF⊃3NF⊃BCNF⊃4NF⊃5NF⊃6NF 第一范式(1NF, First Normal Form)：字段值具有原子性,不能再分(所有关系型数据库系统都满足第一范式); 例如：姓名字段,其中姓和名是一个整体,如果区分姓和名那么必须设立两个独立字段; 第二范式(2NF, Second Normal Form)：一个表必须有主键,即每行数据都能被唯一的区分(2NF必须先满足第一范式); 第三范式(3NF, Third Normal Form)：一个表中不能包涵其他相关表中非关键字段的信息,即数据表不能有冗余字段(3NF必须先满足第二范式); 备注：往往我们在设计表中不能遵守第三范式,因为合理的沉余字段将会给我们减少join的查询; 例如：相册表中会添加图片的点击数字段,在相册图片表中也会添加图片的点击数字段; 2. SQL语句优化2.1 通过慢查日志发现有问题的SQL 查询次数多且每次查询占用时间长的sql 通常为pt-query-digest分析的前几个查询；该工具可以很清楚的看出每个SQL执行的次数及百分比等信息，执行的次数多，占比比较大的SQL IO大的sql 注意pt-query-digest分析中的Rows examine项。扫描的行数越多，IO越大。 未命中的索引的SQL 注意pt-query-digest分析中的Rows examine 和Rows Send的对比。说明该SQL的索引命中率不高，对于这种SQL，我们要重点进行关注。 通过explain查询分析SQL的执行计划, SQL的执行计划侧面反映出了SQL的执行效率， 2.2 常见SQL优化手段 函数Max()的优化 在求max的字段建索引 函数Count()的优化： Count(*):是包含null值；Count(id)：不包含null值 子查询的优化 子查询是我们在开发过程中经常使用的一种方式，在通常情况下，需要把子查询优化为join查询但在优化是需要注意关联键是否有一对多的关系，要注意重复数据(distinct去重)。 在用Join进行多表联合查询时，我们通常使用On来建立两个表的关系。其实还有一个更方便的关键字，那就是Using（如果两个表的关联字段名是一样）。 group by的优化: 最好使用同一表中的列，在子查询中分组 Limit查询的优化：Limit常用于分页处理，时常会伴随order by从句使用，因此大多时候会使用Filesorts这样会造成大量的IO问题。 优化步骤1：使用有索引的列或主键进行order by操作，因为大家知道，innodb是按照主键的逻辑顺序进行排序的。可以避免很多的IO操作。 优化步骤2：记录上次返回的主键， 在下次查询时使用主键过滤。（说明：避免了数据量大时扫描过多的记录） 注意事项：主键要顺序排序并连续的，如果主键中间空缺了某一列，或者某几列，会出现列出数据不足一页的数据；如果不连续的情况，建立一个附加的列index_id列，保证这一列数据要自增的，并添加索引即可。 3. 索引的优化3.1 索引基础概念索引是为了加速对表中数据行的检索而创建的一种分散存储的数据结构。 索引的建立是表中比较有指向性的字段，相当于目录，比如说行政区域代码，同一个地域的行政区域代码都是相同的，那么给这一列加上索引，避免让它重复扫描，从而达到优化的目的！ 创建索引：在执行create table语句时可以创建索引，也可以单独用create index或alter index来为表增加索引。不能用create index语句创建primary key索引。 在创建索引时，可以规定索引能否包含重复值。如果不包含，则索引应该创建为primary key或unique索引。对于单列惟一性索引，这保证单列不包含重复的值。对于多列惟一性索引，保证多个值的组合不重复。 primary key索引和unique索引非常类似。事实上，primary key索引仅是一个具有名称PRIMARY的unique索引。 查看索引：show index from tblname;或show keys from tblname; 3.2 索引的创建与删除 创建普通索引： 方式1：create index 索引名 on 表名(列名) 方式2：alter table 表名 add index 索引名(列名) 方式3：创建表的时候直接指定: ,index [索引名] (列名) 删除索引： drop index [索引名] on 表名 唯一索引：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。 创建唯一索引： 方式1：create unique index 索引名 on 表名(列名) 方式2：alter table 表名 add unique 索引名(列名) 方式3：创建表的时候直接指定: ,unique [索引名] (列名) 3.3 使用索引的场景 表的主键自动建立唯一索引 表的字段唯一约束 直接条件查询的字段（在SQL中用于条件约束的字段） 查询中与其它表关联的字段 查询中排序的字段（排序的字段如果通过索引去访问那将大大提高排序速度） 查询中统计或分组统计的字段 表记录太少（如果一个表只有5条记录，采用索引去访问记录的话，那首先需访问索引表，再通过索引表访问数据表，一般索引表与数据表不在同一个数据块） 经常插入、删除、修改的表（对一些经常处理的业务表应在查询允许的情况下尽量减少索引） 数据重复且分布平均的表字段（假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率大约为50%，那么对这种表A字段建索引一般不会提高数据库的查询速度。） 经常和主字段一块查询但主字段索引值比较多的表字段 对千万级MySQL数据库建立索引的事项及提高性能的手段 3.4 索引的维护及优化（重复及冗余索引）增加索引会有利于查询效率，但会降低insert，update，delete的效率，但实际上往往不是这样的，过多的索引会不但会影响使用效率，同时会影响查询效率，这是由于数据库进行查询分析时，首先要选择使用哪一个索引进行查询，如果索引过多，分析过程就会越慢，这样同样的减少查询的效率，因此我们要知道如何增加，有时候要知道维护和删除不需要的索引 重复索引：重复索引是指相同的列以相同的顺序建立的同类型的索引，如在primary key再建立唯一索引就是重复索引 冗余索引：冗余索引是指多个索引的前缀列相同，或是在联合索引中包含了主键的索引，如对于innodb来说，每一个索引后面，实际上都会包含主键，这时候我们建立的联合索引，又人为的把主键包含进去，那么这个时候就是一个冗余索引。 工具：使用**pt-duplicate-key-checker工具检查重复及冗余索引**: pt-duplicate-key-checker -uroot -padmin -h 127.0.0.1 索引维护的方法: 由于业务变更，某些索引是后续不需要使用的，就要进行删除。 在mysql中，目前只能通过慢查询日志配合pt-index-usage工具来进行索引使用情况的分析；pt-index-usage -uroot -padmin /var/lib/mysql/mysql-host-slow.log 3.5 设计MySql索引的注意事项设计好MySql的索引可以让你的数据库飞起来，大大的提高数据库效率。设计MySql索引的时候有一下几点注意： 创建索引 对于查询占主要的应用来说，索引显得尤为重要。很多时候性能问题很简单的就是因为我们忘了添加索引而造成的，或者说没有添加更为有效的索引导致。如果不加索引的话，那么查找任何哪怕只是一条特定的数据都会进行一次全表扫描，如果一张表的数据量很大而符合条件的结果又很少，那么不加索引会引起致命的性能下降。 但是也不是什么情况都非得建索引不可，比如性别可能就只有两个值，建索引不仅没什么优势，还会影响到更新速度，这被称为过度索引。 复合索引 比如有一条语句是这样的：select * from users where area=’beijing’ and age=22; 如果我们是在area和age上分别创建单个索引的话，由于mysql查询每次只能使用一个索引，所以虽然这样已经相对不做索引时全表扫描提高了很多效率，但是如果在area、age两列上创建复合索引的话将带来更高的效率。如果我们创建了(area, age,salary)的复合索引，那么其实相当于创建了(area,age,salary)、(area,age)、(area)三个索引，这被称为最佳左前缀特性。 因此我们在创建复合索引时应该将最常用作限制条件的列放在最左边，依次递减。 索引不会包含有NULL值的列 只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 使用短索引 对字符串列进行索引，如果可能应该指定一个前缀长度。 例如，如果有一个CHAR(255)的 列，如果在前10 个或20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 排序的索引问题 mysql查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。 like语句操作 一般情况下不鼓励使用like操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引，而like “aaa%”可以使用索引。 不要在列上进行运算 select * from users where YEAR(adddate) 不使用NOT IN操作 NOT IN操作都不会使用索引将进行全表扫描。NOT IN可以NOT EXISTS代替 4. MYSQL数据库设计规范与原则4.1 设计规范 命名规范 采用26个英文字母(区分大小写)和0-9的自然数(经常不需要)加上下划线’_’组成 命名简洁明确,多个单词用下划线’_’分隔,长度不超过30个字符 除非是备份数据库可以加0-9的自然数,如：&#39;user_db_20191210&#39; 表前缀可以有效的把相同关系的表显示在一起,如：&#39;user_&#39; 每个表中必须有自增主键 表与表之间的相关联字段名称要求尽可能的相同 字段类型规范 用尽量少的存储空间来存数一个字段的数据, 例如：能使用int就不要使用varchar、char,能用varchar(16)就不要使用varchar(256); IP地址最好使用int类型; 固定长度的类型最好使用char,例如：邮编; 能使用tinyint就不要使用smallint,int; 最好给每个字段一个默认值, 最好不能为null; 索引规范 命名简洁明确,例如：user_login表user_name字段的索引应为user_name_index唯一索引; 为每个表创建一个主键索引; 为每个表创建合理的索引; 建立复合索引请慎重; 4.2 设计原则 核心原则 不在数据库做运算; cpu计算务必移至业务层; 控制列数量(字段少而精,字段数建议在20以内); 平衡范式与冗余(效率优先；往往牺牲范式) 拒绝3B(拒绝大sql语句：big sql、拒绝大事务：big transaction、拒绝大批量：big batch); 字段类原则 用好数值类型(用合适的字段类型节约空间); 字符转化为数字(能转化的最好转化,同样节约空间、提高查询性能); 避免使用NULL字段(NULL字段很难查询优化、NULL字段的索引需要额外空间、NULL字段的复合索引无效); 少用text类型(尽量使用varchar代替text字段); 索引类原则 合理使用索引(改善查询,减慢更新,索引一定不是越多越好); 字符字段必须建前缀索引; 不在索引做列运算; innodb主键推荐使用自增列(主键建立聚簇索引,主键不应该被修改,字符串不应该做主键)(理解Innodb的索引保存结构就知道了); 不用外键(由程序保证约束); sql类原则 sql语句尽可能简单(一条sql只能在一个cpu运算,大语句拆小语句,减少锁时间,一条大sql可以堵死整个库); 简单的事务; 避免使用trig/func(触发器、函数不用客户端程序取而代之); 不用select *(消耗cpu,io,内存,带宽,这种程序不具有扩展性); OR改写为IN(or的效率是n级别); OR改写为UNION(mysql的索引合并很弱智); 避免负向%; 慎用count(*); limit高效分页(limit越大，效率越低); 使用union all替代union(union有去重开销); 少用连接join; 使用group by; 请使用同类型比较; 打散批量更新; 5. 数据库结构的优化5.1 选择合适的数据类型数据类型的选择，重点在于“合适”二字 使用可以存下你的数据的最小的数据类型。（时间类型数据：可以使用varchar类型，可以使用int类型，也可以使用时间戳类型） 使用简单的数据类型，int要比varchar类型在mysql处理上简单。（int类型存储时间是最好的选择） 尽可能的使用not null定义字段。（innodb的特性所决定，非not null的值，需要额外的在字段存储，同时也会增加IO和存储的开销） 尽量少用text类型，非用不可时最好考虑分表。 5.2 数据库表的范式化优化 表范式化 范式化是指数据库设计的规范，目前说道范式化一般是指第三设计范式。也就是要求数据表中不存在非关键字段对任意候选关键字段的传递函数依赖则符合第三范式。 反范式化 反范式化是指为了查询效率的考虑把原本符合第三范式的表“适当”的增加冗余，以达到优化查询效率的目的，反范式化是一种以空间来换取时间的操作。 5.3 数据库表的垂直拆分所谓的垂直拆分，就是把原来一个有很多列的表拆分成多个表，这解决了表的宽度问题。 垂直拆分原则 把不常用的字段表单独存放到一个表中。 把大字段独立存放到一个表中。 把经常一起使用的字段放到一起。 5.4 数据库表的水平拆分表的水平拆分是为了解决单表数据量过大的问题，水平拆分的表每一个表的结构都是完全一致的 水平拆分原因 如果单表的数据量达到上亿条，那么这时候我们尽管加了完美的索引，查询效率低，写入的效率也相应的降低。 如何将数据平均分为N份 对customer_id进行hash运算，如果要拆分为5个表则使用mod（customer_id，5）取出0-4个值。 针对不动的hashid把数据存储到不同的表中。 水平拆分面临的挑战 夸分区表进行数据查询 前端业务统计：业务上给不同的用户返回不同的业务信息，对分区表没有大的挑战。 统计及后台报表操作 但是对后台进行报表统计时，数据量比较大，后台统计时效性比较低，后台就用汇总表，将前后台的表拆分开。 6. 数据库系统配置优化数据库是基于操作系统的，目前大多数MySQL都是安装在linux系统之上，所以对于操作系统的一些参数配置也会影响到MySQL的性能 6.1 操作系统的优化网络方面的配置，要修改/etc/sysctl.conf 增加tcp支持的队列数 net.ipv4.tcp_max_syn_backlog = 65535 减少断开连接时，资源回收(tcp有连接状态) net.ipv4.tcp_max_tw_buckets = 8000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_fin_timeout = 10 说明： TCP是有连接状态，通过netstat查看连接状态，经常会看到timeout状态或者timewait状态连接，为了加快timewait状态的连接回收，就需要调整上面的四个参数，保持TCP连接数在一个适当的状态。 6.2 打开文件数的限制打开文件数的限制，可以使用ulimit –a查看目录的各个限制，可以修改/etc/security/limits.conf文件 limits.conf中增加以下内容以修改打开文件数量的限制（永久生效） *Soft nofile 65535 *Hard nofile 65535 如果一次有效，就要使用ulimit –n 65535即可。（默认情况是1024） 除此之外最好在MySQL服务器上关闭iptables，selinux等防火墙软件 6.3 MySQL配置文件优化Mysql可以通过启动时指定参数和使用配置文件两种方法进行配置，在大多数情况下配置文件位于/etc/my.cnf或/etc/mysql/my.cnf MySQL查找配置文件的顺序可以通过以下命令获得： /usr/sbin/mysqld --verbose --help | grep -A 1 &#39;default options&#39; 注意：如果存在多个位置存在配置文件，则后面的会覆盖前面的。 6.3.1 my.cnf常用 连接请求 参数 max_connections：最大连接数 如果服务器的并发连接请求量比较大，建议调高此值，以增加并行连接数量，MySQL会为每个连接提供连接缓冲区，连接数越多就会开销越多的内存，所以要适当调整该值，不能盲目提高设值。 数值过小会经常出现ERROR 1040: Too many connections错误，可以过’conn%’通配符查看当前状态的连接数量，以定夺该值的大小。 max_used_connections / max_connections * 100% （**理想值≈ 85%**）: 响应的连接数/最大连接数 back_log：能暂存的连接数量 如果MySQL的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。 默认数值是50，可调优为128，对于Linux系统设置范围为小于512的整数。 interactive_timeout：服务器关闭交互式连接前等待活动的秒数 默认数值是28800，可调优为7200。 wait_timeout：服务器关闭非交互连接之前等待活动的秒数 指定一个请求的最大连接时间，对于4GB左右内存的服务器可以设置为5-10。 6.3.1 my.cnf常用 缓冲区 参数 key_buffer_size: 指定索引缓冲区的大小 它决定索引处理的速度，尤其是索引读的速度。 它只对MyISAM表起作用。即使你不使用MyISAM表，但是内部的临时磁盘表是MyISAM表，也要使用该值 query_cache_size ：查询缓存的内存 使用查询缓冲，MySQL将查询结果存放在缓冲区中，今后对于同样的SELECT语句（区分大小写），将直接从缓冲区中读取结果。 通过检查状态值Qcache_*，可以知道query_cache_size设置是否合理（上述状态值可以使用SHOW STATUS LIKE ‘Qcache%’获得）。如果Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲不够的情况，如果Qcache_hits的值也非常大，则表明查询缓冲使用非常频繁，此时需要增加缓冲大小；如果Qcache_hits的值不大，则表明你的查询重复率很低，这种情况下使用查询缓冲反而会影响效率，那么可以考虑不用查询缓冲。此外，在SELECT语句中加入SQL_NO_CACHE可以明确表示不使用查询缓冲。 与查询缓冲有关的参数还有query_cache_type、query_cache_limit、query_cache_min_res_unit。 query_cache_type指定是否使用查询缓冲，可以设置为0、1、2，该变量是SESSION级的变量。 query_cache_limit指定单个查询能够使用的缓冲区大小，缺省为1M。 query_cache_min_res_unit指定分配缓冲区空间的最小单位，缺省为4K。检查状态值Qcache_free_blocks，如果该值非常大，则表明缓冲区中碎片很多，这就表明查询结果都比较小，此时需要减小query_cache_min_res_unit。 record_buffer_size：顺序扫描缓冲区大小 每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，你可能想要增加该值。 默认数值是131072(128K)，可改为16773120 (16M) read_rnd_buffer_size：随机读缓冲区大小 当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySQL会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySQL会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。一般可设置为16M sort_buffer_size：排序扫描缓冲区大小 每个需要进行排序的线程分配该大小的一个缓冲区。增加这值加速ORDER BY或GROUP BY操作。 默认数值是2097144(2M)，可改为16777208 (16M)。 join_buffer_size：联合查询缓冲区大小 联合查询操作所能使用的缓冲区大小。 record_buffer_size，read_rnd_buffer_size，sort_buffer_size，join_buffer_size为每个线程独占，也就是说，如果有100个线程连接，则占用为16M*100 table_cache：表高速缓存的大小 表高速缓存的大小。每当MySQL访问一个表时，如果在表缓冲区中还有空间，该表就被打开并放入其中，这样可以更快地访问表内容。通过检查峰值时间的状态值Open_tables和Opened_tables，可以决定是否需要增加table_cache的值。如果你发现open_tables等于table_cache，并且opened_tables在不断增长，那么你就需要增加table_cache的值了（上述状态值可以使用SHOW STATUS LIKE ‘Open%tables’获得）。注意，不能盲目地把table_cache设置成很大的值。如果设置得太高，可能会造成文件描述符不足，从而造成性能不稳定或者连接失败。 1G内存机器，推荐值是128－256。内存在4GB左右的服务器该参数可设置为256M或384M。 max_heap_table_size：用户可以创建的内存表(memory table)的大小 这个值用来计算内存表的最大行数值。这个变量支持动态改变，即set @max_heap_table_size=# 这个变量和tmp_table_size一起限制了内部内存表的大小。如果某个内部heap（堆积）表大小超过tmp_table_size，MySQL可以根据需要自动将内存中的heap表改为基于硬盘的MyISAM表。 tmp_table_size：临时表的大小 通过设置tmp_table_size选项来增加一张临时表的大小，例如做高级GROUP BY操作生成的临时表。如果调高该值，MySQL同时将增加heap表的大小，可达到提高联接查询速度的效果，建议尽量优化查询，要确保查询过程中生成的临时表在内存中，避免临时表过大导致生成基于硬盘的MyISAM表。 每次创建临时表，Created_tmp_tables增加，如果临时表大小超过tmp_table_size，则是在磁盘上创建临时表，Created_tmp_disk_tables也增加,Created_tmp_files表示MySQL服务创建的临时文件文件数 比较理想的配置是：Created_tmp_disk_tables / Created_tmp_tables * 100% &lt;= 25%比如上面的服务器Created_tmp_disk_tables / Created_tmp_tables * 100% ＝1.20%，应该相当好了 默认为16M，可调到64-256最佳，线程独占，太大可能内存不够I/O堵塞 thread_cache_size：可以复用的保存在中的线程的数量 可以复用的保存在中的线程的数量。如果有，新的线程从缓存中取得，当断开连接的时候如果有空间，客户的线置在缓存中。如果有很多新的线程，为了提高性能可以这个变量值。 通过比较 Connections和Threads_created状态的变量，可以看到这个变量的作用。默认值为110，可调优为80。 thread_concurrency：同一时间运行的线程系统提示所需数量的线程 推荐设置为服务器 CPU核数的2倍， 例如双核的CPU, 那么thread_concurrency的应该为4；2个双核的cpu, thread_concurrency的值应为8。默认为8 这个参数已经在5.7.2版本的MySQL中被移除 6.3.1 my.cnf常用 配置InnoDB的 参数 innodb_buffer_pool_size：缓冲池大小 对于InnoDB表来说，innodb_buffer_pool_size的作用就相当于key_buffer_size对于MyISAM表的作用一样。 InnoDB使用该参数指定大小的内存来缓冲数据和索引。对于单独的MySQL数据库服务器，最大可以把该值设置成物理内存的80%。 根据MySQL手册，对于2G内存的机器，推荐值是1G（50%）。 innodb_flush_log_at_trx_commit：主要控制了innodb将log buffer中的数据写入日志文件并flush磁盘的时间点，取值分别为0、1、2三个 设置为0，表示当事务提交时，不做日志写入操作，而是每秒钟将log buffer中的数据写入日志文件并flush磁盘一次； 设置为1，则在每秒钟或是每次事物的提交都会引起日志文件写入、flush磁盘的操作，确保了事务的ACID； 设置为2，每次事务提交引起写入日志文件的动作，但每秒钟完成一次flush磁盘操作。 实际测试发现，该值对插入数据的速度影响非常大，设置为2时插入10000条记录只需要2秒，设置为0时只需要1秒，而设置为1时则需要229秒。因此，MySQL手册也建议尽量将插入操作合并成一个事务，这样可以大幅提高速度。 根据MySQL手册，在允许丢失最近部分事务的危险的前提下，可以把该值设为0或2。 innodb_log_buffer_size：log缓存大小 一般为1-8M，默认为1M，对于较大的事务，可以增大缓存大小。可设置为4M或8M。 innodb_additional_mem_pool_size：内存池大小 该参数指定InnoDB用来存储数据字典和其他内部数据结构的内存池大小。缺省值是1M。通常不用太大，只要够用就行，应该与表结构的复杂度有关系。如果不够用，MySQL会在错误日志中写入一条警告信息。 根据MySQL手册，对于2G内存的机器，推荐值是20M，可适当增加。 innodb_thread_concurrency=8，推荐设置为 2*(NumCPUs+NumDisks)，默认一般为8 skip-name-resolve：禁止域名解析 禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。 但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求 7. MySQL的执行顺序MySQL的语句一共分为11步，最先执行的总是FROM操作，最后执行的是LIMIT操作。 其中每一个操作都会产生一张虚拟的表，这个虚拟的表作为一个处理的输入，只是这些虚拟的表对用户来说是透明的，但是只有最后一个虚拟的表才会被作为结果返回。 如果没有在语句中指定某一个子句，那么将会跳过相应的步骤。 ⑧select ⑨distinct &lt;字段名&gt; ①from &lt;表名&gt; ③&lt;连接类型&gt;join &lt;表名&gt; ②on&lt;连接条件&gt; ④where&lt;查询条件&gt; ④group by&lt;分组字段&gt; ⑥with&#123;cube|rollup&#125; ⑦having&lt;查询条件&gt; ⑩order by&lt;排序字段&gt; ⑪limit&lt;分页数量&gt; 查询处理的每一个阶段分析： FORM: 对FROM的左边的表和右边的表计算笛卡尔积。产生虚表VT1 ON: 对虚表VT1进行ON筛选，只有那些符合&lt;连接条件&gt;的行才会被记录在虚表VT2中。 JOIN：如果指定了OUTER JOIN（比如left join、 right join），那么保留表中未匹配的行就会作为外部行添加到虚拟表VT2中，产生虚拟表VT3, rug from子句中包含两个以上的表的话，那么就会对上一个join连接产生的结果VT3和下一个表重复执行步骤1~3这三个步骤，一直到处理完所有的表为止。 WHERE：对虚拟表VT3进行WHERE条件过滤。只有符合&lt;where查询条件&gt;的记录才会被插入到虚拟表VT4中。 GROUP BY: 根据group by子句中的列，对VT4中的记录进行分组操作，产生VT5. CUBE | ROLLUP: 对表VT5进行cube或者rollup操作，产生表VT6. HAVING： 对虚拟表VT6应用having过滤，只有符合&lt;having查询条件&gt;的记录才会被 插入到虚拟表VT7中。 8. MySQL执行引擎8.1 MyISAM存储引擎 不支持事务、也不支持外键，优势是访问速度快，对事务完整性没有 要求或者以select，insert为主的应用基本上可以用这个引擎来创建表 支持3种不同的存储格式，分别是：静态表；动态表；压缩表 静态表： 表中的字段都是非变长字段，这样每个记录都是固定长度的，优点存储非常迅速，容易缓存，出现故障容易恢复；缺点是占用的空间通常比动态表多（因为存储时会按照列的宽度定义补足空格）ps：在取数据的时候，默认会把字段后面的空格去掉，如果不注意会把数据本身带的空格也会忽略。 动态表： 记录不是固定长度的，这样存储的优点是占用的空间相对较少；缺点：频繁的更新、删除数据容易产生碎片，需要定期执行OPTIMIZE TABLE或者myisamchk-r命令来改善性能 压缩表： 因为每个记录是被单独压缩的，所以只有非常小的访问开支 8.2 InnoDB存储引擎 提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比MyISAM引擎，写的处理效率会差一些，并且会占用更多的磁盘空间以保留数据和索引。 InnoDB存储引擎的特点：支持自动增长列，支持外键约束 8.3 MEMORY存储引擎 Memory存储引擎使用存在于内存中的内容来创建表。每个memory表只实际对应一个磁盘文件，格式是.frm。memory类型的表访问非常的快，因为它的数据是放在内存中的，并且默认使用HASH索引，但是一旦服务关闭，表中的数据就会丢失掉。 MEMORY存储引擎的表可以选择使用BTREE索引或者HASH索引，两种不同类型的索引有其不同的使用范围 Hash索引优点： Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。 Hash索引缺点： 那么不精确查找呢，也很明显，因为hash算法是基于等值计算的，所以对于“like”等范围查找hash索引无效，不支持； Memory类型的存储引擎主要用于哪些内容变化不频繁的代码表，或者作为统计操作的中间结果表，便于高效地对中间结果进行分析并得到最终的统计结果，。对存储引擎为memory的表进行更新操作要谨慎，因为数据并没有实际写入到磁盘中，所以一定要对下次重新启动服务后如何获得这些修改后的数据有所考虑。 8.4 MERGE存储引擎 Merge存储引擎是一组MyISAM表的组合，这些MyISAM表必须结构完全相同，merge表本身并没有数据，对merge类型的表可以进行查询，更新，删除操作，这些操作实际上是对内部的MyISAM表进行的。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"【数据库优化】MySQL慢日志查询分析","date":"2019-08-28T16:26:43.000Z","path":"article/20190829.html","text":"同大多数关系型数据库一样，日志文件是MySQL数据库的重要组成部分。MySQL有几种不同的日志文件，通常包括错误日志文件，二进制日志，通用日志，慢查询日志，等等。这些日志可以帮助我们定位mysqld内部发生的事件，数据库性能故障，记录数据的变更历史，用户恢复数据库等等。 错误日志：记录启动、运行或停止mysqld时出现的问题。 通用日志：记录建立的客户端连接和执行的语句。 更新日志：记录更改数据的语句。该日志在MySQL 5.1中已不再使用。 二进制日志：记录所有更改数据的语句。还用于复制。 慢查询日志：记录所有执行时间超过long_query_time秒的所有查询或不使用索引的查询 Innodb日志：InnoDB redo log(记录了事务的行为，可以很好的通过其对页进行“重做”操作) 1. 开启慢查询日志开启慢查询日志，可以让MySQL记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能。通过show variables like &#39;slow_query%&#39;;查询是否开了慢查询(默认禁用OFF) 1234567mysql&gt; show variables like &#x27;%slow_query_log%&#x27;;+---------------------+------------------------------------------------------+| Variable_name | Value |+---------------------+------------------------------------------------------+| slow_query_log | OFF || slow_query_log_file | D:\\mysql-5.7.27-winx64\\data\\DESKTOP-E9F062A-slow.log |+---------------------+------------------------------------------------------+ slow_query_log 慢查询开启状态 OFF 未开启 ON 为开启slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录） 开启慢查询，需要设置slow_query_log参数。当然，如果不是调优需要的话，一般不建议开启该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志写入文件。 12mysql&gt; set global slow_query_log = 1; //设置开启或者关闭，0为关闭，1为开启mysql&gt; set global long_query_time = 3; //设置慢的阙值时间，默认10秒 如果通过终端命令设定的话，需要重新连接或新开一个会话才能看到修改值 使用set global slow_query_log 命令开启慢查询日志，只对当前数据库生效，如果Mysql重启后则会失效。如果要永久生效，必须修改my.cnf配置文件(其他系统变量也是如此) 123456[mysqld]slow_query_log = 1 #开启slow_query_log_file = /mysql-5.7.27-winx64/data/mysql-slow.log #默认host_name_show.loglong_query_time = 3 #默认10秒（查询超过多少秒才记录）log-queries-not-using-indexes = on #如果值设置为ON，则会记录所有没有利用索引的查询，一般在性能调优的时候会暂时开启。log_output = &#x27;FILE,TABLE&#x27; #输出的格式(FILE:文本, TABLE:表中, FILE,TABLE:同时输出到文本和表中) 插入一条测试慢查询 1mysql&gt; select sleep(5); 通过MySQL命令查看有多少慢查询 123456mysql&gt; show global status like &#x27;%Slow_queries%&#x27;;+---------------+-------+| Variable_name | Value |+---------------+-------+| Slow_queries | 1 |+---------------+-------+ 2. 慢查询日志分析工具 工具 一般统计 高级统计 语言 优势 针对log mysqldumpslow √ × perl mysql官方自带 slow myprofi √ × php 简单 slow mysql-log-filter √ 部分√ python 简单 slow mysql-explain-slow-log √ × perl 无 slow mysqlbinlog √ × 二进制 mysql官方自带 binary log mysqlsla √ √ perl 总能强大，使用简单，自定义能力强 所有日志，包括自定义日志 pt-query-digest √ √ perl 总能强大，使用简单，自定义能力强 所有日志，包括自定义日志 2.1 mysqldumpslow MySQL自带的慢查询日志分析工具mysqldumpslow主要功能是, 统计不同慢sql的: 出现次数(Count), 执行最长时间(Time), 累计总耗费时间(Time), 等待锁的时间(Lock), 发送给客户端的行总数(Rows), 扫描的行总数(Rows), 用户以及sql语句本身(抽象了一下格式, 比如 limit 1, 20 用 limit N,N 表示).安装后基本使用：1234mysqldumpslow -s r -t 10 /data/mysql/mysql-slow.log //得到返回记录集最多的10个SQLmysqldumpslow -s c -t 10 /data/mysql/mysql-slow.log //得到访问次数最多的10个SQL mysqldumpslow -s t -t 10 -g &quot;left join&quot; /data/mysql/mysql-slow.log //得到按照时间排序的前10条里面含有做了连接的查询SQLmysqldumpslow -s r -t 10 /data/mysql/mysql-slow.log | more //另外建议在使用这些命令时结合|和more使用，否则有可能出现爆屏情况 2.2 mysqlslahackmysql.com推出的一款日志分析工具(该网站还维护了 mysqlreport, mysqlidxchk 等比较实用的mysql工具) 整体来说, 功能非常强大. 数据报表,非常有利于分析慢查询的原因, 包括执行频率, 数据量, 查询消耗等. 安装后基本使用方法： 1mysqlsla -lt slow -sort t_sum -top 1000 /tmp/slow_query.log 结果选项说明： 总查询次数 (queries total), 去重后的sql数量 (unique), 输出报表的内容排序(sorted by), 最重大的慢sql统计信息(包括 平均执行时间, 等待锁时间, 结果行的总数, 扫描的行总数) Count, sql的执行次数及占总的slow log数量的百分比. Time, 执行时间, 包括总时间, 平均时间, 最小, 最大时间, 时间占到总慢sql时间的百分比. 95% of Time, 去除最快和最慢的sql, 覆盖率占95%的sql的执行时间. Lock Time, 等待锁的时间. 95% of Lock , 95%的慢sql等待锁时间. Rows sent, 结果行统计数量, 包括平均, 最小, 最大数量. Rows examined, 扫描的行数量. Database, 属于哪个数据库 Users, 哪个用户,IP, 占到所有用户执行的sql百分比 Query abstract, 抽象后的sql语句 Query sample, sql语句 mysqlsla常用参数说明： -log-type (-lt) type logs:通过这个参数来制定log的类型，主要有slow, general, binary, msl, udl,分析slow log时通过制定为slow -sort:t_sum:按总时间排序(默认)，c_sum:按总次数排序c_sum_p: sql语句执行次数占总执行次数的百分比。 -top:显示sql的数量，默认是10,表示按规则取排序的前多少条 –statement-filter (-sf) [+-][TYPE]:过滤sql语句的类型，比如select、update、drop，[TYPE] 有SELECT, CREATE, DROP, UPDATE, INSERT，例如”+SELECT,INSERT”，不出现的默认是-，即不包括。 -db：要处理哪个库的日志： 123# 举个例子，只取funsion数据库的select语句，并按照总时间排序，取前1000条数据# 保存到当前目录下的 slow_query.pretty.log文件中mysqlsla -lt slow -sort t_sum -sf &quot;+select&quot; -db funsion -top 1000 /tmp/slow_query.log &gt; ./slow_query.pretty.log 深度使用可参考： MySQL日志分析神器之mysqlsla 2.3 pt-query-digestpt-query-digest是用于分析mysql慢查询的一个工具，它可以分析binlog、General log、slowlog，也可以通过SHOWPROCESSLIST或者通过tcpdump抓取的MySQL协议数据来进行分析。可以把分析结果输出到文件中，分析过程是先对查询语句的条件进行参数化，然后对参数化以后的查询进行分组统计，统计出各查询的执行时间、次数、占比等，可以借助分析结果找出问题进行优化。 12# 分析最近12小时内的查询：pt-query-digest --since=12h slow.log &gt; slow_report2.log pt-query-digest语法及重要选项12345678910111213pt-query-digest [OPTIONS] [FILES] [DSN] --create-review-table 当使用--review参数把分析结果输出到表中时，如果没有表就自动创建。 --create-history-table 当使用--history参数把分析结果输出到表中时，如果没有表就自动创建。 --filter 对输入的慢查询按指定的字符串进行匹配过滤后再进行分析 --limit 限制输出结果百分比或数量，默认值是20,即将最慢的20条语句输出，如果是50%则按总响应时间占比从大到小排序，输出到总和达到50%位置截止。 --host mysql服务器地址 --user mysql用户名 --password mysql用户密码 --history 将分析结果保存到表中，分析结果比较详细，下次再使用--history时，如果存在相同的语句，且查询所在的时间区间和历史表中的不同，则会记录到数据表中，可以通过查询同一CHECKSUM来比较某类型查询的历史变化。 --review 将分析结果保存到表中，这个分析只是对查询条件进行参数化，一个类型的查询一条记录，比较简单。当下次使用--review时，如果存在相同的语句分析，就不会记录到数据表中。 --output 分析结果输出类型，值可以是report(标准分析报告)、slowlog(Mysql slow log)、json、json-anon，一般使用report，以便于阅读。 --since 从什么时间开始分析，值为字符串，可以是指定的某个”yyyy-mm-dd [hh:mm:ss]”格式的时间点，也可以是简单的一个时间值：s(秒)、h(小时)、m(分钟)、d(天)，如12h就表示从12小时前开始统计。 --until 截止时间，配合—since可以分析一段时间内的慢查询。 分析pt-query-digest输出结果 总体统计结果 Overall：总共有多少条查询 Time range：查询执行的时间范围 unique：唯一查询数量，即对查询条件进行参数化以后，总共有多少个不同的查询 total：总计 min：最小 max：最大 avg：平均 95%：把所有值从小到大排列，位置位于95%的那个数，这个数一般最具有参考价值 median：中位数，把所有值从小到大排列，位置位于中间那个数 查询分组统计结果 Rank：所有语句的排名，默认按查询时间降序排列，通过–order-by指定 Query ID：语句的ID，（去掉多余空格和文本字符，计算hash值） Response：总的响应时间 time：该查询在本次分析中总的时间占比 calls：执行次数，即本次分析总共有多少条这种类型的查询语句 R/Call：平均每次执行的响应时间 V/M：响应时间Variance-to-mean的比率 Item：查询对象 每一种查询的详细统计结果 由下面查询的详细统计结果，最上面的表格列出了执行次数、最大、最小、平均、95%等各项目的统计。 ID：查询的ID号，和上图的Query ID对应 Databases：数据库名 Users：各个用户执行的次数（占比） Query_time distribution ：查询时间分布, 长短体现区间占比，本例中1s-10s之间查询数量是10s以上的两倍。 Tables：查询中涉及到的表 Explain：SQL语句 3. explain查看执行计划在上面的慢查询中，我们已经将查询时间超过阀值的sql语句过滤了出来，explain+查询语句具体分析是哪里出了问题。MySQL 提供了一个 Explain 命令, 它可以对 select 语句进行分析, 并输出 select 执行的详细信息, 以供开发人员针对性优化. 123456789101112131415mysql&gt; explain select * from user_info where id = 2\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.06 sec) 各列的含义如下: id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_type: SELECT 查询的类型. SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. table: 查询的是哪个表 partitions: 匹配的分区 type: join 类型 type字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过type字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可. eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询. range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中. index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. possible_keys: 此次查询中可能选用的索引 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key: 此字段是 MySQL 在当前查询时所真正使用到的索引. key_len: 表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到. ref: 哪个字段或常数与 key 一起被使用 rows: 显示此查询一共扫描了多少行. 这个是一个估计值. rows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数, 这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. filtered: 表示此查询条件所过滤的数据的百分比 extra: EXplain 中的很多额外的信息会在 Extra 字段显示 Using filesort: 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. Using index: “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary: 查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化. type 类型的性能比较通常来说, 不同的 type 类型的性能关系如下: ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的. 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快. 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. 4. MySQL性能分析语句show profileQuery Profile是MySQL自带的一种Query诊断分析工具，可以完整的显示一条sql执行的各方面的详细信息，默认关闭; 看看当前的MySQL版本是否支持: show variables like &#39;profiling&#39;;或show variables like &#39;profiling%&#39;;1234567mysql&gt; show variables like &#x27;profiling%&#x27;;+------------------------+-------+| Variable_name | Value |+------------------------+-------+| profiling | OFF || profiling_history_size | 15 |+------------------------+-------+ 使用前需要开启: set profiling = 1; (1:开 / 0:关)1mysql&gt; set profiling = 1; 运行sql后，查询结果show profiles;1234567891011121314151617mysql&gt; SHOW PROFILES\\G*************************** 1. row ***************************Query_ID: 1Duration: 0.02949950 Query: explain select * from user*************************** 2. row ***************************Query_ID: 2Duration: 0.03405350 Query: select * from housedemo*************************** 3. row ***************************Query_ID: 3Duration: 0.07813800 Query: select * from house*************************** 4. row ***************************Query_ID: 4Duration: 0.00018150 Query: show prifiles 诊断SQL, show profile Type io for query Query_ID LIMIT部分的用法与SELECT中LIMIT子句一致，不赘述。 Type是可选的，取值范围可以如下： ALL 显示所有性能信息 BLOCK IO 显示块IO操作的次数 CONTEXT SWITCHES 显示上下文切换次数，不管是主动还是被动 CPU 显示用户CPU时间、系统CPU时间 IPC 显示发送和接收的消息数量 MEMORY [暂未实现] PAGE FAULTS 显示页错误数量 SOURCE 显示源码中的函数名称与位置 SWAPS 显示SWAP的次数 1234567891011121314151617181920mysql&gt; show profile cpu,block io for query 3;+----------------------+----------+----------+------------+--------------+---------------+| Status | Duration | CPU_user | CPU_system | Block_ops_in | Block_ops_out |+----------------------+----------+----------+------------+--------------+---------------+| starting | 0.000077 | 0.000000 | 0.000000 | NULL | NULL || checking permissions | 0.000013 | 0.000000 | 0.000000 | NULL | NULL || Opening tables | 0.031992 | 0.000000 | 0.000000 | NULL | NULL || init | 0.000059 | 0.000000 | 0.000000 | NULL | NULL || System lock | 0.000016 | 0.000000 | 0.000000 | NULL | NULL || optimizing | 0.000007 | 0.000000 | 0.000000 | NULL | NULL || statistics | 0.000017 | 0.000000 | 0.000000 | NULL | NULL || preparing | 0.008535 | 0.000000 | 0.000000 | NULL | NULL || executing | 0.000016 | 0.000000 | 0.000000 | NULL | NULL || Sending data | 0.037234 | 0.000000 | 0.000000 | NULL | NULL || end | 0.000011 | 0.000000 | 0.000000 | NULL | NULL || query end | 0.000012 | 0.000000 | 0.000000 | NULL | NULL || closing tables | 0.000014 | 0.000000 | 0.000000 | NULL | NULL || freeing items | 0.000108 | 0.000000 | 0.000000 | NULL | NULL || cleaning up | 0.000030 | 0.000000 | 0.000000 | NULL | NULL |+----------------------+----------+----------+------------+--------------+---------------+ 从图中可以看到开始，打开表，加载，关闭表，释放资源、记录日志，清理的你工作，在这完全可以看到一条SQL的完整生命周期。 日常开发需要注意 如果show profile … for query id；出现了如下四个，则必须优化这条sql。 converting HEAP to MyISAM 查询结果太大， 内存都不够用了网磁盘上搬了 Creating tmp table 创建临时表 拷贝数据到临时表：假设要查询两百万数据，刚好匹配的条件有一百万，恰巧要把这一百万的数据拷贝到临时表，然后再把数据推送给用户，最后再把临时表删掉，这个时候就是导致SQL变慢的罪魁祸首 用完再删除 Copying to tmp table on disk 把内存中临时表复制到磁盘，危险！！！ locked 5. 全局查询日志切记：永远不要再生产环境开启这个功能。全局查询日志有时也能帮助我们来调SQL。但是，切记，这家伙只能在测试环境使用，绝不可以在生产环境使用。 命令启用12mysql&gt; set global general_log=1; #开启后会把所有的SQL偷偷的记录mysql&gt; set global log_output=&#x27;TABLE&#x27;; 配置启用, 在MySQL的my.cnf中，设置如下：123456#开启general_log=1#记录日志文件的路径general_log_file=/path/logfile#输出格式log_output=file 此后，你所编写的SQL语句，将会记录到MySQL库里的general_log表，可以用下面的命令查看。1select * from mysql.general_log; 场景：如果需要做系统的定案分析(今天下午2点-3点出的故障），如果要观察和复现的话，可以在测试环境下模拟一遍，然后把所有的问题复现一下。那么用general_log这个表来收集什么时间段发生了什么样的SQL，帮助我们定位收集。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"【Java知识梳理】深入JVM(三)-内存模型JMM 与 锁机制","date":"2019-08-27T09:30:45.000Z","path":"article/20190827.html","text":"Java内存模型(Java Memory Model)Java Memory Model(JMM)描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量存储到内存中和从内存中读取变量这样的底层细节(可见性,有序性,原子性)。 所有的变量都存储在主内存中 每个线程都有自己的独立的工作内存，里面保存该线程使用到的变量的副本(来自主内存的拷贝) JMM规定： 线程对共享变量的所有操作都必须在自己的工作内存中进行，不能直接从主内存中读写。 不同线程之间无法直接访问其他线程工作内存中的变量，线程间变量值的传递需要通过主内存来完成。 1. JMM-同步八种操作JMM模型下,线程间通信必须要经过主内存。JMM数据原子操作:lock -&gt; read -&gt; load -&gt; use -&gt; assign -&gt; store -&gt; write -&gt; unlock lock（锁定）：将主内存变量加锁，标识为线程独占状态 read（读取）：从主内存读取数据到工作内存 load（载入）：将读取的数据写入工作内存 use（使用）：将工作内存数据传递给执行引擎来计算 assign（赋值）：将计算好的值赋值给工作内存的变量 store（存储）：把工作内存数据存储到主内存 write（写入）：把store过来的变量值赋值给主内存的变量 unlock（解锁）：将主内存变量解锁，释放后的变量才可以被其他线程锁定。 在执行上述八种基本操作时，必须满足如下规则： 从主复制到工作,必须按顺序执行read-&gt;load操作; 从工作同步到主内存,必须按顺序执行store-&gt;write操作; 但不保证必须是连续执行 不允许read-&gt;load、store-&gt;write操作之一单独出现 assign操作改变数据后必须同步到主内存,不允许把没有发生过assign操作的数据同步到主内存 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量 一个变量在同一时刻只允许一条线程对其进行lock操作,lock和unlock必须成对出现 lock操作会清空工作内存中此变量的值，执行引擎使用前需要重新执行load或assign操作初始化变量的值 不允许去unlock一个未被锁定 或 被其他线程锁定的变量 unlock之前，必须先同步到主内存中（执行store和write操作） 2. JMM-原子性和数据库事务中的原子性一样，满足原子性特性的操作是不可中断的，要么全部执行成功要么全部执行失败。Synchronized能够实现：原子性(同步) 和 可见性 JMM关于synchronized的两条规定： 线程解锁前，必须把共享变量的最新值刷新到主内存中 线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从内存中重新读取最新的值（注意：加锁与解锁需要是同一把锁） 线程执行互斥代码的过程： 获得互斥锁 清空工作内存 从主内存拷贝变量的最新副本到工作内存 执行代码 将更改后的共享变量的值刷新到主内存 释放互斥锁 2. JMM-可见性多个线程访问同一个共享变量时，其中一个线程对这个共享变量值的修改，其他线程能够立刻获得修改以后的值。volatile能够实现可见性，但不保证原子性 深入来说：通过加入内存屏障和禁止重排序优化来实现的。 对volatile变量执行写操作时，会在写操作后加入一条store屏蔽指令 对volatile变量执行读操作时，会在读操作前加入一条load屏蔽指令 通俗地讲：volatile变量在每次被线程访问时，都强迫从主内存中重读该变量的值，而当该变量发生变化时，又会强迫线程将最新的值刷新到主内存。这样任何时刻，不同的线程总能看到该变量的最新值。 线程写volatile变量的过程： 改变线程工作内存中volatile变量副本的值 将改变后的副本的值从工作内存刷新到主内存 线程读volatile变量的过程： 从主内存中读取volatile变量的最新值到线程的工作内存中 从工作内存中读取volatile变量的副本 2.1 happens-before规则在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这2个操作之间必须要存在happens-before关系。 定义: 如果一个操作在另一个操作之前发生(happens-before),那么第一个操作的执行结果将对第二个操作可见, 而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。 happens-before规则： 程序次序规则：在一个线程内一段代码的执行结果是有序的。就是还会指令重排，但是随便它怎么排，结果是按照我们代码的顺序生成的不会变！ 锁定规则：一个unLock操作先行发生于后面对同一个锁的lock操作；论是单线程还是多线程，必须要先释放锁，然后其他线程才能进行lock操作 volatile变量规则：就是如果一个线程先去写一个volatile变量，然后一个线程去读这个变量，那么这个写操作的结果一定对读的这个线程可见。 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 线程启动规则：在主线程A执行过程中，启动子线程B，那么线程A在启动子线程B之前对共享变量的修改结果对线程B可见 线程终止规则：在主线程A执行过程中，子线程B终止，那么线程B在终止之前对共享变量的修改结果在线程A中可见。 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程代码检测到中断事件的发生，可以通过Thread.interrupted()检测到是否发生中断 对象终结规则：这个也简单的，就是一个对象的初始化的完成，也就是构造函数执行的结束一定 happens-before它的finalize()方法。 3. JMM-有序性编译器和处理器为了优化程序性能而对指令序列进行重排序，也就是你编写的代码顺序和最终执行的指令顺序是不一致的，重排序可能会导致多线程程序出现内存可见性问题。 我们编写的源代码到最终执行的指令，会经过三种重排序: 源代码–&gt;编译器优化重排序–&gt;指令级并行重排序–&gt;内存系统重排序–&gt;最终执行的指令 3.1 as-if-serial语义as-if-serial语义：不管怎么重排序(编译器和处理器为了提高并行度做的优化),(单线程)程序的执行结果不会改变。编译器、runtime和处理器都必须遵守as-if-serial语义。多线程中程序交错执行时, 重排序可能造成内存可见性问题, 可能会改变程序的执行结果。 有序性规则表现在以下两种场景: 线程内和线程间 线程内: 指令会按照一种“串行”(as-if-serial)的方式执行，此种方式已经应用于顺序编程语言。 线程间: 一个线程“观察”到其他线程并发地执行非同步的代码时，任何代码都有可能交叉执行。唯一起作用的约束是：对于同步方法，同步块以及volatile字段的操作仍维持相对有序。 As-if-serial只是保障单线程不会出问题，所以有序性保障，可以理解为把As-if-serial扩展到多线程，那么在多线程中也不会出现问题 从底层的角度来看，是借助于处理器提供的相关指令内存屏障来实现的 对于Java语言本身来说，Java已经帮我们与底层打交道，我们不会直接接触内存屏障指令，java提供的关键字synchronized和volatile，可以达到这个效果，保障有序性（借助于显式锁Lock也是一样的，Lock逻辑与synchronized一致） 3.2 著名的双检锁(double-checked locking)模式实现单例1234567891011121314151617public class Singleton &#123; // volatile保证happens-before规则,重排序被禁止 private volatile static Singleton INSTANCE = null; private Singleton() &#123;&#125; public Singleton getInstance() &#123; // 实例没创建,才进入内部的synchronized代码块 if (null == INSTANCE) &#123; synchronized (Singleton.class) &#123; // 判断其他线程是否已经创建实例 if (null == INSTANCE) &#123; INSTANCE = new Singleton(); &#125; &#125; &#125; return INSTANCE; &#125;&#125; 如果不用volatile修饰INSTANCE,可能造成访问的是一个初始化未完成的对象; 使用了volatile关键字后，重排序被禁止，所有的写（write）操作都将发生在读（read）操作之前。 4. 锁机制 锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁（但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级）。 锁的状态是通过对象监视器在对象头中的字段来表明的。 四种状态会随着竞争的情况逐渐升级，而且是不可逆的过程，即不可降级。 这四种状态都不是Java语言中的锁，而是Jvm为了提高锁的获取与释放效率而做的优化(使用synchronized时)。 4.1 对象头Mark Mark Word,对象头的标记,32位: 描述对象的hash,锁信息,垃圾回收标记,分代年龄 指向锁记录的指针 指向monitor的指针 GC标记 偏向锁线程ID 4.2 偏向锁Java偏向锁(Biased Locking)是Java6引入的一项多线程优化 大部分情况锁是没有竞争的,所以可以通过偏向锁来提高性能; 所谓偏向,就是偏心,即锁会偏向于当前已经占有锁的线程,总是由同一线程多次获得; 会在对象头和栈帧中的锁记录里存储锁偏向的线程ID 只要没有竞争,获得偏向锁的线程,在将来进入同步块,不需要做同步 当其他线程请求相同的锁时,偏向模式结束 -XX:+UseBiasedLocking(默认开启) 在竞争激烈的场合,偏向锁会增加系统负担 4.3 轻量级锁轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。 普通的锁处理性能不够理想,轻量级锁是一种快速的锁定方法. 过程: 如果对象没有被锁定: 将对象头的Mark指针保存到锁对象中 将对象头设置为指向锁的指针(在线程栈空间中) 如果轻量级锁失败,表示存在竞争,升级为重量级锁(常规锁) 在没有锁竞争的情况下,减少传统锁使用OS互斥量产生的性能损耗 在竞争激烈的场合,轻量级锁会多做很多额外操作,导致性能下降 4.4 自旋锁自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。 当竞争存在时,如果线程可以很快获得锁,那么可以不在OS层挂起线程,让线程做几个空操作(自旋) 如果同步块很长,自旋失败,会降低系统性能 如果同步块很短,自旋成功,节省线程挂起切换时间,提升系统性能 4.5 重量级锁重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。 4.6 synchronized的执行过程： 检测Mark Word里面是不是当前线程的ID，如果是，表示当前线程处于偏向锁 如果不是，则使用CAS将当前线程的ID替换Mard Word，如果成功则表示当前线程获得偏向锁，置偏向标志位1 如果失败，则说明发生竞争，撤销偏向锁，进而升级为轻量级锁。 当前线程使用CAS将对象头的Mark Word替换为锁记录指针，如果成功，当前线程获得锁 如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 如果自旋成功则依然处于轻量级状态。 如果自旋失败，则升级为重量级锁。 以上几种锁都是JVM自己内部实现，当我们执行synchronized同步块的时候jvm会根据启用的锁和当前线程的争用情况，决定如何执行同步操作； 5. Java语言层面对锁的优化 减少锁持有时间 不需要同步执行的代码，能不放在同步快里面执行就不要放在同步快内，可以让锁尽快释放； 减少锁的粒度 它的思想是将物理上的一个锁，拆成逻辑上的多个锁，增加并行度，从而降低锁竞争。它的思想也是用空间来换时间； java中很多数据结构都是采用这种方法提高并发操作的效率： ConcurrentHashMap: 使用Segment数组,Segment继承自ReenTrantLock，所以每个Segment就是个可重入锁，每个Segment 有一个HashEntry&lt; K,V &gt;数组用来存放数据，put操作时，先确定往哪个Segment放数据，只需要锁定这个Segment，执行put，其它的Segment不会被锁定；所以数组中有多少个Segment就允许同一时刻多少个线程存放数据，这样增加了并发能力。 LongAdder:实现思路也类似ConcurrentHashMap，LongAdder有一个根据当前并发状况动态改变的Cell数组，Cell对象里面有一个long类型的value用来存储值;开始没有并发争用的时候或者是cells数组正在初始化的时候，会使用cas来将值累加到成员变量的base上，在并发争用的情况下，LongAdder会初始化cells数组，在Cell数组中选定一个Cell加锁，数组有多少个cell，就允许同时有多少线程进行修改，最后将数组中每个Cell中的value相加，在加上base的值，就是最终的值；cell数组还能根据当前线程争用情况进行扩容，初始长度为2，每次扩容会增长一倍，直到扩容到大于等于cpu数量就不再扩容，这也就是为什么LongAdder比cas和AtomicInteger效率要高的原因，后面两者都是volatile+cas实现的，他们的竞争维度是1，LongAdder的竞争维度为“Cell个数+1”为什么要+1？因为它还有一个base，如果竞争不到锁还会尝试将数值加到base上； 拆锁的粒度不能无限拆，最多可以将一个锁拆为当前CPU数量即可； 锁粗化 大部分情况下我们是要让锁的粒度最小化，锁的粗化则是要增大锁的粒度(如:循环内的操作); 锁分离 使用读写锁: ReentrantReadWriteLock 是一个读写锁，读操作加读锁，可以并发读，写操作使用写锁，只能单线程写； 读写分离: CopyOnWriteArrayList 、CopyOnWriteArraySet CopyOnWrite容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器 CopyOnWrite并发容器用于读多写少的并发场景，因为，读的时候没有锁，但是对其进行更改的时候是会加锁的，否则会导致多个线程同时复制出多个副本，各自修改各自的； LinkedBlockingQueue: LinkedBlockingQueue也体现了这样的思想，在队列头入队，在队列尾出队，入队和出队使用不同的锁，相对于LinkedBlockingArray只有一个锁效率要高； 锁消除 在即时编译时,如果发现不可能被共享的对象,则可以消除对象的锁操作 无锁(如CAS) 如果需要同步的操作执行速度非常快，并且线程竞争并不激烈，这时候使用CAS效率会更高，因为加锁会导致线程的上下文切换，如果上下文切换的耗时比同步操作本身更耗时，且线程对资源的竞争不激烈，使用volatiled+CAS操作会是非常高效的选择； 消除缓存行的伪共享 除了我们在代码中使用的同步锁和jvm自己内置的同步锁外，还有一种隐藏的锁就是缓存行，它也被称为性能杀手。在多核cup的处理器中，每个cup都有自己独占的一级缓存、二级缓存，甚至还有一个共享的三级缓存，为了提高性能，cpu读写数据是以缓存行为最小单元读写的；32位的cpu缓存行为32字节，64位cup的缓存行为64字节，这就导致了一些问题。 6. CAS与原子类CAS即Compare and Swap翻译过来就是比较并替换, 它体现了一种乐观锁的思想 (synchronized为悲观锁思想); 结合CAS和volatile可以实现无锁并发(非阻塞同步),适用于竞争不激烈,多核CPU的场景下(竞争激烈,重试频繁发生会影响效率); CAS算法涉及到三个操作数: 内存值V, 旧值A, 新值B; 当且仅当V==A时，CAS用新值B来更新V，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 CAS底层依赖一个Unsafe类来直接调用操作系统底层的CAS指令; 6.1 Unsafe类java中CAS操作依赖于Unsafe类，Unsafe类所有方法都是native的，直接调用操作系统底层资源执行相应任务，它可以像C一样操作内存指针，是非线程安全的。 Unsafe里的CAS 操作相关实现: compareAndSwapObject,compareAndSwapInt,compareAndSwapLong12345//第一个参数o为给定对象，offset为对象内存的偏移量，通过这个偏移量迅速定位字段并设置或获取该字段的值，//expected表示期望值，x表示要设置的值，下面3个方法都通过CAS原子指令执行操作。public final native boolean compareAndSwapObject(Object o, long offset,Object expected, Object x);public final native boolean compareAndSwapInt(Object o, long offset,int expected,int x);public final native boolean compareAndSwapLong(Object o, long offset,long expected,long x); 6.2 原子操作类并发包JUC(java.util.concurrent)中的原子操作类(Atomic系列),底层是基于CAS + volatile实现的. AtomicBoolean：原子更新布尔类型 AtomicInteger：原子更新整型 AtomicLong：原子更新长整型 下面看AtomicInteger类的部分源码： 1234567891011121314public class AtomicInteger extends Number implements java.io.Serializable&#123; //获取指针类Unsafe private static final Unsafe unsafe = Unsafe.getUnsafe(); //省略...获取内存偏移量等 //如果当前值为expect，则设置为update(当前值指的是value变量) public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; //当前值加1返回旧值，底层CAS操作 public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1); &#125; //省略...其他方法&#125; AtomicInteger基本是基于Unsafe类中CAS相关操作实现的，是无锁操作。再看Unsafe类中的getAndAddInt()方法，该方法执行一个CAS操作，保证线程安全。 12345678//Unsafe类中的getAndAddInt方法(JDK8)public final int getAndAddInt(Object o, long offset, int delta) &#123; int v; do &#123; v = getIntVolatile(o, offset); &#125; while (!compareAndSwapInt(o, offset, v, v + delta)); return v;&#125; 可看出getAndAddInt通过一个while循环不断的重试更新要设置的值，直到成功为止，调用的是Unsafe类中的compareAndSwapInt方法，是一个CAS操作方法。 6.3 CAS操作中可能会带来的ABA问题ABA问题是指在CAS操作时，其他线程将变量值A改为了B，但是又被改回了A，等到本线程使用期望值A与当前变量进行比较时，发现变量A没有变，于是CAS就将A值进行了交换操作，但是实际上该值已经被其他线程改变过，这与乐观锁的设计思想不符合。 无法正确判断这个变量是否已被修改过，一般称这种情况为ABA问题。 ABA问题一般不会有太大影响，产生几率也比较小。但是并不排除极特殊场景下会造成影响，因此需要解决方法： AtomicStampedReference类 AtomicMarkableReference类 AtomicStampedReference类: 一个带有时间戳的对象引用，每次修改时，不但会设置新的值，还会记录修改时间。在下一次更新时，不但会对比当前值和期望值，还会对比当前时间和期望值对应的修改时间，只有二者都相同，才会做出更新。解决了反复读写时，无法预知值是否已被修改的窘境。 底层实现为：一个键值对Pair存储数据和时间戳，并构造volatile修饰的私有实例；两者都符合预期才会调用Unsafe的compareAndSwapObject方法执行数值和时间戳替换。 AtomicMarkableReference类: 一个boolean值的标识，true和false两种切换状态表示是否被修改。不靠谱。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"}]},{"title":"【Java知识梳理】深入JVM(二)-类文件结构 与 类加载机制","date":"2019-08-25T11:29:48.000Z","path":"article/20190825.html","text":"1. 类文件结构Class文件是一组以 8 位字节为基础单位的二进制流，各个数据严格按照顺序紧凑的排列在 Class 文件中，中间无任何分隔符，这使得整个 Class 文件中存储的内容几乎全部都是程序运行的必要数据，没有空隙存在。当遇到需要占用 8 位字节以上空间的数据项时，会按照高位在前的方式分割成若干个 8 位字节进行存储。Java 虚拟机规范规定 Class 文件格式采用一种类似与 C 语言结构体的伪结构体来存储数据，这种伪结构体中只有两种数据类型：无符号数和表。 无符号数：属于基本数据类型，以u1、u2、u4、u8来代表1个字节、2个字节、4个字节、8个字节的无符号数， 无符号数可以用来描述数字、索引引用、数量值或者按照UTF-8编码构成字符串值。 表：由多个无符号数或者其他表作为数据项构成的复合数据类型，所有表都习惯性地以「_info」结尾。表用于描述有层次关系的复合结构的数据，整个 Class 文件就是一张表。 根据 Java 虚拟机规范，类文件由单个 ClassFile 结构组成： 123456789101112131415161718ClassFile &#123; u4 magic; //Class文件的标志(魔数) u2 minor_version; //Class的小版本号 u2 major_version; //Class的大版本号 u2 constant_pool_count; //常量池的数量 cp_info constant_pool[constant_pool_count-1];//常量池 u2 access_flags; //Class的访问标记 u2 this_class; //当前类 u2 super_class; //父类 u2 interfaces_count; //接口 u2 interfaces[interfaces_count];//一个类可以实现多个接口 u2 fields_count; //Class文件的字段属性 field_info fields[fields_count]; //一个类会可以有个字段 u2 methods_count; //Class文件的方法数量 method_info methods[methods_count];//一个类可以有个多个方法 u2 attributes_count; //此类的属性表中的属性数 attribute_info attributes[attributes_count];//属性表集合&#125; 1.1 魔数 (Magic Number) Class文件的0~3字节(前四个字节: ca fe ba be) 作用: 确定这个文件是否为一个能被虚拟机接收的Class文件 1.2 Class文件版本 4~7字节, 其中45次版本号,67主版本号(如jdk8主版本号是: 00 34) 1.3 常量池 8~9字节表示16进制常量池数量,其后紧跟具体常量池, 常量池的数量是 constant_pool_count-1（常量池计数器是从1开始计数的，将第0项常量空出来是有特殊考虑的，索引值为0代表“不引用任何一个常量池项”） 常量池主要存放两大常量: 字面量和符号引用 字面量: Java语言层面的常量概念(String,final等) 符号引用: 编译原理方面的概念(类和接口的全限定名\\字段的名称和描述符\\方法的名称和描述符) 常量池中每一项常量都是一个表，这14种表有一个共同的特点：开始的第一位是一个 u1 类型的标志位 -tag 来标识常量的类型，代表当前这个常量属于哪种常量类型 .class文件可以通过javap -v class类名 指令来看一下其常量池中的信息(javap -v class类名-&gt; temp.txt ：将结果输出到 temp.txt 文件) 1.4 类的访问标志与继承信息 在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口，是否为 public 或者 abstract 类型，如果是类的话是否声明为 final 等等. access_flags中一共有16个标志位可以使用，当前只定义了其中的8个，没有使用到的标志位要求一律为0。 1.5 当前类索引(this),父类索引(super)与接口索引集合(interfaces) 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，由于 Java 语言的单继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。 接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按implents(如果这个类本身是接口的话则是extends) 后的接口顺序从左到右排列在接口索引集合中。 1.6 成员变量信息(Feild) 字段表（field info）用于描述接口或类中声明的变量。字段包括类级变量以及实例变量，但不包括在方法内部声明的局部变量。 字段信息包括：字段的作用域（public、private、protected修饰符）、是实例变量还是类变量（static修饰符）、可变性（final）、并发可见性（volatile修饰符，是否强制从主内存读写）、可否被序列化（transient修饰符）、字段数据类型（基本类型、对象、数组）、字段名称，以上修饰符都是布尔类型。 方法和字段的描述符作用是用来描述字段的数据类型、方法的参数列表（包括数量、类型以及顺序）和返回值。 根描述规则，基本数据类型（byte、char、double、float、int、long、short、boolean）以及代表无返回值的void类型都用一个大写字符来表示，对象类型使用字符L加对象的全限定名来表示。 B: 基本类型byte C: 基本类型char D: 基本类型double F: 基本类型float I: 基本类型 J: 基本类型long S: 基本类型short Z: 基本类型boolean V: 特殊类型void L: 对象类型，如Ljava/lang/Object 1.7 方法信息(Method) methods_count 表示方法的数量，而 method_info 表示的方法表。 Class 文件存储格式中对方法的描述与对字段的描述几乎采用了完全一致的方式。方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项。 1.8 附加属性信息 attributes_count表示属性表中的属性个数, attribute_info 表示属性表 在 Class 文件，字段表，方法表中都可以携带自己的属性表集合，以用于描述某些场景专有的信息。与 Class 文件中其它的数据项目要求的顺序、长度和内容不同，属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写 入自己定义的属性信息，Java 虚拟机运行时会忽略掉它不认识的属性。 2. 字节码指令Java字节码指令就是Java虚拟机能够识别、可执行的指令，可以说是Jvm的最小执行单元。javac命令会将Java源文件编译成字节码文件，即.class文件，其中就包含了大量的字节码指令，javap命令可以解析字节码(.class文件)，将字节码内部逻辑以可读的方式呈现出来 (javap -v -p HelloWorld)。 按指令的功能分为如下几类： 存储和加载类指令：主要包括load系列(将一个局部变量加载到操作数栈)、store系列(将一个数值从操作数栈存储到局部变量表)和ldc/push/const系列(将一个常量加载到操作数栈)，主要用于在局部变量表、操作数栈和常量池三者之间进行数据调度； 例如: iload_0表示从当前栈帧局部变量表中0号位置取int类型的数值加载到操作数栈 对象操作指令（创建与读写访问）：比如我们刚刚的putfield和getfield就属于读写访问的指令，此外还有putstatic/getstatic，还有new系列指令，以及instanceof等指令。 操作数栈管理指令：如pop和dup，他们只对操作数栈进行操作。 类型转换指令和运算指令：如add(加)/sub(减)/mul(乘)/div(除)/l2i/d2f等系列指令，实际上这类指令一般也只对操作数栈进行操作。 控制跳转指令：这类里包含常用的if系列指令以及goto类指令。 方法调用和返回指令：主要包括invoke系列指令和return系列指令。这类指令也意味这一个方法空间的开辟和结束，即invoke会唤醒一个新的java方法小宇宙（新的栈和局部变量表），而return则意味着这个宇宙的结束回收。 从指令操作的数据类型来讲：指令开头或尾部的一些字母，就往往表明了它所能操作的数据类型： a对应对象，表示指令操作对象性数据，比如aload和astore、areturn等等。 i对应整形。也就有iload，istore等i系列指令。 f对应浮点型，l对应long，b对应byte，d对应double，c对应char。 ia对应int array，aa对应object array，da对应double array。 3. 编译期处理(语法糖)语法糖: 指Java编译器把.java源码编译为.class字节码过程中,自动生成和转换的一些代码. 如:默认构造器,自动拆装箱等. 默认构造器: public class Candy&#123;&#125; 编译后为: public class Candy&#123;public Candy()&#123;super();&#125;&#125; 自动拆装箱: Integer x=1;int y=x; 编译后为: Integer x=Integer.valueOf(1);int y=x.intValue(); 泛型擦除: 擦除的是字节码上的泛型信息. 泛型反射: 通过反射获得泛型信息 可变参数: String... args 可以是一个String[] args foreach: 集合相当于获取迭代器Iterator switch: Jdk7开始可以配合String和枚举 switch-String: 执行了两遍switch,第一遍根据字符串的hashCode和equals将字符串转换为相应的byte类型,第二遍利用byte执行比较. switch-枚举: 会为当前类生成一个静态内部类(合成类,仅JVM使用,对我们不可见),用来映射枚举类的枚举编号(从0开始)与数组元素的关系,数组大小即为枚举元素的个数,里面存储case用来对比的数字,根据这个数字执行switch 枚举类: 继承Enum并且用final修饰类,构造方法私有,枚举量被编译成本类的final类变量,定义私有静态枚举量数组$VALUES,静态方法values()用来返回定义的枚举量数组的clone(),静态方法valueOf()调用父类valueOf(本类.class,名称)根据类型和名称得到相应实例 try-with-resources: 无论try块的异常还是关闭资源时的异常都不会丢。可以在 try-with-resources 语句中同时处理多个资源。 在 Java 7/8 ，try-with-resources 语句中必须声明要关闭的资源。通过这种方式声明的资源属于隐式 final。 Java 9 中甚至能使用预先创建的资源，只要所引用的资源声明为 final 或者是 effective final。 在幕后施展魔法的是 AutoCloseable 或者 Closeable 接口，它们与 try-with-resources 语句协同工作。 重写桥接: 子类重写方法返回值可以是父类返回值的子类,JVM内部使用了桥接方法(synthetic bridge修饰)重写父类方法并返回子类重写的同名方法,并且没有命名冲突,仅对jvm可见. 匿名内部类: 内部创建了final修饰的实现类, 匿名内部类引用局部变量时,局部变量必须是final的:因为内部创建实现类时,将值赋给其对象的valx属性,valx属性没有机会再跟着一起变化. 4. 类加载阶段 隐式加载：new 显式加载：loadClass、forName等(需要调用Class的newInstance方法获取实例) 类的装载阶段：**加载 --&gt; 链接 --&gt; 初始化** 加载：通过Classloader加载class文件字节码，生成class对象 链接：校验–&gt;准备–&gt;解析 校验：检查加载的Class的正确性和安全性 准备：为变量分配存储空间并设置类变量初始值 解析：JVM将常量池内的符号引用转换为直接引用 初始化：执行类变量赋值和静态代码块 4.1 加载 将类的字节码载入方法区中,内部采用C++的instanceKlass描述java类, 它的重要field有: _java_mirror:Java类的镜像, _super:父类, _field:成员变量, _methods:方法, _constants:常量池, _class_loader:类加载器, _vtable:虚方法表, _itable:接口方法表 如果这个类还有父类没加载,先加载父类 加载和链接可能是交替运行的 instanceKlass这样的元数据是存储在方法区(元空间),但_java_mirror存储在堆中; 可通过HSDB工具查看. 4.2 链接 验证: 验证类是否符合JVM规范,安全性检查 准备: 为static变量分配空间,设置默认值 jdk7开始, static变量存储于_java_mirror末尾, jdk7之前是instanceKlass末尾. static变量分配空间和赋值是两个步骤, 分配空间在准备阶段完成,赋值在初始化阶段完成 如果static变量是final的基本类型或字符串常量,那么编译阶段值就确定了,赋值在准备阶段完成 如果static变量是final的引用类型,那么赋值还是会在初始化阶段完成 解析: 将常量池中的符号引用解析为直接引用(确切知道类,方法,属性在内存中的位置) 4.3 初始化 初始化即调用&lt;cinit&gt;()V方法,虚拟机会保证这个类的[构造方法]线程安全 发生的时机: 概括的说,类初始化是[懒惰的] main方法所在的类的,总会被首先初始化 首次访问这个类的静态变量或静态方法时 子类初始化, 如果父类没有初始化,会引发 子类访问父类静态变量, 只会触发父类的初始化 Class.forName 和 new操作 导致初始化 不会导致类初始化的情况 访问类的static final 静态常量(基本类型和字符串常量)不会触发初始化 类对象.class 不会 创建该类的数组 不会 类加载器的loadClass方法 不会 Class.forName的第二个参数为false时 不会 4.4 应用实例-懒惰初始化单例模式(线程安全)1234567891011class Singleton&#123; private Singleton()&#123;&#125; // 内部类中保存单例 private static class LazyHolder&#123; private static final Singleton SINGLETON = new Singleton(); &#125; // 第一次调用getInstance,才会导致内部类加载和初始化其静态成员 public static Singleton getInstance()&#123; return LazyHolder.SINGLETON; &#125;&#125; 5. 类加载器以JDK8为例: 名称 加载哪的类 说明 Bootstrap ClassLoader JAVA_HOME/jre/lib 启动类加载器, 最顶层, 打印显示为null Extension ClassLoader JAVA_HOME/jre/lib/ext 扩展类加载器, 第二级, 打印显示为$ExtClassLoader Application ClassLoader classpath 应用程序类加载器, 第三级, 打印显示为$AppClassLoader 自定义类加载器 自定义 上级为Application 5.1 类加载器-双亲委派机制 类加载器在接到加载类的请求时，首先将加载任务委托给上级加载器，依次递归，如果上级加载器可以完成类加载任务，就成功返回；只有上级加载器无法完成此加载任务时，才自己去加载。 这种双亲委派模式的好处，一个可以避免类的重复加载，另外也避免了java的核心API被篡改。 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * loadClass方法的实现方式 */protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded //【1】 检查该类是否已经加载 Class c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; //【2】 有上级的话,委派上级 loadClass c = parent.loadClass(name, false); &#125; else &#123; //【3】 如果没有上级了(ExtClassLoader),则委派BootstrapClassLoader c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order to find the class. long t1 = System.nanoTime(); //【4】 每一级都找不到,调用findClass(每个类加载器自己扩展)来加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 5.2 线程上下文类加载器 Java 提供了很多服务提供者接口(Service Provider Interface，SPI),允许第三方为这些接口提供实现(常见的 SPI 有 JDBC、JCE、JNDI、JAXP 和 JBI 等)。 SPI接口中的代码经常需要加载具体的实现类; SPI的接口由Java核心库来提供，实现类可能是作为Java应用所依赖的jar包被包含进来，可以通过类路径（CLASSPATH）来找到。 SPI的接口是Java核心库的一部分，是由引导类加载器来加载的；引导类加载器是无法找到SPI的实现类的,这时候需要抛弃双亲委派加载链模式，使用线程上下文里的类加载器加载类。 类 java.lang.Thread中的方法 getContextClassLoader()和 setContextClassLoader(ClassLoader cl)用来获取和设置线程的上下文类加载器。 Java默认的 线程上下文类加载器 是 应用程序类加载器(AppClassLoader)。 5.3 何时使用Thread.getContextClassLoader()? 总的说来动态加载资源时，一般只有两种选择，当前类加载器和线程上下文类加载器。当前类加载器是指当前方法所在类的加载器。这个类加载器是运行时类解析使用的加载器，Class.forName(String)和Class.getResource(String)也使用该类加载器。代码中X.class的写法使用的类加载器也是这个类加载器。 该如何选择类加载器？ 如若代码是限于某些特定框架，这些框架有着特定加载规则，则不要做任何改动，让框架开发者来保证其工作（比如应用服务器提供商，尽管他们并不能总是做对）。如在Web应用和EJB中，要使用Class.gerResource来加载资源。 在其他情况下，我们可以自己来选择最合适的类加载器。可以使用策略模式来设计选择机制。其思想是将“总是使用上下文类加载器”或者“总是使用当前类加载器”的决策同具体实现逻辑分离开。往往设计之初是很难预测何种类加载策略是合适的，该设计能够让你可以后来修改类加载策略。 一般来说，上下文类加载器要比当前类加载器更适合于框架编程，而当前类加载器则更适合于业务逻辑编程。 5.4 类加载器与Web容器以 Apache Tomcat 来说，每个 Web 应用都有一个对应的类加载器实例。该类加载器也使用代理模式，所不同的是它是首先尝试去加载某个类，如果找不到再代理给父类加载器。这与一般类加载器的顺序是相反的。这是 Java Servlet 规范中的推荐做法，其目的是使得 Web 应用自己的类的优先级高于 Web 容器提供的类。这种代理模式的一个例外是：Java 核心库的类是不在查找范围之内的。这也是为了保证 Java 核心库的类型安全。 绝大多数情况下，Web 应用的开发人员不需要考虑与类加载器相关的细节。下面给出几条简单的原则： 每个 Web 应用自己的 Java 类文件和使用的库的 jar 包，分别放在 WEB-INF/classes和 WEB-INF/lib目录下面。 多个应用共享的 Java 类文件和 jar 包，分别放在 Web 容器指定的由所有 Web 应用共享的目录下面。 当出现找不到类的错误时，检查当前类的类加载器和当前线程的上下文类加载器是否正确。 5.5 自定义类加载器 什么时候需要自定义类加载器 加载非classpath路径的任意路径类文件 都是通过接口来使用实现,希望解耦时,常用于框架设计 这些类希望予以隔离,不同应用的同名类都可以加载,不冲突,常见于tomcat容器 如何自定义类加载器 继承ClassLoader类 重写findClass(String className)方法 读取(加载)类文件的字节码。 调用ClassLoader超类的defineClass方法，向虚拟机提供字节码。 使用者调用该自定义类加载器的loadClass方法 1234567891011121314151617181920212223import java.io.IOException;import java.nio.file.Files;import java.nio.file.Paths;public class MyClassLoader extends ClassLoader &#123; /** * @param name 类名称 */ @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; try &#123; String cname = &quot;E:\\\\myclasspath\\\\&quot; + name.replace(&#x27;.&#x27;, &#x27;/&#x27;) + &quot;.class&quot;; byte[] classBytes = Files.readAllBytes(Paths.get(cname)); Class&lt;?&gt; cl = defineClass(name, classBytes, 0, classBytes.length); if (cl == null) &#123; throw new ClassNotFoundException(name); &#125; return cl; &#125; catch (IOException e) &#123; System.out.print(e); throw new ClassNotFoundException(name); &#125; &#125;&#125; 6. 运行期JVM自动优化Java程序最初是通过解释器进行解释执行的，当程序需要迅速启动和执行时，解释器可以首先发挥作用，省去编译时间，立即执行；当程序运行后，随着时间的推移，编译器逐渐发挥作用，把越来越多的代码编译成本地代码，获得更高的执行效率。解释执行节约内存，编译执行提升效率。 同时，解释器可以作为编译器激进优化时的一个“逃生门”，让编译器根据概率选择一些大多数时候都能提升运行速度的优化手段，当激进优化的假设不成立，则通过逆优化退回到解释状态继续执行。HotSpot虚拟机中内置了两个即时编译器，分别称为Client Compiler(C1编译器)**和Server Compiler(C2编译器)**，默认采用Interpreter(解释器)与其中一个编译器直接配合的方式工作，使用哪个编译器取决于虚拟机运行的模式，也可以自己去指定。 分层编译策略, JVM将执行状态分成了5个层次: 0层, 解释执行 1层, 使用C1即时编译器编译执行(不带profiling) 2层, 使用C1即时编译器编译执行(带基本的profiling) 3层, 使用C1即时编译器编译执行(带完全的profiling) 4层, 使用C2即时编译器编译执行 profiling是指在运行过程中收集一些程序执行状态的数据,例如[方法的调用次数],[循环的回边次数]等 即时编译器(JIT)与解释器的区别 解释器是将字节码解释为机器码,下次即便遇到相同的字节码,仍会执行重复的解释 JIT是将一些字节码编译为机器码并存入CodeCache,下次遇到相同的代码,直接执行,无需再编译 解释器是将字节码解释为针对所有平台都通用的机器码 JIT会根据平台类型,生成平台特定的机器码 对于占据大部分的不常用的代码,我们无需耗费时间将其编译成机器码,而是采用解释执行的方式运行;另一方面,对于占据小部分的热点代码,我们则可以将其编译成机器码,以达到理想的运行速度; 执行效率: Interpreter &lt; C1 &lt; C2, 总的目标是发现热点代码(hotpot名称的由来)优化之. 6.1 公共子表达式消除如果一个表达式E已经计算过了，并且先前的计算到现在E中所有变量的值都没有发生变化，那么E的这次出现就成为了公共表达式，可以直接用之前的结果替换。例：int d = (c * b) * 12 + a + (a + b * c) =&gt; int d = E * 12 + a + (a + E) 6.2 数组边界检查消除Java语言中访问数组元素都要进行上下界的范围检查，每次读写都有一次条件判定操作，这无疑是一种负担。编译器只要通过数据流分析就可以判定循环变量的取值范围永远在数组长度以内，那么整个循环中就可以把上下界检查消除，这样可以省很多次的条件判断操作。 6.3 方法内联方法内联能去除方法调用的成本，同时也为其他优化建立了良好的基础，因此各种编译器一般会把内联优化放在优化序列的最靠前位置，然而由于Java对象的方法默认都是虚方法，在编译期无法确定方法版本，就无法内联。 因此方法调用都需要在运行时进行多态选择，为了解决虚方法的内联问题，Java虚拟机团队引入了“类型继承关系分析(CHA)”的技术。 在内联时，若是非虚方法，则可以直接内联 遇到虚方法，首先根据CHA判断此方法是否有多个目标版本，若只有一个，可以直接内联，但是需要预留一个“逃生门”，称为守护内联，若在程序的后续执行过程中，加载了导致继承关系发生变化的新类，就需要抛弃已经编译的代码，退回到解释状态执行，或者重新编译。 若CHA判断此方法有多个目标版本，则编译器会使用“内联缓存”，第一次调用缓存记录下方法接收者的版本信息，并且每次调用都比较版本，若一致则可以一直使用，若不一致则取消内联，查找虚方法表进行方法分派。 6.4 逃逸分析分析对象动态作用域，当一个方法被定以后，它可能被外部方法所引用，称为方法逃逸，甚至还有可能被外部线程访问到，称为线程逃逸。 若能证明一个对象不会逃逸到方法或线程之外，这可以通过栈上分配、同步消除、标量替换来进行优化。 栈上分配：如果确定一个对象不会逃逸，则可以让它分配在栈上，对象所占用的内存空间就可以随栈帧出栈而销毁。这样可以减小垃圾收集系统的压力。 同步消除：线程同步相对耗时，如果确定一个变量不会逃逸出线程，那这个变量的读写不会有竞争，则对这个变量实施的同步措施也就可以消除掉。 标量替换：如果逃逸分析证明一个对象不会被外部访问，并且这个对象可以被拆散的话，那么程序真正执行的时候可以不创建这个对象，改为直接创建它的成员变量，这样就可以在栈上分配。 7. 反射机制简单说，反射机制是程序在运行时能够获取自身的信息。在java中，只要给定类的名字，那么就可以通过反射机制来获得类的所有信息。Class反射对象描述的是类的语义结构，通过class对象，可以获取构造器，成员变量，方法等类元素的反射对象，并且可以用编程的方法通过这些反射对象对目标对象进行操作。这些反射类在java.lang.reflect包中定义，下面是最主要的三个类： Constructor：类的构造函数反射类： 通过Class#getConstructors()方法可以获得类的所有构造函数的反射对象数组。 其中最主要的方法是newInstance(Object[] args),通过该方法可以创建一个对象类的实例，功能和new一样。在jdk5.0之后，提供了newInstance(Object…args)更为灵活。 Method：类方法的反射类。 通过Class#getDeclaredMethods()方法可以获取所有方法的反射类对象数组Method[].其中最主要的方法是: invoke(String name,class parameterTypes),和invoke(Object obj,Object…args)。同时也还有很多其他方法 Class getReturnType（）：获取方法的返回值类型 Class[] getParameterTypes（）：获取方法的参数数组 Field：类成员变量的反射类， 通过Class#getDeclareFields（）可以获取类成员变量反射的数组。 Class#getDeclareField（String name）获取某特定名称的反射对象。 最主要的方法是：set(Object obj,Object value),为目标对象的成员变量赋值。如果是基础类型还可以这样赋值setInt(),setString()… java还提供了包的反射类和注解的反射类。 总结:java反射体系保证了通过程序化的方式访问目标对象的所有元素，对于private 和protected成员变量或者方法，也是可以访问的。 7.1 反射中，Class.forName和classloader的区别 Class.forName()得到的Class是完成初始化的 而ClassLoader.loadClass()得到的Class是还没有链接的。 Spring IoC为了加快初始化速度，因此大量使用了延时加载技术。而使用classloader不需要执行类中的初始化代码，可以加快加载速度，把类的初始化工作留到实际使用到这个类的时候。 7.2 哪里用到反射机制？ JDBC中，利用反射动态加载了数据库驱动程序。 Web服务器中利用反射调用了Sevlet的服务方法。 Eclispe等开发工具利用反射动态刨析对象的类型与结构，动态提示对象的属性和方法。 很多框架都用到反射机制，注入属性，调用方法，如Spring。 7.3 反射机制的优缺点？优点：可以动态执行，在运行期间根据业务功能动态执行方法、访问属性，最大限度发挥了java的灵活性。缺点：对性能有影响，这类操作总是慢于直接执行java代码。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"}]},{"title":"【Java知识梳理】深入JVM(一)-运行时数据区 与 垃圾回收机制","date":"2019-08-23T15:15:19.000Z","path":"article/20190823.html","text":"Java虚拟机运行时数据区 程序计数器（Program Counter Register） 本地方法栈（Native Method Stack） Java虚拟机栈（VM Stack） Java堆（Heap）（线程共享） 方法区（Method Area）（线程共享） Java运行过程 Java源代码 经过Javac编译成 字节码（bytecode).class文件; 在运行时，通过 虚拟机(JVM)内嵌的解释器 将字节码转换成为最终的机器码。 常见的JVM，都提供了 JIT(Just-In-Time)编译器，也就是通常所说的动态编译器，JIT能够在运行时将热点代码编译成机器码，所以准确的说Java代码会解释执行或编译执行。 1.程序计数器（Program Counter Register） 线程私有 不会内存溢出 作用：记住下一条JVM指令的执行地址。 2.Java虚拟机栈（VM Stack） 线程私有 LIFO（后进先出） 存储栈帧，支撑Java方法的调用、执行和退出 可能出现OutOfMemoryError异常（如果被设计成动态扩展，而扩展又未申请到足够的内存抛出）和StackOverflowError异常（如线程请求的栈深度大于最大深度抛出） 2.1 栈帧（Frame） Java虚拟机栈中存储的内容，它被用于存储数据和部分过程结构的数据结构，同时也被用来处理动态链接、方法返回值 和 异常分派 一个完整的栈帧包含：局部变量表、操作数栈、动态连接信息、方法正常完成和异常完成的信息 每个栈由多个栈帧组成，对应着每次方法调用时所占用的内存 每个线程只能有一个活动栈帧，对应着当前正在执行的那个方法 2.2 局部变量表 由若干个Slot组成，长度由编译期决定 单个Slot可以储存一个类型为boolean、byte、char、short、float、reference、returnAddress 的数据，两个Slot可以存储一个类型为long或double的数据 局部变量表用于方法间参数的传递，以及方法执行过程中存储基础数据类型的值和对象的引用 2.3 操作数栈 一个后进先出栈，由若干个Entry组成，长度由编译期决定 单个Entry即可以存储一个Java虚拟机中定义的任意数据类型的值，包括long和double类型，但是存储long和double类型的Entry深度为2，其他类型深度为1 在方法执行过程中，栈帧用于存储计算参数和计算结果；在方法调用时，操作数栈也用来准备调用方法的参数以及接收方法返回结果 2.4 栈的内存溢出（StackOverflowError） 栈帧过多导致内存溢出（方法的递归调用） 栈帧过大导致内存溢出 JSON数据转换可能导致内存溢出（可用@JsonIgnore忽略不能转换的属性） 2.5 线程诊断 案例1：cpu占用过高 Linux下，top打印所有进程，筛选cpu占用高的进程号，如：32655 用ps H -eo pid,tid,%cpu | grep 32655打印32655的所有线程，定位到具体cpu占用过高的线程 jstack 进程id打印该线程的所有线程详情 将线程id换算成16进制，对比打印出的线程详情，定位到具体线程，进一步定位到源代码具体代码行号。 案例2：程序运行很长时间没有结果 前面步骤同上，jstack 进程id打印该线程的所有线程详情 在最后一段找到了 Found one Java-level deadlock，定位死锁的具体行号。 3.本地方法栈（Native Method Stack） 线程私有 LIFO（后进先出） 支撑Native方法的调用、执行和退出 可能出现OutOfMemoryError异常 和 StackOverflowError异常 有一些虚拟机（如HotSpot）将Java虚拟机栈和本地方法栈合并实现 3.1 Java虚拟机栈和本地方法栈可能发生的异常情况： 如果线程请求分配的栈容量超过Java虚拟机栈允许的最大容量时，Java虚拟机将会抛出一个StackOverflowError异常 如果Java虚拟机栈可以动态扩展，并且扩展的动作已经尝试过，但是目前无法申请到足够的内存去完成扩展，或者在建立新的线程时没有足够的内存去创建对应的虚拟机栈，那么Java虚拟机将会抛出一个OutOfMemoryError异常。 4.Java堆（Heap） 全局共享 通常是Java虚拟机中最大的一块内存区域 作用是作为Java对象的主要存储区域（通过new创建的对象都会使用堆内存） 有垃圾回收机制 4.1 Java堆可能发生的异常 如果实际所需的堆超过了自动内存管理系统能提供的最大容量，那Java虚拟机将会抛出一个OutOfMemoryError异常。 4.2 堆内存诊断 jps工具：查看当前系统中有哪些Java进程 jmap工具：查看堆内存占用情况jmap -head 进程id jconsole工具：图形界面，多功能的监测工具，可以连续监测 案例：垃圾回收后，内存占用仍然很高 jps工具定位进程，jmap -head 进程id查看堆使用情况， 可以用jconsole工具手动执行GC 用jvirsualvm抓取堆dump(快照，抓取堆里面有哪些类型的对象及个数等信息) 4.3 字符串常量池 (StringTable) 在JDK6.0及之前版本，字符串常量池是放在Perm Gen区(也就是方法区)中； 在JDK7.0版本，字符串常量池被移到了堆中 字符串手动入池: 调用String.intern() 5.方法区（Method Area） 全局共享 作用是存储Java类的结构信息 JVMS不要求该区域实现自动内存管理，但是商用Java虚拟机都能够自动管理该区域内存 在JDK1.8后，方法区由元空间实现 方法区内存溢出场景：spring、mabatis等动态加载类的场景使用不当会导致方法区内存溢出 5.1 运行时常量池 全局共享 是方法区的一部分 作用是存储Java类文件常量池中的符号信息 可能出现OutOfMemoryError异常 5.2 永久代与方法区 在JDK1.2~6，HotSpot使用永久代实现方法区 在JDK1.7，开始移除（符号表被到Native Heap，字符串常量和类的静态引用被移到Java Head中） 在JDK1.8，永久代被元空间（Metaspace）所替代 6.直接内存 全局共享 并非JVMS定义的标准Java运行时内存区域, 属于操作系统内存 JDK1.4引入NIO，目的是避免Java堆 和 Native堆 中来回 复制数据 带来的性能损耗。 能被自动管理，但是在检测手段上可能会由一些简陋 可能出现OutOfMemoryError异常 常用于NIO操作时，用于数据缓冲区 分配回收成本高，但读写性能高，不受JVM内存回收管理 7.可回收对象的判定 引用计数法：给对象添加一个引用计数器，每当有一个地方引用它时，计数器就+1，当引用失效就-1，任何时候计数器为0时就是可回收对象。 可达性分析：通过一系列名为GC Roots的对象作为起始点，从这些根节点开始向下搜索，搜索所走过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链相连时，则称该对象是不可达的。 目前主流Java虚拟机中并没有选用引用计数法，其中最重要的原因是它很难解决循环引用问题 7.1 Java语言中的GC Roots 在虚拟机栈（栈帧中的本地变量表）中的引用的对象。 在方法区中的类静态属性引用的对象。 在方法区中的常量引用的对象。 在本地方法栈中JNI（即一般说的Native方法）的引用对象。 7.2 Java引用类型 强引用：Java中默认声明的就是强引用 垃圾回收器将永远不会回收被【强引用】对象，哪怕内存不足时，JVM也会直接抛出OutOfMemoryError，不会去回收。可以赋值为null中断强引用。 软引用（SoftReference）：用来描述一些非必需但仍有用的对象，用java.lang.ref.SoftReference类来表示软引用 垃圾回收后，在内存不足时会再次触发垃圾回收，回收【软引用】对象，仍不足，才会抛出内存溢出异常。可以配合引用队列来释放软引用自身。 弱引用（WeakReference）：用 java.lang.ref.WeakReference 来表示弱引用 垃圾回收器将永远都会回收被【弱引用】对象，无论内存是否足够。可以配合引用队列来释放弱引用自身。 虚引用（PhantomReference）：最弱的一种引用关系，用 PhantomReference 类来表示 必须配合引用队列使用，主要配合ByteBuffer使用，被引用对象回收时，会将虚引用入队，由Reference Handler线程调用虚引用相关方法释放直接内存。 8.垃圾回收算法 标记清除算法（Mark-Sweep） 标记整理算法(Mark-Compact) 复制算法（copying） 8.1 分代垃圾回收（Java堆分为新生代和老年代） 对象首先分配在新生代的Eden区 新生代空间不足时，触发 Minor GC，Eden区和From幸存区(Survivor)存活的对象使用coping复制到To幸存区中，存活的年龄+1 并且交换From和To。 Minor GC会引发 STW(Stop the world)，暂停其他用户的线程，等垃圾回收结束后，用户线程才恢复运行 当对象寿命超过阈值时，会晋升至老年代，最大寿命15(4bit) 当老年代空间不足，会先尝试触发 Minor GC，如果之后空间仍不足，那么触发 Full GC，STW的时间更长 8.2 相关JVM参数 堆初始大小： -Xms 堆最大大小： -Xmx 或 -XX:MaxHeapSize=size 新生代大小： -Xmn 或 (-XX:NewSize=size + -XX:MaxNewSize=size) 幸存区比例（动态）： -XX:InitialSurvivorRatio=ratio 和 -XX:+UseAdaptiveSizePolicy 幸存区比例： -XX:SurvivorRatio=ratio 晋升阈值： -XX:MaxTenuringThreshold=threshold 晋升详情： -XX:+PrintTenuringDistribution GC详情： -XX:+PrintGCDetils -verbose:gc FullGC 前 MinorGC： -XX:+ScavengeBeforeFullGC 9.垃圾回收器 串行（开启：-XX:+UseSerialGC=Serial + SerialOld） 单线程 适合堆内存较小，适合个人电脑 吞吐量优先 多线程 堆内存较大，多核CPU 让单位时间内，总STW的时间最短 响应时间优先 多线程 堆内存较大，多核CPU 尽量让单次STW的时间最短 9.1 吞吐量优先（并行）回收器 开启(默认开启)： -XX:+UseParallelGC ~ -XX:+UseParallelOldGC 动态调整堆大小：-XX:+UseAdaptiveSizePolicy 目标吞吐量：-XX:GCTimeRatio=ratio 最大暂停时间的目标值：-XX:MaxGCPauseMillis=ms 线程数：-XX:ParallelGCThreads=n 9.2 响应时间优先（并发）回收器可以和用户线程并发执行，工作在老年代 开启：-XX:+UseConcMarkSweepGC 配合 -XX:UseParNewGC ~ SerialOld 并行和并发线程数：-XX:ParallelGCThreads=n ~ -XX:ConsGCThreads=threads 回收时机（内存占比）:-XX:CMSInitiatingOccupancyFraction=percent 重新标记前对新生代先做垃圾回收：-XX:+CMSScavengeBeforeRemark 9.3 G1（Garbage First）（并发） G1回收器 适用场景 同时注重 吞吐量(Throughput)和低延迟(Low latency)，默认暂停目标是200ms 超大堆内存，会将堆划分为多个大小相等的区域(Region) 整体上是标记+整理算法，两个区域之间是复制算法 相关JVM参数 开启（JDK9默认）：-XX:+UseG1GC 区域大小：-XX:G1HeapRegionSize=size 最大暂停时间：-XX:MaxGCPauseMillis=time G1垃圾回收阶段（三个阶段循环） **Young Collection**：新生代GC（会STW） **Young Collection + Concurrent Mark**： 在YoungGC时会进行GC Root的初始标记 老年代占用堆空间比例达到阈值值，进行并发标记(不会STW)，由下面的JVM参数决定 -XX:InitiatingHeadOccupancyPercent=percent(默认45%) **Mixed Collection**：会对Eden、Survivor、Old进行全面垃圾回收 最终标记(Remark)会STW 拷贝存活(Evacuation)会STW 为达到最大暂停时间短的目标，Old区是优先回收垃圾最多的区域 9.4 Minor GC 和 Full GC SerialGC 新生代内存不足：Minor GC 老年代内存不足：Full GC ParallelGC 新生代内存不足：Minor GC 老年代内存不足：Full GC CMS 新生代内存不足：Minor GC 老年代内存不足：分两种情况（回收速度高于内存产生速度不会触发Full GC） G1 新生代内存不足：Minor GC 老年代内存不足：分两种情况（回收速度高于内存产生速度不会触发Full GC） Minor GC：当Eden区满时，触发Minor GC Full GC： System.gc()方法的调用 老年代空间不足 方法区空间不足 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、From幸存区 向 To幸存区 复制时，对象大小大于To区可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 10.垃圾回收调优 调优领域：内存、锁竞争、CPU占用、IO 调优目标：【低延迟】还是【高吞吐量】（高吞吐量:ParallelGC，低延迟:CMS,G1,ZGC） 最快的GC是不发生GC：查看Full GC前后的内存占用（内存数据太多？数据表示太臃肿？内存泄漏？） 新生代调优：new操作内存分配廉价、死亡对象回收代价是零、大部分对象用过即死、MinorGC时间远低于FullGC 10.1 新生代调优 理想情况：新生代能容纳所有【并发量*(请求-响应)】的数据 幸存区大到能够保留【当前活跃对象+需要晋升对象】 【晋升阈值配置】得当，让长时间存活的对象尽快晋升 调整最大晋升阈值：-XX:MaxTenuringThreshold=threshold 打印晋升详情：-XX:+PrintTenuringDistribution 10.2 老年代调优以CMS为例： CMS的老年代内存越大越好（避免浮动垃圾引起的并发失败） 先尝试不做调优，如果没有FullGC那么已经OK，否则先尝试调优新生代 观察发生Full GC时老年代内存占用，将老年代内存预设调大1/4~1/3 -XX:CMSInitiatingOccupancyPercent=percent 10.3 调优案例 案例1：FullGC 和 MinorGC频繁 可能原因：空间紧张，若业务高峰期时，新生代空间紧张，幸存区的晋升阈值会降低，大量本来生存短对象晋升老年区，进一步触发老年代FullGC的频繁发生 解决方法：经过分析，观察堆空间大小，先试着增大新生代内存，同时增大幸存区的空间以及晋升阈值。 案例2：请求高峰期发生了FullGC，单次暂停时间特别长（CMS） 查看日志，看CMS哪个阶段暂停时间长（重新标记阶段），解决：打开开关参数CMSScavengeBeforeRemark 重新标记前对新生代先做垃圾回收：-XX:+CMSScavengeBeforeRemark 10.4 G1调优最佳实践 不要设置新生代和老年代的大小 G1收集器在运行的时候会调整新生代和老年代的大小。通过改变代的大小来调整对象晋升的速度以及晋升年龄，从而达到我们为收集器设置的暂停时间目标。设置了新生代大小相当于放弃了G1为我们做的自动调优。我们需要做的只是设置整个堆内存的大小，剩下的交给G1自己去分配各个代的大小。 不断调优暂停时间指标 通过XX:MaxGCPauseMillis=x可以设置启动应用程序暂停的时间，G1在运行的时候会根据这个参数选择CSet来满足响应时间的设置。一般情况下这个值设置到100ms或者200ms都是可以的(不同情况下会不一样)，但如果设置成50ms就不太合理。暂停时间设置的太短，就会导致出现G1跟不上垃圾产生的速度。最终退化成Full GC。所以对这个参数的调优是一个持续的过程，逐步调整到最佳状态。 关注Evacuation Failure Evacuation Failure类似于CMS里面的晋升失败，堆空间的垃圾太多导致无法完成Region之间的拷贝，于是不得不退化成Full GC来做一次全局范围内的垃圾收集。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"}]},{"title":"【Java知识梳理】网络协议","date":"2019-08-15T13:04:33.000Z","path":"article/20190815.html","text":"1. 分层网络协议 OSI七层网络协议：物理层，数据链路层，网络层，传输层(TCP/UDP)，会话层，表示层，应用层 TCP/IP协议分层(可以理解为OSI的一种实现)：网络接口层，网络层，传输层(TCP/UDP)，应用层 2. TCP通信协议简介： 面向连接的、可靠的、基于字节流的 传输层通信协议 将应用层的数据流分割成报文段并发送给目标节点的TCP层 数据包都有序号，对方收到则发送ACK确认，未收到则重传 使用校验和来校验数据在传输过程中是否有误 报文头中的ACK(确认序号标志)，SYN(同步序号，用于建立连接过程) 3. TCP建立连接的三次握手 第一次：建立连接时，客户端发送SYN包(syn=j)到服务器，并进入SYS_SEND状态，等待服务器确认； 第二次：服务器收到SYN包，必须确认客户的SYN(ack=j+1)，同时自己也发送一个SYN包(syn=k)，即 SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次：客户端收到SYN+ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务端进入ESTABLISHED状态，完成三次握手。 4. 为什么需要三次握手 为了初始化Sequence Number的初始值（通信双方要互相通知对方自己的Sequence Number，要作为以后数据通信的序号，以保证接收到的数据不会因为网络传输问题而乱序，TCP会用这个序号拼接数据） 5. 首次握手的隐患—SYN超时 服务端收到客户端的SYN，回复SYN-ACK的时候未收到ACK确认 服务端不断尝试(重发SYN-ACK)直至超时，Linux默认等待63秒才断开连接(默认重试5次，重试间隔1s开始，每次翻倍，即1+2+4+8+16+32=63) 可能遭受SYN Flood的风险(syn攻击，又称为ddos攻击) 6. 什么是SYN Flood攻击 客户端恶意的向某个服务器端口发送大量的SYN包，则可以使服务器打开大量的半开连接，分配TCB，从而消耗大量的服务器资源，同时也使得正常的连接请求无法被相应。而攻击发起方的资源消耗相比较可忽略不计。 SYN Flood是当前最流行的DoS（拒绝服务攻击）与DDoS（分布式拒绝服务攻击）的方式之一。 7. Linux针对SYN Flood的防护措施 SYN队列满后，通过tcp_syncookies参数回发SYN Cookies 若为正常连接则客户端会回发SYN Cookies，直接建立连接 8. 建立连接后，客户端出现故障怎么办（保活机制） 向对方发送保活探测报文，如果未收到响应则继续发送 尝试次数达到保活探测树仍未收到响应则中断连接 9. TCP终止连接的四次挥手（以客户端主动为例） 第一次：客户端发送一个FIN(seq=u)，用来关闭客户端到服务器的数据传送，客户端进入FIN_WAIT_1状态； 第二次：服务器收到FIN，发回一个ACK(ack=u+1)，确认序号为收到的序号+1(和SYN一样，一个FIN将占用一个序号)，服务端进入CLOSE_WAIT状态； 第三次：服务端发送一个FIN(seq=w)，用来关闭服务端到客户端的数据传送，服务端进入LAST_ACK状态； 第四次：客户端收到FIN，发回一个ACK(ack=w+1)，将确认序号设置为收到序号+1，客户端进入TIME_WAIT状态，服务端进入CLOSED状态，完成四次挥手。 10. 存在TIME_WAIT状态的原因 保证TCP全双工连接的可靠释放，确保有足够时间让对方收到ACK包 避免新旧来凝结混淆，使旧数据包在网络中因过期而失效 11. 为什么需要四次挥手 因为全双工，发送方和接收方都需要FIN报文和ACK报文 12. 服务器出现大量CLOSE_WAIT状态的原因 对方关闭socket连接，我方忙于读或写，没有及时关闭连接 检查代码，特别是释放资源的代码 检查配置，特别是处理请求的线程配置 13. UDP简介 面向非连接 不维护连接状态，支持同时向多个客户端传输相同消息 数据包报头只有8个字节，额外开销小 吞吐量只受限于数据生成速率、传输速率以及机器性能 尽最大努力交付，不保证可靠性，不需要维持复杂的链接状态表 面向报文，不对应用程序提交的报文信息进行拆分或者合并 14. TCP和UDP的区别 面向连接 vs 无连接 可靠性和有序性 vs 不保证 全双工的字节流 vs 全双工的数据报 效率低 vs 速度快 重量级 vs 轻量级 15. Http协议简介 基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等），Hyper Text Transfer Protocol（超文本传输协议）的缩写。 简单快速：客户向服务器请求服务时，只需传送请求方法(GET、HEAD、POST等)和路径。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。 灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。 无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快 支持B/S及C/S模式。 16. HTTP 请求/响应的步骤 客户端连接到Web服务器 一个HTTP客户端，通常是浏览器，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。例如，https://chaooo.github.io。 发送HTTP请求 通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。 服务器接受请求并返回HTTP响应 Web服务器解析请求，定位请求资源。服务器将资源复本写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。 释放连接TCP连接 若connection 模式为close，则服务器主动关闭TCP连接，客户端被动关闭连接，释放TCP连接;若connection 模式为keepalive，则该连接会保持一段时间，在该时间内可以继续接收请求; 客户端浏览器解析HTML内容 客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据HTML，根据HTML的语法对其进行格式化，并在浏览器窗口中显示。 17. 在浏览器地址栏键入URL，按下回车之后会经历以下流程： 浏览器向 DNS 服务器请求解析该 URL 中的域名所对应的 IP 地址; 解析出 IP 地址后，根据该 IP 地址和默认端口 80，和服务器建立TCP连接; 浏览器发出读取文件(URL 中域名后面部分对应的文件)的HTTP 请求，该请求报文作为 TCP 三次握手的第三个报文的数据发送给服务器; 服务器对浏览器请求作出响应，并把对应的 html 文本发送给浏览器; 释放 TCP连接; 浏览器将该 html 文本并显示内容; 18. HTTP之状态码 状态代码有三位数字组成，第一个数字定义了响应的类别，共分五种类别: 1xx：指示信息–表示请求已接收，继续处理 2xx：成功–表示请求已被成功接收、理解、接受 3xx：重定向–要完成请求必须进行更进一步的操作 4xx：客户端错误–请求有语法错误或请求无法实现 5xx：服务器端错误–服务器未能实现合法的请求 常见状态码： 200 OK //客户端请求成功 400 Bad Request //客户端请求有语法错误，不能被服务器所理解 401 Unauthorized //请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden //服务器收到请求，但是拒绝提供服务 404 Not Found //请求资源不存在，eg：输入了错误的URL 500 Internal Server Error //服务器发生不可预期的错误 503 Server Unavailable //服务器当前不能处理客户端的请求，一段时间后可能恢复正常 19. HTTPS和HTTP的区别： https协议需要到CA申请证书(收费)，http不需要。 https密文传输，http明文传输。 http使用80端口，https默认使用443端口。 https = http + 加密 + 认证 + 完整性保护 20. Socket简介 Socket是对TCP/IP协议的抽象，是操作系统对外开发的接口 基于tcp协议的编程模型 服务器： 创建ServerSocket类型的对象并提供端口号； 等待客户端的连接请求，调用accept方法； 使用输入输出流进行通信； 关闭Socket； 客户端： 创建Socket类型的对象并提供服务器的通信地址和端口号； 使用输入输出流进行通信； 关闭Socket； 基于udp协议的编程模型 主机A(接收方): 创建DatagramSocket类型的对象，并提供端口号； 创建DatagramPacket类型的对象，用于接收发来的数据； 从Socket中接收数据，调用**receive()**方法； 关闭Socket并释放有关的资源； 主机B(发送方) 创建DatagramSocket类型的对象； 创建DatagramPacket类型的对象，并提供接收方的IP地址和端口号； 通过Socket发送数据，调用**send()**方法； 关闭Socket并释放有关的资源；","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"}]},{"title":"【安全认证】JSON Web Token 入门","date":"2019-08-06T07:50:53.000Z","path":"article/20190806.html","text":"JSON Web TokenJSON Web Token（缩写 JWT）基于JSON格式信息一种Token令牌，是目前最流行的跨域认证解决方案。 JWT 的原理是，服务器认证以后，生成一个 JSON 对象，发回给用户。 此后，用户与服务端通信的时候，都要发回这个 JSON 对象。服务器完全只靠这个对象认定用户身份。为了防止用户篡改数据，服务器在生成这个对象的时候，会加上签名。 服务器就不保存任何 session 数据了，也就是说，服务器变成无状态了，从而比较容易实现扩展。 1. JWT数据结构它是一个很长的字符串，中间用点（.）分隔成三个部分。 例如：eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjaGFvIiwidWlkIjoyOSwiZXhwIjoxNTY3OTM2NzgwfQ.6zvimBNs_MCiov4MOkkUodgKmRFBS2dVhmhIb1MV6m4。 JWT 的三个部分(Header.Payload.Signature)依次如下: Header（头部） Payload（负载） Signature（签名） 1.1 Header（头部）Header 部分是一个 JSON 对象，描述 JWT 的元数据。 1234&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125; alg：签名的算法（algorithm），默认是 HMAC SHA256（写成HS256） typ：表示这个令牌（token）的类型（type），JWT令牌统一写为JWT。 最后，将上面的 JSON 对象使用 Base64URL算法转成字符串。 1.2 Payload（负载）Payload 部分也是一个 JSON 对象，用来存放实际需要传递的数据。JWT 规定了7个官方字段(Reserved claims)，供选用。标准中建议使用这些字段，但不强制。 iss (issuer)：签发人 exp (expiration time)：过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号，JWT唯一标识，能用于防止JWT重复使用 除了官方字段，还有公共声明的字段（见：http://www.iana.org/assignments/jwt/jwt.xhtml）也可以定义私有字段，如： 12345&#123; &quot;sub&quot;: &quot;1234567890&quot;, &quot;name&quot;: &quot;John Doe&quot;, &quot;admin&quot;: true&#125; 注意，JWT 默认是不加密的，任何人都可以读到，所以不要把秘密信息放在这个部分。 这个 JSON 对象也要使用 Base64URL算法转成字符串。 1.3 Signature（签名）Signature 部分是对前两部分的签名，防止数据篡改。该签名信息是通过header和payload，加上secret，通过算法加密生成。 首先，需要指定一个密钥（secret）。这个密钥只有服务器才知道，不能泄露给用户。然后，使用 Header 里面指定的签名算法（默认是 HMAC SHA256），按照下面的公式产生签名。 HMACSHA256(base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), secret) 算出签名以后，把 Header、Payload、Signature 三个部分拼成一个字符串，每个部分之间用”点”（.）分隔，就可以返回给用户。 2. Base64URL算法前面提到，Header 和 Payload 串型化的算法是 Base64URL。这个算法跟 Base64 算法基本类似，但有一些小的不同。 JWT 作为一个令牌（token），有些场合可能会放到 URL（比如 api.example.com/?token=xxx）。Base64 有三个字符+、/和=，在 URL 里面有特殊含义，所以要被替换掉：=被省略、+替换成-，/替换成_ 。这就是 Base64URL 算法。 3. JWT 的使用方式及特点 认证原理： 客户端向服务器申请授权，服务器认证以后，生成一个token字符串并返回给客户端，此后客户端在请求受保护的资源时携带这个token，服务端进行验证再从这个token中解析出用户的身份信息。 JWT的使用方式： 客户端收到服务器返回的JWT，存储在浏览器（Cookie或localStorage） 此后，客户端每次与服务器通信，都要带上这个JWT。 一种做法是放在HTTP请求的头信息Authorization字段里面，格式如下： Authorization: &lt;token&gt; 需要将服务器设置为接受来自所有域的请求，用Access-Control-Allow-Origin: * 另一种做法是，跨域的时候，JWT就放在POST请求的数据体里面。 对JWT实现token续签的做法： 额外生成一个refreshToken用于获取新token，refreshToken需存储于服务端，其过期时间比JWT的过期时间要稍长。 用户携带refreshToken参数请求token刷新接口，服务端在判断refreshToken未过期后，取出关联的用户信息和当前token。 使用当前用户信息重新生成token，并将旧的token置于黑名单中，返回新的token。 JWT 的几个特点 JWT 默认是不加密，但也是可以加密的。生成原始 Token 以后，可以用密钥再加密一次。 JWT 不加密的情况下，不能将秘密数据写入JWT。 JWT 不仅可以用于认证，也可以用于交换信息。有效使用 JWT，可以降低服务器查询数据库的次数。 JWT 的最大缺点是，由于服务器不保存 session 状态，因此无法在使用过程中废止某个 token，或者更改 token 的权限。也就是说，一旦 JWT 签发了，在到期之前就会始终有效，除非服务器部署额外的逻辑。 JWT 本身包含了认证信息，一旦泄露，任何人都可以获得该令牌的所有权限。为了减少盗用，JWT 的有效期应该设置得比较短。对于一些比较重要的权限，使用时应该再次对用户进行认证。 为了减少盗用，JWT 不应该使用 HTTP 协议明码传输，要使用 HTTPS 协议传输。 4. Java中JWT的使用java-jwt工具包提供了JWT算法的封装 导入java-jwt，选择一种算法（HMAC256为例） Algorithm algorithm = Algorithm.HMAC256(&quot;secret&quot;); 算法定义了一个令牌是如何被签名和验证的。 创建一个签名的JWT token（通过调用jwt.create()创建一个JWTCreator实例） String token = JWT.create().withIssuer(&quot;auth0&quot;).sign(algorithm); 如果Claim不能转换为JSON，或者在签名过程中使用的密钥无效，那么将会抛出JWTCreationException异常 验证令牌（调用jwt.require()和传递算法实例来创建一个JWTVerifier实例。方法build()返回的实例是可重用的，因此可以定义一次，并使用它来验证不同的标记。最后调用verifier.verify()来验证token） JWTVerifier verifier = JWT.require(algorithm).withIssuer(&quot;auth0&quot;).build(); verifier.verify(token); 如果令牌有一个无效的签名，或者没有满足Claim要求，那么将会抛出JWTVerificationException异常 jwt时间的验证（当验证一个令牌时，时间验证会自动发生；JWT令牌可能包括可用于验证的DateNumber字段） &quot;iat&quot; &lt; TODAY：这个令牌发布了一个过期的时间 &quot;exp&quot; &gt; TODAY：这个令牌还没过期 &quot;nbf&quot; &gt; TODAY：这个令牌已经被使用了 解码一个jwt令牌 DecodedJWT jwt = JWT.decode(token); jwt.getAlgorithm();:返回jwt的算法值,如果没有定义则返回null jwt.getType();:返回jwt的类型值，如果没有定义则返回null（多数情况类型值为jwt） 如果令牌有无效的语法，或者消息头或有效负载不是JSONs，那么将会抛出JWTDecodeException异常 5. Java中JWT的使用实例封装一个JWT工具类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import java.util.Date;import java.util.HashMap;import java.util.Map;import com.auth0.jwt.JWT; //导入java-jwtimport com.auth0.jwt.algorithms.Algorithm;import com.auth0.jwt.interfaces.DecodedJWT;import com.auth0.jwt.interfaces.JWTVerifier;import com.entity.User; //引入User实体类public class JwtUtil &#123; //设置过期时间，这里设置15分钟 private static final long EXPIRE_TIME = 15 * 60 * 1000; //服务端的私钥secret,在任何场景都不应该流露出去 private static final String TOKEN_SECRET = &quot;zhengchao&quot;; /** * 生成签名 * @param **User** * @param **password** * @return */ public static String createToken(User user) &#123; try &#123; // 设置过期时间 Date date = new Date(System.currentTimeMillis() + EXPIRE_TIME); // 私钥和加密算法 Algorithm algorithm = Algorithm.HMAC256(TOKEN_SECRET); // 设置头部信息 Map&lt;String, Object&gt; header = new HashMap&lt;&gt;(2); header.put(&quot;typ&quot;, &quot;JWT&quot;); header.put(&quot;alg&quot;, &quot;HS256&quot;); // 返回token字符串 return JWT.create() .withHeader(header) .withClaim(&quot;aud&quot;, user.getName()) .withClaim(&quot;uid&quot;, user.getId()) .withExpiresAt(date) .sign(algorithm); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 检验token是否正确 * @param **token** * @return */ public static boolean isVerify(String token)&#123; try &#123; Algorithm algorithm = Algorithm.HMAC256(TOKEN_SECRET); JWTVerifier verifier = JWT.require(algorithm).build(); verifier.verify(token); return true; &#125; catch (Exception e)&#123; return false; &#125; &#125; /** *从token解析出uid信息,用户ID * @param token * @param key * @return */ public static int parseTokenUid(String token) &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(&quot;uid&quot;).asInt(); &#125; /** *从token解析出aud信息,用户名 * @param token * @param key * @return */ public static String parseTokenAud(String token) &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(&quot;aud&quot;).asString(); &#125;&#125; 登录成功后，生成token给浏览器，存储在浏览器（Cookie或localStorage） 1String token = JwtUtil.createToken(user); 此后，客户端每次与服务器通信（需权限的资源），都要带上这个JWT。 一种做法是放在HTTP请求的头信息Authorization字段里面，格式如下： Authorization: &lt;token&gt; 需要将服务器设置为接受来自所有域的请求，用Access-Control-Allow-Origin: * 另一种做法是，跨域的时候，JWT就放在POST请求的数据体里面。 jwt 适合做简单的 restful api 认证，颁发一个固定有效期的 jwt，降低 jwt 暴露的风险，尽量不要对 jwt 做服务端的状态管理，这样才能体现出 jwt 无状态的优势。 附：java-jwt已经实现的算法 JWS 算法 介绍 HS256 HMAC256 HMAC with SHA-256 HS384 HMAC384 HMAC with SHA-384 HS512 HMAC512 HMAC with SHA-512 RS256 RSA256 RSASSA-PKCS1-v1_5 with SHA-256 RS384 RSA384 RSASSA-PKCS1-v1_5 with SHA-384 RS512 RSA512 RSASSA-PKCS1-v1_5 with SHA-512 ES256 ECDSA256 ECDSA with curve P-256 and SHA-256 ES384 ECDSA384 ECDSA with curve P-384 and SHA-384 ES512 ECDSA512 ECDSA with curve P-521 and SHA-512","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"ssafe","permalink":"http://chaooo.github.io/tags/ssafe/"}]},{"title":"【安全认证】MD5算法加盐实现用户密码加密","date":"2019-07-28T13:49:28.000Z","path":"article/20190728.html","text":"1. MD5加密算法介绍MD5的全称是Message-Digest Algorithm 5（信息-摘要算法 第五版），经MD2、MD3和MD4发展而来的一种加密算法，是典型的消息摘要算法，属Hash算法一类。作用是让大容量信息在用数字签名软件签署私人密匙前被”压缩”成一种保密的格式（就是把一个任意长度的字节串变换成一定长的大整数）。通过MD5算法进行加密获得一个随机长度的信息并产生一个128位的信息摘要。如果将这个128位的二进制摘要信息换算成十六进制，可以得到一个32位的字符串，因此我们加密完成后的16进制的字符串长度为32位。 2. MD5加密算法特点： 压缩性：任意长度的数据，算出的MD5值长度都是固定的。 容易计算：从原数据计算出MD5值很容易。 抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。 强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。 3. 盐（Salt）在密码学中，是指通过在密码任意固定位置插入特定的字符串，让散列后的结果和使用原始密码的散列结果不相符，这种过程称之为“加盐”。 4. java.security.MessageDigest类JDK中的java.security.MessageDigest用于为应用程序提供信息摘要算法的功能，如 MD5 或 SHA 算法。 MessageDigest 通过其getInstance系列静态函数来进行实例化和初始化。 MessageDigest 对象通过使用 update 方法处理数据。任何时候都可以调用 reset 方法重置摘要。一旦所有需要更新的数据都已经被更新了，应该调用 digest 方法之一完成哈希计算并返回结果。 对于给定数量的更新数据，digest 方法只能被调用一次。digest 方法被调用后，MessageDigest 对象被重新设置成其初始状态。 5. 封装一个MD5加密工具类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.security.MessageDigest;import java.util.UUID;public class MD5Util &#123; /** * md5加密 * @param s：待加密字符串 * @return 加密后16进制字符串 */ public static String md5(String s) &#123; try &#123; //实例化MessageDigest的MD5算法对象 MessageDigest md = MessageDigest.getInstance(&quot;MD5&quot;); //通过digest方法返回哈希计算后的字节数组 byte[] bytes = md.digest(s.getBytes(&quot;utf-8&quot;)); //将字节数组转换为16进制字符串并返回 return toHex(bytes); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * 获取随即盐 * @return */ public static String salt()&#123; //利用UUID生成随机盐 UUID uuid = UUID.randomUUID(); //返回a2c64597-232f-4782-ab2d-9dfeb9d76932 String[] arr = uuid.toString().split(&quot;-&quot;); return arr[0]; &#125; /** * 字节数组转换为16进制字符串 * @param bytes数组 * @return 16进制字符串 */ private static String toHex(byte[] bytes) &#123; final char[] HEX_DIGITS = &quot;0123456789ABCDEF&quot;.toCharArray(); StringBuilder ret = new StringBuilder(bytes.length * 2); for (int i=0; i&lt;bytes.length; i++) &#123; ret.append(HEX_DIGITS[(bytes[i] &gt;&gt; 4) &amp; 0x0f]); ret.append(HEX_DIGITS[bytes[i] &amp; 0x0f]); &#125; return ret.toString(); &#125;&#125; 6. 使用封装的MD5工具类完成用户注册(主要代码)12345678910111213141516public Object register(String name, String password) &#123; //添加用户信息 user = new User(); //设置用户名 user.setName(name); //密码加密后再保存 String salt = MD5Util.salt(); String md5Password = MD5Util.md5(password+salt); //存入MD5加密后的密码 user.setPassword(md5Password); //随机盐存入数据库，用于登录校验 user.setSalt(salt); //最后将用户数据数据存入数据库 int row = userDao.insert(user); return ...&#125; 7. 使用封装的MD5工具类完成用户登录(主要代码)123456789101112public Object login(String name, String password) &#123; //根据用户名在数据库查找用户 User user = userDao.selectByName(name); //取出用户信息比对 String dbPassword = user.getPassword(); String salt = user.getSalt(); //通过密码+盐 重新生成 MD5密码 String md5Password = MD5Util.md5(password+salt); if(md5Password.equals(dbPassword)) &#123; //登录成功 &#125;&#125; 8. 扩展：MessageDigest类常用方法8.1 构造方法摘要MessageDigest(String algorithm) –创建具有指定算法名称的MessageDigest 实例对象。 MessageDigest类是一个工厂类，其构造器是受保护的，不允许直接使用new MessageDigist( )来创建对象，而必须通过其静态方法getInstance( )生成MessageDigest对象。其中传入的参数指定计算消息摘要所使用的算法，常用的有”MD5”，”SHA”等。 8.2 成员方法摘要： 返回值 方法名 描述 Object clone() 如果实现是可复制的，则返回一个副本。 byte[] digest() 通过执行诸如填充之类的最终操作完成哈希计算。 byte[] digest(byte[] input) 使用指定的字节数组对摘要进行最后更新，然后完成摘要计算。 int digest(byte[] buf, int offset, int len) 通过执行诸如填充之类的最终操作完成哈希计算。 String getAlgorithm() 返回标识算法的独立于实现细节的字符串。 int getDigestLength() 返回以字节为单位的摘要长度，如果提供程序不支持此操作并且实现是不可复制的，则返回 0。 static MessageDigest getInstance(String algorithm) 生成实现指定摘要算法的 MessageDigest 对象。 static MessageDigest getInstance(String algorithm, Provider provider) 生成实现指定提供程序提供的指定算法的 MessageDigest 对象，如果该算法可从指定的提供程序得到的话。 static MessageDigest getInstance(String algorithm, String provider) 生成实现指定提供程序提供的指定算法的 MessageDigest 对象，如果该算法可从指定的提供程序得到的话。 Provider getProvider() 返回此信息摘要对象的提供程序。 static boolean isEqual(byte[] digesta, byte[] digestb) 比较两个摘要的相等性。 void reset() 重置摘要以供再次使用。 String toString() 返回此信息摘要对象的字符串表示形式。 void update(byte input) 使用指定的字节更新摘要。 void update(byte[] input) 使用指定的字节数组更新摘要。 void update(byte[] input, int offset, int len) 使用指定的字节数组，从指定的偏移量开始更新摘要。 void update(ByteBuffer input) 使用指定的 ByteBuffer 更新摘要。 ★ 编程思路：java.security包中的MessageDigest类提供了计算消息摘要（即生成散列码）的方法，首先生成对象，执行其update( )方法可以将原始数据传递给该对象，然后执行其digest( )方法即可得到消息摘要。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"ssafe","permalink":"http://chaooo.github.io/tags/ssafe/"}]},{"title":"【数据库】嵌入式SQL语言","date":"2019-07-16T06:32:28.000Z","path":"article/20190716.html","text":"概述 交互式SQL语言有很多优点：记录集合操作、非过程性操作、一条语句就可实现复杂查询的结果， 然而，交互式SQL本身也有很多局限： 从使用者角度：专业人员可熟练写出SQL语句，但大部分的普通用户并非可以 从SQL本身角度：特别复杂的检索结果难以用一条交互式SQL语句完成，此时需要结合高级语言中经常出现的顺序、分支和循环结构来帮助处理 因此，高级语言+SQL语言： 既继承高级语言的过程控制性 又结合SQL语言的复杂结果集操作的非过程性 同时又为数据库操作者提供安全可靠的操作方式：通过应用程序进行操作 嵌入式SQL语言 将SQL语言嵌入到某一种高级语言中使用 这种高级语言，如C/C++, Java, PowerBuilder等，又称宿主语言(Host Language) 嵌入在宿主语言中的SQL与前面介绍的交互式SQL有一些不同的操作方式 目录： 变量声明与数据库连接 事务Transaction 数据集与游标 状态捕获及错误处理机制 动态SQL 数据字典与SQLDA ODBC简介 JDBC简介 嵌入式SQL-ODBC-JDBC三者比较 1. 变量声明与数据库连接 以宿主语言C语言为例，对比交互式SQL语言与嵌入式SQL语言 交互式SQL:select Sname, Sage from Student where Sname=&#39;张三&#39;; 嵌入式SQL:exec sql select Sname, Sage into :vSname, :vSage from Student where Sname=&#39;张三&#39;; 典型特点 exec sql引导SQL语句: 提供给C编译器，以便对SQL语句预编译成C编译器可识别的语句 增加一 into子句: 该子句用于指出接收SQL语句检索结果的程序变量 由冒号引导的程序变量,如: ‘:vSname’, ‘:vSage’ 1.1 变量的声明与使用 在嵌入式SQL语句中可以出现宿主语言语句所使用的变量，这些变量需要特殊的声明：1234exec sql begin declare section; char vSname[10], specName[10]=&quot;张三&quot;; int vSage;exec sql end declare section; 变量声明和赋值中，要注意： 宿主程序的字符串变量长度应比字符型字段的长度多1个。因宿主程序的字符串尾部多一个终止符为’\\0’，而程序中用双引号来描述。 宿主程序变量类型与数据库字段类型之间有些是有差异的,有些DBMS可支持自动转换，有些不能。 声明的变量，可以在宿主程序中赋值，然后传递给SQL语句的where等子句中，以使SQL语句能够按照指定的要求(可变化的)进行检索。 嵌入式比交互式SQL语句灵活了一些：只需改一下变量值，SQL语句便可反复使用，以检索出不同结果。 示例：12345678exec sql begin declare section; char vSname[10], specName[10]=&quot;张三&quot;; int vSage;exec sql end declare section;//用户可在此处基于键盘输入给specName赋值exec sql select Sname, Sage into :vSname, :vSage from Student where Sname = :specName;//比较相应的交互式SQL语句：select Sname, Sage from Student where Sname = &#x27;张三&#x27;; 1.2 程序与数据库的连接和断开1.2.1 数据库的连接connect在嵌入式SQL程序执行之前，首先要与数据库进行连接, 不同DBMS，具体连接语句的语法略有差异 SQL标准中建议的连接语法为： execsql connecttotarget-server asconnect-name useruser-name; 或 execsql connecttodefault; Oracle中数据库连接: execsql connect:user_name identified by :user_pwd; DB2 UDB中数据库连接: execsql connecttomydb user:user_name using:user_pwd; 1.2.1 数据库的断开disconnect在嵌入式SQL程序执行之后，需要与数据库断开连接 SQL标准中建议的断开连接的语法为： exec sql disconnect connect-name; 或 exec sql disconnect current; Oracle中断开连接: exec sql commit release; 或 exec sql rollback release; DB2 UDB中断开连接: exec sql connect reset; exec sql disconnect current; 1.3 SQL执行的提交与撤消SQL语句在执行过程中，必须有提交和撤消语句才能确认其操作结果 SQL执行的提交： execsql commitwork; SQL执行的撤消： execsql rollbackwork; 为此，很多DBMS都设计了捆绑提交/撤消与断开连接在一起的语句,以保证在断开连接之前使用户确认提交或撤消先前的工作，例如Oracle中： execsql commitrelease; 或 execsql rollbackrelease; 2. 事务Transaction 从应用程序员角度：事务是一个存取或改变数据库内容的程序的一次执行，或者说一条或多条SQL语句的一次执行被看作一个事务 从微观角度，或者从DBMS角度：事务是数据库管理系统提供的控制数据操作的一种手段，通过这一手段，应用程序员将一系列的数据库操作组合在一起作为一个整体进行操作和控制，以便数据库管理系统能够提供一致性状态转换的保证。 简单来说：事务是作为单个逻辑工作单元执行的一系列操作；多个操作作为一个整体向系统提交，要么都执行，要么都不执行；事务是一个不可分割的工作逻辑单元。 2.1 事务的特性: ACID 原子性Atomicity : DBMS能够保证事务的一组更新操作是原子不可分的，即对DB而言，要么都执行，要么都不执行 一致性Consistency: DBMS保证事务的操作状态是正确的，符合一致性的操作规则，它是进一步由隔离性来保证的 隔离性Isolation: DBMS保证并发执行的多个事务之间互相不受影响。例如两个事务T1和T2, 即使并发执行，也相当于或者先执行了T1,再执行T2;或者先执行了T2, 再执行T1。 持久性Durability: DBMS保证已提交事务的影响是持久的，被撤销事务的影响是可恢复的。 换句话说：具有ACID特性的若干数据库基本操作的组合体被称为事务。 3. 数据集与游标读取单行结果处理与多行结果处理的差异：Into子句与游标(Cursor) 检索单行结果，可将结果直接传送到宿主程序的变量中(Into) 示例：exec sql select Sname,Sage into :vSname,:vSage from Student where Sname = :specName; 检索多行结果，则需使用游标(Cursor) 游标是指向某检索记录集的指针 通过这个指针的移动，每次读一行，处理一行，再读一行… , 直至处理完毕 读一行操作是通过Fetch…into语句实现的：每一次Fetch, 都是先向下移动指针，然后再读取 记录集有结束标识EOF, 用来标记后面已没有记录了 游标(Cursor)的使用需要先定义、再打开(执行)、接着一条接一条处理，最后再关闭 游标可以定义一次，多次打开(多次执行)，多次关闭 3.1 游标的使用方法 Cursor的定义：declare cursor 123456789EXEC SQL DECLARE cursor_name CURSOR FOR Subquery [ORDER BY result_column [ASC | DESC][, result_column …] [FOR [ READ ONLY | UPDATE [OF columnname [, columnname…]]]];//示例:exec sql declare cur_student cursor for select Sno, Sname, Sclass from Student where Sclass= :vClass order by Sno for read only ; Cursor的打开和关闭：open cursor //close cursor EXEC SQL OPEN cursor_name; EXEC SQL CLOSE cursor_name; Cursor的数据读取：Fetch 1234567891011EXEC SQL FETCH cursor_name INTO host-variable , [host-variable, …];//示例:exec sql declare cur_student cursor for select Sno, Sname, Sclass from Student where Sclass= :vClass order by Sno for read only ;exec sql open cur_student;…exec sql fetch cur_student into :vSno, :vSname, :vSage…exec sql close cur_student; 3.2 可滚动游标 ODBC支持的可滚动Cursor 标准的游标始终是自开始向结束方向移动的，每fetch一次，向结束方向移动一次；一条记录只能被访问一次；再次访问该记录只能关闭游标后重新打开 ODBC(OpenDataBase Connectivity)是一种跨DBMS的DB操作平台，它在应用程序与实际的DBMS之间提供了一种通用接口 许多实际的DBMS并不支持可滚动游标，但通过ODBC可以使用该功能 可滚动游标是可使游标指针在记录集之间灵活移动、使每条记录可以反复被访问的一种游标 可滚动游标移动时需判断是否到结束位置，或到起始位置 可通过判断是否到EOF位置(最后一条记录的后面),或BOF位置(起始记录的前面) 如果不需区分，可通过whenevernotfound语句设置来检测 123456789EXEC SQL DECLARE cursor_name [INSENSITIVE] [SCROLL] CURSOR[WITH HOLD] FOR Subquery[ORDER BY result_column [ASC | DESC][, result_column …][FOR READ ONLY | FOR UPDATE OF columnname [,columnname ]…];EXEC SQL FETCH[ NEXT | PRIOR | FIRST | LAST| [ABSOLUTE | RELATIVE] value_spec ]FROM cursor_name INTO host-variable [, host-variable …]; NEXT向结束方向移动一条； PRIOR向开始方向移动一条； FIRST回到第一条； LAST移动到最后一条； ABSOLUTvalue_spec定向检索指定位置的行,value_spec由1至当前记录集最大值； RELATIVEvalue_spec相对当前记录向前或向后移动，value_spec为正数向结束方向移动，为负数向开始方向移动 3.3 数据库记录的增删改 数据库记录的删除 一种是查找删除(与交互式DELETE语句相同)，一种是定位删除 1234567891011121314EXEC SQL DELETE FROM tablename [corr_name] WHERE search_condition | WHERE CURRENT OF cursor_name;//示例：查找删除exec sql delete from customers c where c.city = ‘Harbin’ and not exists ( select * from orders o where o.cid = c.cid);//示例：定位删除exec sql declare delcust cursor for select cid from customers c where c.city =‘harbin’ and not exists ( select * from orders o where o.cid = c.cid) for update of cid;exec sql open delcustWhile (TRUE) &#123; exec sql fetch delcust into :cust_id; exec sql delete from customers where current of delcust ; &#125; 数据库记录的更新 一种是查找更新(与交互式Update语句相同)，一种是定位更新 1234567891011121314EXEC SQL UPDATE tablename [corr_name] SET columnname = expr [, columnname = expr …] [ WHERE search_condition ] | WHERE CURRENT OF cursor_name;//示例：查找更新exec sql update student s set sclass = ‘035102’ where s.sclass = ‘034101’// 示例：定位更新exec sql declare stud cursor for select * from student s where s.sclass =‘034101’ for update of sclass;exec sql open studWhile (TRUE) &#123; exec sql fetch stud into :vSno, :vSname, :vSclass; exec sql update student set sclass = ‘035102’ where current of stud ; &#125; 数据库记录的插入 只有一种类型的插入语句 12345678EXEC SQL INSERT INTO tablename [ (columnname [,columnname, …] )] [ VALUES (expr [ , expr , …] ) | subqurey ] ;//示例：插入语句exec sql insert into student ( sno, sname, sclass) values (‘03510128’, ‘张三’, ‘035101’) ;//示例：插入语句exec sql insert into masterstudent ( sno, sname, sclass) select sno, sname, sclass from student; 4. 状态捕获及错误处理机制4.1 基本机制 状态，是嵌入式SQL语句的执行状态，尤其指一些出错状态；有时程序需要知道这些状态并对这些状态进行处理 嵌入式 SQL程序中，状态捕获及处理有三部分构成 设置SQL通信区:一般在嵌入式SQL程序的开始处便设置 exec sql include sqlca; 设置状态捕获语句:在嵌入式SQL程序的任何位置都可设置；可多次设置；但有作用域 exec sql whenever sqlerror goto report_error; 状态处理语句:某一段程序以应对SQL操作的某种状态 report_error: exec sql rollback; SQL通信区: SQLCA SQLCA是一个已被声明过的具C语言的结构形式的内存信息区，其中的成员变量用来记录SQL语句执行的状态，便于宿主程序读取与处理 SQLCA是DBMS(执行SQL语句)与宿主程序之间交流的桥梁之一 状态捕获语句: exec sql whenever condition action; Whenever语句的作用是设置一个“条件陷阱”, 该条语句会对其后面的所有由Exec SQL语句所引起的对数据库系统的调用自动检查它是否满足条件(由condition指出). SQLERROR: 检测是否有SQL语句出错。其具体意义依赖于特定的DBMS NOT FOUND: 执行某一SQL语句后，没有相应的结果记录出现 SQLWARNING: 不是错误，但应引起注意的条件 如果满足condition, 则要采取一些动作(由action指出) CONTINUE: 忽略条件或错误，继续执行 GOTO 标号: 转移到标号所指示的语句，去进行相应的处理 STOP: 终止程序运行、撤消当前的工作、断开数据库的连接 DO函数或 CALL函数: 调用宿主程序的函数进行处理，函数返回后从引发该condition的Exec SQL语句之后的语句继续进行 状态捕获语句Whenever的作用范围是其后的所有Exec SQL语句，一直到程序中出现另一条相同条件的Whenever语句为止，后面的将覆盖前面的。1234567891011int main() &#123; exec sql whenever sqlerror stop; … … goto s1 … … exec sql whenever sqlerror continue; s1: exec sql update agents set percent = percent + 1; … …&#125;//S1标号指示的语句受第二个Whenever语句约束。//注意：作用域是语句在程序中的位置，而不是控制流程(因是预编译程序处理条件陷阱) 状态捕获语句Whenever的使用容易引发无限循环123456789101112int main() &#123; exec sql whenever sqlerror goto handle_error; exec sql create table customers(cid char(4) not null, cname varchar(13), … … ); … … handle_error: exec sql whenever sqlerror continue;// 控制是否无限循环：无，则可能；有，则不会 exec sql drop customers; exec sql disconnect; fprintf(stderr,”could not create customers table\\n”); return -1;&#125; 4.2 状态信息典型DBMS系统记录状态信息的三种方法 状态记录: sqlcode: 典型DBMS都提供一个sqlcode变量来记录其执行sql语句的状态，但不同DBMS定义的sqlcode值所代表的状态意义可能是不同的。 sqlcode== 0, successful call; sqlcode &lt; 0, error, e.g., from connect, database does not exist , –16; sqlcode &gt; 0, warning, e.g., no rows retrieved from fetch sqlca.sqlcode: 支持SQLCA的产品一般要在SQLCA中填写sqlcode来记录上述信息; 除此而外，sqlca还有其他状态信息的记录 sqlstate: 有些DBMS提供的记录状态信息的变量是sqlstate或sqlca.sqlstate 当我们不需明确知道错误类型，而只需知道发生错误与否，则我们只要使用前述的状态捕获语句即可，而无需关心状态记录变量(隐式状态处理) 但我们程序中如要自行处理不同状态信息时，则需要知道以上信息，但也需知道正确的操作方法(显式状态处理) 4.3 程序自身进行错误信息的处理正确的显式状态处理示例: 1234567891011exec sql begin declar section; char SQLSTATE[6];exec sql end declare section;exec sql whenever sqlerror goto handle_error;… …exec sql whenever sqlerror continue;exec sql create table custs (cid char(4) not null, cname varchar(13), … … );if (strcmp(SQLSTATE, “82100”)==0) &lt;处理82100错误的程序&gt; … … 上述的if语句是能被执行的，因为createtable发生错误时是继续向下执行的。 5. 动态SQL5.1 动态SQL的概念动态SQL是相对于静态SQL而言的 静态SQL特点：SQL语句在程序中已经按要求写好，只需要把一些参数通过变量(高级语言程序语句中不带冒号) 传送给嵌入式SQL语句即可(嵌入式SQL语句中带冒号) 动态SQL特点：SQL语句可以在程序中动态构造，形成一个字符串，然后再交给DBMS执行，交给DBMS执行时仍旧可以传递变量 5.2 动态SQL的两种执行方式如SQL语句已经被构造在host-variable字符串变量中,则： 立即执行语句: 运行时编译并执行 EXEC SQL EXECUTE IMMEDIATE :host-variable; Prepare-Execute-Using语句:PREPARE语句先编译，编译后的SQL语句允许动态参数，EXECUTE语句执行，用USING语句将动态参数值传送给编译好的SQL语句 EXEC SQL PREPARE sql_temp FROM :host-variable; EXEC SQL EXECUTE sql_temp USING :cond-variable 6. 数据字典与SQLDA6.1 数据字典的概念数据字典(Data dictionary)，又称为系统目录(System Catalogs) 是系统维护的一些表或视图的集合，这些表或视图存储了数据库中各类对象的定义信息，这些对象包括用Create语句定义的表、列、索引、视图、权限、约束等,这些信息又称数据库的元数据–关于数据的数据。 不同DBMS术语不一样：数据字典(DataDictionary(Oracle))、目录表(DB2UDB)、系统目录(INFORMIX)、系统视图(X/Open) 不同DBMS中系统目录存储方式可能是不同的,但会有一些信息对DBA公开。这些公开的信息,DBA可以使用一些特殊的SQL命令来检索。 6.2 数据字典的内容构成数据字典通常存储的是数据库和表的元数据，即模式本身的信息： 与关系相关的信息 关系名字 每一个关系的属性名及其类型 视图的名字及其定义 完整性约束 用户与账户信息，包括密码 统计与描述性数据：如每个关系中元组的数目 物理文件组织信息： 关系是如何存储的(顺序/无序/散列等) 关系的物理位置 索引相关的信息 6.3 数据字典的结构 也是存储在磁盘上的关系 专为内存高效访问设计的特定的数据结构 可能的字典数据结构 Relation_metadata = (relation_name, number_of_attributes, storage_organization, location) Attribute_metadata = (attribute_name, relation_name, domain_type, position, length) User_metadata = (user_name, encrypted_password, group) Index_metadata = (index_name, relation_name, index_type, index_attributes) View_metadata = (view_name, definition) 6.4 X/Open标准的系统目录 X/Open标准中有一个目录表Info_Schem.Tables, 该表中的一行是一个已经定义的表的有关信息 Table_Schem：表的模式名(通常是表所有者的用户名) Table_Name：表名 Table_Type：&#39;Base_Table&#39;或&#39;View&#39; 可以使用SQL语句来访问这个表中的信息，比如了解已经定义了哪些表，可如下进行： Select Table_Name From Tables; 模式的含义是指某一用户所设计和使用的表、索引及其他与数据库有关的对象的集合，因此表的完整名应是：模式名.表名。这样做可允许不同用户使用相同的表名，而不混淆。 一般而言，一个用户有一个模式。可以使用CreateSchema语句来创建模式(用法参见相关文献)，在CreateTable等语句可以使用所定义的模式名称。 6.5 Oracle的数据字典 Oracle数据字典由视图组成，分为三种不同形式，由不同的前缀标识 USER_ :用户视图，用户所拥有的对象，在用户模式中 ALL_ :扩展的用户视图，用户可访问的对象 DBA_ :DBA视图(所有用户都可访问的DBA对象的子集) Oracle数据字典中定义了三个视图USER_Tables,ALL_Tables,和DBA_Tables供DBA和用户使用数据字典中关于表的信息 同样,Oracle数据字典中也定义了三个视图USER_TAB_Columns,ALL_TAB_Columns(Accessible_Columns),和DBA_TAB_Columns供DBA和用户使用数据字典中关于表的列的信息 可以使用SQL语句来访问这些表中的信息： Select Column_Name From ALL_TAB_Columns Where Table_Name = ‘STUDENT’; Oracle数据字典中还定义了其他视图 TABLE_PRIVILEDGE(或ALL_TAB_GRANTS) COLUMN_PRIVILEDGE(或ALL_COL_GRANTS)可访问表的权限，列的权限 CONSTRAINT_DEFS(或ALL_CONSTRAINTS)可访问表的各种约束 可以使用下述命令获取Oracle定义的所有视图信息 Select view_name from all_views where owner = ‘SYS’ and view_name like ‘ALL_%’ or view_name like ‘USER_%’; 如果用户使用Oracle,可使用其提供的SQL*PLUS进行交互式访问 动态SQL: 表和列都已知，动态构造检索条件。 动态SQL:检索条件可动态构造，表和列也可动态构造。 6.6 SQLDA构造复杂的动态SQL需要了解数据字典及SQLDA，已获知关系模式信息 SQLDA: SQLDescriptorArea,SQL描述符区域。 SQLDA是一个内存数据结构，内可装载关系模式的定义信息，如列的数目，每一列的名字和类型等等 通过读取SQLDA信息可以进行更为复杂的动态SQL的处理 不同DBMS提供的SQLDA格式并不是一致的。 7. ODBC简介7.1 ODBC定义ODBC：Open DataBase Connection，ODBC是一种标准—不同语言的应用程序与不同数据库服务器之间通讯的标准。 一组API(应用程序接口)，支持应用程序与数据库服务器的交互 应用程序通过调用ODBC API, 实现 与数据服务器的连接 向数据库服务器发送SQL命令 一条一条的提取数据库检索结果中的元组传递给应用程序的变量 具体的DBMS提供一套驱动程序，即Driver库函数，供ODBC调用，以便实现数据库与应用程序的连接。 ODBC可以配合很多高级语言来使用，如C,C++, C#, Visual Basic, PowerBuilder等等； 7.2 通过ODBC连接数据库 ODBC应用前，需要确认具体DBMS Driver被安装到ODBC环境中 当应用程序调用ODBC API时，ODBC API会调用具体DBMS Driver库函数，DBMS Driver库函数则与数据库服务器通讯，执行相应的请求动作并返回检索结果 ODBC应用程序首先要分配一个SQL环境，再产生一个数据库连接句柄 应用程序使用SQLConnect()，打开一个数据库连接，SQLConnect()的具体参数: connection handle, 连接句柄 the server，要连接的数据库服务器 the user identifier，用户 password ，密码 SQL_NTS 类型说明前面的参数是空终止的字符串 示例12345678910111213141516int ODBCexample()&#123; RETCODE error; /* 返回状态吗 */ HENV env; /* 环境变量 */ HDBC conn; /* 连接句柄 */ SQLAllocEnv(&amp;env); SQLAllocConnect(env, &amp;conn); //分配数据库连接环境 SQLConnect(conn, &quot;aura.bell-labs.com&quot;, SQL_NTS, &quot;avi&quot;, SQL_NTS, avipasswd&quot;, SQL_NTS); //打开一个数据库连接 &#123; …. Do actual work … &#125; //与数据库通讯 SQLDisconnect(conn); SQLFreeConnect(conn); SQLFreeEnv(env); //断开连接与释放环境&#125; 7.3 通过ODBC与数据库服务器进行通讯 应用程序使用SQLExecDirect()向数据库发送SQL命令； 使用SQLFetch()获取产生的结果元组； 使用SQLBindCol()绑定C语言变量与结果中的属性 当获取一个元组时，属性值会自动地传送到相应的C语言变量中 SQLBindCol()的参数： ODBC定义的stmt变量,查询结果中的属性位置 SQL到C的类型变换,变量的地址. 对于类似字符数组一样的可变长度类型，应给出 •变量的最大长度 •当获取到一个元组后，实际长度的存储位置. •注:当返回实际长度为负数，说明是一个空值。 示例123456789101112131415161718192021char branchname[80]; float balance;int lenOut1, lenOut2;HSTMT stmt;SQLAllocStmt(conn, &amp;stmt);//分配一个与指定数据库连接的新的语句句柄char * sqlquery = &quot;select branch_name, sum (balance) from account group by branch_name&quot;;error = SQLExecDirect(stmt, sqlquery, SQL_NTS);//执行查询，stmt句柄指向结果集合if (error == SQL_SUCCESS) &#123;SQLBindCol(stmt, 1, SQL_C_CHAR, branchname , 80, &amp;lenOut1);SQLBindCol(stmt, 2, SQL_C_FLOAT, &amp;balance, 0 , &amp;lenOut2);//绑定高级语言变量与stmt句柄中的属性while (SQLFetch(stmt) &gt;= SQL_SUCCESS) &#123;//提取一条记录，结果数据被存入高级语言变量中 printf (&quot; %s %g\\n&quot;, branchname, balance); &#125;&#125;SQLFreeStmt(stmt, SQL_DROP);//释放语句句柄 7.4 ODBC的其他功能 动态SQL语句的预编译-动态参数传递功能 获取元数据特性 发现数据库中的所有关系的特性 以及 发现每一个查询结果的列的名字和类型等； 默认, 每一条SQL语句都被作为一个独立的能够自动提交的事务来处理。 应用程序可以关闭一个连接的自动提交特性 SQLSetConnectOption(conn, SQL_AUTOCOMMIT, 0)&#125; 此时事务要显式地给出提交和撤销的命令 SQLTransact(conn, SQL_COMMIT) or SQLTransact(conn, SQL_ROLLBACK) 8. JDBC简介8.1 JDBC定义JDBC：Java DataBase Connection，JDBC是一组Java版的应用程序接口API，提供了Java应用程序与数据库服务器的连接和通讯能力。 JDBCAPI分成两个程序包： Java.sql 核心API –J2SE(Java2标准版)的一部分。使用java.sql.DriverManager类、java.sql.Driver和java.sql.Connection接口连接到数据库 Javax.sql 可选扩展API–J2EE(Java2企业版)的一部分。包含了基于JNDI(JavaNamingandDirectoryInterface,Java命名和目录接口)的资源，以及管理连接池、分布式事务等，使用DataSource接口连接到数据库。 8.2 JDBC的功能 java.sql.DriverManager——处理驱动的调入并且对产生新数据库连接提供支持 Java.sql.Driver——通过驱动进行数据库访问，连接到数据库的应用程序必须具备该数据库的特定驱动。 java.sql.Connection——代表对特定数据库的连接。 Try &#123;…&#125; Catch &#123;…&#125; ——异常捕获及其处理 java.sql.Statement——对特定的数据库执行SQL语句 java.sql.PreparedStatement —— 用于执行预编译的SQL语句 java.sql.CallableStatement ——用于执行对数据库内嵌过程的调用。 java.sql.ResultSet——从当前执行的SQL语句中返回结果数据。 8.3 使用JDBC API访问数据库的过程 概念性的基本过程 打开一个连接；创建“Statement”对象，并设置查询语句；使用Statement对象执行查询，发送查询给数据库服务器和返回结果给应用程序；处理错误的例外机制 具体实施过程 •传递一个Driver给DriverManager，加载数据库驱动。 Class.forName() •通过URL得到一个Connection对象, 建立数据库连接 DriverManager.getConnection(sDBUrl) DriverManager.getConnection(sDBUrl,sDBUserID,sDBPassword) •接着创建一个Statement对象(PreparedStatement或CallableStatement)，用来查询或者修改数据库。 Statement stmt=con.createStatement() •查询返回一个ResultSet。 ResultSet rs=stmt.executeQuery(sSQL) 示例：12345678910111213141516public static void JDBCexample(String dbid, String userid, String passwd)&#123; try &#123; //错误捕获 Class.forName (&quot;oracle.jdbc.driver.OracleDriver&quot;); Connection conn = DriverManager.getConnection( &quot;jdbc:oracle:thin:@db.yale.edu:1521:univdb&quot;, userid, passwd); //加载数据库驱动，建立数据库连接 Statement stmt = conn.createStatement(); //创建一个语句对象 … Do Actual Work …. //进行SQL语句的执行与处理工作 stmt.close(); conn.close(); //关闭语句对象，关闭连接&#125; catch (SQLException sqle) &#123; System.out.println(&quot;SQLException : &quot; + sqle); &#125;&#125; 完整的示例程序1234567891011121314151617181920212223public static void JDBCexample(String dbid, String userid, String passwd)&#123; try &#123; Class.forName (&quot;oracle.jdbc.driver.OracleDriver&quot;); Connection conn = DriverManager.getConnection( &quot;jdbc:oracle:thin:@db.yale.edu:1521:univdb&quot;, userid, passwd); Statement stmt = conn.createStatement(); try &#123; stmt.executeUpdate( &quot;insert into instructor values (‘77987&#x27;, ‘Kim&#x27;, ‘Physics’,98000)&quot;); &#125; catch (SQLException sqle) &#123; System.out.println(&quot;插入错误:&quot; + sqle); &#125; ResultSet rset = stmt.executeQuery( &quot;select dept_name, avg(salary) from instructor group by dept_name&quot;); while ( rset.next() ) &#123; System.out.println(rset.getString(“dept_name&quot;) + &quot; &quot; + rset.getFloat(2)); &#125; stmt.close(); conn.close();&#125; catch (SQLException sqle) &#123; System.out.println(&quot;SQLException:&quot; + sqle);&#125;&#125; 9. 嵌入式SQL-ODBC-JDBC三者比较执行一条SQL语句，读取执行的结果集合 嵌入式SQL的思维模式 建立数据库连接 声明一个游标 打开游标 读取一条记录(循环) 关闭游标 断开数据库连接 ODBC的思维模式 建立数据库连接 分配语句句柄 用句柄执行SQL 建立高级语言变量与句柄属性的对应 读取一条记录(循环) 释放语句句柄 断开数据库连接 JDBC的思维模式 建立数据库连接 创建语句对象 用语句对象执行SQL，并返回结果对象 从结果对象获取一条记录 提取对象的属性值传给高级语言变量(返回上一步) 释放语句对象 断开数据库连接 相同点: 都是建立数据库连接, 执行sql, 处理结果, 释放连接, 流程基本一致 不同点, 操作方式的不同: 嵌入式SQL按照语句进行操作 ODBC按照函数来进行操作 JDBC按照对象来进行操作","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"【数据库】数据库语言SQL","date":"2019-07-01T11:36:31.000Z","path":"article/20190701.html","text":"SQL语言概述结构化查询语言(Structured Query Language)简称SQL，是一种特殊目的的编程语言，是一种数据库查询和程序设计语言，用于存取数据以及查询、更新和管理关系数据库系统。 SQL语言是集DDL、DML和DCL于一体的数据库语言 DDL语句引导词：Create(建立),Alter(修改),Drop(撤消) 模式的定义和删除，包括定义Database,Table,View,Index,完整性约束条件等，也包括定义对象(RowType行对象,Type列对象) DML语句引导词：Insert ,Delete, Update, Select 各种方式的更新与检索操作，如直接输入记录，从其他Table(由SubQuery建立)输入 各种复杂条件的检索，如连接查找，模糊查找，分组查找，嵌套查找等 各种聚集操作，求平均、求和、…等，分组聚集，分组过滤等 DCL语句引导词：Grant,Revoke 安全性控制：授权和撤消授权 目录： 利用SQL建立数据库 利用SQL简单查询 利用SQL多表联合查询 利用SQL进行增-删-改 利用SQL语言修正与撤销数据库 SQL Server介绍 SQL语言-子查询 SQL语言-结果计算与聚集计算 SQL语言-分组查询与分组过滤 SQL语言实现关系代数操作 SQL语言之视图及其应用 数据库完整性 数据库的静态完整性(约束) 数据库的动态完整性(触发器) 数据库索引 数据库序列 数据库安全性 数据库自主安全性机制 1. 利用SQL建立数据库DDL：数据定义语言（Data Definition Language)，DDL通常由**DBA(数据库管理员)**来使用，也有经DBA授权后由应用程序员来使用 创建数据库(DB)：Create Database 数据库(Database)是若干具有相互关联关系的Table/Relation的集合 简单语法形式：create database database 数据库名; 创建DB中的Table(定义关系模式)：Create Table Create table 表名(列名 数据类型 [Primary key|Unique] [Not null][,列名 数据类型 [Not null], …]); []表示其括起的内容可以省略，|表示其隔开的两项可取其一 Primary key: 主键约束。每个表只能创建一个主键约束 Unique: 唯一性约束(即候选键)。可以有多个唯一性约束 Not null: 非空约束。 数据类型（SQL-92标准） char(n):固定长度的字符串 varchar(n):可变长字符串 int:整数 //有时不同系统也写作integer numeric(p，q):固定精度数字，小数点左边p位，右边(p-q)位 real:浮点精度数字 //有时不同系统也写作float(n)，小数点后保留n位 date:日期 (如 2003-09-12) time:时间 (如 23:15:003) 注意: 现行商用DBMS的数据类型有时有些差异 2. 利用SQL简单查询DML：数据操纵语言（Data Manipulation Language)，DML通常由用户或应用程序员使用，访问经授权的数据库 向Table中添加数据(追加元组)：Insert into insert into insert into 表名[(列名[, 列名] …] values (值[,值], …); values值的排列，须与列名排列一致 若所有列名省略，则values值的排列须与该表存储中的列名排列一致 单表查询Select Select Select 列名[[,列名] …] From 表名[Where 检索条件]; 语义：从表名所给出的表中，查询出满足检索条件的元组，并按给定的列名及顺序进行投影显示。 相当于：Π[列名,...,列名](σ检索条件(表名)) Select语句中的select … , from… , where…, 等被称为子句，在以上基本形式基础上会增加许多构成要素，也会增加许多新的子句，满足不同的需求。 检索条件的书写Where 与选择运算σF(R)的条件F书写一样，只是其逻辑运算符用 and,or,not 来表示, 同时也要注意运算符的优先次序及括弧的使用。书写要点是注意对自然语言检索条件的正确理解。 Select Tname From Teacher Where Salary &gt; 2000 and D# = ’03’;//检索教师表中所有工资大于2000元 并且是03系的教师姓名 排重(DISTINCT) 关系模型不允许出现重复元组。但现实DBMS，却允许出现重复元组。 在Table中要求无重复元组是通过定义Primary key或Unique来保证的; 而在检索结果中要求无重复元组, 是通过DISTINCT保留字的使用来实现的。 Select DISTINCT S# From SC Where Score &gt; 80; 排序(ORDER BY) Select语句中结果排序是通过增加order by子句实现的 order by 列名 [asc|desc] 意义为检索结果按指定列名进行排序，若后跟asc或省略，则为升序；若后跟desc, 则为降序。 模糊查询(*LIKE*) _：一个字符，%：任意长度字符。 Select Sname From Student Where Sname Like &#39;张_ _&#39;;//检索名字为张某某的所有同学姓名 Select Sname From Student Where Sname Not Like &#39;张%&#39;;//检索名字不姓张的所有同学姓名 3. 利用SQL多表联合查询多表联合检索可以通过连接运算来完成，而连接运算又可以通过广义笛卡尔积后再进行选择运算来实现。 检索语句: Select 列名[[,列名] …] From 表名1,表名2,… Where 检索条件; 相当于Π[列名,...,列名](σ检索条件(表名1 × 表名2 × …)) 检索条件中要包含连接条件，通过不同的连接条件可以实现等值连接、不等值连接及各种θ-连接 θ-连接之等值连接 多表连接时，如两个表的属性名相同，则需采用**表名.属性名**方式来限定该属性是属于哪一个表 Select Sname From Student, SC Where Student.S#=SC.S# and SC.C#=&#39;001&#39; Order By Score DESC;//按“001”号课成绩由高到低顺序显示所有学生的姓名(二表连接) 属性重名重名处理(表别名) 连接运算涉及到重名的问题，如两个表中的属性重名，连接的两个表重名(同一表的连接)等，因此需要使用**别名**以便区分 Select 列名 as 列别名[[,列名 as 列别名] …] From 表名1 as 表别名1,表名2 as 表别名2,… Where Where 检索条件; 当定义了别名后，在检索条件中可以使用别名来限定属性 as 可以省略 θ-连接之不等值连接 Select T1.Tname as Teacher1, T2.Tname as Teacher2 From Teacher T1, Teacher T2 Where T1.Salary&gt;T2.Salary;//求有薪水差额的任意两位教师 实例： Select S1.S# From SC S1, SC S2 Where S1.S# = S2.S# and S1.C#=&#39;001&#39; and S2.C#=&#39;002&#39; and S1.Score &gt; S2.Score;//求“001”号课成绩比“002”号课成绩高的所有学生的学号 4. 利用SQL进行增-删-改 SQL-之更新操作 元组新增Insert：新增一个或一些元组到数据库的Table中 元组更新Update:对某些元组中的某些属性值进行重新设定 元组删除Delete：删除某些元组 SQL-DML既能单一记录操作，也能对记录集合进行批更新操作 SQL-DML之更新操作需要利用前面介绍的子查询(Subquery)的概念，以便处理“一些”、“某些”等 SQL-之INSERT 单一元组新增命令形式：插入一条指定元组值的元组 insert into 表名 [(列名[,列名]…)] values (值 [,值]…); 批数据新增命令形式：插入子查询结果中的若干条元组。待插入的元组由子查询给出。 insert into 表名 [(列名[，列名]…)] 子查询; 示例：Insert Into St (S#,Sname) Select S#,Sname From Student Where Sname like &#39;%伟&#39;;//将检索到的满足条件的同学新增到该表中 注意：当新增元组时，DBMS会检查用户定义的完整性约束条件等，如不符合完整性约束条件，则将不会执行新增动作。 SQL-之DELETE 元组删除Delete命令: 删除满足指定条件的元组 Delete From 表名 [ Where 条件表达式]; 如果Where条件省略，则删除所有的元组(清空表)。 示例：Delete From Student Where S# in ( Select S# From SC Where Score &lt; 60 Group by S# Having Count(*)&gt;= 4);//删除有四门不及格课程的所有同学 SQL-之UPDATE 元组更新Update命令: 用指定要求的值更新指定表中满足指定条件的元组的指定列的值 Update 表名 Set 列名=表达式 | (子查询) [[,列名=表达式 | (子查询) ] …] [ Where 条件表达式]; 如果Where条件省略，则更新所有的元组。 示例：Update Teacher Set Salary=Salary*1.1 Where D# in (Select D# From Dept Where Dname=&#39;计算机&#39;);//将所有计算机系的教师工资上调10% 5. 利用SQL语言修正与撤销数据库 修正基本表的定义 alter table tablename [add &#123;colname datatype, …&#125;] //增加新列 [drop &#123;完整性约束名&#125;] //删除完整性约束 [modify &#123;colname datatype, …&#125;] //修改列定义 示例：Alter Table Student Drop Unique(Sname);删除学生姓名必须取唯一值的约束 示例：Alter Table Student Add Saddr char[40],PID char[18];在学生表Student上增加二列Saddr, PID SQL-DDL之撤销与修改 drop table 表名; //撤消基本表 drop database 数据库名; //撤消数据库 SQL-DDL之数据库指定与关闭命令 有些DBMS提供了操作多个数据库的能力，此时在进行数据库操作时需要指定待操作数据库与关闭数据库的功能。 use 数据库名; //指定当前数据库 close 数据库名; //关闭当前数据库 6. SQL Server介绍SQL Server 是 Microsoft提供的一款关系数据库管理系统 SQL Server 的系统数据库 Master：是SQL Server中最重要的系统数据库，存储SQL Server中的元数据。 Model：模板数据库，在创建新的数据库时，SQL Server将会复制此数据库作为新数据库的基础。 Msdb：代理服务数据库，提供一个存储空间。 Tempdb：临时数据库，为所有的临时表、临时存储过程及其他临时操作提供存储空间，断开连接时，临时表与存储过程自动被删除。 SQL Server的数据库 文件：有三种文件扩展名：.mdf、.ndf、.ldf 主数据库文件：扩展名为.mdf，是存储数据库的启动信息和部分或全部数据。一个数据库可以有多个数据库文件，但主数据库文件只有一个。 辅助数据文件：扩展名为.ndf，用于放置主数据库文件中所定义数据库的其它数据，可有多个。在数据庞大时，可以帮助存储数据。 日志文件：扩展名.ldf。每个数据库至少有一个事务日志文件。 页面：是SQL Server存储的最小单位。一页为8K或8192字节。 空间(extent)：是8个连续的页面，即64K数据，是分配数据表存储空间的一种单位 6.1 SQL Server数据库的创建-删除与维护 创建数据库 语法形式：Create Database 库名 可视化操作(查询分析器)：Database(鼠标右键) -&gt; new Database… -&gt; 填写数据库名及配置 创建数据库的过程就是为数据库设计名称、设计所占用存储空间和存 放文件位置的过程。特别是在网络数据库中，对数据库的设计显得尤为重要。如估计数据可能占用的磁盘空间有多大，日志文件及其他要占用多大空间。 创建数据库的用户自动成为数据库的拥有者。 删除数据库 语法形式：Drop Database 库名 可视化操作(查询分析器)：数据库名(鼠标右键) -&gt; Delete 对不再需要的数据库，应删除以释放空间。删除的结果将是所有数据库文件都一并被删除。 当数据库处于正在使用或正在恢复状态时，不能删除。 备份数据库 可视化操作(查询分析器)：数据库名(鼠标右键) -&gt; Tasks -&gt; Back Up… 备份就是对数据库或事务日志进行备份。SQL的备份是动态的，备份的过程还可以让用户继续改写。只有系统管理员、数据库的拥有者及数据库的备份者才有权限进行数据备份。可以通过企业管理器进行数据库备份。 完全数据库备份：完全备份数据文件和日志文件。 差异备份（增量备份）：对最近一次数据库备份以来发生的数据变化进行备份。这要在完全备份的基础上进行。特点是速度快。 事务日志备份：对数据库发生的事务进行备份。包括从上次进行事务日志备份、差异备份和数据库完全备份之后，所有已经完成的事务。能尽可能的恢复最新的数据库记录。特点是所需磁盘空间小，时间少。 数据库文件和文件组备份：用在数据库相当大的情况下。 恢复数据库 可视化操作(查询分析器)：数据库名(鼠标右键) -&gt; Tasks -&gt; Restore 数据库的恢复是指将数据库备份加载到系统中的过程。在根据数据库备份文件恢复过程中，系统将自动执行安全性检查、重建数据库结构及完成填写数据库内容。 数据库的恢复是静态的。所以在恢复前，应将需要恢复的数据库访问属性设为单用户，不要让其他用户操作。 可以通过企业管理器来完成数据库恢复。 数据库授权: 语法形式：grant 权限 on 表名 to 用户名 权限有：select,update,insert,delete,exec,dri。 对被授权的用户，要先成为该数据库的使用者，即要把用户加到数据库里,才能授权. 6.2 SQL Server数据表的创建-与增/删/改/查 创建表 同一用户不能建立同一个表名的表，同一表名的表可有多个拥有者。但在使用时，需要在这些表上加上所有者的表名。 用T-SQL语句创建表，语法形式：CREATE TABLE [数据库名.所有者名.]表名 (&#123;&lt;列名 数据类型&gt;&#125; [缺省值][约束][是否为空] …) 注意：T-SQL是SQL Server软件的SQL语言，与标准版有些差异。但标准版SQL，一般情况下SQL Server软件也都支持 可视化操作(查询分析器)：数据库名 -&gt; Tables -&gt; New Table… 增加、修改表字段 语法形式：ALTER TABLE ADD | ALTER 字段名 &lt;类型&gt; 创建、删除与修改约束 约束是SQL提供自动保持数据库完整性的一种方法，共5种。 用T-SQL语句建立约束，语法形式：CONSTRAINT 约束名 约束类型 (列名) 约束名：在库中应该唯一，如不指定，系统会给出 约束类型 (5种)： primary key constraint (主键值) unique constraint (唯一性) check constraint (检查性) default constraint (默认) foreign key constraint (外部键) 列名：要约束的字段名 示例:Create Table Course ( C# char(3) , Cname char(12), Chours integer, Credit float(1), T# char(3) ) constraint pk primary key(C# )); 7. SQL语言-子查询 子查询：出现在Where子句中的Select语句被称为子查询(subquery) , 子查询返回了一个集合，可以通过与这个集合的比较来确定另一个查询集合。 三种类型的子查询：(NOT) IN-子查询；θ-Some/θ-All子查询；(NOT) EXISTS子查询 7.1 (NOT) IN子查询 基本语法：表达式 [not] in (子查询) 语法中，表达式的最简单形式就是列名或常数。 语义：判断某一表达式的值是否在子查询的结果中。 示例： Select * From Student Where Sname in (&#39;张三&#39;, &#39;王三&#39;);//列出张三、王三同学的所有信息 Select S#, Sname From Student Where S# in (Select S# From SC Where C#=&#39;001&#39;);//列出选修了001号课程的学生的学号和姓名 非相关子查询：内层查询独立进行，没有涉及任何外层查询相关信息的子查询前面的子查询示例都是非相关子查询 相关子查询：内层查询需要依靠外层查询的某些参量作为限定条件才能进行的子查询 外层向内层传递的参量需要使用外层的表名或表别名来限定 示例：Select Sname From Student Stud Where S# in ( Select S# From SC Where S# = Stud.S# and C#=&#39;001&#39;);//求学过001号课程的同学的姓名 注意：相关子查询只能由外层向内层传递参数，而不能反之；这也称为变量的作用域原则。 7.2 θ-Some/θ-All子查询 基本语法：表达式 θ some (子查询) / 表达式 θ all (子查询) 语法中，θ是比较运算符：&lt;, &gt;, &gt;=, &lt;=, =, &lt;&gt;。 语义：将表达式的值与子查询的结果进行比较： 如果表达式的值至少与子查询结果的某一个值相比较满足 关系，则表达式 θ some (子查询)的结果便为真 如果表达式的值与子查询结果的所有值相比较都满足 关系，则表达式 θ all (子查询)的结果便为真 示例： Select Tname From Teacher Where Salary &lt;= all ( Select Salary From Teacher);//找出工资最低的教师姓名 Select S# From SC Where C# = “001” and Score &lt; some ( Select Score From SC Where C#=&#39;001&#39;);//找出001号课成绩不是最高的所有学生的学号 在SQL标准中，也有θ-Any谓词，但由于其语义的模糊性：any,“任一”是指所有呢？还是指某一个？不清楚，所以被θ-Some替代以求更明晰。 等价性变换需要注意 表达式 = some (子查询)和表达式 in (子查询)含义相同 表达式 &lt;&gt; some (子查询)和表达式 not in (子查询)含义不同 表达式 &lt;&gt; all (子查询)和表达式 not in (子查询)含义相同 7.3 (NOT) EXISTS子查询 基本语法：[not] Exists [not] Exists (子查询) 语义：子查询结果中有无元组存在 1234567891011121314--示例：检索选修了赵三老师主讲课程的所有同学的姓名Select DISTINCT Sname From Student Where exists ( Select * From SC, Course, Teacher Where SC.C#=Course.C# and SC. S#=Student.S# and Course.T# = Teacher.T# and Tname=&#x27;赵三&#x27;);--示例：检索学过001号教师主讲的所有课程的所有同学的姓名Select Sname From Student Where not exists //不存在 ( Select * From Course //有一门001教师主讲课程 Where Course.T# = ‘001’ and not exists //该同学没学过 ( Select * From SC Where S# = Student.S# and C# = Course.C#));--上述语句的意思：不存在有一门001号教师主讲的课程该同学没学过 8. SQL语言-结果计算与聚集计算8.1 结果计算Select-From-Where语句中，Select子句后面不仅可是列名，而且可是一些计算表达式或聚集函数，表明在投影的同时直接进行一些运算 Select Select 列名 | expr | agfunc(列名) [[, 列名 | expr | agfunc(列名) ] … ] From 表名1 [, 表名2 … ] [ Where Where 检索条件 ]; expr可以是常量、列名、或由常量、列名、特殊函数及算术运算符构成的算术运算式。特殊函数的使用需结合各自DBMS的说明书 agfunc()是一些聚集函数 1234--示例：求有差额(差额&gt;0)的任意两位教师的薪水差额Select T1.Tname as TR1, T2.Tname as TR2, T1.Salary – T2.Salary From Teacher T1, Teacher T2 Where T1.Salary &gt; T2.Salary; 8.2 聚集函数SQL提供了五个作用在简单列值集合上的内置聚集函数agfunc,分别是：COUNT、SUM、AVG、MAX、MIN 聚合函数 支持的数据类型 描述 count() 任何类型/* 计算结果集中的总行数 sum() Numeric 计算指定列中所有非空值的总和 avg() numeric 计算指定列中所有非空值的平均值 max() char/numeric 返回指定列中最大值 min() char/numeric 返回指定列中最小值 12345678--示例：求教师的工资总额Select Sum(Salary) From Teacher;--示例：求计算机系教师的工资总额Select Sum(Salary) From Teacher T, Dept Where Dept.Dname = ‘计算机’ and Dept.D# = T.D#;--示例：求数据库课程的平均成绩Select AVG(Score) From Course C, SC Where C.Cname = ‘数据库’ and C.C# = SC.C#; 9. SQL语言-分组查询与分组过滤9.1 分组查询分组：SQL可以将检索到的元组按照某一条件进行分类，具有相同条件值的元组划到一个组或一个集合中，同时处理多个组或集合的聚集运算。 分组的基本语法：1234Select Select 列名 | expr | agfunc(列名) [[, 列名 | expr | agfunc(列名) ] … ] From 表名1 [, 表名2 … ] [ Where Where 检索条件 ] [ Group by Group by 分组条件 ] ; 分组条件可以是：列名1, 列名2, … 示例： 求每一个学生的平均成绩 Select S#, AVG(Score) From SC Group by S#; 9.2 分组过滤聚集函数是不允许用于Where子句中的：Where子句是对每一元组进行条件过滤，而不是对集合进行条件过滤 分组过滤：若要对集合(即分组)进行条件过滤，即满足条件的集合/分组留下，不满足条件的集合/分组剔除。 Having子句，又称分组过滤子句。需要有Groupby子句支持，换句话说，没有Groupby子句，便不能有Having子句。 基本语法：1234Select Select 列名 | expr | agfunc(列名) [[, 列名 | expr | agfunc(列名) ] … ] From 表名1 [, 表名2 … ] [ Where Where 检索条件 ] [ Group by Group by 分组条件 [ Having Having 分组过滤条件] ] ; 示例：求不及格课程超过两门的同学的学号 Select S# From SC Where Score&lt;60 Group by S# Having Count(*)&gt;2; 9.3 where子句与having子句的区别 聚合函数是比较where、having 的关键。在from后面的执行顺序： where -&gt; 聚合函数(sum,min,max,avg,count) -&gt;having 列出group by来比较二者: where子句：是在分组之前使用，表示从所有数据中筛选出部分数据，以完成分组的要求，在where子句中不允许使用统计函数，没有group by子句也可以使用。 having子句：是在分组之后使用的，表示对分组统计后的数据执行再次过滤，可以使用统计函数，有group by子句之后才可以出现having子句。 注意事项 ： where 后不能跟聚合函数，因为where执行顺序大于聚合函数。 where 子句的作用是在对查询结果进行分组前，将不符合where条件的行去掉，即在分组之前过滤数据，条件中不能包含聚组函数，使用where条件显示特定的行。 having 子句的作用是筛选满足条件的组，即在分组之后过滤数据，条件中经常包含聚组函数，使用having 条件显示特定的组，也可以使用多个分组标准进行分组。 10. SQL语言实现关系代数操作SQL语言：并运算UNION, 交运算INTERSECT, 差运算EXCEPT。 基本语法形式： 子查询 &#123;Union [ALL] | Intersect [ALL] | Except [ALL] 子查询&#125; 通常情况下自动删除重复元组：不带ALL。若要保留重复的元组，则要带ALL。 假设子查询1的一个元组出现m次，子查询2的一个元组出现n次，则该元组在： 子查询1 Union ALL 子查询2 ，出现m + n次 子查询1 Intersect ALL 子查询2 ，出现min(m,n)次 子查询1 Except ALL 子查询2 ，出现max(0, m – n)次 UNION运算符是Entry-SQL92的一部分, INTERSECT、EXCEPT运算符是Full-SQL92的一部分,它们都是Core-SQL99的一部分，但有些DBMS并不支持这些运算，使用时要注意。 10.1 SQL并运算(UNION) 示例：已知两个表 Customers(Cid, Cname, City, Discnt) Agents(Aid, Aname, City, Percent) 求客户所在的或者代理商所在的城市123Select City From CustomersUNIONSelect City From Agents; 10.2 SQL交运算(INTERSECT) 示例：求既学过002号课，又学过003号课的同学学号 123Select S# From SC Where C# = ‘002’INTERSECTSelect S# From SC Where C# = ‘003’; 上述语句也可采用如下不用INTERSECT的方式来进行 Select S# From SC Where C# = ‘002’ and S# IN (Select S# From SC Where C# = ‘003’); 交运算符Intersect并没有增强SQL的表达能力，没有Intersect， SQL也可以用其他方式表达同样的查询需求。只是有了Intersect更容易表达一些，但增加了SQL语言的不唯一性。 10.3 SQL差运算(EXCEPT) 示例： 假定所有学生都有选课，求没学过002号课程的学生学号123Select DISTINCT S# From SCEXCEPTSelect S# From SC Where C# = ‘002’; 上述语句也可采用如下不用INTERSECT的方式来进行123Select DISTINCT S# From SC SC1 Where not exists ( Select * From SC Where C# = ‘002’ and S# = SC1.S#); 差运算符Except也没有增强SQL的表达能力，没有Except， SQL也可以用其他方式表达同样的查询需求。只是有了Except更容易表达一些，但增加了SQL语言的不唯一性。 10.4 空值的处理空值是其值不知道、不确定、不存在的值；数据库中有了空值，会影响许多方面，如影响聚集函数运算的正确性，不能参与算术、比较或逻辑运算等 在SQL标准中和许多现流行的DBMS中，空值被用一种特殊的符号Null来标记，使用特殊的空值检测函数来获得某列的值是否为空值。 空值检测： is [not ] null //测试指定列的值是否为空值 示例：找出年龄值为空的学生姓名 Select Sname From Student Where Sage is null; 现行DBMS的空值处理小结 除is[not]null之外，空值不满足任何查找条件 如果null参与算术运算，则该算术表达式的值为null 如果null参与比较运算，则结果可视为false。在SQL-92中可看成unknown 如果null参与聚集运算，则除count(*)之外其它聚集函数都忽略null 10.5 内连接、外连接 标准SQL语言中连接运算通常为： Select Select 列名[[,列名]… ] From 表名1,表名2,… Where 检索条件; 即相当于采用Π[列名,…,列名](σ 检索条件(表名1 × 表名2 × …))。 SQL的高级语法中引入了内连接与外连接运算，具体形式：12345Select Select 列名 [ [, 列名] … ] From 表名1 [NATURAL] [ INNER | &#123; LEFT | RIGHT | FULL&#125; [OUTER]] JOIN 表名2 &#123; ON 连接条件 | Using (Colname &#123;, Colname …&#125;) &#125; [ Where Where 检索条件 ] … ; 由 连接类型 和 连接条件 构成连接运算。 **Natural**：出现在结果关系中的两个连接关系的元组在公共属性上取值相等，且公共属性只出现一次 Inner Join: 即关系代数中的θ-连接运算 Left Outer Join, Right Outer Join, Full Outer Join: 即关系代数中的外连接运算 **on &lt;连接条件&gt;**：出现在结果关系中的两个连接关系的元组取值满足连接条件，且公共属性出现两次 **using (Col1, Col2, …, Coln)**：Col是两个连接关系的公共属性的子集，元组在(Col1,Col2,…,Coln)上取值相等，且(Col1,Col2,…,Coln)只出现一次 示例: 1234567891011-- (Inner Join)求所有教师的任课情况并按教师号排序(没有任课的教师也需列在表中)Select Teacher.T#, Tname, Cname From Teacher Inner Join Course ON Teacher.T# = Course.T# Order by Teacher.T# ASC;--(Outer Join)求所有教师的任课情况(没有任课的教师也需列在表中)Select Teacher. T#, Tname, Cname From Teacher Left Outer Join Course ON Teacher.T# = Course.T# Order by Teacher.T# ASC ; 11. SQL语言之视图及其应用 数据库的三级模式两层映像 三级模式：数据库系统是由外模式、模式(概念模式)和内模式三级构成 应用–&gt; 外模式(多个) –&gt; 概念模式(一个) –&gt; 内模式(一个) –&gt; 数据库 两层映像：E-C映像(外模式-&gt;概念模式)、C-I映像(概念模式-&gt;内模式)。 对应概念模式的数据在SQL中被称为基本表(Table),而对应外模式的数据称为视图(View)**。视图不仅包含外模式，而且包含其E-C映像**。 基本表是实际存储于存储文件中的表，基本表中的数据是需要存储的 视图在SQL中只存储其由基本表导出视图所需要的公式，即由基本表产生视图的映像信息，其数据并不存储，而是在运行过程中动态产生与维护的 对视图数据的更改最终要反映在对基本表的更改上。 11.1 视图的定义视图需要“先定义，再使用”；定义视图，有时可方便用户进行检索操作。 定义视图: create view view_name [(列名[列名] …)] as 子查询 [with check option] 如果视图的属性名缺省，则默认为子查询结果中的属性名；也可以显式指明其所拥有的列名。 with checkoption指明当对视图进行insert，update，delete时，要检查进行insert/update/delete的元组是否满足视图定义中子查询中定义的条件表达式 示例：定义一个视图 CompStud 为计算机系的学生，通过该视图可以将Student表中其他系的学生屏蔽掉1234Create View CompStud AS (Select * From Student Where D# in (Select D# From Dept Where Dname = ‘计算机’)); 11.2 视图的使用使用视图：定义好的视图，可以像Table一样，在SQL各种语句中使用 示例：检索计算机系的所有学生，我们可使用CompStud Select * From CompStud; 示例：检索计算机系的年龄小于20的所有学生，我们可使用CompStud Select * From CompStud Where Sage&lt;20; 11.3 视图的更新SQL视图更新：是比较复杂的问题，因视图不保存数据，对视图的更新最终要反映到对基本表的更新上，而有时，视图定义的映射不是可逆的。 SQL视图更新的可执行性 如果视图的select目标列包含聚集函数，则不能更新 如果视图的select子句使用了unique或distinct，则不能更新 如果视图中包括了groupby子句，则不能更新 如果视图中包括经算术表达式计算出来的列，则不能更新 如果视图是由单个表的列构成，但并没有包括主键，则不能更新 对于由单一Table子集构成的视图，即如果视图是从单个基本表使用选择、投影操作导出的，并且包含了基本表的主键，则可以更新 可更新SQL视图示例： 1234567-- 定义视图create view CStud(S#, Sname, Sclass)as ( select S#, Sname, Sclass from Student where D# =&#x27;03&#x27;);-- 更新视图Insert into CStud Values (&#x27;98030104&#x27;, &#x27;张三丰&#x27;, &#x27;980301&#x27;);-- 更新视图 将转换为 更新基本表insert into Student values (&#x27;98030104&#x27;, &#x27;张三丰&#x27;, Null, Null, &#x27;03&#x27;, &#x27;980301&#x27;) 11.4 视图的撤销已经定义的视图也可以撤消 撤消视图：Drop View view_name 不仅视图可以撤消，基本表、数据库等都可以撤消 撤消基本表：Drop Table 表名 12. 数据库完整性数据库完整性(DB Integrity)是指：DBMS应保证的DB的一种特性–在任何情况下的正确性、有效性和一致性 广义完整性：语义完整性、并发控制、安全控制、DB故障恢复等 狭义完整性：专指语义完整性，DBMS通常有专门的完整性管理机制与程序来处理语义完整性问题。 12.1 基本概念关系模型中有完整性要求：实体完整性、参照完整性、用户自定义完整性 数据库完整性管理的作用 防止和避免数据库中不合理数据的出现 DBMS应尽可能地自动防止DB中语义不合理现象 如DBMS不能自动防止，则需要应用程序员和用户在进行数据库操作时处处加以小心，每写一条SQL语句都要考虑是否符合语义完整性，这种工作负担是非常沉重的，因此应尽可能多地让DBMS来承担 DBMS怎样自动保证完整性： DBMS允许用户定义一些完整性约束规则(用SQL-DDL来定义) 当有DB更新操作时，DBMS自动按照完整性约束条件进行检查，以确保更新操作符合语义完整性 完整性约束条件(或称完整性约束规则)的一般形式：Integrity Constraint::=(O,P,A,R) O：数据集合：约束的对象(列、多列(元组)、元组集合) P：谓词条件：需要定义什么样的约束 A：触发条件：默认更新时检查 R：响应动作：默认拒绝 12.2 数据库完整性的分类 按约束对象分类: 域完整性约束条件：施加于某一列上，对给定列上所要更新的某一候选值是否可以接受进行约束条件判断，这是孤立进行的 关系完整性约束条件：施加于关系/table上，对给定table上所要更新的某一候选元组是否可以接受进行约束条件判断，或是对一个关系中的若干元组和另一个关系中的若干元组间的联系是否可以接受进行约束条件判断 按约束来源分类: 结构约束：来自于模型的约束，例如函数依赖约束、主键约束(实体完整性)、外键约束(参照完整性)，只关心数值相等与否、是否允许空值等； 内容约束：来自于用户的约束，如用户自定义完整性，关心元组或属性的取值范围。例如Student表的Sage属性值在15岁至40岁之间等。 按约束状态分类: 静态约束：要求DB在任一时候均应满足的约束；例如Sage在任何时候都应满足大于0而小于150(假定人活最大年龄是150)。 动态约束：要求DB从一状态变为另一状态时应满足的约束；例如工资只能升，不能降：工资可以是800元，也可以是1000元；可以从800元更改为1000元，但不能从1000元更改为800元。 13. 数据库的静态完整性(约束) SQL语言支持的约束类别： 静态约束 列完整性—域完整性约束 表完整性–关系完整性约束 动态约束 触发器 CreateTable有三种功能：定义关系模式、定义完整性约束 和定义物理存储特性 定义完整性约束条件：列完整性、表完整性 列约束：一种域约束类型，对单一列的值进行约束 1234567&#123; NOT NULL | //列值非空[ CONSTRAINT constraintname ] //为约束命名，便于以后撤消&#123; UNIQUE //列值是唯一| PRIMARY KEY //列为主键| CHECK (search_cond) //列值满足条件,条件只能使用列当前值| REFERENCES tablename [(colname) ][ON DELETE &#123; CASCADE | SET NULL &#125; ] &#125; &#125; 表约束：一种关系约束类型，对多列或元组的值进行约束 1234567[ CONSTRAINT constraintname ] //为约束命名，便于以后撤消&#123; UNIQUE (colname &#123;,colname…&#125;) //几列值组合在一起是唯一| PRIMARY KEY (colname &#123;,colname…&#125;) //几列联合为主键| CHECK (search_condition) //元组多列值共同满足条件 //条件中只能使用同一元组的不同列当前值| FOREIGN KEY (colname &#123;,colname…&#125;)REFERENCES tablename [(colname &#123;,colname…&#125;)]//引用另一表tablename的若干列的值作为外键 check中的条件可以是Select-From-Where内任何Where后的语句，包含子查询。 Create Table中定义的表约束或列约束可以在以后根据需要进行撤消或追加。撤消或追加约束的语句是 Alter Table(不同系统可能有差异) 示例：撤消SC表的ctscore约束(由此可见，未命名的约束是不能撤消) Alter Table SC DROP CONSTRAINT ctscore; 有些DBMS支持独立的追加约束,注意书写格式可能有些差异 示例：Alter Table SC Add Constraint nctscore check (Score&gt;=0.0 and Score&lt;=150.0)); 现约束的方法-断言ASSERTION 一个断言就是一个谓词表达式，它表达了希望数据库总能满足的条件 表约束和列约束就是一些特殊的断言 SQL还提供了复杂条件表达的断言。其语法形式为： CREATE ASSERTION &lt;assertion-name&gt; CHECK &lt;predicate&gt; 当一个断言创建后，系统将检测其有效性，并在每一次更新中测试更新是否违反该断言。 1234567891011-- 示例: “每个分行的贷款总量必须小于该分行所有账户的余额总和”create assertion sum_constraint check (not exists (select * from branch where (select sum(amount ) from loan where loan.branch_name = branch.branch_name ) &gt;= (select sum (balance ) from account where account.branch_name = branch.branch_name )))-- 数据表：account(branch_name, account_number,…, balance) //分行，账户及其余额loan(branch_name , loan_number, amount,) //分行的每一笔贷款branch(branch_name, … ) //分行 断言测试增加了数据库维护的负担，要小心使用复杂的断言。 14. 数据库的动态完整性(触发器)实现数据库动态完整的方法—触发器Trigger 触发器Trigger Create Table中的表约束和列约束基本上都是静态的约束，也基本上都是对单一列或单一元组的约束(尽管有参照完整性)，为实现动态约束以及多个元组之间的完整性约束，就需要触发器技术Trigger Trigger是一种过程完整性约束(相比之下，Create Table中定义的都是非过程性约束),是一段程序，该程序可以在特定的时刻被自动触发执行，比如在一次更新操作之前执行，或在更新操作之后执行。 基本语法 12345678CREATE TRIGGER trigger_name BEFORE | AFTER &#123; INSERT | DELETE | UPDATE [OF colname &#123;, colname...&#125;] &#125; ON tablename [REFERENCING corr_name_def &#123;, corr_name_def...&#125; ] [FOR EACH ROW | FOR EACH STATEMENT] //对更新操作的每一条结果(前者)，或整个更新操作完成(后者) [WHEN (search_condition)] //检查条件，如满足执行下述程序 &#123; statement //单行程序直接书写，多行程序要用下行方式 | BEGIN ATOMIC statement; &#123; statement;...&#125; END &#125; 触发器Trigger意义： 当某一事件发生时(Before|After),对该事件产生的结果(或是每一元组，或是整个操作的所有元组), 检查条件search_condition,如果满足条件，则执行后面的程序段。条件或程序段中引用的变量可用corr_name_def来限定。 事件：BEFORE | AFTER { INSERT | DELETE | UPDATE …} 当一个事件(Insert, Delete, 或Update)发生之前Before或发生之后After触发 操作发生，执行触发器操作需处理两组值：更新前的值和更新后的值，这两个值由corr_name_def的使用来区分 corr_name_def的定义 12345&#123; OLD [ROW] [AS] old_row_corr_name //更新前的旧元组命别名为| NEW [ROW] [AS] new_row_corr_name //更新后的新元组命别名为| OLD TABLE [AS] old_table_corr_name //更新前的旧Table命别名为| NEW TABLE [AS] new_table_corr_name //更新后的新Table命别名为&#125; corr_name_def将在检测条件或后面的动作程序段中被引用处理 示例1: 设计一个触发器当进行Teacher表更新元组时, 使其工资只能升不能降 12345678create trigger teacher_chgsal before update of salary on teacher referencing new x, old y for each row when (x.salary &lt; y.salary)begin raise_application_error(-20003, &#x27;invalid salary on update&#x27;); //此条语句为Oracle的错误处理函数end; 示例2: 假设student(S#, Sname, SumCourse), SumCourse为该同学已学习课程的门数，初始值为0，以后每选修一门都要对其增1 。设计一个触发器自动完成上述功能。 1234567create trigger sumc after insert on sc referencing new row newi for each rowbegin update student set SumCourse = SumCourse + 1 where S# = :newi.S# ;end; 示例3：假设student(S#, Sname, SumCourse), 当删除某一同学S#时，该同学的所有选课也都要删除。设计一个触发器完成上述功能 123456create trigger delS# after delete on Student referencing old oldi for each rowbegin delete sc where S# = :oldi.S# ;end; 15. 数据库索引索引是对数据库表中一列或多列的值进行排序的一种数据结构（最常见的是B-Tree） 索引的作用 快速取数据； 保证数据记录的唯一性； 实现表与表之间的参照完整性； 在使用ORDER by、group by子句进行数据检索时，利用索引可以减少排序和分组的时间。 创建索引：CREATE INDEX 索引名称 on 表名(字段名); 删除索引：DROP INDEX 索引名称 索引注意事项： 查询时减少使用*返回全部列，不要返回不需要的列 where表达式子句包含索引的表达式置前 避免在Order by中使用表达式 索引技术是数据库自动使用，一个表格只存在一个索引就够了 缺点 索引的缺点是创建和维护索引需要耗费时间和空间 索引可以提高查询速度，会减慢写入速度 索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 15.1 索引主要种类根据数据库的功能，可以在数据库设计器中创建三种索引：唯一索引、主键索引和聚集索引。提示：尽管唯一索引有助于定位信息，但为获得最佳性能结果，建议改用主键或唯一约束。 唯一索引 唯一索引是不允许其中任何两行具有相同索引值的索引。当现有数据中存在重复的键值时，大多数数据库不允许将新创建的唯一索引与表一起保存。数据库还可能防止添加将在表中创建重复键值的新数据。例如，如果在employee表中职员的姓(lname)上创建了唯一索引，则任何两个员工都不能同姓。 主键索引 数据库表经常有一列或多列组合，其值唯一标识表中的每一行。该列称为表的主键。在数据库关系图中为表定义主键将自动创建主键索引，主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时，它还允许对数据的快速访问。 聚集索引 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。聚集索引和非聚集索引的区别，如字典默认按字母顺序排序，读者如知道某个字的读音可根据字母顺序快速定位。因此聚集索引和表的内容是在一起的。如读者需查询某个生僻字，则需按字典前面的索引，举例按偏旁进行定位，找到该字对应的页数，再打开对应页数找到该字。这种通过两个地方而查询到某个字的方式就如非聚集索引。 索引列 可以基于数据库表中的单列或多列创建索引。多列索引可以区分其中一列可能有相同值的行。如果经常同时搜索两列或多列或按两列或多列排序时，索引也很有帮助。例如，如果经常在同一查询中为姓和名两列设置判据，那么在这两列上创建多列索引将很有意义。 16. 数据库序列序列(SEQUENCE)是序列号生成器，可以为表中的行自动生成序列号，产生一组等间隔的数值(类型为数字)。其主要的用途是生成表的主键值，可以在插入语句中引用，也可以通过查询检查当前值，或使序列增至下一个值。创建序列需要CREATE SEQUENCE系统权限。 16.1 Oracle中的序列（Sequence） 创建序列1234567create sequence 序列名 [increment by n] --每次增加n个，默认为1 [start with n] --起始值n，默认为1 [&#123;maxvalue n | nomaxvalue&#125;] --最大值设置，递增默认10的27次方，递减默认-1 [&#123;minvalue n | nominvalue&#125;] --最小值设置，递增默认1，递减默认-10的26次方 [&#123;cycle | nocycle&#125;] --是否循环 [&#123;cache n | nocache&#125;] --是否对序列进行内存缓冲，默认为20 查询序列 NEXTVAL:返回序列中下一个有效的值，任何用户都可以引用。 CURRVAL:中存放序列的当前值,NEXTVAL 应在 CURRVAL 之前指定 ，二者应同时有效。 1234--查询下一个将要使用的序列select 序列名.nextval from dual--查询当前序列select 序列名.currval from dual Oracle将sequence的定义存储在数据字典之中。 Sequence是独立于事务的，就是说序列的增加不需要等待事务的完成，也就是说序列是异步于事务而增长的。这说明，你访问不到别的用户使用该sequence产生的值，也就是说你只能访问到你当前产生的值，即使其他用户已经增加了sequence的值；还说明如果事务回滚，sequence不会回滚，它所发生的改变是一维的。 删除序列：Drop sequence 序列名 更改序列：Alter sequence 序列名 [其余参数同创建序列] 使用序列示例： 123456789101112-- 1.直接使用insert into person (id, name, password) values (序列名.nextval, &#x27;张三&#x27;, &#x27;123&#x27;)-- 2.也可以通过建立触发器，当有数据插入表person时，使用oracle序列为其去的递增的主键值-- 2.1创建触发器create or replace trigger 触发器名 before insert on personfor each rowbegin select 序列名.nextval into :new.id from dual;end;-- 2.2插入数据insert into person ( username, age, password) values (&#x27;张三&#x27;, 20, &#x27;zhang123&#x27;) 注意点： 一个序列可以被多张别使用，不过一般建议为每个表建立单独的序列。 当使用到序列的事务发生回滚。会造成序列号不连续。在用生成的序列值作为编号做插入数据库操作时，可能遇到事务提交失败，从而导致序号不连续。 大量语句发生请求，申请序列时，为了避免序列在运用层实现序列而引起的性能瓶颈。Oracle序列允许将序列提前生成 n个先存入内存，在发生大量申请序列语句时，可直接到运行最快的内存中去得到序列。但cache个数最好不要设置过大，因为在数据库重启时，会清空内存信息，预存在内存中的序列会丢失，当数据库再次启动后，序列从上次内存中最大的序列号+1 开始存入n个。这种情况也能会在数据库关闭时也会导致序号不连续。 16.2 Mysql中的序列（AUTO_INCREMENT）MySQL中最简单使用序列的方法就是使用AUTO_INCREMENT来定义列。 orale没有类似mysql的AUTO_INCREMENT这样的自增长字段，实现插入一条记录，自动增加1.oracle是通过sequence（序列）来完成的。 首先mysql的自增长“序列”和序列是两回事，mysql本身不提供序列机制。 mysql的AUTO_INCREMENT可以设置起始值，但是不能设置步长，其步长默认就是1. mysql一个表只能有一个自增长字段。自增长只能被分配给固定表的固定的某一字段，不能被多个表共用。并且只能是数字型。 17. 数据库安全性数据库安全性是指DBMS应该保证的数据库的一种特性(机制或手段)：免受非法、非授权用户的使用、泄漏、更改或破坏 数据库安全性管理涉及许多方面 社会法律及伦理方面：私人信息受到保护，未授权人员访问私人信息会违法 公共政策/制度方面：例如，政府或组织的信息公开或非公开制度 安全策略：政府、企业或组织所实施的安全性策略，如集中管理和分散管理，需者方知策略(也称最少特权策略) 数据的安全级别: 绝密(Top Secret), 机密(Secret),可信(Confidential)和无分类(Unclassified) 数据库系统DBS的安全级别：物理控制、网络控制、操作系统控制、DBMS控制 DBMS的安全机制 自主安全性机制：存取控制(AccessControl) 通过权限在用户之间的传递，使用户自主管理数据库安全性 强制安全性机制： 通过对数据和用户强制分类，使得不同类别用户能够访问不同类别的数据 推断控制机制： 防止通过历史信息，推断出不该被其知道的信息； 防止通过公开信息(通常是一些聚集信息)推断出私密信息(个体信息)，通常在一些由个体数据构成的公共数据库中此问题尤为重要 数据加密存储机制： 通过加密、解密保护数据，密钥、加密/解密方法与传输 DBA的责任和义务 熟悉相关的法规、政策，协助组织的决策者制定好相关的安全策略 规划好安全控制保障措施，例如，系统安全级别、不同级别上的安全控制措施，对安全遭破坏的响应， 划分好数据的安全级别以及用户的安全级别 实施安全性控制：DBMS专门提供一个DBA账户，该账户是一个超级用户或称系统用户。DBA利用该账户的特权可以进行用户账户的创建以及权限授予和撤消、安全级别控制调整等 18. 数据库自主安全性机制 通常情况下，自主安全性是通过授权机制来实现的。 用户在使用数据库前必须由DBA处获得一个账户，并由DBA授予该账户一定的权限，该账户的用户依据其所拥有的权限对数据库进行操作; 同时，该帐户用户也可将其所拥有的权利转授给其他的用户(账户)，由此实现权限在用户之间的传播和控制。 授权者：决定用户权利的人 授权：授予用户访问的权利 DBMS自动实现自主安全性： DBMS允许用户定义一些安全性控制规则(用SQL-DCL来定义) 当有DB访问操作时，DBMS自动按照安全性控制规则进行检查，检查通过则允许访问，不通过则不允许访问 DBMS将权利和用户(账户)结合在一起，形成一个访问规则表，依据该规则表可以实现对数据库的安全性控制 AccessRule ::=(S, O, t, P) S: 请求主体(用户) O: 访问对象 t: 访问权利 P: 谓词 {AccessRule｝通常存放在数据字典或称系统目录中，构成了所有用户对DB的访问权利; 用户多时，可以按用户组建立访问规则 访问对象可大可小(目标粒度Object granularity):属性/字段、记录/元组、关系、数据库 权利：包括创建、增、删、改、查等 谓词：拥有权利需满足的条件 示例：员工管理数据库的安全性控制示例Employee(P#,Pname,Page,Psex,Psalary,D#,HEAD) 示例要求： 员工管理人员：能访问该数据库的所有内容，便于维护员工信息 收发人员：访问该数据库以确认某员工是哪一个部门的，便于收发工作，只能访问基本信息，其他信息不允许其访问 每个员工：允许其访问关于自己的记录，以便查询自己的工资情况，但不能修改 部门领导：能够查询其所领导部门人员的所有情况 高层领导：能访问该数据库的所有内容，但只能读 两种控制示例 按名控制安全性：存储矩阵 按内容控制安全性：视图 视图是安全性控制的重要手段 通过视图可以限制用户对关系中某些数据项的存取,例如： 视图1：CreateEmpV1as select*fromEmployee 视图2：CreateEmpV2as selectPname,D#fromEmployee 通过视图可将数据访问对象与谓词结合起来，限制用户对关系中某些元组的存取，例如： 视图1： CreateEmpV3asselect*fromEmployeewhereP#=:UserId 视图2： CreateEmpV4asselect*fromEmployeewhereHead=:UserId 用户定义视图后，视图便成为一新的数据对象，参与到存储矩阵与能力表中进行描述 18.1 SQL语言的用户与权利 SQL语言包含了DDL,DML和DCL。数据库安全性控制是属于DCL范畴 授权机制—自主安全性；视图的运用 关系级别(普通用户) &lt;– 账户级别(程序员用户) &lt;– 超级用户(DBA) (级别1)Select : 读(读DB, Table, Record, Attribute, … ) (级别2)Modify : 更新 Insert : 插入(插入新元组, … ) Update : 更新(更新元组中的某些值, …) Delete : 删除(删除元组, …) (级别3)Create : 创建(创建表空间、模式、表、索引、视图等) Create : 创建 Alter : 更新 Drop : 删除 级别高的权利自动包含级别低的权利。如某人拥有更新的权利，它也自动拥有读的权利。在有些DBMS中，将级别3的权利称为账户级别的权利，而将级别1和2称为关系级别的权利。 授权命令GRANT 1234GRANT &#123;all PRIVILEGES | privilege &#123;,privilege…&#125;&#125; ON [TABLE] tablename | viewname TO &#123;public | user-id &#123;, user-id…&#125;&#125; [WITH GRANT OPTION]; user-id ，某一个用户账户，由DBA创建的合法账户 public, 允许所有有效用户使用授予的权利 privilege是下面的权利 SELECT | INSERT | UPDATE | DELETE | ALL PRIVILEDGES WITH GRANT OPTION选项是允许被授权者传播这些权利 SQL-DCL的控制安全性-授权示例: 假定高级领导为Emp0001, 部门领导为Emp0021, 员工管理员为Emp2001,收发员为Emp5001(均为UserId, 也即员工的P#) Grant All Priviledges ON Employee TO Emp2001; Grant SELECT ON EmpV2 TO Emp5001; Grant SELECT ON EmpV3 TO public; Grant SELECT ON EmpV4 TO Emp0021; 授予视图访问的权利，并不意味着授予基本表访问的权利(两个级别：基本关系级别和视图级别) 授权者授予的权利必须是授权者已经拥有的权利 收回授权命令REVOKE 123REVOKE &#123;all privilEges | priv &#123;, priv…&#125; &#125; ON tablename | viewname FROM &#123;public | user &#123;, user…&#125; &#125;; 示例: revoke select on employee from UserB; 18.2 自主安全性的授权过程及其问题18.2.1 授权过程: 第一步：DBA创建DB, 并为每一个用户创建一个账户 假定建立了五个用户：UserA, UserB, UserC, UserD, UserE 第二步：DBA授予某用户账户级别的权利 假定授予UserA 第三步：具有账户级别的用户可以创建基本表或视图, 他也自动成为该表或该视图的属主账户，拥有该表或该视图的所有访问 权利 假定UserA创建了Employee, 则UserA就是Employee表的属主账户 第四步：拥有属主账户的用户可以将其中的一部分权利授予另外的用户，该用户也可将权利进一步授给其他的用户… 假定UserA将读权限授予UserB, 而userB又将其拥有的权限授予UserC,如此将权利不断传递下去。 注意授权的传播范围 传播范围包括两个方面：水平传播数量和垂直传播数量 水平传播数量是授权者的再授权用户数目(树的广度) 垂直传播数量是授权者传播给被授权者，再被传播给另一个被授权者, …传播的深度(树的深度) 有些系统提供了传播范围控制，有些系统并没有提供，SQL标准中也并没有限制。 当一个用户的权利被收回时，通过其传播给其他用户的权利也将被收回 如果一个用户从多个用户处获得了授权，则当其中某一个用户收回授权时，该用户可能仍保有权利。例如UserC从UserB和UserE处获得了授权，当UserB收回时，其还将保持UserE赋予其的权利。 18.2.2 强制安全性机制 强制安全性机制 强制安全性通过对数据对象进行安全性分级 绝密(Top Secret), 机密(Secret), 可信(Confidential) 和 无分类(Unclassified) 同时对用户也进行上述的安全性分级 从而强制实现不同级别用户访问不同级别数据的一种机制 强制安全性机制的实现 DBMS引入强制安全性机制, 可以通过扩展关系模式来实现 关系模式: R(A1: D1, A2: D2, …, An:Dn) 对属性和元组引入安全性分级特性或称分类特性 R(A1: D1, C1, A2: D2, C2…, An:Dn, Cn, TC)其中 C1,C2,…,Cn分别为属性D1,D2,…,Dn的安全分类特性; TC为元组的分类特性 这样, 关系中的每个元组, 都将扩展为带有安全分级的元组 强制安全性机制使得关系形成为多级关系(不同级别用户所能看到的关系的子集)，也出现多重实例、多级关系完整性等许多新的问题或新的处理技巧，在使用中需注意仔细研究。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"【数据库】数据库系统基础","date":"2019-06-22T12:20:19.000Z","path":"article/20190622.html","text":"概述 数据库 是 电子化信息的集合 将信息规范化并使之电子化，形成电子信息’库’，以便利用计算机对这些信息进行快速有效的存储、检索、统计与管理。 表(Table)：以按行按列形式组织及展现的数据 数据库起源于规范化“表(Table)”的处理，Table中描述了一批相互有关联关系的数据–&gt;关系 数据库系统的构成（概念层次）: 数据库(DB):Database：相互之间有关联关系的数据的集合 数据库管理系统(DBMS):Database Management System 数据库应用(DBAP):Database Application 数据库管理员(DBA):Database Asministrator 计算机基本系统 目录： 数据库管理系统(DBMS) 数据库系统的标准结构 三级模式两层映像 数据模型 关系模型 关系模型中的完整性 关系代数 关系演算 1. 数据库管理系统(DBMS)1.1 从用户角度看DBMS(数据库管理系统) 数据库定义：定义数据库中的Table的表名、标题(属性以及属性值的要求)等 DBMS提供了一套数据定义语言(DDL: Data Definition Language)给用户 用户使用DDL描述其所要建立的表的格式 DBMS依照用户的定义，创建数据库及其中的表 数据库操作：向数据库的Table中增加/删除/更新数据及对数据进行查询、检索、统计等 DBMS提供了一套数据库操纵语言(DML: Data Manipulation Language)给用户 用户使用DML描述其所要进行的增、删、改、查等操作 DBMS依照用户的操作描述，实际执行这些操作 数据库控制：控制数据库中数据的使用(哪些用户可以使用，哪些不可以) DBMS提供了一套数据控制语言(DCL: Data Control Language)给用户 用户使用DCL描述其对数据库所要实施的控制 DBMS依照用户描述，实际ijnx控制 数据库维护：转储/恢复/重组/性能监测/分析… DBMS提供了一系列程序(实用程序/例行程序)给用户 在这些程序中提供了对数据库维护的各种功能 用户使用这些程序进行各种数据库维护操作 (数据库维护的实用程序，一般由数据库管理员(DBA)来使用和掌握的) 1.2 数据库语言 使用者使用数据库语言，利用DBMS操纵数据库 SQL语言：结构化的数据库语言 高级语言：一条数据库语言相当于高级语言的一个或多个循环程序，数据库语言可以嵌入到高级语言(宿主语言)中使用 1.3 从系统实现角度看DBMS的功能 数据库管理系统的实现：形式 –&gt; 构造 –&gt; 自动化 DBMS为完成DB管理，在后台运行着一系列程序… 语言编译器：将数据库语言书写的内容，翻译成BDMS可执行的命令。例如：DDL编译器，DML编译器，DCL编译器等 查询优化(执行引擎)与查询实现(基本命令的不同执行算法)：提高数据库检索速度的手段。例如贯穿于数据存取各个阶段的优化程序 数据存取与索引：提供数据在磁盘/磁带等上的搞笑存取手段。例如：存储管理器，缓冲区管理器，索引/文件和记录管理器等 通信控制：提供网络环境下数据库操作与数据传输的手段 事务管理：提供提高可靠性并避免并发操作错误的手段 故障恢复：使用数据库自动恢复到故障发生前正确状态的手段。例如备份、运行日志操控等实用程序 安全性控制：提供合法性检验，避免非授权非法用户访问数据库的手段 完整性控制：提供数据及数据操作正确性检查的手段 数据字典管理：管理用户已经定义的信息 **应用程序接口(API)**：提供应用程序使用DBMS特定功能的二首段 数据库数据装载、重组等实用程序 数据库性能分析：统计在运行过程中数据库的各种性能数据，便于优化运行 典型的数据库管理系统(DBMS)：Oracle、DB2(IBM)、Sybase、Microsoft SQL Server、Microsoft Access、PostgreSQL 2. 数据库系统的标准结构DBMS管理数据的三个层次： External Level = User Level（外部级别 = 用户级别） 某一用户能够看到与处理的数据, 全局数据中的某一部分 Conceptual Level = Logic level（概念级别 = 逻辑级别） 从全局角度理解/管理的数据, 含相应的关联约束 Internal Level = Physical level（内部级别 = 物理级别） 存储在介质上的数据，含存储路径、存储方式 、索引方式等 3. 三级模式两层映像数据库的三级模式结构是指：数据库系统是由外模式、模式(概念模式)和内模式三级构成 应用–&gt; 外模式(多个) –&gt; 模式(一个) –&gt; 内模式(一个) –&gt; 数据库 3.1 数据(视图)与模式(数据的结构) 模式(Schema):对数据库中数据所进行的一种结构性的描述，所观察到数据的结构信息 视图(View)/数据(Data)：某一种表现形式下表现出来的数据库中的数据 3.2 三级模式(三级视图) External Schema —-(External) View 外模式：某一用户能够看到与处理的数据的结构描述 (Conceptual) Schema —- Conceptual View 模式(概念模式)：从全局角度理解/管理的数据的结构描述, 含相应的关联约束 体现在数据之间的内在本质联系 Internal Schema —- Internal View 内模式：存储在介质上的数据的结构描述，含存储路径、存储方式 、索引方式等 3.3 两层映像 E-C Mapping：External Schema-Conceptual Schema Mapping 将外模式映射为概念模式，从而支持实现数据概念视图向外部视图的转换 便于用户观察和使用 C-I Mapping：Conceptual Schema-Internal Schema Mapping 将概念模式映射为内模式，从而支持实现数据概念视图向内部视图的转换 便于计算机进行存储和处理 3.4 标准结构的两个独立性 逻辑数据独立性 当概念模式变化时，可以不改变外部模式(只需改变E-C Mapping)，从而无需改变应用程序 物理数据独立性 当内部模式变化时，可以不改变概念模式(只需改变C-I Mapping) ，从而不改变外部模式 4. 数据模型 数据模型：模式 与 模式的结构 规定模式统一描述方式的模型，包括：数据结构、操作和约束 数据模型是对模式本身结构的抽象，模式是对数据本身结构形式的抽象 比如：关系模型：所有模式都可为抽象表(Table)的形式[数据结构]，而每一个具体的模式都是拥有不同列名的具体的表。对这种表形式的数据有哪些[操作]和[约束] 三大经典数据模型 关系模型：表的形式组织数据 层次模型：树的形式组织数据 网状模型：图的形式组织数据 5. 关系模型 形象地说，一个关系(relation)就是一个Table，关系模型就是处理Table的，它由三个部分组成： 描述DB各种数据的基本结构形式(Table/Relation) 描述Table与Table之间所可能发生的各种操作(关系运算) 描述这些操作所应遵循的约束条件(完整性约束) 关系模型的三个要素： 基本结构：Relation/Table 基本操作：Relation Operator 基本的:(并, UNION)、(差, DIFFERENCE)、(广义积,PRODUCT)、(选择, SELECTION)、(投影, PROJECTION)。 扩展的:(交, INTERSECTION)、(连接, JOIN)、(除, DIVISION)运算 完整性约束：实体完整性、参照完整性和用户自定义的完整性 表(Table)的基本构成要素 列/字段/属性/数据项：列名，列值 行/元组/记录 标题/模式 5.1 “表”的严格定义 域(Domain)：“列”的取值范围，一组值的集合，这组值具有相同的数据类型 笛卡尔积(Cartesian Product)：“元组”及所有可能组合成的元组 关系(Relation)：一组域D1,D2,…,Dn的笛卡尔积的子集，笛卡尔积中具有某一方面意义的那些元组被称作一个关系(Relation) 5.2 关系模式与关系 同一关系模式下，可有很多的关系 关系模式是关系的结构, 关系是关系模式在某一时刻的数据 关系模式是稳定的；而关系是某一时刻的值，是随时间可能变化的 5.3 关系的特性 列是同质：即每一列中的分量来自同一域，是同一类型的数据 不同的列可来自同一个域，称其中的每一列为一个属性，不同的属性要给予不同的属性名。 列位置互换性：区分哪一列是靠列名 行位置互换性：区分哪一行是靠某一或某几列的值(关键字/键字/码字) 关系是以内容(名字或值)来区分的，而不是属性在关系的位置来区分 理论上，关系的任意两个元组不能完全相同。(集合的要求：集合内不能有相同的两个元素)；现实应用中，表(Table)可能并不完全遵守此特性。元组相同是指两个元组的每个分量(列值)都相同。 属性不可再分特性:又被称为关系第一范式 5.4 关系的一些重要概念 候选码(Candidate Key)/候选键 关系中的一个属性组，其值能唯一标识一个元组，若从该属性组中去掉任何一个属性，它就不具有这一性质了，这样的属性组称作候选码。 主码(Primary Key)/主键 当有多个候选码时，可以选定一个作为主码。DBMS以主码为主要线索管理关系中的各个元组 主属性与非主属性 包含在任何一个候选码中的属性被称作主属性，而其他属性被称作非主属性 最简单的，候选码只包含一个属性； 极端的，所有属性构成这个关系的候选码，称为全码(All-Key) 外码(Foreign Key)/外键 关系R中的一个属性组，它不是R的候选码，但它与另一个关系S的候选码相对应，则称这个属性组为R的外码或外键。 两个关系通常是靠外码连接起来的。 6. 关系模型中的完整性6.1 实体完整性 关系的主码中的属性值不能为空值； 意义：关系中的元组对应到现实世界相互之间可区分的一个个个体，这些个体是通过主码来唯一标识的；若主码为空，则出现不可标识的个体，这是不容许的。 6.2 参照完整性 如果关系R1的外码Fk与关系R2的主码Pk相对应，则R1中的每一个元组的Fk值或者等于R2 中某个元组的Pk 值，或者为空值 意义：如果关系R1的某个元组t1参照了关系R2的某个元组t2，则t2必须存在 6.3 用户自定义完整性 用户针对具体的应用环境定义的完整性约束条件 6.4 DBMS对关系完整性的支持 实体完整性和参照完整性由DBMS系统自动支持 DBMS系统通常提供了如下机制： 它使用户可以自行定义有关的完整性约束条件 当有更新操作发生时，DBMS将自动按照完整性约束条件检验更新操作的正确性，即是否符合用户自定义的完整性 7. 关系代数7.1 关系代数的特点 基于集合，提供了一系列的关系代数操作：并、差、笛卡尔积(广义积)、选择、投影和更名等基本操作 以及交、 连接和关系除等扩展操作，是一种集合思维的操作语言。 关系代数操作以一个或多个关系为输入，结果是一个新的关系。 用对关系的运算来表达查询，需要指明所用操作, 具有一定的过程性。 是一种抽象的语言，是学习其他数据库语言，如SQL等的基础 7.2 关系代数的约束某些关系代数操作，如并、差、交等，需满足”并相容性” 并相容性： 参与运算的两个关系及其相关属性之间有一定的对应性、可比性或意义关联性 定义：关系R与关系S存在相容性，当且仅当： (1) 关系R和关系S的属性数目必须相同； (2) 对于任意i，关系R的第i个属性的域必须和关系S的第i个属性的域相同 示例：关系R：STUDENT(SID char(10), Sname char(8), Age char(3)) 示例：关系S：TEACHER(TID char(10), Tname char(8), Age char(3)) 7.3 关系代数的基本操作 集合操作 并（UNIO）：R∪S 交（INTERSECTION）：R∩S 差（DIFFERENCE）：R-S 笛卡儿积（Cartesian PRODUCT）：R×S 纯关系操作 选择（SELECT）：σF(R) 投影（PROJECT）：ΠA(R) 连接（JOIN）：R⋈S 除（DIVISION）：R÷S 7.3.1 并(Union) 操作 定义：设关系R和关系S是并相容的，则关系R与关系S的并运算结果也是一个关系，记作：**R∪S**, 它由 或者出现在关系R中，或者出现在S中的元组构成。 数学描述：R∪S=&#123;t|t∈R∨t∈S&#125;，其中t是元组 并运算是将两个关系的元组合并成一个关系，在合并时去掉重复的元组。 汉语中的“或者…或者…”通常意义是并运算的要求。 R∪S 与 S∪R 运算的结果是同一个关系 7.3.2 差(Difference) 操作 定义：设关系R 和关系S是并相容的，则关系R与关系S的差运算结果也是一个关系，记作：**R-S**, 它由出现在关系R中但不出现在关系S中的元组构成。 数学描述：R－S=&#123;t|t∈R∧t∉S&#125;，其中t是元组 汉语中的“是…但不含…”通常意义是差运算的要求。 R-S 与 S-R 是不同的 7.3.3 交（Intersection Referential integrity） 操作 定义：设关系R和关系S具有相同的目n，且相应的属性取自同一个域，则关系R与关系S的交由既属于R又属于S的元组组成。其结果关系仍为n目关系。 数学描述：R∩S=&#123;t|t∈R∧t∈S&#125;，其中t是元组 7.3.4 广义笛卡尔积(Extended cartesian product) 操作 定义：关系R(&lt;a1,a2, …,an&gt;)与关系S(&lt;b1,b2, …,bm &gt;)的广义笛卡尔积(简称广义积,或 积 或笛卡尔积)运算结果也是一个关系，记作：**RxS**；两个分别为n目和m目的关系R和S的广义笛卡尔积是一个(n+m)列的元组的集合，元组的前n列是关系R的一个元组，后m列是关系S的一个元组，若R有k1个元组，S有k2个元组，则关系R和关系S的广义笛卡尔积有k1×k2个元组。 数学描述：RxS = &#123;&lt;a1,a2,…,an,b1,b2,…,bm&gt;|&lt;a1,a2,…,an&gt;∈R ∧ &lt;b1,b2,…,bm&gt;∈S&#125; RxS=SxR：RxS为R中的每一个元组都和S中的所有元组进行串接。SxR为S中的每一个元组都和R中的所有元组进行串接。结果是相同的。 两个关系R和S，它们的属性个数分别为n和m(R是n度关系，S是m度关系)则笛卡尔积R×S的属性个数=n+m。即元组的前n个分量是R中元组的分量，后m个分量是S中元组的分量(R×S是n+m度关系). 两个关系R和S，它们的元组个数分别为x和y(关系R的基数x,S的基数y),则笛卡尔积R×S的元组个数=x×y。(R×S的基数是x×y). 7.3.5 选择(Select) 定义：给定一个关系R, 同时给定一个选择的条件condition(简记F), 选择运算结果也是一个关系，记作**σF(R)**, 它从关系R中选择出满足给定条件condition的元组构成。 数学描述：σF(R) = &#123;t|t∈R ∧ F(t)=&#39;真&#39;&#125;,其中F表示选择条件，它是一个逻辑表达式，取逻辑值‘真’或‘假’。 选择操作从给定的关系中选出满足条件的行,条件的书写很重要，尤其是当不同运算符在一起时，要注意运算符的优先次序，优先次序自高至低为{ 括弧()；θ；¬；∧；∨ } 7.3.6 投影(Project) 定义：给定一个关系R, 投影运算结果也是一个关系，记作**A(R)**, 它从关系R中选出属性包含在A中的列构成。 数学描述：ΠA(R) = &#123;t[A] | t∈R&#125;,其中A为R中的属性列 投影操作从给定关系中选出某些列组成新的关系, 而选择操作是从给定关系中选出某些行组成新的关系 7.4 关系代数的扩展操作7.4.1 交(Intersection) 定义：假设关系R和关系S是并相容的，则关系R与关系S的交运算结果也是一个关系，记作：**R∩S**, 它由同时出现在关系R和关系S中的元组构成。 数学描述：R∩S = &#123;t|t∈R ∧ t∈S&#125;，其中t是元组 R∩S 和 S∩R 运算的结果是同一个关系 交运算可以通过差运算来实现：R∩S = R-(R-S) = S-(S-R) 汉语中的“既…又…”，“…, 并且…”通常意义是交运算的要求 7.4.2 θ-连接(θ-Join, theta-Join) 投影与选择操作只是对单个关系(表)进行操作, 而实际应用中往往涉及多个表之间的操作, 这就需要θ-连接操作 定义：给定关系R和关系S, R与S的连接运算结果也是一个关系，记作 **R⋈S[AθB]**：(括号内AθB是⋈的下标)，它由关系R和关系S的笛卡尔积中, 选取R中属性A与S中属性B之间满足 θ 条件的元组构成。 数学描述：R⋈S[AθB] = σ t[A]θs[B] (R×S)，σF(RxS)其中t是R中的元组，s是S中的元组 在实际应用中，θ-连接操作经常与投影Π、选择σ操作一起使用 特别注意：当引入θ-连接操作后，DBMS可直接进行连接操作，而不必先形成笛卡尔积。 7.4.3 等值连接(Equi-Join) 定义：给定关系R和关系S, R与S的等值连接运算结果也是一个关系，记作**R⋈S[A=B]**：(括号内A=B是⋈的下标)，它由关系R和关系S的笛卡尔积中选取R中属性A与S中属性B上值相等的元组所构成。 数学描述：R⋈S[A=B] = σ t[A]=s[B] (R×S) 当θ-连接中运算符为“＝”时，就是等值连接，等值连接是θ-连接的一个特例； 广义积的元组组合并不是都有意义的，另广义积的元组组合数目也非常庞大，因此采用θ-连接/等值连接运算可大幅度降低中间结果的保存量，提高速度。 7.4.4 自然连接(Natural-Join) 定义：给定关系R和关系S, R与S的自然连接运算结果也是一个关系，记作 ，它由关系R和关系S的笛卡尔积中选取相同属性组B上值相等的元组所构成。 数学描述：R⋈S = σ t[B]=s[B] (R×S) 自然连接是一种特殊的等值连接，要求关系R和关系S必须有相同的属性组B，R, S属性相同，值必须相等才能连接，要在结果中去掉重复的属性列 7.5 关系代数的基本书写思路 选出将用到的关系/表 做”积”运算（可用连接运算替换） 做选择运算保留所需的行/元组 做投影运算保留所需的列/属性 基本思路： 检索是否涉及多个表，如不涉及，则可直接采用并、差、交、选择与投影，只要注意条件书写正确与否即可 如涉及多个表，则检查： 能否使用自然连接，将多个表连接起来(多数情况是这样的) 如不能，能否使用等值或不等值连接(θ-连接) 还不能，则使用广义笛卡尔积，注意相关条件的书写 连接完后，可以继续使用选择、投影等运算，即所谓数据库的“选投联”操作 7.6 关系代数之复杂扩展操作7.6.1 除(Division) 除法运算经常用于求解“查询… 全部的/所有的…”问题 前提条件：给定关系R(A1 ,A2 , … ,An)为n度关系，关系S(B1 ,B2 , … ,Bm)为m度关系 。如果可以进行关系R与关系S的除运算，当且仅当：属性集{ B1 ,B2 , … , Bm }是属性集{ A1 ,A2 , … ,An }的真子集，即m &lt; n。 定义：关系R 和关系S的除运算结果也是一个关系，记作R÷S，分两部分来定义。 数学描述：12R÷S &#x3D; &#123;t|t∈Π[R-S](R) ∧ ∀u∈S(tu∈R) &#125; &#x3D; Π[R-S](R) - Π[R-S]((Π[R-S](R)×S)-R) 其中[R-S]为投影Π的下标(属性) 7.6.2 外连接(Outer-Join) 定义：两个关系R与S进行连接时，如果关系R(或S)中的元组在S(或R)中找不到相匹配的元组，则为了避免该元组信息丢失，从而将该元组与S(或R)中假定存在的全为空值的元组形成连接，放置在结果关系中，这种连接称之为外连接(Outer Join)。 外连接 = 自然连接 (或θ连接) + 失配的元组(与全空元组形成的连接) 外连接的形式：左外连接、右外连接、全外连接 左外连接 = 自然连接(或连接) + 左侧表中失配的元组 右外连接 = 自然连接(或连接) + 右侧表中失配的元组 全外连接 = 自然连接(或连接) + 两侧表中失配的元组 左外连接(Left Outer Join)记为：⋊ 右外连接(Right Outer Join)记为：⋉ 全外连接(Full Outer Join)记为：⋊⋉ 8. 关系演算关系演算是描述关系运算的另一种思维方式，它是以数理逻辑中的谓词演算为基础的，SQL语言是继承了关系代数和关系演算各自的优点所形成的 按照谓词变量的不同，可分为关系元组演算和关系域演算 关系元组演算是以元组变量作为谓词变量的基本对象 关系域演算是以域变量作为谓词变量的基本对象 8.1 关系元组演算 关系元组演算公式：{ t | P(t) } 表示：所有使谓词 P 为真的元组 t 的集合 t 是元组变量 t ∈ r 表示元组 t 在关系 r 中 t[A] 表示元组 t 的分量，即 t 在属性 A 上的值 P是与谓词逻辑相似的公式, P(t)表示以元组 t 为变量的公式 关系元组演算公式的基本形式：{ t | P(t) } P(t)可以是如下三种形式之一的原子公式： t∈R：t 是关系 R 中的一个元组，例如： { t | t∈Student} s[A] θ c：元组分量s[A]与常量 c 之间满足比较关系θ，θ:比较运算符&lt;,&lt;=,=,&lt;&gt;,&gt;,&gt;= s[A] θ u[B]：s[A] 与 u[B] 为元组分量，A和B分别是某些关系的属性，他们之间满足比较关系θ， P(t)可以由公式加运算符 ∧(与)、∨(或)、¬(非)递归地构造 如果F是一个公式，则 ¬F 也是公式 如果F1、F2是公式，则 F1∧F2, F1∨F2也是公式 P(t)运算符优先次序(括弧；θ；∃；∀；¬；∧；∨)示例 构造P(t)还有两个运算符：∃(存在)、∀(任意) 如果F是一个公式，则 ∃(t∈r)(F(t)) 也是公式 如果F是一个公式，则 ∀(t∈r)(F(t)) 也是公式 运算符∃和∀，又称为量词，前者称“存在量词”，后者称“全称量词” 而被∃或∀限定的元组变量 t , 或者说，元组变量 t 前有存在量词或全称量词，则该变量被称为“约束变量”，否则被称为“自由变量”。 元组演算的等价性变换 符号&lt;=&gt;表示表示等价于,如：¬(A&gt;B) &lt;=&gt; A&lt;=B &lt;=&gt; A&lt;B∨A=B 8.2 关系域演算 关系域演算公式的基本形式：&#123;&lt;x1,x2, …,xn&gt; | P(x1,x2, …,xn)&#125;,其中 xi 代表域变量或常量, P为以xi为变量的公式。 元组演算是以元组为变量，以元组为基本处理单位，先找到元组，然后再找到元组分量，进行谓词判断； 域演算是以域变量为基本处理单位，先有域变量，然后再判断由这些域变量组成的元组是否存在或是否满足谓词判断。 公式的运算符(∧(与)、∨(或)、¬(非)、∀(全称量词)和∃(存在量词))是相同的，只是其中的变量不同。 元组演算和域演算可以等价互换。 8.2.1 基于关系域演算的QBE语言QBE: Query By Example，1975年由M. M. Zloof提出，1978年在IBM370上实现，是一种高度非过程化的查询语言，特别适合于终端用户的使用。 特点：操作独特，基于屏幕表格的查询语言，不用书写复杂的公式，只需将条件填在表格中即可 QBE操作框架由四个部分构成 关系名区：用于书写欲待查询的关系名 属性名区：用于显示对应关系名区关系的所有属性名 操作命令区：用于书写查询操作的命令 查询条件区：用于书写查询条件 QBE的操作命令 Print 或 P. —- 显示输出操作 Delete或D. —- 删除操作 Insert或I. —- 插入操作 Update或U. —- 更新操作 构造查询的几个要素 示例元素: 即域变量， 一定要加下划线 示例元素是这个域中可能的一个值， 它不必是查询结果中的元素 打印操作符P.: 指定查询结果所含属性列 查询条件: 不用加下划线 可使用比较运算符＞， ≥，＜， ≤，＝和≠ 其中＝可以省略 排序要求 升序排序(AO.)，降序排序（DO.）,多列排序，用‚AO(i).‛ 或‚DO(i).‛ 表示 ，其中i为排序的优先级， i值越小，优先级越高 8.3 安全性关系运算的安全性：不产生无限关系和无穷验证的运算被称为是安全的 关系代数是一种集合运算，是安全的 集合本身是有限的，有限元素集合的有限次运算仍旧是有限的。 关系演算不一定是安全的 如：{t|¬(R(t))}, {t R(t)∨t[2]&gt;3}可能表示无限关系 需要对关系演算施加约束条件，即任何公式都在一个集合范围内操作，而不是无限范围内操作，才能保证其安全性。 8.3.1 安全约束有限集合DOM DOM(ψ)是一个有限集合，其中的每个符号要么是ψ中明显出现的符号，要么是出现在ψ中的某个关系R的某元组的分量。 DOM主要用于约束ψ中一些谓词的计算范围，它不必是最小集合。 安全元组演算表达式，满足三个条件： 只要t满足ψ，t的每个分量就是DOM(ψ)的一个成员。 { t|ψ(t) }中t的取值只能是DOM中的值，有限的。 对于ψ中形如(∃u)(ω(u))的子表达式，若u满足ω,则u的每个分量都是DOM(ω)中的成员。 { t|ψ(t) }中的每个(∃u)(ω(u))子表达式，只需要验证DOM中的元素是否有使ω(u)为真的元素。而对于DOM以外的元素，已经明确其都不满足ω(u)，无需验证。 对于ψ中形如(∀u)(ω(u))的子表达式，若u不满足ω,则u的每个分量都是DOM(ω)中的成员。 { t|ψ(t) }中的每个(∀u)(ω(u))子表达式，只需要验证DOM中的元素是否有使ω(u)为假的元素。而对于DOM以外的元素，已经明确其都满足ω(u)，无需验证。 8.4 关于三种关系运算的一些观点 关系运算有三种：关系代数、关系元组演算和关系域演算 三种关系运算都是抽象的数学运算，体现了三种不同的思维 关系代数—以集合为对象的操作思维，由集合到集合的变换 元组演算—以元组为对象的操作思维，取出关系的每一个元组进行验证，有一个元组变量则可能需要一个循环，多个元组变量则需要多个循环 域演算—以域变量为对象的操作思维，取出域的每一个变量进行验证看其是否满足条件 三种运算之间是等价的 关系代数 与 安全的元组演算表达式 与 安全的域演算表达式 是等价的。即一种形式的表达式可以被等价地转换为另一种形式 三种关系运算都可说是非过程性的 相比之下：域演算的非过程性最好，元组演算次之，关系代数最差 三种关系运算虽是抽象的，但却是衡量数据库语言完备性的基础 一个数据库语言如果能够等价地实现这三种关系运算的操作，则说该语言是完备的 目前多数数据库语言都能够实现这三种运算的操作，在此基础上还增加了许多其他的操作，如赋值操作、聚集操作等 数据库语言可以基于这三种抽象运算来设计 用“键盘符号”来替换抽象的数学符号 用易于理解的符号组合来表达抽象的数学符号 例如：ISBL语言—基于关系代数的数据库语言 再例如：Ingres系统的QUEL语言","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"【杂谈】对开源的理解","date":"2019-04-11T12:13:15.000Z","path":"article/20190411.html","text":"什么是开源所谓的「开源」，原本指的是开放其设计让所有使用者自由修改的一项机制。 在IT领域来说的话，字面意思就是开放源代码，开源软件的源代码任何人都可以审查、修改和增强。 虽然大多被用于软件开发过程中，但这项机制已逐渐演变为泛指在产品、计划与专案方面，透过开放大众的参与、讨论与修改，进而加速其发展、增加透明度及大众福利的方式。 开源是否等于免费开源等于免费是对开源的最大误解。 商业软件（Business Software）、自由软件（Free Software）和开源软件（Open Source Software，此处为狭义的开源）。它们之间的根本区别并不是在是否收费上，而是在于License（许可协议）。 商业软件用的是商业License，以保障软件商的利益为第一位，基本没有考虑用户的利益。 开源软件既然源码都是开放的，所以直接拿来免费使用基本都是没有问题的，但是这并不意味着使用开源软件是完全没限制的。每个开源软件都受License(开源协议)的约束和保护。 目前使用最广泛的一种开源协议便是MIT License，MIT允许别人用作者的代码做任何事情，但必须保证作者的所有权，并且作者无须承担代码使用产生的风险。比如Vue.js、React、Element、Bootstrap都是用的MIT协议。 为什么要选择开源软件开源软件的出现给了用户更多更好的选择，商业软件要想在这样的竞争环境下生存下去，唯一的办法就是把你的东西做得比开源软件更好！ 对于程序员来说，我们不但可以以开源软件为基础，根据自己的需要进行开发；也可以通过分享、观摩他人的源代码，进一步相互切磋与学习。 开源这件事情，不论对用户还是开发者来说，都只是一种选择。商业是商业，开源是开源，没有谁比谁了不起。 最后最后还要说一点：不要把开源软件与盗版混为一谈。 如前面所说，开源也是有License的，违反License的行为就是对开源的盗版。在反盗版问题上，所有类型软件的立场应该是一致的。 开源的观念并不只限于软件的开发与使用，而是希望能够抱着开放的心态，分享与合作的精神，相互切磋与学习，当你的代码被分享时它会变得更好。 参考连接： https://blog.csdn.net/happmaoo/article/details/83201544 https://www.oschina.net/news/58921/what-is-open-source","tags":[{"name":"杂谈","slug":"other","permalink":"http://chaooo.github.io/tags/other/"},{"name":"开源","slug":"open-source","permalink":"http://chaooo.github.io/tags/open-source/"}]},{"title":"【环境配置】Win10下配置Nginx+PHP-7+MySQL-5.6","date":"2018-10-22T03:39:12.000Z","path":"article/20181022.html","text":"1. 软件下载 Windows操作系统。 Nginx，下载地址：http://nginx.org/en/download.html。 PHP，下载地址：http://php.net/downloads.php（nginx下php是以FastCGI的方式运行，所以我们下载非线程安全也就是nts的php包）。 MySQL，下载地址：https://www.mysql.com/downloads/。（选择社区版Community-&gt;MySQL Community Server-&gt;MySQL Community Server 5.6，根据Windows系统选择对应zip包）。 2. 软件安装在C盘新建安装目录C:\\PHP。2.1 Nginx安装Nginx本身就是绿色软件，下载zip安装包解压到C:\\PHP，打开目录C:\\PHP\\nginx-1.15.8双击nginx.exe就可以运行，然后在浏览器打开http://127.0.0.1，出现欢迎界面表示NGINX正常工作。确认NGINX正常工作后在任务管理器中结束nginx.exe任务。2.2 PHP安装把PHP的zip安装包解压到C:\\PHP，解压后PHP安装目录为：C:\\PHP\\php-7.3.2。cmd进行到安装目录，输入php.exe -v,正常会显示版本信息。将C:\\PHP\\php-7.3.2加入系统环境变量。2.3 准备网站根目录准备一个文件夹，作为网站的根目录，这个在下面的配置文件中会多次用到，我把C:\\PHP\\web作为我的网站根目录。在根目录C:\\PHP\\web下新建一个info.php文件，输入如下内容：123&lt;?php phpinfo();?&gt; 2.4 让nginx识别PHP配置PHP (C:\\PHP\\php-7.3.2)在PHP根目录下找到php.ini-development文件，编辑器打开nginx.conf:在PHP根目录下修改配置文件C:\\PHP\\php-7.3.2\\php.ini-development并另存为php.ini,在其中修改或添加配置：1cgi.fix_pathinfo&#x3D;1 配置nginx conf(C:\\PHP\\nginx-1.15.8\\conf)在Nginx根目录下找到conf目录，编辑器打开C:\\PHP\\nginx-1.15.8\\confnginx.conf:123456789101112131415161718192021222324252627282930error_log logs&#x2F;error.log; #打开error_loghttp &#123; # ... server &#123; # ... location &#x2F; &#123; root C:\\PHP\\web; #配置根目录 index index.html index.htm index.php; &#125; # ... # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # 打开下面几行注释 location ~ \\.php$ &#123; root C:\\PHP\\web; #配置根目录 fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; #重要: 把下面 &#x2F;scripts 修改成 $document_root fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; # ... &#125;&#125; 2.5 运行与测试nginx是一个反向代理的web服务器，因此它其实必须依赖一个真正的web服务器才能执行动态的网页内容，因此这里php就是使用fastcgi来充当这个真正的web服务器，它运行在9000端口上，这也是为什么nginx.conf中有这样一句fastcgi_pass 127.0.0.1:9000;。 在任务管理器中结束nginx.exe任务，然后到C:\\PHP\\nginx-1.15.8目录双击nginx.exe开启服务。 在命令行中，cd到php的home目录C:\\PHP\\php-7.3.2，然后执行如下命令：1php-cgi.exe -b 127.0.0.1:9000 -c php.ini 打开浏览器，输入 http://127.0.0.1/info.php，这时候可以看到phpinfo页面：页面内容包含了PHP 当前状态的大量信息，包含了 PHP 编译选项、启用的扩展、PHP 版本、服务器信息和环境变量（如果编译为一个模块的话）、PHP环境变量、操作系统版本信息、path 变量、配置选项的本地值和主值、HTTP 头和PHP授权信息(License)。 2.6 MySQL安装 把MySQL的zip安装包解压到C:\\PHP，解压后PHP安装目录为：C:\\PHP\\mysql-5.6.43-winx64。 将C:\\PHP\\mysql-5.6.43-winx64\\bin加入系统环境变量。 修改配置文件C:\\PHP\\mysql-5.6.43-winx64\\my-default.ini并另存为my.ini,在其中修改或添加配置 （my.ini文件的编码必须是英文编码（如windows中的ANSI），不能是UTF-8或GBK等）： 12basedir&#x3D;C:\\PHP\\mysql-5.6.43-winx64 #mysql所在目录datadir&#x3D;C:\\PHP\\mysql-5.6.43-winx64\\data #mysql所在目录\\data 以管理员身份运行cmd,到安装目录的bin下，输入mysqld -install： 12C:\\PHP\\mysql-5.6.43-winx64\\bin&gt; mysqld -installService successfully installed. 输入命令:mysql --version,正常会显示版本信息。 输入命令:net start mysql启动服务(停止命令：net stop mysql): 123C:\\PHP\\mysql-5.6.43-winx64\\bin&gt;net start mysqlMySQL 服务正在启动 ..MySQL 服务已经启动成功。 服务启动成功之后，输入命令：mysql -u root -p（第一次登录没有密码，直接按回车过）: 1234567891011C:\\PHP\\mysql-5.6.43-winx64\\bin&gt;mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 1Server version: 5.6.43 MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and&#x2F;or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and&#x2F;or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#39;help;&#39; or &#39;\\h&#39; for help. Type &#39;\\c&#39; to clear the current input statement.mysql&gt; 如出现mysql&gt;,即登录成功。 输入命令exit,退出登录。 123mysql&gt; exitByeC:\\PHP\\mysql-5.6.43-winx64\\bin&gt; 3. 制作自动启动脚本控制台就一直开着，很不方便。这个时候可以使用 RunHiddenConsole.zip 来得管理服务的启动与关闭。 3.1 启动脚本在目录C:\\PHP下新建一个start.bat作为启动脚本文件： 12345678910111213141516171819202122:启动脚本@echo offset php_home=./php-7.3.2set nginx_home=./nginx-1.15.8REM Windows 下无效REM set PHP_FCGI_CHILDREN=5REM 每个进程处理的最大请求数，或设置为 Windows 环境变量set PHP_FCGI_MAX_REQUESTS=1000echo Starting PHP FastCGI...RunHiddenConsole %php_home%/php-cgi.exe -b 127.0.0.1:9000 -c %php_home%/php.iniecho FastCGI 启动成功echo.echo Starting nginx...RunHiddenConsole %nginx_home%/nginx.exe -p %nginx_home%echo nginx 启动成功echo.:echo 15秒后自动退出:ping 0.0.0.0 -n 15 &gt; null:请按任意键继续. . .pause 3.2 停止脚本在目录C:\\PHP下新建一个stop.bat作为停止脚本文件： 123456789101112:停止脚本@echo offecho Stopping nginx... taskkill /F /IM nginx.exe &gt; nulecho nginx 已停止:换行echo.echo Stopping PHP FastCGI...taskkill /F /IM php-cgi.exe &gt; nulecho FastCGI 已停止:请按任意键继续. . .pause 3.3 重启脚本在目录C:\\PHP下新建一个restart.bat作为重启脚本文件： 12345678910111213141516171819202122232425262728293031323334:停止脚本@echo offecho Stopping nginx... taskkill /F /IM nginx.exe &gt; nulecho nginx 已停止:换行echo.echo Stopping PHP FastCGI...taskkill /F /IM php-cgi.exe &gt; nulecho FastCGI 已停止echo.:启动脚本@echo offset php_home=./php-7.3.2set nginx_home=./nginx-1.15.8REM Windows 下无效REM set PHP_FCGI_CHILDREN=5REM 每个进程处理的最大请求数，或设置为 Windows 环境变量set PHP_FCGI_MAX_REQUESTS=1000echo Starting PHP FastCGI...RunHiddenConsole %php_home%/php-cgi.exe -b 127.0.0.1:9000 -c %php_home%/php.iniecho FastCGI 启动成功echo.echo Starting nginx...RunHiddenConsole %nginx_home%/nginx.exe -p %nginx_home%echo nginx 启动成功echo.:echo 15秒后自动退出:ping 0.0.0.0 -n 15 &gt; null:请按任意键继续. . .pause 4.最后我的根目录结构 123456789101112131415161718C:\\PHP&gt;dir 驱动器 C 中的卷是 系统 卷的序列号是 09C1-B27D C:\\PHP 的目录2019/02/22 15:46 &lt;DIR&gt; .2019/02/22 15:46 &lt;DIR&gt; ..2019/02/22 11:23 &lt;DIR&gt; mysql-5.6.43-winx642018/12/25 17:54 &lt;DIR&gt; nginx-1.15.82019/02/21 15:59 &lt;DIR&gt; php-7.3.22019/02/22 15:41 758 restart.bat2010/10/26 11:43 1,536 RunHiddenConsole.exe2019/02/22 15:41 549 start.bat2019/02/22 15:41 227 stop.bat2019/02/21 16:56 &lt;DIR&gt; web 4 个文件 3,070 字节 6 个目录 100,959,772,672 可用字节","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"}]},{"title":"【SpringBoot】MVC应用","date":"2018-06-20T09:31:30.000Z","path":"article/20180620.html","text":"对Spring Web MVC封装，简化MVC结构web应用开发。 1. SpringBoot MVC开发Restful服务（前后分离）*按rest规则发送HTTP请求–&gt;Spring MVC–&gt;返回JSON结果 主要步骤： 导入spring-boot-starter-web（springmvc、rest、jackson、tomcat） 在application.properties修改tomcat端口 定义启动类RunBoot，追加@SpringBootApplication 定义Controller、Service、Dao组件 2. SpringBoot MVC开发JSP应用（PC浏览器）HTTP请求–&gt;Spring MVC–&gt;JSP–&gt;HTML响应输出结果 主要步骤： 导入spring-boot-starter-web、jasper解析器、jstl 在application.properties修改tomcat端口、viewResolver 定义启动类RunBoot，追加@SpringBootApplication 定义Controller组件，返回ModelAndView 在src/main/webapp下定义JSP组件 3. SpringBoot MVC开发Thymeleaf应用（PC浏览器）*HTTP请求–&gt;Spring MVC–&gt;Thymeleaf模板–&gt;HTML响应输出结果 主要步骤： 导入spring-boot-starter-web、spring-boot-starter-thymeleaf 在application.properties修改tomcat端口 定义启动类RunBoot，追加@SpringBootApplication 定义Controller组件，返回ModelAndView 在src/main/resources/templates下定义模板文件1234&lt;html xmlns:th=&quot;https://www.thymeleaf.org/&quot;&gt; &lt;h1&gt;Hello&lt;/h1&gt; &lt;h2 th:text=&quot;$&#123;data&#125;&quot;&gt;&lt;/h2&gt;&lt;/html&gt; th:text表达式作用：将模型中的数据以只读文本显示到元素中 th:text表达式作用：将模型中的数据以只读文本显示到元素中。 th:if 表达式作用：if判断逻辑 th:each 表达式作用：循环逻辑 th:href 表达式作用：动态生成href链接 Thymeleaf模板和JSP区别 运行机制不同 JSP–&gt;Servlet–&gt;HTML 模板+数据–&gt;HTML输出 模板简单易用;JSP相对复杂些 JSP:9大内置对象、EL、JSTL、嵌入Java代码、框架标签 模板：模板表达式 模板效率高,比JSP性能好 模板：缓存 4. SpringBoot MVC静态资源处理静态资源包含图片、js、css等，动态资源servlet、jsp等。 SpringBoot中src/main/resources目录下有几个约定的静态资源存放位置 META-INF/resources（优先级最高） resources static public（优先级最低） 自定义静态资源访问路径，编写一个配置文件 123456789101112//@Configuration@Componentpublic class MyStaticConfiguration implements WebMvcConfigurer&#123; public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(&quot;/**&quot;) .addResourceLocations( &quot;classpath:/images/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot;); &#125;&#125; 5. SpringBoot MVC异常处理 异常处理机制 SpringBoot底层提供了异常处理机制。SpringBoot提供了一个ErrorMvcAutoConfiguration自动配置组件，创建了一个BasicErrorController对象，提供两个/error请求处理，一个返回html，另一个返回json。当MVC底层遇到异常会用转发方式发出/error请求。 可以自定义ErrorController替代底层BasicErrorController，将错误提示转发到自定义提示界面(全局) 123456789101112131415161718192021@Controller//@RequestMapping(&quot;/error&quot;)public class MyErrorController implements ErrorController&#123; @RequestMapping(value=&quot;/error&quot;,produces= MediaType.TEXT_HTML_VALUE) public ModelAndView errorHtml() &#123; ModelAndView mav = new ModelAndView(); mav.setViewName(&quot;myerror&quot;); return mav; &#125; @RequestMapping(value=&quot;/error&quot;) @ResponseBody public Object error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(&quot;msg&quot;, &quot;程序发生了异常&quot;); return map; &#125; @Override public String getErrorPath() &#123; return &quot;/error&quot;; &#125;&#125; @ExceptionHandler异常处理（局部） ErrorController管理全局异常，@ExceptionHandler管理所在Controller组件的异常。12345678@ExceptionHandler@ResponseBodypublic Object error(Exception ex) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(&quot;msg&quot;, &quot;发生异常&quot;); map.put(&quot;type&quot;, ex.getClass()); return map;&#125; 可以将上述方法封装成一个BasicController，通过@ControllerAdvice作用到所有Controller组件上。1234567891011@ControllerAdvice//等价于所有Controller都继承它public class BasicController &#123; @ExceptionHandler @ResponseBody public Object error(Exception ex) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(&quot;msg&quot;, &quot;发生异常&quot;); map.put(&quot;type&quot;, ex.getClass()); return map; &#125;&#125; 6. SpringBoot AOP 引入spring-boot-starter-aop 定义一个切面组件123456789101112131415161718192021@Component//将Bean组件纳入Spring容器@Aspect//将Bean组件定义为Aspect切面public class MyAspectBean &#123; @Before(&quot;within(cn.xdl.controller.*)&quot;)//前置通知 public void before() &#123; System.out.println(&quot;----开始处理----&quot;); &#125; @After(&quot;within(cn.xdl.controller.*)&quot;)//最终通知 public void after() &#123; System.out.println(&quot;----处理完毕----&quot;); &#125; @Around(&quot;within(cn.xdl.controller.*)&quot;) public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; StopWatch watch = new StopWatch(); watch.start(); Object obj = pjp.proceed();//调用目标组件方法 watch.stop(); System.out.println(&quot;处理时间:&quot;+watch.getTotalTimeMillis()+&quot; 毫秒&quot;); return obj; &#125;&#125; 配置切面组件 @Aspect、@Before、@After、@Around、@AfterReturning、@AfterThrowing等 7. SpringBoot MVC拦截器 编写一个拦截器组件,实现HandlerInterceptor接口1234567891011121314@Componentpublic class MyInterceptor implements HandlerInterceptor &#123; public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;执行了MyInterceptor拦截器&quot;); String user = (String)request.getSession().getAttribute(&quot;user&quot;); if(user == null) &#123; response.sendRedirect(&quot;/tologin&quot;); return false;//阻止后续流程执行 &#125; return true;//继续执行后续处理 &#125;&#125; 配置拦截器组件12345678@Configurationpublic class MyInterceptorConfiguration implements WebMvcConfigurer&#123; @Autowired private MyInterceptor my; public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(my).addPathPatterns(&quot;/direction/list&quot;); &#125;&#125; 8. SpringBoot整合Servlet/Filter8.1 整合Servlet首先导入spring-boot-starter-web 8.1.1 整合Servlet方式一： 编写一个Servlet组件，继承HttpServlet 在Servlet类定义前使用@WebServlet123456789@WebServlet(name=&quot;helloservlet&quot;,urlPatterns= &#123;&quot;/hello.do&quot;&#125;,loadOnStartup=1)public class HelloServlet extends HttpServlet&#123; public void service( HttpServletRequest request, HttpServletResponse response ) throws IOException &#123; response.getWriter().println(&quot;Hello SpringBoot Servlet&quot;); &#125;&#125; 启动类前需要使用@ServletComponentScan扫描@WebServlet配置1234567@SpringBootApplication@ServletComponentScan //扫描@WebServlet、@WebFilter、@WebListener组件public class RunBoot &#123; public static void main(String[] args) &#123; SpringApplication.run(RunBoot.class, args); &#125;&#125; 8.1.2 整合Servlet方式二： 编写一个Servlet组件，继承HttpServlet12345678public class SomeServlet extends HttpServlet &#123; public void service( HttpServletRequest request, HttpServletResponse response ) throws IOException &#123; response.getWriter().println(&quot;Hello Spring Some Servlet&quot;); &#125;&#125; 使用ServletRegistrationBean+@Bean1234567891011121314151617@SpringBootApplicationpublic class RunBoot &#123; public static void main(String[] args) &#123; SpringApplication.run(RunBoot.class, args); &#125; @Bean public ServletRegistrationBean&lt;Servlet&gt; someservlet()&#123; ServletRegistrationBean&lt;Servlet&gt; bean = new ServletRegistrationBean&lt;Servlet&gt;(); bean.setServlet(new SomeServlet()); bean.setLoadOnStartup(1); List&lt;String&gt; urls = new ArrayList&lt;&gt;(); urls.add(&quot;/some.do&quot;); bean.setUrlMappings(urls); return bean; &#125;&#125; 8.2 整合Filter在SpringBoot整合Servlet的基础上整合Filter 8.2.1 整合Filter方式一： 编写一个Filter组件，继承Filter 在Filter类定义前使用@WebFilter1234567891011@WebFilter(urlPatterns=&quot;/hello.do&quot;)public class HelloFilter implements Filter&#123; @Override public void doFilter( ServletRequest request, ServletResponse response, FilterChain chain ) throws IOException, ServletException &#123; System.out.println(&quot;-----hello filter------servlet执行之前&quot;); chain.doFilter(request, response); System.out.println(&quot;-----hello filter------servlet执行之后&quot;); &#125;&#125; 启动类前需要使用@ServletComponentScan扫描@WebServlet配置1234567@SpringBootApplication@ServletComponentScan //扫描@WebServlet、@WebFilter、@WebListener组件public class RunBoot &#123; public static void main(String[] args) &#123; SpringApplication.run(RunBoot.class, args); &#125;&#125; 8.2.2 整合Filter方式二： 编写一个Filter组件，继承Filter12345678910public class SomeFilter implements Filter&#123; @Override public void doFilter( ServletRequest request, ServletResponse response, FilterChain chain ) throws IOException, ServletException &#123; System.out.println(&quot;-----som filter------servlet执行之前&quot;); chain.doFilter(request, response); System.out.println(&quot;-----som filter------servlet执行之后&quot;); &#125;&#125; 使用FilterRegistrationBean+@Bean 注册过滤器并设置拦截的请求地址123456789101112131415@SpringBootApplicationpublic class RunBoot &#123; public static void main(String[] args) &#123; SpringApplication.run(RunBoot.class, args); &#125; ... @Bean public FilterRegistrationBean&lt;Filter&gt; somefilter()&#123; FilterRegistrationBean&lt;Filter&gt; bean = new FilterRegistrationBean&lt;Filter&gt;(); bean.setFilter(new SomeFilter()); // 配置要拦截的请求 bean.addUrlPatterns(&quot;/some.do&quot;); return bean; &#125;&#125; 9. SpringBoot 任务调度9.1 服务器启动后自动调用tomcat服务器启动后自动调用任务，可以使用ApplicationRunner或CommandLineRunner接口。 123456789101112131415161718@Component@Order(2)public class SomeTask1 implements ApplicationRunner &#123; @Override public void run(ApplicationArguments args) throws Exception &#123; System.out.println(&quot;----服务器启动后自动执行SomeTask1任务---&quot; + new Date()); &#125;&#125;@Component@Order(1)public class SomeTask2 implements CommandLineRunner&#123; @Override public void run(String... args) throws Exception &#123; System.out.println(&quot;----服务器启动后自动执行SomeTask2任务-----&quot;+new Date()); Thread.sleep(5000); &#125;&#125; 多个Task任务，可以通过@Order指定先后顺序，多个任务是线程同步调用。 9.2 程序运行后定时调用任务Spring提供了一个Spring Schedule模块，封装了任务调用，之前都是采用Quartz组件调用。 123456789101112131415@Component@EnableScheduling//开启Schedule模块public class SomeTask3 &#123; //在服务器启动1秒后调用任务，每隔3秒调用一次 @Scheduled(initialDelay=1000,fixedRate=3000) public void execute() &#123; System.out.println(&quot;-----周期性调用SomeTask3-----&quot;+new Date()); &#125; //在服务器启动0秒后调用任务，每隔5秒调用一次 @Scheduled(cron=&quot;0/5 * * * * ?&quot;)//秒 分 时 日 月 星期 public void execute2() &#123; System.out.println(&quot;-----周期性调用SomeTask4-----&quot;+new Date()); &#125;&#125; 使用Spring Schedule还需要指定cron表达式，表达式具体规则： 123456789101112秒 分 时 日 月 星期 年（可省略）0 0 10 1 10 ？秒： 0-59分： 0-59时： 0-23日： 1-31月： 1-12星期：1-7，1表示星期日，7表示星期六* ： 表示每一分、每一秒、每一天，任何一个可能值? ： 只用在日和星期部分，如果指定日，星期用？;如果指定星期，日用?，避免日和星期冲突 &#x2F; ： 表示增量，0&#x2F;1表示0\\1\\2\\3\\4递增加1；0&#x2F;5表示0\\5\\10\\15；1&#x2F;5表示1\\6\\11\\16\\21L ： 只用在日和星期部分，表示最后一天、周六 cron表达式案例： 123456789101112131415161718&quot;30 * * * * ?&quot; 每半分钟触发任务&quot;30 10 * * * ?&quot; 每小时的10分30秒触发任务&quot;30 10 1 * * ?&quot; 每天1点10分30秒触发任务&quot;30 10 1 20 * ?&quot; 每月20号1点10分30秒触发任务&quot;30 10 1 20 10 ? *&quot; 每年10月20号1点10分30秒触发任务&quot;30 10 1 20 10 ? 2011&quot; 2011年10月20号1点10分30秒触发任务&quot;30 10 1 ? 10 * 2011&quot; 2011年10月每天1点10分30秒触发任务&quot;30 10 1 ? 10 SUN 2011&quot; 2011年10月每周日1点10分30秒触发任务&quot;15,30,45 * * * * ?&quot; 每15秒，30秒，45秒时触发任务&quot;15-45 * * * * ?&quot; 15到45秒内，每秒都触发任务&quot;15&#x2F;5 * * * * ?&quot; 每分钟的每15秒开始触发，每隔5秒触发一次&quot;15-30&#x2F;5 * * * * ?&quot; 每分钟的15秒到30秒之间开始触发，每隔5秒触发一次&quot;0 0&#x2F;3 * * * ?&quot; 每小时的第0分0秒开始，每三分钟触发一次&quot;0 15 10 ? * MON-FRI&quot; 星期一到星期五的10点15分0秒触发任务&quot;0 15 10 L * ?&quot; 每个月最后一天的10点15分0秒触发任务&quot;0 15 10 LW * ?&quot; 每个月最后一个工作日的10点15分0秒触发任务&quot;0 15 10 ? * 5L&quot; 每个月最后一个星期四的10点15分0秒触发任务&quot;0 15 10 ? * 5#3&quot; 每个月第三周的星期四的10点15分0秒触发任务 9.3 SpringBoot+Quartz导入spring-boot-starter-quartz, 编写Job任务组件，继承QuartzJobBean 123456public class MyTask5 extends QuartzJobBean&#123; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; System.out.println(&quot;通过Quartz工具调用定时任务&quot;+new Date()); &#125;&#125; 配置Job组件（JobDetail、Tigger） 123456789101112131415161718@Configurationpublic class QuartzConfiguration &#123; @Bean//将MyTask5任务组件封装成JobDetail public JobDetail task5() &#123; return JobBuilder.newJob(MyTask5.class) .withIdentity(&quot;task5&quot;).storeDurably().build(); &#125; @Bean//为JobDetail指定触发时间cron表达式 public Trigger task5Trigger() &#123; CronScheduleBuilder cronScheduleBuilder = CronScheduleBuilder.cronSchedule(&quot;0/5 46 10 * * ?&quot;); return TriggerBuilder.newTrigger() .forJob(task5()) .withIdentity(&quot;task5&quot;) .withSchedule(cronScheduleBuilder) .build(); &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"}]},{"title":"【SpringBoot】数据库访问","date":"2018-06-14T09:29:31.000Z","path":"article/20180614.html","text":"Springboot对于数据访问层，无论是SQL还是NOSQL，都默认采用整合Spring Data的方式进行统一处理，Springboot添加大量自动配置，屏蔽了很多设置。并引入各种Template，Repository来简化我们对数据访问层的操作。 1.SpringBoot数据库访问1.1 Spring DAO JdbcTemplate引入spring-boot-starter-jdbc后（hikari、spring-jdbc包），就可以借助DataSourceAutoConfiguration、JdbcTemplateAutoConfiguration自动配置组件创建出HikariDataSource、JdbcTemplate对象。 引入jdbc启动器、驱动包，创建连接池 根据要操作表定义entity（pojo，属性名与字段名一致） 12345678910111213141516public class Direction &#123; private int id; private String name; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 定义Dao接口 123public interface DirectionDao &#123; public List&lt;Direction&gt; findAll();&#125; 定义Dao实现类，扫描并注入JdbcTemplate使用 123456789101112@Repository//通过组件扫描加载到Spring容器public class JdbcDirectionDao implements DirectionDao &#123; @Autowired private JdbcTemplate template;//通过自动配置加载到Spring容器 @Override public List&lt;Direction&gt; findAll() &#123; String sql = &quot;select * from direction&quot;; RowMapper&lt;Direction&gt; rowMapper = new BeanPropertyRowMapper&lt;Direction&gt;(Direction.class); return template.query(sql, rowMapper); &#125;&#125; 1.2 Spring MyBatis（XML SQL版本） 引入spring-boot-starter-jdbc、驱动包、mybatis-spring-boot-starter 引入application.properties（连接池参数） 实体类(同上) SQL定义 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;cn.xdl.dao.DirectionMapper&quot;&gt; &lt;select id=&quot;selectAll&quot; resultType=&quot;cn.xdl.entity.Direction&quot;&gt; select * from direction &lt;/select&gt; &lt;select id=&quot;selectById&quot; parameterType=&quot;int&quot; resultType=&quot;cn.xdl.entity.Direction&quot;&gt; select * from direction where id=#&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; Mapper接口1234public interface DirectionMapper &#123; public List&lt;Direction&gt; selectAll(); public Direction selectById(int id);&#125; 使用@MapperScan和mybatis.mapperLocations=classpath:sql/*.xml 在启动类前追加@MapperScan 12345@SpringBootApplication@MapperScan(basePackages=&quot;cn.xdl.dao&quot;)//扫描Mapper接口创建对象加载到Spring容器public class RunBoot &#123; ... ...&#125; 在application.properties追加mybatis.mapperLocations 1mybatis.mapperLocations=classpath:sql/*.xml 1.3 Spring MyBatis（注解 SQL版本） 引入spring-boot-starter-jdbc、驱动包、mybatis-spring-boot-starter 引入application.properties（连接池参数） 实体类(同上) 定义Mapper接口，使用@Select、@Update、@Insert、@Delete注解定义SQL 12345678public interface DirectionMapper &#123; @Select(&quot;select * from direction&quot;) public List&lt;Direction&gt; findAll(); @Select(&quot;select * from direction where id=#&#123;id&#125;&quot;) public Direction findById(int id); @Update(&quot;update direction set name=#&#123;name&#125; where id=#&#123;id&#125;&quot;) public int updateName(@Param(&quot;id&quot;)int id,@Param(&quot;name&quot;)String name);&#125; 使用@MapperScan（同上） 2. Spring Data JPA2.1 JpaJpa (Java Persistence API) 是 Sun 官方提出的 Java 持久化规范。中文名Java持久层API，是JDK 5.0注解或XML描述对象－关系表的映射关系，并将运行期的实体对象持久化到数据库中。 Sun引入新的JPA ORM规范主要是为了简化现有的持久化开发工作和整合 ORM 技术，结束现在 Hibernate，TopLink，JDO 等 ORM 框架各自为营的局面。 注意:Jpa 是一套规范，不是一套产品，那么像 Hibernate,TopLink,JDO 他们是一套产品，如果说这些产品实现了这个 Jpa 规范，那么我们就可以叫他们为 Jpa 的实现产品。 2.2 Spring Boot JpaSpring Boot Jpa 是 Spring 基于 ORM 框架、Jpa 规范的基础上封装的一套 Jpa 应用框架，可使开发者用极简的代码即可实现对数据的访问和操作。 Spring Boot Jpa 让我们解脱了 DAO 层的操作，基本上所有 CRUD 都可以依赖于它来实现 在Spring中使用JPA访问数据库，需要使用Spring Data模块支持。 - SpringData是对Spring框架一个扩展模块，包含对JPA、Redis、MongoDB等技术的访问支持。 Spring Boot Jpa的使用 引入spring-boot-starter-jdbc、spring-boot-starter-data-jpa、驱动包 在application.properties定义db连接池参数（同上） 定义RunBoot启动类，使用@SpringBootApplication标记（同上） 根据要操作的表定义实体类，使用@Entity、@Table、@Id、@Column定义该对象和表结构之间的映射关系123456789101112131415161718192021222324@Entity@Table(name=&quot;direction&quot;)public class Direction &#123; @Id @Column(name=&quot;id&quot;) private int id; @Column(name=&quot;name&quot;) private String name; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 定义Dao接口，可以选择继承JpaRepository、PagingAndSortingRepository、CrudRepository等1234public interface DirectionDao extends JpaRepository&lt;Direction, Integer&gt;&#123; //...&#125; 2.3 Dao扩展操作分页查询1234567Pageable pageable = PageRequest.of(1, 3);//of(页数从0开始,记录条数)Page&lt;Direction&gt; page = dao.findAll(pageable);List&lt;Direction&gt; list = page.getContent();list.forEach(d-&gt;&#123;System.out.println(d.getId()+&quot; &quot;+d.getName());&#125;);System.out.println(&quot;总记录数:&quot;+page.getTotalElements()+&quot; 页数:&quot;+(page.getNumber()+1)+&quot;/&quot;+page.getTotalPages());List&lt;Direction&gt; list1 = dao.findByIdGreaterThan2(5); 按方法名规则扩展12//where id&gt;?public List&lt;Direction&gt; findByIdGreaterThan(int id); 定义SQL语句扩展12@Query(nativeQuery=true,value=&quot;select * from direction where id&gt;:id&quot;)public List&lt;Direction&gt; findByIdGreaterThan1(@Param(&quot;id&quot;)int id); 定义JPQL面向查询语句扩展12@Query(&quot;from Direction where id&gt;:id&quot;) //使用类型名和属性名替代表名和字段名public List&lt;Direction&gt; findByIdGreaterThan2(@Param(&quot;id&quot;)int id); 按名称模糊查询，带分页支持123@Query(nativeQuery=true,value=&quot;select * from direction where name like :name&quot; ,countQuery=&quot;select count(*) from direction where name like :name&quot;)public Page&lt;Direction&gt; findByNameLike1(@Param(&quot;name&quot;)String name,Pageable pageable);","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"}]},{"title":"【SpringBoot】Boot 入门","date":"2018-06-06T15:06:01.000Z","path":"article/20180606.html","text":"1. 项目管理工具Maven的基本使用Maven是一个使用java编写的开源的项目管理工具，可以方便灵活的控制项目，不必浪费时间去在不同的环境中配置依赖的jar包，而专心于业务逻辑。 1.1 配置Maven的系统环境变量 下载并解压到目录，如D:\\apache-maven-3.6.1 添加新的系统环境变量MAVEN_HOME=安装的目录：MAVEN_HOME=D:\\apache-maven-3.6.1 添加%MAVEN_HOME%\\bin到系统PATH变量. 测试Maven配置是否成功，打开命令行窗口，输入mvn -v，如果有maven 版本信息输出则证明配置成功，否则请查看自己配置路径等是否正确。 注意：安装Maven前请确保已安装JDK并成功配置其环境变量。 1.2 maven中的术语 maven插件：maven主要定义了项目对象模型的生命周期。实际上每个任务都是交由插件完成的。maven的生命周期与插件目标相互绑定，来完成每个具体的任务。 maven坐标：就是对项目的定位。groupId：组id，机构名。artifactId：构建id ，产品名或者产品的id。version ：版本号。 坐标形式：groupId + artifactId+ version maven仓库：存放maven共享构建的位置。 本地仓库：localRepository（使用conf/settings.xml设置） 私服仓库：部署在局域网中的仓库，方便整个团队的开发使用。 中央仓库：远程仓库下载地址：http://repo1.maven.org/maven2 1234&lt;!-- conf/settings.xml设置本地仓库路径 --&gt;&lt;settings ... &lt;localRepository&gt;D:/apache-maven-3.6.1/.m2/repository&lt;/localRepository&gt;... 1.3 maven构建的生命周期清除–&gt; 编译–&gt; 测试–&gt; 报告–&gt; 打包(jar\\war)–&gt; 安装–&gt; 部署 清除：mvn clean 编译：mvn compile 测试：mvn test 打包：mvn package 安装：mvn install 部署：mvn deploy 1.4 MAVEN优点 模块化项目 项目非常大时，可借助Maven将一个项目拆分成多个工程，最好是一个模块对应一个工程，利于分工协作。而且模块可以通信。 实现Jar包共享 借助Maven，可将jar包仅仅保存在“仓库”中，有需要该文件时，就引用该文件接口，不需要复制文件过来占用空间。 jar包的依赖 借助Maven可以以规范的方式下载jar包，因为所有的知名框架或第三方工具的jar包已经按照统一的规范存放到了Maven的中央仓库中。 jar包的自动导入 通过xml定义引入jar包，Maven会自动导入jar包及其依赖jar包进来。 1.5 MAVEN工具 可以命令行使用，也可以结合Eclipse和Idea使用 简化项目搭建、编译、打包、发布等工作 2. SpringBoot基础 SpringBoot是对Spring框架的封装，用于简化Spring应用搭建和开发过程。 SpringBoot是pivotal公司产品、SpringCloud也是。 2.1 SpringBoot典型特点： 去除XML配置，完全采用Java配置方式 内置tomcat服务器 利用自动配置创建很多对象（DataSource、JdbcTemplate、DispatcherServlet等） 提供一系列启动器（jar包集合） 采用properties或yml做配置文件 应用采用jar包发布 2.2 SpringBoot程序构成 创建工程，导入boot启动器（jar包） spring-boot-starter (核心、包含ioc、yml、自动配置、Log日志) spring-boot-starter-parent（包含参数设置、文件编码、jdk版本等） spring-boot-starter-jdbc（包含连接池、jdbcTemplate等） spring-boot-starter-web（包含mvc、restful、tomcat等） spring-boot-starter-test（包含junit、spring-test等） 添加配置文件application.properties或application.yml 2.3 SpringBoot配置文件application.properties 123spring.datasource.username=rootspring.datasource.password=123456server.port=8888 application.yml 123456spring: datasource: username: root password: 123456server: port: 8888 2.4 SpringBoot启动类定义启动类，通过main方法启动 123456@SpringBootApplicationpublic class Xxxx&#123; public static void main(String[] args)&#123; SpringApplication.run(Xxxx.class); &#125;&#125; 2.5 SpringBoot数据库访问在pom.xml定义spring-boot-starter-jdbc、mysql驱动包 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在application.properties定义数据库连接参数 1234spring.datasource.username=rootspring.datasource.password=123456spring.datasource.url=jdbc:mysql://localhost:3306/ydmaspring.datasource.driverClassName=com.mysql.jdbc.Driver 定义启动类，内部会根据自动配置机制生成DataSource和JdbcTemplate 12345678910111213141516@SpringBootApplicationpublic class RunBoot &#123; public static void main(String[] args) throws SQLException &#123; //ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); ApplicationContext ctx = SpringApplication.run(RunBoot.class, args); DataSource ds = ctx.getBean(DataSource.class); System.out.println(ds.getConnection()); JdbcTemplate template = ctx.getBean(JdbcTemplate.class); System.out.println(template); String sql = &quot;insert into paper_score (total_score,my_score,user_id) values (?,?,?)&quot;; Object[] params = &#123;100,90,1&#125;; template.update(sql,params); &#125;&#125;//提示：DataSource和JdbcTemplate都是基于自动配置机制产生，直接注入使用即可。 2.6 打包发布SpringBoot程序： 在pom.xml定义spring-boot-maven-plugin插件12345678&lt;build&gt;&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt;&lt;/plugins&gt;&lt;/build&gt; 点击工程右键选择run as- maven build … 执行完毕后会在项目target目录下生成一个jar包，该包就是发布包 可以采用java -jar xxxx.jar命令启动 提示：eclipse设置jdk必须指向到JDK路径，不要JRE路径。 3. SpringBoot启动过程 调用SpringApplication的静态的run方法启动 静态的run方法调用SpringApplication对象的run方法 (SpringApplication对象创建时加载spring.factories文件中Initializer和Application Listeners组件，判断程序类型servlet、reactive、default) 对象的run方法会创建Spring的ApplicationContext容器对象 获取启动Listener组件 获取environment环境参数 获取启动Logo信息Banner 根据程序类型不同创建不同类型的ApplicationContext对象 将Listener、environment、banner设置到ApplicationContext容器对象中 为ApplicationContext容器对象加载程序中各种Bean组件 开始执行启动任务ApplicationRunner、CommandLineRunner等 返回ApplicationContext容器对象 4. @SpringBootApplication作用SpringApplication.run方法在启动中，加载一个带有@SpringBootApplication标记的参数，该标记具有以下几种功能。 4.1 @SpringBootConfiguration（SpringBoot Bean定义） spring中bean定义&lt;bean id=&quot;&quot; class=&quot;&quot;&gt; SpringBoot通过@Bean、@Primary标记定义。 案例代码： 12345678910111213141516@SpringBootConfiguration//开启Bean定义功能public class BeanConfiguration &#123; @Bean//将返回的UserDao对象放入Spring容器，默认方法名为id public UserDao userdao() &#123; return new UserDao(); &#125; @Bean(&quot;dao2&quot;)//将返回的UserDao对象放入Spring容器，指定id为dao2 @Primary//默认注入该对象 public UserDao userdao1() &#123; return new UserDao(); &#125; @Bean(&quot;userService&quot;) public UserService userService() &#123; return new UserService(); &#125;&#125; @SpringBootConfiguration标记是对Spring的@Configuration封装，所以直接用@Configuration也可以。 4.2 @ComponentScan（SpringBoot组件扫描） spring中组件扫描&lt;context:component-scan base-package=&quot;&quot;/&gt; SpringBoot通过@ComponentScan 扫描指定包路径组件，带@Controller、@Service、@Repository、@Component注解标记组件 @ComponentScan(basePackages= &#123;&quot;cn.xdl.dao&quot;,&quot;cn.xdl.service&quot;&#125;) 扫描cn.xdl包及子包下的组件 @ComponentScan(basePackages=&quot;cn.xdl&quot;) 扫描当前包及子包下的组件 @ComponentScan 扫描当前包及子包组件，并且将DeptService组件纳入 @ComponentScan(includeFilters= &#123;@Filter(type=FilterType.ASSIGNABLE_TYPE,classes=DeptService.class)&#125;) 扫描当前包及子包组件，带有@Controller、@Service…、@MyComponent注解有效 @ComponentScan(includeFilters= &#123;@Filter(type=FilterType.ANNOTATION,classes=MyComponent.class)&#125;) 4.3 @EnableAutoConfiguration（SpringBoot自动配置）自动配置机制是SpringBoot框架特有功能，能在启动后自动创建一些常用对象，例如DataSource、JdbcTemplate等。 自动配置原理： 在xxx-autoconfigure.jar包中META-INF目录下有一个spring.factories文件，其中定义了大量的XxxAutoConfiguration配置组件。当开启@EnableAutoConfiguration标记时，标记内部会触发AutoConfigurationImportSelector组件调用SpringFactoriesLoader加载spring.factories文件。 自动配置组件就是采用@Configuration+@Bean+@Primary标记事先定义好的配置组件，通过Boot启动自动去spring.factories文件加载，然后在Spring容器中创建出约定对象。 1234DataSourceAutoConfiguration//创建dataSource对象JdbcTemplateAutoConfiguration//创建jdbcTemplateDispatcherServletAutoConfiguration//创建DispatcherServlet对象RedisAutoConfiguration//创建RedisTemplate对象 通过自动配置机制创建DataSource对象 引入spring-boot-starter-jdbc（hikari）、驱动包 在application.properties文件追加db参数 在启动类使用@EnableAutoConfiguration标记 DataSourceAutoConfiguration默认会创建Hikari、tomcat、dbcp2连接池对象，优先级hikari最高，依次tomcat、dbcp2. 如果通过spring.datasource.type属性指定其他类型连接池组件，SpringBoot可以按指定类型创建连接池。12spring.datasource.type=org.apache.commons.dbcp2.BasicDataSourcespring.datasource.type=com.alibaba.druid.pool.DruidDataSource 4.4 MAVEN如何排除某个jar包（扩展）在引入spring-boot-starter-jdbc启动器时，由于jar包依赖会自动引入HikariCP，可以通过&lt; exclusion&gt;标记排除依赖。12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"}]},{"title":"【Spring】SSM框架整合(Spring+SpringMVC+MyBatis)","date":"2018-05-11T14:52:55.000Z","path":"article/20180511.html","text":"SSM框架是spring MVC ，spring和mybatis框架的整合，是标准的MVC模式，将整个系统划分为表现层，controller层，service层，DAO层。 使用spring MVC负责请求的转发和视图管理 spring实现业务对象管理 mybatis作为数据对象的持久化引擎 1.搭建SSM架构步骤： 设计数据库 先写实体类entity，定义对象的属性，（参照数据库中表的字段来设置）。 编写Mapper.xml（Mybatis），定义功能，对应要对数据库进行的那些操作，比如 insert、selectAll、selectByKey、delete、update等。 编写Mapper.java(DAO接口)，将Mapper.xml中的操作按照id映射成Java函数。 配置spring和mybatis框架的整合(applicationContext.xml) 编写Service.java，为控制层提供服务，接受控制层的参数，完成相应的功能，并返回给控制层。 配置SpringMVC(web.xml) 编写Controller.java，连接页面请求和服务层，获取页面请求的参数，通过自动装配，映射不同的URL到相应的处理函数，并获取参数，对参数进行处理，之后传给服务层。 编写JSP页面调用，请求哪些参数，需要获取什么数据。 DataBase –&gt; Entity –&gt; Mapper.xml –&gt; Mapper.Java(DAO) –&gt; Service.java –&gt; Controller.java –&gt; Jsp 2.搭建SSM架构实例（管理员登录）1. 设计数据库(以MySql为例)建立web项目，在src下新建sql脚本(admin.sql)，并在数据库中执行 1234567891011121314CREATE DATABASE exam_sys;/** 管理员表 */DROP TABLE admin;CREATE TABLE admin( id INT AUTO_INCREMENT COMMENT &#x27;管理员ID&#x27;, name VARCHAR(30) NOT NULL COMMENT &#x27;管理员账号&#x27;, password VARCHAR(30) COMMENT &#x27;管理员密码&#x27;, CONSTRAINT et_admin_id_pk PRIMARY KEY(id), CONSTRAINT et_admin_name_uk UNIQUE(NAME));/** 插入数据 */INSERT INTO admin (name, password) VALUES(&#x27;admin&#x27;, &#x27;123456&#x27;);SELECT * FROM admin;COMMIT; 2. 先写实体类entity，定义对象的属性参照数据库中表的字段来设置 123456789101112package com.exam.entity;public class Admin &#123; private int id; private String name; private String password; /** 添加 getter/setter方法 * 添加 无参，有参构造 * 重写toString()以便于测试 */ // ...&#125; 3. 编写AdminMapper.xml（Mybatis），定义功能对应要对数据库进行的那些操作，比如 insert、selectAll、selectByKey、delete、update等。 123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//ibatis.apache.org//DTD Mapper 3.0//EN&quot; &quot;http://ibatis.apache.org/dtd/ibatis-3-mapper.dtd&quot;&gt;&lt;!-- namespace指定和哪个Mapper映射器接口对应 --&gt;&lt;mapper namespace=&quot;com.exam.mapper.AdminDao&quot;&gt; &lt;!-- 定义SQL语句 --&gt; &lt;select id=&quot;findByNameAndPassword&quot; resultType=&quot;com.exam.entity.Admin&quot;&gt; select * from admin where name=#&#123;name, jdbcType=VARCHAR&#125; and password=#&#123;password, jdbcType=VARCHAR&#125; &lt;/select&gt;&lt;/mapper&gt; 4. 编写AdminDao.java，将AdminMapper.xml中的操作按照id映射成Java函数。导入Mybatis相关jar包：mybatis.jar、mysql-connector-java.jar(数据库驱动)、mybatis-spring.jar(SM整合) 12345678package com.exam.mapper;import org.apache.ibatis.annotations.Param;import com.exam.entity.Admin;public interface AdminDao &#123; public Admin findByNameAndPassword(@Param(&quot;name&quot;) String name, @Param(&quot;password&quot;) String password);&#125; 5. 配置spring和mybatis框架的整合导入Spring相关jar包：ioc/aop/dao/连接池；添加Spring配置文件（applicationContext.xml）到src下。 123456789101112131415161718192021222324252627&lt;!-- 配置连接池对象 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/exam_sys&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置SqlSessionFactoryBean来创建SqlSessionFactory 属性dataSource：注入连接池对象 属性mapperLocations：指定MyBatis的映射器XML配置文件的位置 属性typeAliasesPackage：对应我们的实体类所在的包，配置此项可在Mapper映射器直接使用类名，而非包名.类名 --&gt;&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:com/exam/mapper/*.xml&quot;&gt;&lt;/property&gt; &lt;!-- &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;com.exam.entity&quot;&gt;&lt;/property&gt; --&gt;&lt;/bean&gt;&lt;!-- 批量生产DAO接口实现类 ,实现类id为类名首字母小写 --&gt;&lt;bean id=&quot;mapperScanner&quot; class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;!-- &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; --&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.exam.mapper&quot;&gt;&lt;/property&gt; &lt;!-- 自定义注解可以让只让有注解的接口产生实现类，另一部分一部分不产生 --&gt; &lt;!-- &lt;property name=&quot;annotationClass&quot; value=&quot;com.annotation.MyAnnotation&quot;&gt;&lt;/property&gt; --&gt;&lt;/bean&gt;&lt;!-- 开启服务层组件扫描 --&gt;&lt;context:component-scan base-package=&quot;com.exam.service&quot;/&gt; 6. 编写Service.java，为控制层提供服务接受控制层的参数，完成相应的功能，并返回给控制层。 1234567891011121314151617181920package com.exam.service;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import com.exam.mapper.AdminDao;@Service(&quot;adminService&quot;)public class AdminService &#123; @Autowired private AdminDao dao; public boolean Login(String name, String password) &#123; try &#123; return dao.findByNameAndPassword(name, password)!=null?true:false; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125;&#125; 7. 配置SpringMVC导入jar包（spring-web.jar，spring-webmvc.jar）,生成web.xml并配置DispatcherServlet分发请求。 12345678910111213141516171819202122232425262728&lt;!-- 配置编码过滤器 --&gt;&lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;!-- 配置DispatcherServlet分发请求 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;!-- 在applicationContext.xml对静态资源进行放行 ：mvc:default-servlet-handler--&gt; 在applicationContext.xml中开启组件扫描(com.controller)，开启标注形式mvc，配置视图处理器 并 对静态资源进行放行。 1234567891011&lt;!-- 开启控制器组件扫描 --&gt;&lt;context:component-scan base-package=&quot;com.exam.controller&quot;/&gt;&lt;!-- 开启标注形式mvc --&gt;&lt;mvc:annotation-driven /&gt;&lt;!-- 配置视图处理器 --&gt;&lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/&quot;&gt;&lt;/property&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 对静态资源进行放行 --&gt;&lt;mvc:default-servlet-handler/&gt; 8. 编写Controller.java，连接页面请求和服务层获取页面请求的参数，通过自动装配，映射不同的URL到相应的处理函数，并获取参数，对参数进行处理，之后传给服务层。（导入Json相关包：jackson-core.jar，jackson-databind.jar，jackson-annotations.jar） 12345678910111213141516171819202122232425262728293031323334353637package com.exam.controller;import javax.servlet.http.HttpServletRequest;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.ResponseBody;import com.exam.entity.Admin;import com.exam.service.AdminService;@Controller@RequestMapping(&quot;/admin&quot;)public class AdminController &#123; @Autowired private AdminService as; @RequestMapping(&quot;/tologin&quot;) public String toLogin() &#123; return &quot;admin/login&quot;; &#125; @RequestMapping(value=&quot;/login&quot;,method=RequestMethod.POST) @ResponseBody public boolean addUser(Admin admin, HttpServletRequest request) &#123; System.out.println(&quot;add:&quot;+admin); System.out.println(admin.getName()+&quot;---&quot;+admin.getPassword()); boolean bl = as.Login(admin.getName(), admin.getPassword()); if(bl) &#123; //登录成功的逻辑 request.getSession().setAttribute(&quot;admin&quot;, admin); return true; &#125; //登录失败的逻辑 request.setAttribute(&quot;msg&quot;, &quot;登录失败&quot;); return false; &#125;&#125; 9. 编写JSP页面调用123456789101112131415161718192021&lt;form&gt; 管理员: &lt;input id=&quot;aName&quot; type=&quot;text&quot;&gt;&lt;br&gt; 密码:&lt;input id=&quot;aPassword&quot; type=&quot;text&quot;&gt;&lt;br&gt; &lt;input id=&quot;loginBtn&quot; type=&quot;button&quot; value=&quot;登录&quot;&gt;&lt;/form&gt;&lt;script src=&quot;js/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;$(&quot;#loginBtn&quot;).on(&quot;click&quot;, function()&#123; $.ajax(&#123; url: &quot;admin/login&quot;, type: &quot;post&quot;, data: &#123; name: $(&quot;#aName&quot;).val(), password: $(&quot;#aPassword&quot;).val() &#125;, success: function(res)&#123; alert(res); &#125; &#125;);&#125;);&lt;/script&gt;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"【Spring】持久层框架Mybatis","date":"2018-05-06T14:51:34.000Z","path":"article/20180506.html","text":"Mybatis支持普通sql操作，存储过程的调用，它是一个高级的ORM框架(Object Relation Mapping对象关系映射–以面向对象思想访问数据库)，是一个基于Java的持久层框架。 MyBatis封装了几乎所有的JDBC操作和参数的手工设置，它会对结果集自动封装成对象，以及直接把对象存入数据库，甚至可以做到对象与对象的关系维护；诸如：建立连接、操作 Statment、ResultSet，处理 JDBC 相关异常等等都可以交给 MyBatis 去处理，我们的关注点于是可以就此集中在 SQL 语句上，关注在增删改查这些操作层面上。 Mybatis框架的构成 使用Mybatis访问数据库 Mybatis的CRUD操作 Mapper映射器 向mapper传多个参数 结果集列名和属性名不一致的解决方法 类型的别名和日志输出 JdbcType 1. Mybatis框架的构成 实体类 ： 封装记录信息（JavaBean） SQL定义文件 ：定义sql语句（编写SQL语句的XML） 主配置文件 ：定义连接信息、加载SQL文件 以及其他设置的XML 框架API ：用于实现数据库增删改查操作（主要通过SqlSession） 2. 使用Mybatis访问数据库以员工表Emp(id,name,salary)为例 准备数据库及创建项目（需要mybatis的jar包和数据库驱动包） 根据表建立对应的实体类：Emp(id,name,salary) 在【src】目录下创建 MyBaits 的主配置文件 mybatis-config.xml ，其主要作用是提供连接数据库用的驱动，数据名称，编码方式，账号密码等 123456789101112131415161718&lt;configuration&gt; &lt;environments default=&quot;environment&quot;&gt; &lt;environment id=&quot;environment&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot; /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=&quot;com/mapper/EmpMapper.xml&quot; /&gt; &lt;/mappers&gt;&lt;/configuration&gt; 在【src】包路径下创建配置文件（com/mapper/EmpMapper.xml）,然后根据需求定义sql 1234567891011&lt;mapper namespace=&quot;com.mapper.EmpMapper&quot;&gt; &lt;!-- 定义SQL语句 --&gt; &lt;select id=&quot;findById&quot; parameterType=&quot;int&quot; resultType=&quot;com.mapper.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125; &lt;/select&gt; &lt;select id=&quot;findByName&quot; parameterType=&quot;String&quot; resultType=&quot;com.mapper.Emp&quot;&gt; select * from emp32 where name = #&#123;name&#125; &lt;/select&gt;&lt;/mapper&gt; parameterType：要求输入参数的类型 resultType：输出的类型 封装工具类获取SQLSession 123456789101112131415public class SqlSessionUtil &#123; public static SqlSessionFactory ssf; static &#123; // 先构建SQLSession工厂构建器 SqlSessionFactoryBuilder ssfb = new SqlSessionFactoryBuilder(); // 构建SqlSessionFactory关联主配置文件 InputStream inputStream = SqlSessionUtil.class.getClassLoader().getResourceAsStream(&quot;mybatis-config.xml&quot;); ssf = ssfb.build(inputStream); &#125; // 获取SQLSession public static SqlSession getSqlSession() &#123; // 通过SqlSession 工厂对象 来获取SqlSession return ssf.openSession(); &#125;&#125; 编写测试类 1234567public class EmpTest &#123; public static void main(String[] args) &#123; SqlSession ss =SqlSessionUtil.getSqlSession(); Emp emp = ss.selectOne(&quot;findById&quot;, 6); System.out.println(emp); &#125;&#125; 基本原理 应用程序找 MyBatis 要数据 MyBatis 从数据库中找来数据 通过 mybatis-config.xml 定位哪个数据库 通过 EmpMapper.xml 执行对应的 sql 语句 基于 EmpMapper.xml 把返回的数据库封装在 Emp 对象中 返回一个 Emp 对象 3. Mybatis的CRUD操作以员工表Emp(id,name,salary)为例 第一步：配置EmpMapper.xml 123456789101112131415&lt;insert id=&quot;insertEmp&quot; parameterType=&quot;com.mapper.Emp&quot;&gt; insert into emp32(name, salary) values(#&#123;name&#125;, #&#123;salary&#125;)&lt;/insert&gt;&lt;delete id=&quot;deleteEmpById&quot; parameterType=&quot;int&quot;&gt; delete from emp32 where id=#&#123;id&#125;&lt;/delete&gt;&lt;update id=&quot;updateEmpById&quot; parameterType=&quot;com.mapper.Emp&quot;&gt; update emp32 set name=#&#123;name&#125; where id=#&#123;id&#125;&lt;/update&gt;&lt;select id=&quot;findById&quot; parameterType=&quot;int&quot; resultType=&quot;com.mapper.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125;&lt;/select&gt;&lt;select id=&quot;findAll&quot; resultType=&quot;com.mapper.Emp&quot;&gt; select * from emp32&lt;/select&gt; parameterType：要求输入参数的类型 resultType：输出的类型 第二步：SQLSession实现增删改查 123456789101112131415161718192021// 先构建SQLSession工厂构建器SqlSessionFactoryBuilder ssfb = new SqlSessionFactoryBuilder();// 构建SqlSessionFactory关联主配置文件InputStream inputStream = EmpTest.class.getClassLoader().getResourceAsStream(&quot;sqlmap-config.xml&quot;);SqlSessionFactory ssf = ssfb.build(inputStream);// 通过SqlSession 工厂对象 来获取SqlSessionSqlSession ss = ssf.openSession();//增加Emp emp = new Emp(0,&quot;ef2&quot;,50000);int addRows = ss.insert(&quot;insertEmp&quot;, emp);//删除int delRows = ss.delete(&quot;deleteEmpById&quot;, 12);//更新Emp emp2 = new Emp(1,&quot;hello&quot;,0);int updateRows = ss.update(&quot;updateEmpById&quot;, emp2);//查找Emp emp3 = ss.selectOne(&quot;findById&quot;, 6);List&lt;Emp&gt; empList = ss.selectList(&quot;findAll&quot;);ss.commit(); SqlSession对象的操作方法如下： insert(..) 插入操作 update(..) 更新操作 delete(..) 删除操作 selectOne(..) 单行查询操作 selectList(..) 多行查询操作 通过 session.commit() 来提交事务，也可以简单理解为更新到数据库 4. Mapper映射器使用规则： 接口的方法名和SQL定义文件中的id保持一致 接口方法的返回值类型 要和resultType 保持一致 单行：resultType 多行：List&lt;resultType&gt; 增删改返回值，推荐int，也可以是void 接口方法参数和parameterType保持 一致，如果没有parameterType则参数任意 SQL定义文件中的namespace必须包名.接口名 5. 向mapper传多个参数5.1 第一种方案：#{0}，#{1} / #{param1} 和 #{param2}DAO层的函数方法 1public Emp findByIdAndName(int id, String name); 对应的Mapper.xml 123&lt;select id=&quot;findByIdAndName&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where id = #&#123;0&#125; and name = #&#123;1&#125;&lt;/select&gt; 其中，#{0}代表接收的是dao层中的第一个参数，#{1}代表dao层中第二参数，更多参数一致往后加即可。也可以用#{param1} 和 #{param2}实现同意效果。 5.2 第二种方案@paramDao层的函数方法 1public Emp findByIdAndName(@param(&quot;id&quot;)int id, @param(&quot;name&quot;)String name); 对应的Mapper.xml 123&lt;select id=&quot;findByIdAndName&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125; and name = #&#123;name&#125;&lt;/select&gt; 5.3 第三种方案：采用对象或Map传多参数Dao层的函数方法 12public Emp findByIdAndName(Emp emp);public Emp findByIdAndName2(Map&lt;String, Object&gt; params); 对应的Mapper.xml 1234567&lt;select id=&quot;findByIdAndName&quot; parameterType=&quot;com.bean.Emp&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125; and name = #&#123;name&#125;&lt;/select&gt;&lt;select id=&quot;findByIdAndName2&quot; parameterType=&quot;map&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125; and name = #&#123;name&#125;&lt;/select&gt; 6. 结果集列名和属性名不一致的解决方法在SQL定义中，resultType属性用于指定查询数据采用哪种类型封装，规则为结果集列名和属性名一致，如果不一致将不能接收查询结果。解决方法： 使用别名，select语句使用与属性一致的别名 使用resultMap替换resultType，用resultMap指定结果集列名和属性名的对应关系 123456789101112131415161718&lt;!-- 定义resultMap将sql 结果集列名(数据库中的字段)和Emp类中的属性做一个映射关系 type:resultMap最终所映射的Java对象类型，可以使用别名 id:对resultMap的唯一标识 --&gt;&lt;resultMap type=&quot;com.bean.Emp&quot; id=&quot;empMap&quot;&gt; &lt;!-- id表示查询结果集中唯一标识 column:查询出的列名 property:type所指定的类中的属性名 --&gt; &lt;id column=&quot;e_id&quot; property=&quot;id&quot;/&gt; &lt;!-- 对普通列的映射定义 --&gt; &lt;result column=&quot;salary&quot; property=&quot;sal&quot;/&gt;&lt;/resultMap&gt;&lt;!-- 使用resultMap --&gt;&lt;select id=&quot;findEmpById&quot; parameterType=&quot;int&quot; resultMap=&quot;empMap&quot;&gt; select * from emp32 where id = #&#123;id&#125;&lt;/select&gt; 7. 类型的别名和日志输出在mybatis-config.xml中自定义类型的别名 123&lt;typeAliases&gt; &lt;typeAlias alias=&quot;emp&quot; type=&quot;com.bean.Emp&quot;/&gt;&lt;/typeAliases&gt; 在EmpMapper.xml中使用别名 resultType=”emp” 123&lt;select id=&quot;findById&quot; parameterType=&quot;int&quot; resultType=&quot;emp&quot;&gt; select id,name,salary sal from emp32 where id = #&#123;id&#125;&lt;/select&gt; 设置MyBatis的日志输出到控制台 123456&lt;settings&gt; &lt;!--设置是否允许缓存--&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; &lt;!--设置日志输出的目标--&gt; &lt;setting name=&quot;logImpl&quot; value=&quot;STDOUT_LOGGING&quot;/&gt;&lt;/settings&gt; 8. JdbcType在执行SQL时MyBatis会自动通过对象中的属性给SQL中参数赋值，它会自动将Java类型转换成数据库的类型。而一旦传入的是null它就无法准确判断这个类型应该是什么，就有可能将类型转换错误，从而报错。 所以 MyBatis 插入空值时，需要指定JdbcType，这样相对来说是比较安全的。 一般情况下，我们没有必要按个字段去识别/判断它是否可以为空，而是将所有的字段都当做可以为空，全部手动设置转换类型。 MyBatis包含的JdbcType类型，主要有下面这些： BIT、FLOAT、CHAR 、TIMESTAMP 、 OTHER 、UNDEFINEDTINYINT 、REAL 、VARCHAR 、BINARY 、BLOB NVARCHAR、SMALLINT 、DOUBLE 、LONGVARCHAR 、VARBINARY 、CLOB、NCHAR、INTEGER、 NUMERIC、DATE 、LONGVARBINARY 、BOOLEAN 、NCLOB、BIGINT 、DECIMAL 、TIME 、NULL、CURSOR 123&lt;select id=&quot;findByName&quot; parameterType=&quot;String&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where name = #&#123;name, jdbcType=VARCHAR&#125;&lt;/select&gt; 9. Mabatis中#{}和${}的区别 $&#123;&#125;是字符串替换，底层使用的Statement（sql注入问题，效率低，编写sql复杂） 支持${param1}或${变量名},不支持${0}，Dao层必须使用@Param(),用到字符串时需要手动加单引号 #&#123;&#125;是预编译处理命令，底层使用PreparedStatement（可以有效防止sql注入） 不支持表名、排序方式等的占位，默认会将其当成字符串 10. 分页 在主配置文件中配置 分页拦截器（依赖于pageHelper、sqlparse相关jar） 1234&lt;!-- 配置分页拦截器 --&gt;&lt;plugins&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageHelper&quot;&gt;&lt;/plugin&gt;&lt;/plugins&gt; 查询前使用分页API 12345PageHelper.startPage(2, 2);List&lt;Emp&gt; emps = dao.orderBySalary();for(Emp emp: emps) &#123; System.out.println(emp);&#125; 11. Spring+MyBatis整合Spring与MyBatis整合需要引入一个mybatis-spring.jar文件包，该包提供了下面几个与整合相关的API: SqlSessionFactoryBean 创建SqlSessionFactory对象，为整合应用提供SqlSession对象资源 依赖于dataSource 和加载SQL定义文件 MapperFactoryBean 根据指定的某一个Mapper接口生成Bean实例 依赖于SqlSessionFactory 和 MApper接口 MapperScannerConfigurer 根据指定包批量扫描Mapper接口并生成实例 SqlSessionTemplate 类似于JdbcTemplate，便于程序员自己编写Mapper实现类 12. Spring+MyBatis完成sql操作第一步：使用Mybatis（同上） 导jar包(mybatis包/数据库驱动包)，建立实体类，定义SQL文件，编写Mapper映射接口 第二步：配置SqlSessionFactoryBean 导入jar包（mabatis-spring/ioc/aop/dao/连接池） 配置SqlSessionFactoryBean注入dataSource和指定sql定义文件 123456789101112&lt;!-- 配置SqlSessionFactory --&gt;&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:com/mapper/*.xml&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置连接池对象 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 第三步： 方式一： 使用SqlSessionFactoryBean结合接口和SqlSessionFactory 最终产生Mapper接口的 实现类，注意这是实现类 123456789&lt;!-- 配置SqlSessionFactoryBean 产生Mapper接口的 实现类 --&gt;&lt;bean id=&quot;empDao&quot; class=&quot;org.mybatis.spring.mapper.MapperFactoryBean&quot;&gt; &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; &lt;property name=&quot;mapperInterface&quot; value=&quot;com.dao.EmpDao&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;empDao2&quot; class=&quot;org.mybatis.spring.mapper.MapperFactoryBean&quot;&gt; &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; &lt;property name=&quot;mapperInterface&quot; value=&quot;com.dao.EmpDao2&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 方式二： MapperScannerConfigurer MapperFactoryBean一次只能生产一个DAO的实现类，可以通过MapperScannerConfigurer批量生产DAO接口实现类 1234567&lt;!-- 批量生产DAO接口实现类 ,实现类id为类名首字母小写 --&gt;&lt;bean id=&quot;mapperScanner&quot; class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;!-- &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; --&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.dao&quot;&gt;&lt;/property&gt; &lt;!-- 自定义注解可以让只让有注解的接口产生实现类，另一部分一部分不产生 --&gt; &lt;property name=&quot;annotationClass&quot; value=&quot;com.annotation.MyAnnotation&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 13. 使用SqlSessionTemplate模板来完成DAO接口的实现类 使用Mybatis（同上） 配置SqlSessionFactoryBean（同上） 编写DAO接口的实现类 开启组件扫描，注入SqlSessionTemplate,依赖于SqlSessionFactory 使用SqlSessionTemplate对应API完成增删改查 123456&lt;!-- 开启组件扫描 --&gt;&lt;context:component-scan base-package=&quot;com.mapper&quot;&gt;&lt;/context:component-scan&gt;&lt;!-- 创建SqlSessionTemplate --&gt;&lt;bean id=&quot;sqlSessionTemplate&quot; class=&quot;org.mybatis.spring.SqlSessionTemplate&quot;&gt; &lt;constructor-arg index=&quot;0&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 123456789@Repository(&quot;empDao&quot;)public class EmpDaoImpl implements EmpDao &#123; @Autowired private SqlSessionTemplate sqlSessionTemplate; @Override public Emp findById(int id) &#123; return sqlSessionTemplate.selectOne(&quot;findById&quot;, id); &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"【Spring】Spring MVC框架","date":"2018-04-27T14:50:22.000Z","path":"article/20180427.html","text":"Spring MVC是Spring提供的一个强大而灵活的web框架。借助于注解，Spring MVC提供了几乎是POJO的开发模式，使得控制器的开发和测试更加简单。这些控制器一般不直接处理请求，而是将其委托给Spring上下文中的其他bean，通过Spring的依赖注入功能，这些bean被注入到控制器中。 Spring MVC基本概念 Spring MVC的编写步骤 标注(注解)形式的MVC mvc控制器接收页面参数 mvc控制器把数据传递给页面 Spring MVC实现重定向 Spring MVC 中文参数的乱码问题 Spring MVC 拦截器 Spring MVC 拦截器的使用步骤 Spring MVC异常处理 Spring MVC文件上传 文件上传与异常处理的结合 Spring MVC响应JSON REST REST实例 1. Spring MVC基本概念1.1 Spring MVC 五大核心组件Spring MVC主要由DispatcherServlet、处理器映射、处理器(控制器)、视图解析器、视图组成。 DispatcherServlet：控制器，请求入口 HandlerMapping：控制器，分发请求，让请求和控制器建立一一对应关系 Controller：控制器，处理请求 ModelAndView：封装了 数据信息和视图信息 ViewResolver：视图处理器 他的两个核心是两个核心： 处理器映射：选择使用哪个控制器来处理请求 视图解析器：选择结果应该如何渲染 通过以上两点，Spring MVC保证了如何选择控制处理请求和如何选择视图展现输出之间的松耦合。 1.2 SpringMVC运行原理 Http请求：客户端请求提交到DispatcherServlet。 寻找处理器：由DispatcherServlet控制器查询一个或多个HandlerMapping，找到处理请求的Controller。 调用处理器：DispatcherServlet将请求提交到Controller。 调用业务处理和返回结果：Controller调用业务逻辑处理后，返回ModelAndView。 处理视图映射并返回模型： DispatcherServlet查询一个或多个ViewResoler视图解析器，找到ModelAndView指定的视图。 Http响应：视图负责将结果显示到客户端。 1.3 SpringMVC接口解释 DispatcherServlet接口：Spring提供的前端控制器，所有的请求都有经过它来统一分发。在DispatcherServlet将请求分发给Spring Controller之前，需要借助于Spring提供的HandlerMapping定位到具体的Controller。它是整个Spring MVC的核心。它负责接收HTTP请求组织协调Spring MVC的各个组成部分。其主要工作有以下三项： 截获符合特定格式的URL请求。 初始化DispatcherServlet上下文对应WebApplicationContext，并将其与业务层、持久化层的WebApplicationContext建立关联。 初始化Spring MVC的各个组成组件，并装配到DispatcherServlet中。 HandlerMapping接口：能够完成客户请求到Controller映射。 Controller接口： 需要为并发用户处理上述请求，因此实现Controller接口时，必须保证线程安全并且可重用。 Controller将处理用户请求，这和Struts Action扮演的角色是一致的。一旦Controller处理完用户请求，则返回ModelAndView对象给DispatcherServlet前端控制器，ModelAndView中包含了模型（Model）和视图（View）。 从宏观角度考虑，DispatcherServlet是整个Web应用的控制器；从微观考虑，Controller是单个Http请求处理过程中的控制器，而ModelAndView是Http请求过程中返回的模型（Model）和视图（View）。 ViewResolver接口：Spring提供的视图解析器（ViewResolver）在Web应用中查找View对象，从而将相应结果渲染给客户。 1.4 SpringMVC配置 在web.xml文件中进行配置applicationContext.xml路径1234567891011121314&lt;!-- 配置DispatcherServlet --&gt;&lt;servlet&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 配置applicationContext.xml，开启注解功能、配置试图解析器123456789101112131415&lt;!-- 配置HandlerMapping --&gt;&lt;bean id=&quot;handlerMapping&quot; class=&quot;org.springframework.web.servlet.handler.SimpleUrlHandlerMapping&quot;&gt; &lt;property name=&quot;mappings&quot;&gt; &lt;props&gt; &lt;prop key=&quot;/toHello.do&quot;&gt;helloController&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 控制器对象 --&gt;&lt;bean id=&quot;helloController&quot; class=&quot;com.controller.MyHelleController&quot;&gt;&lt;/bean&gt;&lt;!-- 配置视图处理器 --&gt;&lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/&quot;&gt;&lt;/property&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 2. Spring MVC的编写步骤 建立一个项目，导入jar包(ioc mvc) 拷贝spring配置文件到src下，同时在WEB-INF下建立jsp文件。 在web.xml中配置DisappearServlet，并通过contextConfigLocation这个初始化参数关联Spring容器对应的配置文件。 在 Spring配置文件中配置HandlerMapping的实现类SimpleUrlHandlerMapping需要通过mappings属性指定请求和控制器对应的关系。 编写一个类实现Controller接口，实现接口方法，返回ModelAndView，并且在容器创建Controller对象 在Spring配置文件中配置ViewResolver的实现类InternalResourceViewResolver，需要配置前缀prefix和后缀suffix。 3. 标注(注解)形式的MVC 建立项目，导入jar(ioc aop mvc)，拷贝spring配置文件到src下，同时在WEB-INF下建立jsp文件。 在web.xml中配置DispatcherServlet，并通过contextConfigLocation关联配置文件。 开启组件扫描 和 标注形式mvc (容器帮你创建了一个HandlerMapping对象，类型时RequestMappingHandlerMapping)。12&lt;context:component-scan base-package=&quot;包名&quot; /&gt;&lt;mvc:annotation-driven /&gt; 编写一个Java类，不用实现Controller接口，方法返回值类型可以时String也可以是ModelAndView（方法名与参数都自由了） 使用@Controller 可以把普通Java类转换成控制器，同时在容器中创建对象 使用@RequestMapping(&quot;/路径&quot;) 设置方法上 在Spring配置文件中配置ViewResolver的实现类InternalResourceViewResolver，需要配置前缀prefix和后缀suffix。 4. mvc控制器接收页面参数 使用HttpServletRequest类型的参数来接收123456@RequestMapping(&quot;/login.do&quot;)public String login(HttpServletRequest request) &#123; String acc_no = request.getParameter(&quot;acc_no&quot;); String acc_pwd = request.getParameter(&quot;acc_password&quot;); return &quot;main&quot;;&#125; 直接定义和页面请求参数同名的控制器参数12345@RequestMapping(&quot;/login2.do&quot;)public ModelAndView login2(String acc_no,String acc_password, ModelAndView mav) &#123; mav.setViewName(&quot;main&quot;); return mav;&#125; 当页面参数和控制器参数名字不一致，@RequestParam(“acc_no”) 让请求参数和控制器参数对应12345@RequestMapping(&quot;/login3.do&quot;)public ModelAndView login3(@RequestParam(&quot;acc_no&quot;) String a,String acc_password, ModelAndView mav) &#123; mav.setViewName(&quot;main&quot;); return mav;&#125; 控制器中 直接定义对象类型的参数12345@RequestMapping(&quot;/login4.do&quot;)public ModelAndView login4(Account acc, ModelAndView mav) &#123; mav.setViewName(&quot;main&quot;); return mav;&#125; 5. mvc控制器把数据传递给页面使用EL表达式在jsp页面接收数据&lt;h1&gt;欢迎 $&#123;acc_no&#125; &lt;/h1&gt; 使用域对象 进行传输 (request session ServletContext )12345@RequestMapping(&quot;/login6.do&quot;)public String login6(String acc_no, HttpServletRequest req) &#123; req.setAttribute(&quot;acc_no&quot;, acc_no); return &quot;main&quot;;&#125; 使用ModelAndView进行数据传输 mav.getModel().put(&quot;acc_no&quot;, acc_no); mav.getModelMap().put(key, value); mav.getModelMap().addAttribute(&quot;acc_no&quot;, acc_no); 12345678@RequestMapping(&quot;/login7.do&quot;)public ModelAndView login7(String acc_no, ModelAndView mav) &#123; mav.setViewName(&quot;main&quot;); //mav.getModel().put(&quot;acc_no&quot;, acc_no); //mav.getModelMap().put(key, value) mav.getModelMap().addAttribute(&quot;acc_no&quot;, acc_no); return mav;&#125; 使用Model进行数据传输12345@RequestMapping(&quot;/login8.do&quot;)public String login8(String acc_no, Model m) &#123; m.addAttribute(&quot;acc_no&quot;, acc_no); return &quot;main&quot;;&#125; 使用ModelMap进行数据传输123456@RequestMapping(&quot;/login9.do&quot;)public String login9(String acc_no, ModelMap m) &#123; //m.addAttribute(&quot;acc_no&quot;, acc_no); m.put(&quot;acc_no&quot;, acc_no); return &quot;main&quot;;&#125; 使用自定义的对象类型默认传输（默认名类型首字母小写，可以通过@ModelAttribute(“新名”)修改） 默认名：&lt;h1&gt;欢迎 $&#123; account.acc_no &#125; &lt;/h1&gt; @ModelAttribute(“acc”)：&lt;h1&gt;欢迎 $&#123; acc.acc_no &#125; &lt;/h1&gt; 1234@RequestMapping(&quot;/login10.do&quot;)public String login10(@ModelAttribute(&quot;acc&quot;) Account acc) &#123; return &quot;main&quot;;&#125; 6. Spring MVC实现重定向 控制器方法返回String redirect:请求路径 12345678910@RequestMapping(&quot;/login11.do&quot;)public String login11(@ModelAttribute(&quot;acc&quot;) Account acc) &#123; //return &quot;forward:toMain.do&quot;; return &quot;redirect:toMain.do&quot;;&#125;@RequestMapping(&quot;/toMain.do&quot;)public String toMain() &#123; // 干其它的事情 return &quot;main&quot;;&#125; 控制器方法返回ModelAndView 使用RedirectView 完成 12345678@RequestMapping(&quot;/login12.do&quot;)public ModelAndView login12(@ModelAttribute(&quot;acc&quot;) Account acc) &#123; ModelAndView mav = new ModelAndView(); //重定向 RedirectView rv = new RedirectView(&quot;toMain.do&quot;); mav.setView(rv); return mav;&#125; 7. Spring MVC 中文参数的乱码问题tomcat8中 get 没有乱码问题，post 请求有乱码问题 参数为页面(HttpServletRequest request)与(HttpServletResponse response)时 12request.setCharacterEncoding(&quot;UTF-8&quot;);response.setContentType(&quot;application/json;charset=UTF-8&quot;); 传入参数为@RequestParam时，可以通过字符串重新编码来解决 1new String(string.getBytes(&quot;ISO-8859-1&quot;),&quot;UTF-8&quot;); 方法名前出现@RequestMapping(value=&quot;XXX&quot;)时可以在value属性后再加一个属性produces=&quot;text/html;charset=UTF-8&quot;来解决 在web.xml或者dispatcher-servlet.xml或者其他配置servlet的配置文件中添加编码过滤器 123456789101112&lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 8. Spring MVC 拦截器 拦截器和fiter的作用几乎一样，它是Spring提供的一个组件，可以用在HandlerMapping组件之后（用于身份认证，登录检查，编码设置） HandlerMapping接口 preHandle：在HandlerMapping之后控制器之前调用，返回boolean(true:继续其他拦截器和处理器，false:终止后续调用)。 postHandle：处理器执行后、视图处理前调用。 afterCompletion：整个请求处理完毕后调用。 9. Spring MVC 拦截器的使用步骤 搭建一个基于标注的mvc 编写一个类实现HandlerInterceptor接口 在Spring配置文件中配置拦截器1234567&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;mvc:exclude-mapping path=&quot;/login.do&quot;/&gt; &lt;bean class=&quot;com.xdl.interceptor.SomeInterceptor&quot;/&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 10. Spring MVC异常处理 配置spring系统提供的简单异常处理器 SimpleMappingExceptionResolver 处理所有Controller异常12345678&lt;bean id=&quot;simpleExceptionResolver&quot; class=&quot;org.springframework.web.servlet.handler.SimpleMappingExceptionResolver&quot;&gt; &lt;property name=&quot;exceptionMappings&quot;&gt; &lt;props&gt; &lt;prop key=&quot;java.lang.RuntimeException&quot;&gt;error&lt;/prop&gt; &lt;prop key=&quot;java.lang.Exception&quot;&gt;error2&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 自定义异常处理器，实现HandlerExceptionResolver接口，处理所有Controller异常12345678910111213@Controllerpublic class MyExceptionResolver implements HandlerExceptionResolver &#123; @Override public ModelAndView resolveException(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, Exception e) &#123; ModelAndView mav = new ModelAndView(); if(e instanceof RuntimeException) &#123; mav.setViewName(&quot;error&quot;); &#125;else if(e instanceof Exception) &#123; mav.setViewName(&quot;error2&quot;); &#125; return mav; &#125;&#125; 使用@ExceptionHandler注解实现异常处理，处理某一个Controller异常public String execute(HttpServletRequest request, Exception ex)1234567//@Controller//public class MyController &#123;@ExceptionHandlerpublic String processException(Exception e) &#123; System.out.println(e.getMessage()); return &quot;error3&quot;;&#125; 11. Spring MVC文件上传 jsp页面（method=”POST” enctype=”multipart/form-data type=”file”）1234&lt;form action=&quot;upload.do&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 头像：&lt;input type=&quot;file&quot; name=&quot;head_img&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;&gt;&lt;br&gt;&lt;/form&gt; 控制器（MultipartFile类型来接收文件数据，需要配置文件解析器-需要依赖文件上传jar包-commons包）123&lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt;&lt;/bean&gt; 12. 文件上传与异常处理的结合12345678910111213141516171819202122232425262728293031323334@Controllerpublic class fileController &#123; @RequestMapping(&quot;/toFile.do&quot;) public String tofile() &#123; return &quot;file&quot;; &#125; @RequestMapping(&quot;/upload.do&quot;) public String upload(String acc_no, MultipartFile head_img) &#123; System.out.println(&quot;acc_no:&quot; + acc_no ); if(head_img.getSize()&gt;1024*10) &#123; throw new RuntimeException(&quot;文件过大！&quot;); &#125; // 把文件写入磁盘 String uniqueStr = UUID.randomUUID().toString(); String oriFilename = head_img.getOriginalFilename(); String suffix = oriFilename.substring(oriFilename.lastIndexOf(&quot;.&quot;)); File file = new File(&quot;F:/Eclipse/datas/&quot;+uniqueStr+suffix); try &#123; head_img.transferTo(file); &#125; catch (IllegalStateException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; System.out.println(oriFilename); System.out.println(file); return &quot;file&quot;; &#125; /** 局部异常 */ @ExceptionHandler public String processError(Exception e) &#123; return &quot;error4&quot;; &#125;&#125; 13. Spring MVC响应JSON 搭建基于标注的mvc 在控制器中，设计控制方法，控制方法返回值数据类型对应的对象转换为JSON 给方法加@RequestMapping(“/请求路径”)、@ResponseBody，它能把Java对象转换为JSON直接返回，依赖json转换包 14. RESTREST即表述性状态传递（Representational State Transfer），使用这种软件架构风格，可以降低开发的复杂性，提高系统的可伸缩性，便于分布式应用的开发。 REST两个核心规范 url请求路径的格式，由原来的基于操作的设计改变了基于资源的设计（如:http://test/source/1234） 对http请求的方式做了规范，GET代表查询，POST增加，DELETE删除，PUT更新 restful 符合REST设计规范和风格的应用程序或设计 就是RESTful Spring MVC对REST的支持 @RequestMapping支持URI的模板，以及http请求方式设定的支持 @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;,method=RequestMethod.POST) 对URI上路径变量的处理的支持，@PathVariable @PathVariable(&quot;id&quot;) int id rest请求路径是没有后缀的，需要把url-parttern修改成/ &lt;servlet-mapping&gt;&lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt;&lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 需要对静态资源进行放行&lt;mvc:default-servlet-handler/&gt; 15. REST实例 配置web.xml与applicationContext.xml(部分配置) 12345678910&lt;!-- 修改rest请求路径 --&gt;&lt;!-- web.xml --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;!-- 对静态资源进行放行 --&gt;&lt;!-- applicationContext.xml --&gt; &lt;mvc:default-servlet-handler/&gt; 编写控制类 12345678910111213141516171819202122232425262728293031323334353637383940@Controllerpublic class AccountController &#123; @RequestMapping(&quot;/toLogin.do&quot;) public String toLogin() &#123; return &quot;login&quot;; &#125;/** 根据id查询账户 GET */ @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;, method=RequestMethod.GET) @ResponseBody public Account getAccountById(@PathVariable(&quot;id&quot;) int id) &#123; Random rm = new Random(); Account acc = new Account(id, &quot;test&quot;+rm.nextInt(100),&quot;123&quot;, rm.nextInt(999)+1000); return acc; &#125;/** 新增账户 POST */ @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;,method=RequestMethod.POST) @ResponseBody public boolean addAccount(Account acc) &#123; System.out.println(&quot;add:&quot;+acc); if(acc.getId()&gt;100) return true; return false; &#125;/** 根据id删除帐户对象 DELETE */ @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;,method=RequestMethod.DELETE) @ResponseBody public boolean deleteAccountById(@PathVariable(&quot;id&quot;) int id) &#123; System.out.println(&quot;delete:&quot;+id); if(id&gt;100) return true; return false; &#125;/** 根据id更新帐户 PUT */ @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;,method=RequestMethod.PUT) @ResponseBody public boolean putAccount(@RequestBody Account acc) &#123; //@RequestBody将接收的ajax请求的json字符串写入Account对象中 System.out.println(&quot;update:&quot;+acc); if(acc.getId()&gt;100) return true; return false; &#125;&#125; 编写jsp页面 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;form&gt; &lt;p&gt;ID：&lt;input id=&quot;accountId&quot;&gt;&lt;/p&gt; &lt;p&gt;姓名：&lt;input id=&quot;accountNo&quot;&gt;&lt;/p&gt; &lt;p&gt;密码：&lt;input id=&quot;accountPassword&quot;&gt;&lt;/p&gt; &lt;p&gt;金额：&lt;input id=&quot;accountMoney&quot;&gt;&lt;/p&gt; &lt;button id=&quot;findBtn&quot; type=&quot;button&quot;&gt;查询&lt;/button&gt; &lt;button id=&quot;addBtn&quot; type=&quot;button&quot;&gt;添加&lt;/button&gt; &lt;button id=&quot;updateBtn&quot; type=&quot;button&quot;&gt;更新&lt;/button&gt; &lt;button id=&quot;delBtn&quot; type=&quot;button&quot;&gt;删除&lt;/button&gt;&lt;/form&gt;&lt;script src=&quot;js/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;$(&quot;#findBtn&quot;).on(&quot;click&quot;, function()&#123; findAccount();&#125;);$(&quot;#addBtn&quot;).on(&quot;click&quot;, function()&#123; addAccount();&#125;);$(&quot;#updateBtn&quot;).on(&quot;click&quot;, function()&#123; updateAccount();&#125;);$(&quot;#delBtn&quot;).on(&quot;click&quot;, function()&#123; delAccount();&#125;);function getDatas()&#123; var accountId = $(&quot;#accountId&quot;).val(); var accountNo = $(&quot;#accountNo&quot;).val(); var accountPassword = $(&quot;#accountPassword&quot;).val(); var accountMoney = $(&quot;#accountMoney&quot;).val(); return &#123; id: accountId, acc_no: accountNo, acc_password: accountPassword, acc_money: accountMoney &#125;;&#125;function findAccount()&#123; var datas = getDatas(); $.ajax(&#123; url: &quot;account/&quot; + datas.id, type: &quot;get&quot;, success: function(res)&#123; $(&quot;#accountNo&quot;).val(res.acc_no); $(&quot;#accountPassword&quot;).val(res.acc_password); $(&quot;#accountMoney&quot;).val(res.acc_money); &#125;, &#125;);&#125;function addAccount()&#123; var datas = getDatas(); $.ajax(&#123; url: &quot;account/&quot; + datas.id, type: &quot;post&quot;, data: datas, success: function(res)&#123; alert(res); &#125;, &#125;);&#125;function delAccount()&#123; var datas = getDatas(); $.ajax(&#123; url: &quot;account/&quot; + datas.id, type: &quot;delete&quot;, success: function(res)&#123; alert(res); &#125;, &#125;);&#125;function updateAccount()&#123; var datas = getDatas(); $.ajax(&#123; url:&quot;account/&quot;+ datas.id, type:&quot;put&quot;, data:JSON.stringify(datas), contentType:&quot;application/json&quot;,//以json字符串提交数据 success: function(res)&#123; alert(res); &#125;, &#125;);&#125;&lt;/script&gt; 注意： PUT需要以json字符串提交数据contentType:&quot;application/json&quot; @RequestBody将接收的ajax请求的json字符串写入Account对象中 JSON.stringify()：将json对象转换为json字符串 JSON.parse()：将json字符串转换为json对象","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"【Spring】面向切面编程(AOP模块)","date":"2018-04-21T14:48:36.000Z","path":"article/20180421.html","text":"AOP（Aspect Oriented Programming）：面向切面编程，它是面向对象基础上发展来的技术，是面向对象更高层次的应用，它可以在不修改原有代码的情况给组件增强功能。 AOP涉及到的概念 编写AOP程序步骤 切点表达式 通知的五种类型 标注形式AOP步骤 AOP 通知对应的标注 @Around具体用法 异常通知 1. AOP涉及到的概念 Aspect：切面，用来封装共通业务逻辑；其类叫切面类，其创建的对象叫切面对象。 JoinPoint：连接点，用来封装切面所要嵌入的位置信息的对象，（主要封装了方法信息） Pointcut：切点，是一堆连接点的集合，后面会使用切点表达式来表述切点 Target：目标，要被切入共通业务逻辑的对象 Proxy：代理，被增强之后的目标对象就是代理 Advice：通知，时机，切面逻辑在目标方法执行之前调用，执行之后调用，目标方法前后，目标方法最终，目标方法出现异常 2. 编写AOP程序步骤 编写一个Sevice类，里面有登录和注册两个方法，然后使用Spring容器获取Service类对应的对象，调用登录和注册方法 在不修改登录和注册原有代码的情况下，让两个方法调用前输出****** 添加aop的jar包到lib 编写一个类，定义共同业务逻辑 配置aplicationContext.xml，创建切面对象 配置aop:config，切面–&gt;通知–&gt;切点 3. 切点表达式 Bean限定表达式 bean(&quot;容器内组件id&quot;)，支持通配符*，如：bean(&quot;*Dao&quot;)，bean(&quot;acc*&quot;) 类型限定表达式 within(&quot;包名.类型&quot;)，要求表达式最后一部分必须是类型，如：com.dao.impl.类型，com.dao.impl.*，com.dao..* 方法限定表达式 execution(&quot;表达式&quot;)，可以有 权限修饰 返回值类型 方法名(参数类型)throws 异常，必须有:返回值类型 方法名() 4. 通知的五种类型 &lt;aop:before：前置通知，目标方法执行之前调用 &lt;aop:after-returning：后置通知，目标方法执行之后调用（目标方法出异常，通知方法无法执行） &lt;aop:after-throwing：异常通知，目标方法出异常才调用 &lt;aop:after：最终通知，目标方法之后一定会执行 &lt;aop:around：环绕通知，目标方法执行前后都调用 5. 标注形式AOP步骤 建项目，添加jar包(ioc,aop)，src下添加配置文件 编写一个Sevice类，里面有登录和注册两个方法 开启组件扫描，在类上打对应标注，创建Spring容器 测试逻辑 定义一个切面类，定义切面方法，并在容器中使用标注@Component创建切面对象 开启标注形式aop：&lt;aop:aspectj-autoproxy proxy-target-class=&quot;true|false&quot; /&gt; 使用切面对应的标注以及通知对应的标注结合切点表达式完成aop： @Aspect，@Before... 6. AOP 通知对应的标注 @Before：前置通知，目标方法执行之前调用 @AfterReturning：后置通知，目标方法执行之后调用（目标方法出异常，通知方法无法执行） @AfterThrowing：异常通知，目标方法出异常才调用 @After：最终通知，目标方法之后一定会执行 @Around：环绕通知，目标方法执行前后都调用 7. @Around具体用法@Around既可以在目标方法之前织入增强动作，也可以在执行目标方法之后织入增强动作； 12345678@Around(&quot;within(com..*)&quot;)public Object showAfterDate(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println(&quot;开始时间：&quot; + new Date().getTime()); Object obj = pjp.proceed(); System.out.println(&quot;结束时间：&quot; + new Date().getTime()); System.out.println(&quot;执行时间：&quot;date2.getTime() - date.getTime()); return obj;&#125; 虽然Around功能强大，但通常需要在线程安全的环境下使用。因此，如果使用普通的Before、AfterReturing增强方法就可以解决的事情，就没有必要使用Around增强处理了。 8. 异常通知JoinPoint可以获取出异常的方法 1234@AfterThrowing(value=&quot;within(com..*)&quot;, throwing=&quot;e&quot;)public void processException(JoinPoint jp, Exception e) &#123; System.out.println(&quot;捕获到异常&quot; + jp.getSignature() + &quot;:\\n【&quot; + e +&quot;】&quot;);&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"【Spring】事务管理","date":"2018-04-15T14:47:30.000Z","path":"article/20180415.html","text":"事务的基本概念：事务指的是逻辑上的一组操作，这组操作要么全部成功，要么全部失败。 事务的特性(ACID) Spring提供事务管理的3个接口 TransactionDefinition接口 TransactionStatus接口 PlatformTransactionManager接口（事务管理器） 基于AspectJ的xml方式的声明式事务管理 基于注解的声明式事务管理 1. 事务的特性(ACID) 事务的特性：原子性、一致性、隔离性、持久性。 原子性（Atomicity）：事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency）：事务前后数据的完整性必须保持一致。 隔离性（Isolation）：多个用户并发访问数据库时，一个用户的事务不能被其他用户的事务所干扰，多个并发事务之间数据要相互隔离（数据库中相应的数据隔离级别，通过它避免事务间的冲突）。 持久性（Durability）:一个事务一旦被提交，它对数据库中数据的改变是永久性的，即使数据库发生故障也不应该对其有任何影响。 2. Spring提供事务管理的3个接口： PlatformTransactionManager：事务管理器，用来管理事务的接口，定义了事务的提交、回滚等方法。 TransactionDefinition：事务定义信息（隔离级别、传播行为、是否超时、是否只读）。 TransactionStatus：事务具体运行状态（事务是否提交，事务是否有保存点，事务是否是新事物等状态）。 Spring事务管理时，这三个接口是有联系的，Spring首先会根据事务定义信息TransactionDefinition获取信息,然后由事务管理器PlatformTransactionManager进行管理，在事务管理过程中，会产生一个事务的状态，这个状态就保存在事务具体运行状态TransactionStatus中了。 3. TransactionDefinition接口TransactionDefinition定义事务隔离级别(Isolation)、定义事务传播行为(Propagation) 如果不考虑隔离性,就会引发安全问题：脏读、不可重复读、以及虚读或者叫做幻读。 事务的传播行为：解决业务层方法之间相互调用时,使用何种事务的问题。 3.1 安全问题 脏读：一个事务读取了另一个事务改写但还未提交的数据，如果这些数据被回滚，则读到的数据是无效的。 不可重复读：同一事务中，多次读取同一数据返回的结果有所不同（读取到另一个事务已经提交的更新的数据）。 幻读：一个事务读取了几行记录后，另一个事务插入一些记录，幻读就发生了。再后来的查询中，第一个事务就会发现有些原来没有的记录。 3.2 事务的隔离级别(Isolation)： READ_UNCOMMITED(读未提交)：允许读取未提交的改变了的数据（最低级别），可能导致脏读、不可重复读、幻读等。 READ_COMMITED(读提交)：允许在并发事务提交后读取，可防止脏读，但可能导致不可重复读、幻读。 REPEATABLE_READ(可重复读)：多次读取相同字段是一致的,除非数据被事务本身改变，可防止脏读、不可重复读，但可能导致幻读。 SERIALIZABLE(序列化)：事务是串行的,完全服从ACID的级别隔离，确保不发生脏读、不可重复读、幻读等。这在所有的隔离基本中是最慢的，它是典型的通过完全锁定在事务中涉及的数据表来完成的。 DEFAULT(Spring提供)：使用数据库默认的隔离级别（Mysql默认采用REPEATABLE_READ隔离级别，Oracle默认采用READ_COMMITTED隔离级别）。 3.3 事务的传播特性(Propagation)： 第一类：运行在同一个事务 **REQUIRED**：默认，支持当前事务，如果当前没有事务，就新建一个事务。 SUPPORTS：支持当前事务，如果当前没有事务，就不使用事务(以非事务方式执行) MANDATORY：支持当前事务，如果当前没有事务，就抛出异常 第二类：运行在不同事务 **REQUIRES_NEW**：新建事务，如果当前存在事务，把当前事务挂起 NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起 NEVER：以非事务方式执行，如果当前存在事务，则抛出异常 第三类：嵌套执行–即外层事务如果失败，内层事务要么回滚到保存点要么回滚到初始状态 **NESTED**：如果当前事务存在，则嵌套事务执行 4. TransactionStatus接口平台事务管理器(PlatformTransactionManager)会根据TransactionDefinition中定义的事务信息(包括隔离级别、传播行为)来进行事务的管理,在管理的过程中事务可能产生了保存点或事务是新的事务等情况,那么这些信息都会记录在TransactionStatus的对象中。 5. PlatformTransactionManager接口（事务管理器）该接口有许多实现类例如：DataSourceTransactionManager、HibernateTransactionManager等。 5.1 Spring支持两种方式事务管理： 编程式事务管理 手动编写代码进行事务管理，通过TransactionTemlate手动管理事务（很少使用） 声明式事务管理 基于TransactionProxyFactoryBean的方式（很少使用） 基于AspectJ的xml方式，配置稍复杂,但清晰可见事务使用范围（经常使用） 基于注解的方式，配置简单,需要在使用事务管理的业务层类或方法添加@Transactional注解（经常使用） 6. 基于AspectJ的xml方式的声明式事务管理123456789101112131415161718192021222324&lt;!-- 配置事务管理器 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;jdbc连接池对象id&quot;/&gt;&lt;/bean&gt;&lt;!-- 配置事务的通知（事务的增强） --&gt;&lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;!-- propagation:事务传播行为 isolation:事务的隔离级别 read-only:只读 rollback-for:发生哪些异常回滚 no-rollback-for:发生哪些异常不回滚 timeout:过期信息 --&gt; &lt;tx:method name=&quot;transfer&quot; propagation=&quot;REQUIRED&quot; isolation=&quot;DEFAULT&quot; read-only=&quot;false&quot; rollback-for=&quot;&quot; timeout=&quot;&quot; no-rollback-for=&quot;&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 配置切面 --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;aop:pointcut id=&quot;pointcut1&quot; expression=&quot;execution(*cn.muke.spring.demo3.AccountService+.*(.))&quot;/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;pointcut1&quot;/&gt;&lt;/aop:config&gt; 7. 基于注解的声明式事务管理 配置事务管理器1234567&lt;!-- 1.创建一个事务管理器对象 --&gt;&lt;bean id=&quot;事务管理器id&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;jdbc连接池对象id&quot;/&gt;&lt;/bean&gt;&lt;!-- 2.开启声明式事务 --&gt;&lt;tx:annotation-driven transaction-manager=&quot;事务管理器id&quot; proxy-target-class=&quot;true|false&quot; /&gt; transaction-manager：指定事务管理器(由框架提供类，在容器中创建这个对象并依赖于dataSource) proxy-target-class：决定是基于接口的还是基于类的代理被创建；为true则是基于类的代理将起作用(需要cglib库)，为false(默认)则标准的JDK 基于接口的代理将起作用。 使用，在类上或者方法上标注@Transactional123456@Transactional( rollbackFor=&#123;Exception.class&#125;, readOnly=false, isolation=Isolation.DEFAULT, propagation=Propagation.REQUIRED)public void transfer()&#123;..&#125; @Transactional的属性 rollbackFor：设置检查异常也回滚 noRollbackFor：指定运行时异常不回滚 readOnly： 只读属性，当事务方法都是select语句时，可以将readOnly设置成true优化方法，提高方法执行效率。当有DML操作时这个属性必须时false。 isolation：事务的隔离级别(枚举:DEFAULT,READ_UNCOMMITTED,READ_COMMITTED,REPEATABLE_READ,SERIALIZABLE) propagation：事务的传播特性(枚举:REQUIRED,SUPPORTS,MANDATORY,REQUIRES_NEW,NOT_SUPPORTED,NEVER) Spring中事务管理器默认值针对运行时异常回滚，对检查异常不回滚。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"【Spring】JDBC详解","date":"2018-04-09T14:46:07.000Z","path":"article/20180409.html","text":"Spring对JDBC做了简化和封装；简化了DAO实现类编写；提供了基于AOP的声明式事务管理；对JDBC中异常做了封装，把原来检查异常封装成了继承自RuntimeException的异常（DataAcessException）。 数据源配置 JdbcTemplate的使用 通过实现RowMapper接口把查询结果映射到Java对象 JdbcTemplate对象的主要方法 异常转换 1. 数据源配置1234567891011121314151617@Configuration@ComponentScan(&quot;com.jdbc&quot;)public class MyConfiguration &#123; @Bean public DataSource mysqlDataSource() &#123; BasicDataSource dataSource = new BasicDataSource(); dataSource.setDriverClassName(&quot;com.mysql.cj.jdbc.Driver&quot;); dataSource.setUrl(&quot;jdbc:mysql://localhost:3306/test&quot;); dataSource.setUsername(&quot;root&quot;); dataSource.setPassword(&quot;123456&quot;); return dataSource; &#125; @Bean public JdbcTemplate jdbcTemplate() &#123; return new JdbcTemplate(mysqlDataSource()); &#125;&#125; 也可以使用XML配置来实现配置效果： 12345678910111213&lt;!-- 配置连接池对象 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 定义jdbcTemplate对象 --&gt;&lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;constructor-arg index=&quot;0&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- 开启组件扫描 --&gt;&lt;context:component-scan base-package=&quot;com.jdbc&quot;&gt;&lt;/context:component-scan&gt; 2. JdbcTemplate的使用JdbcTemplate模板是Spring JDBC模块中主要的API，它提供了常见的数据库访问功能。JdbcTemplate类执行SQL查询、更新语句和存储过程调用，执行迭代结果集和提取返回参数值。 基本的查询： 1234567891011//DAO实现类@Repository(&quot;empDao&quot;)public class EmpDaoImpl implements EmpDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public int getCount() &#123; String sql = &quot;select count(*) from emp32&quot;; return jdbcTemplate.queryForObject(sql, Integer.class); &#125;//... 3. 通过实现RowMapper接口把查询结果映射到Java对象12345678910public class EmpRowMapper implements RowMapper&lt;Emp&gt; &#123; @Override public Emp mapRow(ResultSet rs, int n) throws SQLException &#123; return new Emp( rs.getInt(&quot;id&quot;), rs.getString(&quot;name&quot;), rs.getDouble(&quot;salary&quot;) ); &#125;&#125; 1234567891011//DAO实现类@Repository(&quot;empDao&quot;)public class EmpDaoImpl implements EmpDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public Emp getEmpById(int id) &#123; String sql = &quot;select * from emp32 where id=?&quot;; return jdbcTemplate.queryForObject(sql, new EmpRowMapper(), id); &#125;//... 4. JdbcTemplate对象的主要方法 queryForInt()： 12345//查询一个整数类型int count = jdbcTemplateObject.queryForInt(&quot;select count(*) from emp32&quot;);//一个使用绑定变量的简单查询int age = jdbcTemplateObject.queryForInt(&quot;select age from emp32 where id = ?&quot;, new Object[]&#123;10&#125;); queryForLong()： 12//查询一个 long类型long count = jdbcTemplateObject.queryForLong(&quot;select count(*) from emp32&quot;); queryForObject()： 1234567891011//查询字符串String SQL = &quot;select name from emp32 where id = ?&quot;;String name = jdbcTemplateObject.queryForObject(SQL, new Object[]&#123;10&#125;, String.class);//查询并返回一个对象：String SQL = &quot;select * from emp32 where id = ?&quot;;emp32 student = jdbcTemplateObject.queryForObject(SQL, new Object[]&#123;10&#125;, new EmpRowMapper());//查询并返回多个对象：String SQL = &quot;select * from emp32&quot;;List&lt;emp32&gt; students = jdbcTemplateObject.query(SQL, new EmpRowMapper()); update()： 1234567891011//在表中插入一行：String SQL = &quot;insert into emp32 (name, age) values (?, ?)&quot;;jdbcTemplateObject.update( SQL, new Object[]&#123;&quot;Zara&quot;, 11&#125; );//更新表中的一行：String SQL = &quot;update emp32 set name = ? where id = ?&quot;;jdbcTemplateObject.update( SQL, new Object[]&#123;&quot;Zara&quot;, 10&#125; );//从表中删除一行：String SQL = &quot;delete emp32 where id = ?&quot;;jdbcTemplateObject.update( SQL, new Object[]&#123;20&#125; ); execute()：执行DDL语句 12345678String SQL = &quot;CREATE TABLE emp32( id INT AUTO_INCREMENT, NAME VARCHAR(30), salary DOUBLE DEFAULT 5000, CONSTRAINT student_id_pk PRIMARY KEY(id), CONSTRAINT student_name_uk UNIQUE(NAME))&quot;;jdbcTemplateObject.execute( SQL ); 5. 异常转换 Spring提供了自己的开箱即用的数据异常分层——DataAccessException作为根异常，它负责转换所有的原始异常。 所以开发者无需处理底层的持久化异常，因为Spring JDBC模块已经在DataAccessException类及其子类中封装了底层的异常。 这样可以使异常处理机制独立于当前使用的具体数据库。 除了默认的SQLErrorCodeSQLExceptionTranslator类，开发者也可以提供自己的SQLExceptionTranslator实现。 例如：自定义SQLExceptionTranslator实现的简单例子，当出现完整性约束错误时自定义错误消息： 12345678910public class CustomSQLErrorCodeTranslator extends SQLErrorCodeSQLExceptionTranslator &#123; @Override protected DataAccessException customTranslate (String task, String sql, SQLException sqlException) &#123; if (sqlException.getErrorCode() == -104) &#123; return new DuplicateKeyException(&quot;完整性约束冲突&quot;, sqlException); &#125; return null; &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"【Spring】IoC注解实现","date":"2018-04-03T14:44:07.000Z","path":"article/20180403.html","text":"回顾xml方式管理Java Bean 注解方式管理Java Bean 注解方式Bean的注入 注解方式Bean的常用配置项(作用域,生命周期,懒加载等) 1. 回顾xml方式管理Java Bean 将一个Bean交由Spring创建并管理 &lt;baen id=&quot;bean&quot; class=&quot;包名.Bean&quot;&gt;&lt;/baen&gt; 获取Spring上下文 ApplicationContext app = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); 获取Bean Bean bean = app.getBean(&quot;bean&quot;, Bean.class); 2. 注解方式管理Java Bean一、创建一个class配置文件 12345678@Configurationpublic class MyConfiguration&#123; //将一个Bean交由Spring创建并管理 @Bean(name=&quot;bean1&quot;) public Bean bean()&#123; return Bean = new Bean(); &#125;&#125; 二、获取Spring上下文 12ApplicationContext context = new AnnotationConfigApplicationContext(MyConfiguration.class); 三、获取Bean 1Bean1 bean1 = context.getBean(&quot;bean1&quot;, Bean1.class); 2.1 简化注解方式的步骤1一、 开启组件扫描（去掉上述步骤1中MyConfiguration实例化Bean的方法） 123@Configuration //该注解可理解当前class等同于一个xml文件@ComponentScan(&quot;包路径&quot;) //开启组件扫描public class MyConfiguration&#123;&#125; 在applicationContext.xml中开启组件扫描方式&lt;context:component-scan base-package=&quot;包路径&quot;/&gt;。 二、 将交由Spring管理的类加上@Component注解，或（@Repository，@Controller，@Service） 1234@Component(&quot;bean1&quot;)//通过构造方法实例化Bean1public class Bean1&#123; //...&#125; @Component是通用注解，其他三个注解是这个注解的拓展，并且具有了特定的功能 @Repository注解在持久层中，具有将数据库操作抛出的原生异常翻译转化为spring的持久层异常的功能。 @Controller层是spring-mvc的注解，具有将请求进行转发，重定向的功能。 @Service层是业务逻辑层注解，这个注解只是标注该类处于业务逻辑层。 2.2 Bean别名一、 xml形式：通过name属性或alias标签 12&lt;bean id=&quot;bean1&quot; name=&quot;bean2,bean3&quot; class=&quot;com...Bean&quot;/&gt;&lt;alias name=&quot;bean1&quot; alias=&quot;bean4&quot;/&gt; 二、 注解形式 1234567@Configurationpublic class MyConfiguration&#123; @Bean(name=&#123;&quot;bean1&quot;,&quot;bean2&quot;,&quot;bean3&quot;&#125;) public Bean1 bean1()&#123; return Bean1 = new Bean1(); &#125;&#125; 注意：@Component只能指定一个名字，@Component默认值为类名首字母小写，也可以自定义，如:@Component(&quot;bean1&quot;)； 默认@scope为singleton单例，也可以进行指定 3. 注解方式Bean的注入一、 **@Value(&quot;值&quot;)**：常用于基本数据类型值注入，值可用EL表达式。 123456@Componentpublic class Player&#123; @Value(&quot;张三&quot;) private String name; //...&#125; 二、 @Autowired**：常用于复杂类型值的注入 + @Autowired：可以用在成员变量，setter方法，构造方法上；优先按照类型进行匹配，匹配不上启用名字进行匹配。 + @Qualifier(&quot;名字&quot;) 根据名字匹配，配合@Autowired，不能用在构造方法上；@Qualifier指定对象必须存在，否则程序报错，可以使用@Autowired的required属性来解除这种强依赖，@Autowired(required=false):尽量去找，组件不存在也不报错。 + @Autowired的原理**：在启动spring IoC时，容器自动装载了一个AutowiredAnnotationBeanPostProcessor后置处理器，当容器扫描到@Autowied、@Resource或@Inject时，就会在IoC容器自动查找需要的bean，并装配给该对象的属性 1234567891011121314151617181920212223@Componentpublic class Player&#123; @Value(&quot;张三&quot;) private String name; /** 用于成员变量 */ //@Autowired //@Qualifier(&quot;card1&quot;) private Card card; /** 用于构造方法 */ //@Autowired public Player(Card card) &#123; super(); this.card = card; &#125; /** 用于setter方法 */ @Autowired(required=false) public void setCard(Card card) &#123; this.card = card; &#125;&#125; 三、 @Resource**：常用于复杂类型值的注入 + @Resource：用在成员变量和setter方法上，是JDK**1.6支持的注解，优先按照名字匹配，可以通过@Resource(name=&quot;名&quot;)指定；如果没有指定name属性，用在成员变量上默认取字段名，用在setter方法上默认取属性名进行装配。名字匹配不上，会动用类型匹配。但注意：如果name属性一旦指定，就只会按照名称进行装配。 123456@Componentpublic class Player&#123; @Resource(name=&quot;card&quot;) private Card card; //...&#125; 集合类型值注入实例 123456789101112131415161718@Configuration@ComponentScan(&quot;包路径&quot;)public class MyConfiguration&#123; @Bean public List&lt;String&gt; list()&#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;aaa&quot;); list.add(&quot;bbb&quot;); return list; &#125;&#125;@Componentpublic class Player&#123; @Autowired private List&lt;String&gt; list; //...&#125; 4. 注解方式Bean的常用配置项(作用域,生命周期,懒加载等)4.1 注解方式Bean的作用域12345678910111213@Configuration@ComponentScan(&quot;包路径&quot;)public class MyConfiguration&#123; @Bean(name=&quot;bean1&quot;) @Scope(&quot;singleton&quot;) public Bean1 bean1()&#123; return Bean1 = new Bean1(); &#125;&#125;@Component@Scope(&quot;singleton&quot;)public class Bean&#123;&#125; 4.2 注解方式Bean的懒加载1234567891011121314@Configuration@ComponentScan(&quot;包路径&quot;)@Lazy //相当于xml中default-lazy-init=&quot;true&quot;public class MyConfiguration&#123; @Bean(name=&quot;bean1&quot;) @Lazy public Bean1 bean1()&#123; return Bean1 = new Bean1(); &#125;&#125;@Component@Lazypublic class Bean&#123;&#125; 4.3 Bean初始化和销毁一、实现InitializingBean和DisposableBean接口（xml和注解都支持）。 12345678910111213public class Bean implements InitializingBean&#123; @Override public void afterPropertiesSet()&#123; //执行一些初始化后的工作 &#125;&#125;public class Bean implements DisposableBean&#123; @Override public void destroy()&#123; //执行一些销毁前的工作 &#125;&#125; 二、xml形式 12345678public class Bean&#123; public void init()&#123; //执行一些初始化后的工作 &#125; public void cleanup()&#123; //执行一些销毁前的工作 &#125;&#125; 123&lt;bean id=&quot;bean&quot; class=&quot;example.Bean&quot; init-method=&quot;init&quot; destroy-method=&quot;cleanup&quot;&gt;&lt;/bean&gt; 三、注解形式1，@Bean(initMethod=”init”, destroyMethod=”cleanup”) 12345678910111213141516public class Bean&#123; public void init()&#123; //执行一些初始化后的工作 &#125; public void cleanup()&#123; //执行一些销毁前的工作 &#125;&#125;@Configurationpublic class MyConfiguration&#123; @Bean(initMethod=&quot;init&quot;, destroyMethod=&quot;cleanup&quot;) public Bean bean()&#123; return new Bean(); &#125;&#125; 四、注解形式2，添加@PostConstruct，@PreDestroy 1234567891011@Componentpublic class Bean&#123; @PostConstruct public void init()&#123; //执行一些初始化后的工作 &#125; @PreDestroy public void cleanup()&#123; //执行一些销毁前的工作 &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"【Spring】IoC控制反转","date":"2018-03-27T14:38:23.000Z","path":"article/20180327.html","text":"Spring是一个开源的轻量级控制反转(IOC)和面向切面(AOP)的容器框架，它主要是为了解决企业应用开发的复杂性而诞生的，但现在已不止应用于企业服务。 IOC：Inversion Of Control（控制反转），构成Spring框架的核心基础 DAO：Data Access Object（数据 访问对象），Spring对JDBC访问数据库的简化和封装 WebMVC：Spring对Web部分(jsp,servlet,ajax)以及MVC设计模式的支持 AOP：是在面向对象的基础上发展来的更高级的技术 ORM：Object Relation Mapping（对象关系映射），以面向对象的思想来访问数据库 JEE：Java的消息服务，远程调用，邮件服务等 IoC（控制反转） Spring容器初始化 spring容器创建对象(实例化) Spring DI注入的实现 DI的参数的注入 Bean的常用配置项(作用域,生命周期,懒加载等) 1. IoC（控制反转）IoC：(Inversion of Control),控制反转：控制权的转移，应用程序本身不负责依赖对象的创建和维护，而是由外部容器负责创建和维护。 控制：控制对象的创建及销毁（生命周期） 反转：将对象的控制权交给IoC容器 DI：(Dependence Injection),依赖注入(注射)是IoC控制反转的一种具体实现方法，通过参数的方式从外部传入依赖，将依赖的创建由主动变为被动。 简单来说， 当 组件A 依赖 组件B 时，IoC容器通过设置A的属性，把B传入的过程叫依赖注入 IoC的好处：降低了组件的依赖程度，让组件之间变成低耦合设计。 2. Spring容器初始化任何Java类都可以在Spring容器中创建对象 并交由容器来进行管理和使用，Spring容器 实现了 IOC 和 AOP 机制，Spring容器的类型是 BeanFactory 或者 ApplicationContext BeanFactory提供配置结构和基本功能，加载并初始化Bean ApplicationContext保存了Bean对象并在Spring中被广泛使用 2.1 初始化ApplicationContext的几种方式： 本地文件12FileSystemXmlApplicationContext app = new FileSystemXmlApplicationContext(&quot;F:/workspace/appcontext.xml&quot;); Classpath12ClassPathXmlApplicationContext app = new ClassPathXmlApplicationContext(&quot;classath:applicationContext.xml&quot;); Web应用中依赖Servlet或Listener123&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; 2.2 Spring容器完成IOC的步骤 建立一个动态的Web项目，导入jar包(ioc) 拷贝Spring容器配置文件到src(Source classpath)下 在spring容器配置文件中配置文件中配置一个对象的创建 &lt;baen id=&quot;对象引用名&quot; class=&quot;包名.类名&quot;&gt;&lt;/baen&gt; 写一个测试类 创建Spring容器对象，然后从容去中获取创建的组件 applicationContext.getBean(&quot;对象引用名&quot;, 类名.class) 3. spring容器创建对象(实例化)3.1 构造器方式实例化 配置文件：&lt;baen id=&quot;对象引用名&quot; class=&quot;包名.类名&quot;&gt;&lt;/baen&gt; applicationContext.getBean(&quot;对象引用名&quot;, 类名.class)默认调用类型对应的无参构造方法1&lt;bean id=&quot;date&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt; 12ApplicationContext app = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);Date date = app.getBean(&quot;date&quot;, Date.class); 3.2 静态工厂方法实例化 使用一个类型对应的静态方法来获取这个类型的对象 &lt;bean id=&quot;对象引用名&quot; class=&quot;包名.工厂类名&quot; factory-method=&quot;静态方法名&quot;&gt;&lt;/bean&gt;1&lt;bean id=&quot;cal&quot; class=&quot;java.util.Calendar&quot; factory-method=&quot;getInstance&quot;&gt;&lt;/bean&gt; 12ApplicationContext app = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);Calendar cal = app.getBean(&quot;cal&quot;, Calendar.class); 3.3 实例工厂方法实例化 使用一个已经存在的对象，来调用对应的成员方法来获取另一个类型的对象 &lt;bean id=&quot;对象的引用名&quot; factory-bean=&quot;工厂方法的id&quot; factory-method=&quot;成员方法名&quot;&gt;&lt;/bean&gt;12&lt;bean id=&quot;cal&quot; class=&quot;java.util.Calendar&quot; factory-method=&quot;getInstance&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;time&quot; factory-bean=&quot;cal&quot; factory-method=&quot;getTime&quot;&gt;&lt;/bean&gt; 12ApplicationContext app = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);Date time = app.getBean(&quot;time&quot;, Date.class); 4. Spring DI注入的实现Spring注入是指在启动Spring容器加载bean配置的时候，完成对变量的赋值行为。Bean属性值：基本数据类型用value，复杂数据类型用ref(传入组件id)。DI的实现方法：设值注入(setter注入)、构造注入、自动化注入(自动装配) 实例：准备两个实体类Card，Player：Card有suit(花色)和point(点数)，Player有name(名字)和card(牌)。 4.1 设值注入property(属性)的name参考对象set方法 123456789&lt;bean id=&quot;card&quot; class=&quot;bean.Card&quot;&gt; &lt;property name=&quot;suit&quot; value=&quot;黑桃&quot;&gt;&lt;/property&gt; &lt;property name=&quot;point&quot; value=&quot;A&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- Player参考其setCard方法 --&gt;&lt;bean id=&quot;player&quot; class=&quot;bean.Player&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;玩家1&quot;&gt;&lt;/property&gt; &lt;property name=&quot;card&quot; ref=&quot;card&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 4.2 构造注入（Constructor arguments）构建对象时赋值，参考对应构造方法（name为构造方法参数名，也可以用index:0开始） 123456789&lt;bean id=&quot;card2&quot; class=&quot;bean.Card&quot;&gt; &lt;constructor-arg name=&quot;suit&quot; value=&quot;红桃&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;point&quot; value=&quot;K&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- Player参考其构造方法Player(name,card) --&gt;&lt;bean id=&quot;player2&quot; class=&quot;bean.Player&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;玩家2&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;card&quot; ref=&quot;card2&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 4.3 自动化注入（Autowiring mode）一般用来解决复杂值的注入，可以通过bean标记的autowrie属性(autowire=”byName/byType/constructor”)指定对应的自动化的注入方式 1&lt;bean id=&quot;bean1&quot; class=&quot;example.exampleBean&quot; autowire=&quot;&quot; /&gt; 自动装配autowire属性 有五种自动装配的方式： No：默认，需要通过ref属性来连接bean。 byName： 与当前组件属性名 和 容器中其他组件的id 一致的bean，自动装配。1234567&lt;bean id=&quot;card3&quot; class=&quot;bean.Card&quot;&gt; &lt;property name=&quot;suit&quot; value=&quot;方片&quot;&gt;&lt;/property&gt; &lt;property name=&quot;point&quot; value=&quot;J&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- Player中必须要有setCard3 方法(setter方法名要与注入组件id对应) 否则Spring会将id为card的bean通过setter方法进行自动装配(若有setCard方法)--&gt;&lt;bean id=&quot;player3&quot; class=&quot;bean.Player&quot; autowire=&quot;byName&quot;&gt;&lt;/bean&gt; byType：与当前组件属性类型 和 容器中其他组件的class 一致的bean，自动装配，如果存在多个则抛出异常。123456&lt;bean class=&quot;bean.Card&quot;&gt; &lt;property name=&quot;suit&quot; value=&quot;方片&quot;&gt;&lt;/property&gt; &lt;property name=&quot;point&quot; value=&quot;J&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- Spring会将类型为Card的bean通过setter方法进行自动装配(setter参数类型与注入组件类型对应) --&gt;&lt;bean id=&quot;player4&quot; class=&quot;bean.Player&quot; autowire=&quot;byType&quot;&gt;&lt;/bean&gt; constructor：与当前组件 构造方法的参数 容器中其他组件的id 一致的bean，自动装配，不匹配再和 容器中其他组件的class 一致的bean，自动装配（如果存在多个则不装配），如果构造方法中第一个参数不匹配，则终止后续赋值。123456&lt;bean id=&quot;card5&quot; class=&quot;bean.Card&quot;&gt; &lt;property name=&quot;suit&quot; value=&quot;方片&quot;&gt;&lt;/property&gt; &lt;property name=&quot;point&quot; value=&quot;J&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- Player添加构造方法Player(Card card5)，构造方法参数名与注入组件id对应，不匹配再用构造方法参数类型和注入组件class匹配，如果存在多个则不装配 --&gt;&lt;bean id=&quot;player5&quot; class=&quot;bean.Player&quot; autowire=&quot;constructor&quot;&gt;&lt;/bean&gt; autodetect：如果有默认的构造器，则通过constructor方式进行自动装配，否则使用byType方式进行自动装配。 5. DI的参数的注入Bean对象 注入类型 可以是 字符串、集合、bean对象。 5.1 注入字符串12345&lt;bean id=&quot;msg&quot; class=&quot;com.xdl.bean.OracleDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;scott&quot;/&gt; &lt;property name=&quot;password&quot;&gt;&lt;value&gt;tiger&lt;/value&gt;&lt;/property&gt; &lt;property name=&quot;msg&quot;&gt;&lt;null/&gt;&lt;/property&gt;&lt;/bean&gt; 5.2 注入集合12345678910111213141516171819202122232425262728&lt;!-- 1. 定义list集合 --&gt;&lt;property name=&quot;friends&quot;&gt; &lt;list&gt; &lt;value&gt;值1&lt;/value&gt; &lt;value&gt;值2&lt;/value&gt; &lt;/list&gt;&lt;/property&gt;&lt;!-- 2. 定义set集合 --&gt;&lt;property name=&quot;friends2&quot;&gt; &lt;set&gt; &lt;value&gt;值1&lt;/value&gt; &lt;value&gt;值2&lt;/value&gt; &lt;/set&gt;&lt;/property&gt;&lt;!-- 3. 定义map集合 --&gt;&lt;property name=&quot;phones&quot;&gt; &lt;map&gt; &lt;entry key=&quot;1594546454&quot; value=&quot;值1&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;1594546464&quot; value=&quot;值2&quot;&gt;&lt;/entry&gt; &lt;/map&gt;&lt;/property&gt;&lt;!-- 4. props集合 --&gt;&lt;property name=&quot;phones2&quot;&gt; &lt;props&gt; &lt;prop key=&quot;164545564&quot;&gt;值1&lt;/prop&gt; &lt;prop key=&quot;164546756&quot;&gt;值2&lt;/prop&gt; &lt;/props&gt;&lt;/property&gt; 5.3 集合参数的单独定义注入集合–引入：List、Set、Map、Properties集合也可以先独立定义，再注入的方式使用，这样便于重复利用。 123456789101112131415161718192021222324&lt;!-- 1. 定义list集合 --&gt;&lt;util:list id=&quot;ref_friends&quot;&gt; &lt;value&gt;值1&lt;/value&gt; &lt;value&gt;值2&lt;/value&gt;&lt;/util:list&gt;&lt;!-- 2. 定义set集合 --&gt;&lt;util:set id=&quot;ref_buddys&quot;&gt; &lt;value&gt;值&lt;/value&gt; &lt;value&gt;值2&lt;/value&gt;&lt;/util:set&gt;&lt;!-- 3. 定义map集合 --&gt;&lt;util:map id=&quot;ref_phones&quot;&gt; &lt;entry key=&quot;159454644&quot; value=&quot;值1&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;1594546454&quot; value=&quot;值2&quot;&gt;&lt;/entry&gt;&lt;/util:map&gt;&lt;!-- 4. props集合 --&gt;&lt;util:properties id=&quot;ref_phonePro&quot;&gt; &lt;prop key=&quot;164545564&quot;&gt;值1&lt;/prop&gt; &lt;prop key=&quot;16454675665564&quot;&gt;值2&lt;/prop&gt;&lt;/util:properties&gt;&lt;util:properties id=&quot;ref_db&quot; location=&quot;classpath:db.properties&quot;&gt;&lt;/util:properties&gt;&lt;!-- 使用 --&gt;&lt;property name=&quot;phones&quot; ref=&quot;ref_phones&quot;&gt;&lt;/property&gt;&lt;property name=&quot;phones2&quot; ref=&quot;ref_phonePro&quot;&gt;&lt;/property&gt; 5.3 Spring的’EL’表达式它和EL在语法上很 相似，可以读取一个bean对象/集合中的数据。Spring EL 采用 #{Sp Expression Language} 即 #&#123;spring表达式&#125;，可在xml配置和注解中使用。 Spring EL配置连接池对象12345678&lt;!-- 引入数据库配置文件 --&gt;&lt;util:properties id=&quot;db&quot; location=&quot;classpath:db.properties&quot;/&gt;&lt;!-- 配置连接池 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.xdl.bean.OracleDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;#&#123;db.name&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;#&#123;db.password&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;#&#123;db.url&#125;&quot;/&gt;&lt;/bean&gt; 6. Bean的常用配置项(作用域,生命周期,懒加载等)Bean的常用配置项：Id、Class、Scope、Constructor arguments、Propertties、Autowiring mode、Lazy-initialization mode、Initialization/destruction method 6.1 Bean作用域（Scope） Singleton作用域 单例，指一个Bean容器只存在一份 prototype作用域 每次请求(使用)创建新的实例，destroy方式不生效 Web环境作用域： request作用域：每个request请求都会创建一个单独的实例。 session作用域：每个session都会创建一个单独的实例。 application作用域：每个servletContext都会创建一个单独的实例。 websocket作用域：每个websocket连接都会创建一个单独的实例。 自定义作用域 SimpleThreadScope作用域：每个线程都会创建一个单独的实例。 6.2 Bean的生命周期（Initialization/destruction method）Bean的生命周期：定义 –&gt; 初始化 –&gt; 使用 –&gt; 销毁 6.2.1 Bean初始化如果需要在Bean实例化之后执行一些逻辑，有两种方法： 实现InitializingBean接口(org.springframework.beans.factory.InitializingBean)，覆盖afterPropertiesSet方法，在afterPropertiesSet中执行一些初始化后的工作。 配置init-method 配置**beans**的default-init-method属性 来指定一个初始化方法，这个指定针对容器中所有的对象，由于这样影响的范围比较广，所以当对象没有对应的初始化方法程序也不会报错。 配置**bean**的init-method来指定初始化方法，这样只影响包含init-method属性所在的bean标记创建的对象，这样控制的对象比较精准，所以当类型中没有这个初始化方法则程序崩溃。1&lt;bean id=&quot;exampleId&quot; class=&quot;example.exampleBean&quot; init-method=&quot;init&quot;&gt;&lt;/bean&gt; 12345public class ExampleBean&#123; public void init()&#123; //执行一些初始化后的工作 &#125;&#125; 6.2.2 Bean销毁如果需要在Bean销毁之前执行一些逻辑，有两种方法： 实现DisposableBean接口(org.springframework.beans.factory.DisposableBean)覆盖destroy方法，，在destroy中执行一些销毁前的工作。 配置destroy-method 配置**beans**的default-destroy-method属性 来指定一个销毁方法，这个指定针对容器中所有的对象，由于这样影响的范围比较广，所以当对象没有对应的销毁方法程序也不会报错。 配置**bean**的destroy-method来指定销毁方法，这样只影响包含destroy-method属性所在的bean标记创建的对象，这样控制的对象比较精准，所以当类型中没有这个销毁方法则程序崩溃。1&lt;bean id=&quot;exampleId&quot; class=&quot;example.exampleBean&quot; destroy-method=&quot;cleanup&quot;&gt;&lt;/bean&gt; 12345public class ExampleBean&#123; public void cleanup()&#123; //执行一些销毁前的工作 &#125;&#125; 注意：销毁方法只针对单例模式的对象 6.3 Bean的懒加载（Lazy-initialization mode）Spring容器会在创建容器时提前初始化Singleton作用域的bean，可以通过bean标记lazy-init=&quot;true&quot;延迟实例化(对象被使用时才创建)。 配置lazy-init 配置**beans**的default-lazy-init=&quot;true&quot;为所有Bean设定懒加载。 配置**bean**的lazy-init=&quot;true&quot;为单独的某个Bean设定懒加载。1&lt;bean id=&quot;bean1&quot; class=&quot;example.exampleBean&quot; lazy-init=&quot;true&quot;/&gt; 适用场景：如果某个Bean在程序整个运行周期都可能不会被使用，可以考虑设定该Bean为懒加载 优点：尽可能的节约了资源 缺点：可能导致某个操作响应时间增加 6.4 Bean装配的Aware接口实现了Aware接口的bean在初始化后可以获取相应资源并进行相应的操作。 ApplicationContextAware 接口方法：setApplicationContext 作用：通常用来获取上下文对象，声明全局变量后在方法中对变量进行初始化并供其他方法调用 实现过程：创建一个类并实现ApplicationContextAware接口，重写setApplicationContext方法；在xml文件中配置该类；当spring加载该配置文件时即调用接口方法。 BeanNameAware 接口方法：setBeanName 作用：获取声明的类名，声明全局变量后在方法中对变量进行初始化并供其他方法调用 实现过程：创建一个类并实现BeanNameAware接口，重写setBeanName方法；在xml文件中配置该类；当spring加载该配置文件时即调用接口方法。 6.4 Bean装配之ResourceResources（针对于资源文件的统一接口） UrlResource：URL 对应的资源，根据一个 URL 地址即可获取 ClassPathResource：获取类路径下的资源 FileSystemResource：获取文件系统里面的资源 ServletContextResource：ServletContext 封装的资源，用于访问 ServletContext 环境下的资源 InputStreamResource：获取输入流封装的资源 ByteArrayResource：获取字节数组封装的资源 ResourceLoader: 所有的 application contexts 都实现了 ResourceLoader 接口，因此所有的 application contexts 都能通过getResource()获取Resource实例。 getResource()参数： classPath方式：”classPath:class路径下文件” file方式： “file:本地磁盘文件绝对地址” url方式： “url:URL地址下文件” 没有前缀时依赖applicationContext的配置文件路径: “文件全名” eg:applicationContext.getResource(&quot;classpath:config.txt&quot;)","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"【Java教程】Web编程基础","date":"2017-06-20T12:59:40.000Z","path":"article/20170620.html","text":"JavaWeb是用Java技术来解决相关web互联网领域的技术总和。Java提供了技术方案可以解决客户端和服务器端的实现，特别是服务器的应用，比如Servlet，JSP和第三方框架等等。 http协议 Servlet 请求的转发与重定向 上下文对象ServletContext 会话跟踪（状态管理） JSP 内置对象(隐含对象) taglib指令 JavaWeb三大组件 JSON在Java中的使用 AJAX 1. http协议超文本传输协议，是一种应用层的网络传输协议 http协议的特点： 简单，快速：支持多种不同的的数据提交方式，如get/post 数据传输灵活，支持任意类型数据的传输 无连接协议：每次连接，只处理一次请求，进行一次响应，响应完毕，立即断开。 无状态协议：处理请求与响应时没有记忆能力，如果需要处理之间的信息，只能重新传递。 http协议的组成部分： 请求：浏览器连接服务器的过程 响应：服务器回复浏览器的过程 http协议的请求： 请求头：描述客户端的信息 请求体：GET没有请求体，请求体用于存储POST请求发送的数据。 请求空行：请求头与请求体之间的一行空白 请求行：描述请求方式，服务器地址，协议版本等 http协议的响应： 响应头：描述服务器的信息 响应体：响应的内容，文本，json数据等。 响应行：描述服务器协议版本，响应状态码，以及响应成功或失败的解释。 2. Servletservlet是一个运行在tomcat上的Java类，用户通过浏览器输入地址，触发这个类，这个类执行完毕，准备一个响应体，发送给浏览器。 2.1 Servlet编写步骤： 编写一个Java类，继承HttpServlet类 重新service方法 在service方法中，对用户请求进行响应。 123456789101112131415//注解：添加访问的网址@WebServlet(&quot;/hello&quot;)public class MyServlet extends HttpServlet &#123; private static final long serialVersionUID = 1L; @Override public void service(ServletRequest req, ServletResponse res) throws IOException &#123; //1.设置响应体的编码，以及内容类型 res.setContentType(&quot;text/html;charset=utf-8&quot;); //2.得到响应体输出的打印流 PrintWriter out = res.getWriter(); //3.打印文字 out.println(&quot;&lt;h1&gt;Hello Servlet!&lt;/h1&gt;&quot;); &#125;&#125; 2.2 配置ervlet类的访问网址 web3.0版本之后使用注解的方式配置ervlet类的访问网址 web3.0版本之前配置Servlet访问网址的方式： 将Servlet类，配置到web.xml中，告知tomcat，servlet的类名 配置Servlet类的别名，并给指定别名的Servlet添加映射网址。 123456789101112&lt;!-- 将servlet类，配置到web.xml中，告知tomcat，servlet的类名 --&gt;&lt;servlet&gt; &lt;!-- Servlet类别名，用于后续添加映射网址 --&gt; &lt;servlet-name&gt;demo1&lt;/servlet-name&gt; &lt;!-- Servlet类全名 --&gt; &lt;servlet-class&gt;day01_Servlet.demo1.MyServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;!-- 给指定别名的Servlet添加映射网址 --&gt; &lt;servlet-name&gt;demo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/hello&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 2.3 Servlet生命周期 实例化 –&gt; 初始化(init) –&gt; 服务(service) –&gt; 销毁(销毁之前调用destory) –&gt; 不可用 创建时机：默认情况下，当用户第一次访问Servlet的映射网址是Servlet对象被创建，后续用户再次访问，是重复利用此对象。 销毁时机：当tomcat关闭时 或 应用从tomcat卸载时。 tomcat为了便于我们进行资源的合理缓存，为生命周期事件提供了三个方法： init(); 当Servlet对象被创建时，方法执行，通常在这里进行一些可重用资源的初始化工作。 service(); 服务方法，当用户每次发起请求时，此方法用于处理请求，并进行响应，此方法每次都执行在新的线程中。 destory(); 当Servlet即将被销毁时，方法执行，释放资源的代码可写在此方法中。 2.4 get和post区别 GET请求： 没有请求体，请求时携带参数在url中，参数在url地址的?后，参数由=连接的键值对组成，&amp;连接键值对。 只能传输字符串类型参数 浏览器url地址最大长度4kb 数据传输时，参数在url中明文显示，不安全。 POST请求： 有请求体，是一个单独的数据包，用于存储请求中的多个参数 可传输任意类型的数据，进行文件上传必须POST请求 可以传递的数据大小，理论上没有上限 数据传输时在单独的数据包，较为安全。 2.5 接收请求中的参数 根据参数的名称，接收参数的单个值 String value = request.getParameter(String name); 根据参数的名称，接收一组参数的值 String[] values = request.getParameterValues(String name); 1234567891011121314protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.setCharacterEncoding(&quot;UTF-8&quot;); response.setContentType(&quot;text/html;charset=utf-8&quot;); //1.接收 String username = request.getParameter(&quot;username&quot;); String[] password = request.getParameterValues(&quot;password&quot;); //2.打印 System.out.println(&quot;username:&quot; + username); System.out.println(&quot;password:&quot; + password[0]); System.out.println(&quot;password2:&quot; + password[1]); //3.浏览器输出 response.getWriter().append(&quot;&lt;div&gt;很遗憾注册失败，点击&lt;a href=\\&quot;demo1.html\\&quot;&gt;重新注册&lt;/a&gt;&lt;/div&gt;&quot;);&#125; 2.6 乱码处理2.6.1 乱码情况： 浏览器提交表单时，会对中文参数值进行自动编码。Tomcat服务器接收到的浏览器请求后，默认使用iso-8859-1去解码，当编码与解码方式不一致时，就会乱码。 tomcat8版本之前(不包含tomcat8版本), GET请求乱码 任何版本, POST请求乱码 2.6.2 请求乱码处理： 适用于所有乱码问题：(Tomcat8之后get无乱码) 指定浏览器打开页面的编码&lt;meta charset=&quot;UTF-8&quot;&gt;; 将接收到的中文乱码重新编码： 12String name = request.getParameter(&quot;userName&quot;);String userName = new String( name.getByte(&quot;ISO-8859-1&quot;),&quot;utf-8&quot;); 仅适用于POST请求： 指定浏览器打开页面的编码&lt;meta charset=&quot;UTF-8&quot;&gt;; Servlet接收之前设置解码（需在调用request.getParameter(“key”)之前设置）request.setCharacterEncoding(&quot;utf-8&quot;); 2.6.3 响应乱码的处理： 方式一：设置响应的内容类型, 以及编码格式:response.setContentType(&quot;text/html;charset=utf-8&quot;); 方式二：进设置编码格式, 不设置响应内容类型:response.setCharacterEncoding(&quot;UTF-8&quot;)(常用于客户端不是浏览器的情况, 如果在浏览器的环境下设置, 有部分浏览器无法识别, 依然会乱码); 2.7 Servlet的创建时机 通过web.xml配置Servlet, 可以修改Servlet加载的时机。 可以给Servlet节点，添加&lt;load-on-startup&gt;节点来制定servlet启动顺序。 节点中的值为数字： -1：默认-1，表示当用户第一次请求时，创建对象 &gt;=0：大于等于0，当服务器启动时，创建对象，值越小创建越早，值相同按web.xml配置顺序创建 123456789101112&lt;servlet&gt; &lt;servlet&gt; &lt;servlet-name&gt;s1&lt;/servlet-name&gt; &lt;servlet-class&gt;demo.ServletDemo&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;s1&lt;/servlet-name&gt; &lt;url-pattern&gt;/s1&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt;&lt;/servlet-mapping&gt; 3. 请求的转发与重定向3.1 请求对象request的常用操作 getMethod() : 得到请求的方式 getRequestURI() : 获取浏览器请求地址 getRemoteAddr() : 获取客户端ip地址 getRemoteHost() : 获取客户端名称 getServerName() : 获取服务器名称 getServerPort() : 获取服务器端口号 getQueryString() : 获取get请求参数字符串，其他请求返回null 3.1 请求的转发与重定向注意事项 请求转发与重定向操作，必须要有出口。 当一个请求在servlet中进行了重定向，那么这个servlet就不要再进行响应了 3.2 转发* 一个web组件，将未处理完毕的请求，通过tomcat转交给另一个web组件处理 步骤： 获取请求转发器：RequestDispather rd = request.getRequestDispacher(&quot;转发地址&quot;); 进行转发操作：rd.forward(request, response); 因为通常请求转发器获取后, 只会使用一次 , 一般不给对象起名, 简写: request.getRequestDispacher(&quot;转发地址&quot;).forward(request, response); 特点： 转发过程中，多个web组件之间共享一个请求对象request与响应对象response 在转发过程中，无论转发多少次，浏览器只发起了一次请求，所以浏览器地址不会改变 转发不能跨项目实现 比重定向效率更高 3.3 重定向* 一个web组件，处理完毕请求后，告知浏览器，将请求转向另一个地址 格式：response.sendRedirect(&quot;重定向地址&quot;)； 原理：当客户端请求服务器时，发起重定向流程： 给浏览器响应302的状态码 , 以及一个键值对, 键为: location , 值为重定向新地址. 当浏览器接收到302的状态码时, HTTP协议规定了浏览器会寻找location对象的新地址. 浏览器自动发起新的请求 , 跳转到新地址. 特点： 重定向会产生两个请求对象，多个请求对象中数据不互通 浏览器地址发生了改变 重定向可以跨域实现 比转发效率低 4. 上下文对象ServletContext 用于关联多个servlet，是servlet之间通讯的桥梁，用于多个servlet之间的信息共享 每一个项目运行时，tomcat会为这个项目创建一个servletContext，项目关闭时销毁。 获取ServletContext对象：ServletContext context = getServletContext(); 常用方法 context.setAttributes(String key, Objexct value); //设置替换数据 context.getAttributes(String key); //获取数据 context.removeAttributes(String key); //删除数据 context.getRealPath(“/“); //获取项目运行时所在文件路径 5. 会话跟踪（状态管理） 存在两种实现： cookie: 将浏览器产生的状态存储在浏览器中 Session: 将浏览器产生的状态存储在服务器中 cookie技术原理： 服务器向客户端响应时，将数据以set-Cookie消息头（响应头）的方式发给浏览器， 浏览器接收到cookie后，会将这些数据以文本文件的方式（.txt文件）保存起来 当浏览器再次发起相同请求时，浏览器会将之前存储的cookie,添加到请求头，发给服务器 Session技术原理： 当浏览器访问服务器时，服务器可以选择为用户创建一个Session对象(类似于map集合)， 该Session对象有一个id属性，称之为SessionId，服务器会将这个SessionId以cookie方式发送给浏览器 浏览器再次访问服务器时，同时会传递SessionId的cookie给i服务器，服务器根据sessionId找到Session对象，供程序使用。 5.1 Cookie 创建Cookie：Cookie在Java中是一个类，每个cookie的对象都表示一个键值对 Cookie cookie = new Cookie(String key, String value); 注意：tomcat8.5版本之前，cookie无法出场中文 通过响应对象，将cookie添加到响应头,可添加多个 response.addCookie(Cookie cookie); 通过请求头得到cookie数组，没有则返回null Cookie[] cookies = request.getCookies(); 取键：cookie.getName(); 取值：cookie.getValue() Cookie的存储时长： cookie.setMaxAge(int 秒)； 正数：倒计时秒数 0：表示立即删除此cookie，常用于覆盖一个存活时长较长的cookie,用于删除它 负数：默认-1，表示会话结束时自动删除（关闭浏览器） Cookie的存储路径问题 存储的cookie发送到服务器时，判断是否发送的依据是：域名相同，路径相同 为了避免路径问题，通常会将cookie设置统一路径为根路径：cookie.setPath(“/“); 5.2 Cookie的优缺点 缺点： Cookie技术存储的数据类型，只能是字符串，且早期版本(8.5之前)不可存储中文。 数据存储在客户的计算机中，不安全，不建议存储安全敏感数据 保存数据量有限制，大约4kb左右 依赖于用户的浏览器设置，用户可以金庸cookie，可能被用户主动删除 优点： 分散服务器的压力 5.3 Session 获取Session 格式1：request.getSession();//等价参数传true 格式2：request.getSession(boolean isNew); true，根据浏览器的SessionId查找一个session，若没有就新创建一个对象并返回 false，根据浏览器的SessionId查找一个session，若没有就返回null Session常用方法 session.setAttribute(String key, object value);//设置/替换值 session.getAttribute(String key);//获取值 session.invalidate();//销毁 设置session存活时长 默认会话时长30分钟，当浏览器最后一次访问服务器后30分钟后，若没有再次连接，则session被销毁。 可以通过修改配置文件，修改所有的session时长 修改conf/web.xml的&lt;session-config&gt;&lt;session-tiomeout&gt;数值分钟&lt;/session-tiomeout&gt;&lt;/session-config&gt; 可以通过session对象，修改单个对象的session时长 void session.setMaxInactiveInterval(int seconds) 5.4 Session的优缺点 缺点： 数据存储在服务器端，当用户量大时，对服务器造成极大的压力，很容易耗尽服务器资源 优点： 数据存储在服务器中，安全 数据类型为Object，在Java中表示可以存储所有类型的数据 session存储的数据大小，理论上无限的。 5.5 Cookie和Session的使用 Cookie和Session不是互斥的，是相辅相成的 在项目开发时： 对安全敏感的数据，存储在session中 对安全不敏感的字符串数据，可以选择存储在Cookie中 对于大的数据，应该存在数据库和文件中 注意：cookie和session是为了管理状态而非存储数据。 6.JSP6.1 JSP语法基础 Java Server Pages：java动态网页技术 JSP引擎原理：JSP引擎读取JSP文件，将文件转换为Servlet，由servlet给用户响应 注意： JSP文件的转换 发生在服务器启动时，当用户访问JSP时，其实访问的是JSP文件转换的Servlet 执行流程：浏览器请求–&gt;tomcat–&gt;JSP引擎转换为Servlet–&gt;转换的Servlet–&gt;准备响应体–&gt;响应给浏览器–&gt;浏览器解析html JSP语法结构 html代码 Java代码 Jsp特有的语法结构 Java代码声明区：指的是类的成员位置 123&lt;%! // Java代码声明区%&gt; Java代码执行区：指的是Servlet的service方法中，每次用户请求，执行区的代码都会执行起来 123&lt;% // Java代码执行区%&gt; JSP输出表达式 用于快速的将Java中的数据，输出到网页中.. 语法格式：&lt;%=数据 %&gt;，编译后被转换成out.print(数据) JSP注释： html中可以用&lt;!-- --&gt; java中可以用//，/**/，/** */ jsp注释&lt;%-- --%&gt; html和java注释会被编译，其中html注释会被编译到页面，jsp注释编译器会自动忽略 6.2 JSP三大指令 page指令 include指令 taglib指令 指令使用格式：&lt;%@ 指令名称 属性1=值 属性2=值 属性n=值 %&gt;*语法上，JSP允许在单个页面出现多个相同的JSP指令 6.2.1 page指令 用于配置页面信息 12345678910111213&lt;%@ page language=&quot;java&quot;：语言 contentType=&quot;text/html;charset=utf-8&quot;：响应的内容类型，以及响应的编码格式 pageEncoding=&quot;UTF-8&quot;：文件存储的编码格式 extends=&quot;继承的父类&quot; buffer=&quot;数字/none&quot;：是否允许缓存，默认值8kb autoFlush=&quot;true/false&quot;：是否自动清除缓存，默认true session=&quot;true/false&quot;：是否提前准备session对象，默认true isThreadSafe=&quot;true/false&quot;：是否线程安全的 import=&quot;java.util.List&quot;：用于导包，多个包使用&quot;,&quot;隔开 errorPage=&quot;网址&quot;：当页面发生BUG后，显示哪个页面 isErrorPage=&quot;true/false&quot;：当前页面是否是一个错误处理页面，如果结果为true，当别的页面产生错误，跳转到此页面，会提前准备好一个对象exception，此对象封装了错误信息%&gt; 6.3 项目发生错误时，统一的处理方式 打开项目的web.xml 加入子节点&lt;error-page&gt;&lt;error-code&gt;错误码&lt;/error-code&gt;&lt;location&gt;处理网址&lt;/location&gt;&lt;/error-page&gt; 12345678&lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/error.jsp&lt;/location&gt;&lt;/error-page&gt;&lt;error-page&gt; &lt;error-code&gt;404&lt;/error-code&gt; &lt;location&gt;/404.jsp&lt;/location&gt;&lt;/error-page&gt; include指令：用于将jsp或html引入到另一个jsp中 语法格式：&lt;%@ include file=&quot;地址&quot; %&gt; include动作：用于将jsp或html引入到另一个jsp中 语法格式：&lt;jsp:include page=&quot;地址&quot;&gt; include指令 与 include动作区别： include指令：引入文件操作，是在JSP引擎的转换时发生，将多个jsp文件，生产为了一个Servlert（多个jsp =&gt; 一个Servlet） include动作：引入文件操作，是在浏览器请求时，将引用文件的响应体添加到了请求文件的响应体中（多个jsp =&gt; 多个Servlet） 7.内置对象(隐含对象) 在JSP中，我们的代码执行在service中，所谓内置对象，指的是在JSP引擎转换时期，在我们代码生成位置的上面，提前准备好的一些变量，对象。 内置对象通常是我们会主动创建的对象 7.1 九大内置对象 request 对象类型：java.servlet.HttpServletRequest request内置对象中包含了有关浏览器请求的信息，提供了大量get方法，用于获取cookie、header以及session内数据等。 response 对象类型：javax.servlet.HttpServletResponse response对象提供了多个方法用来处理HTTP响应，可以调用response中的方法修改ContentType中的MIME类型以及实现页面的跳转等。 config 对象类型：javax.servlet.ServletConfig 在Servlet初始化的时候，JSP引擎通过config向它传递信息。这种信息可以是属性名/值匹配的参数，也可以是通过ServletContext对象传递的服务器的有关信息。 out 对象类型：javax.servlet.jsp.JspWriter 在JSP开发过程中使用得最为频繁的对象 page 对象类型：java.lang.Object page对象有点类似于Java编程中的this指针，就是指当前JSP页面本身。 pageContext 对象类型：pageContext pageContext对象是一个比较特殊的对象。它相当于页面中所有其他对象功能的最大集成者，即使用它可以访问到本页面中所有其他对象 session 对象类型：java.servlet.http.HttpSession session是与请求有关的会话期，用来表示和存储当前页面的请求信息。 application 对象类型：javax.servlet.ServletContext 用于实现用户之间的数据共享（多使用于网络聊天系统）。 exception 对象类型：java.lang.Throwable 作用 exception内置对象是用来处理页面出现的异常错误。 7.2 JSP四大域对象 九大内置对象中，存在四个较为特殊的对象，这四个对象用户在不同的作用域中存储数据，获取数据，删除数据 域对象的特点：每一个内置对象，都类似一个Map集合，可以存取删除数据，都具备如下三个方法： 存储数据：setAttribute(String key, Object value); 获取数据：Object value = getAttribute(String); 删除数据： removeAttribute(String key); 四大内置对象，分别指的是： pageContext: (作用域：1个页面) 页面上下文，存储在pageContext中的数据, 作用域是最小的, pageContext在JSP代码执行时 创建, 在JSP代码执行完毕时, 销毁. request: (作用域：一次请求，如果请求被转发，可能跨越多个页面) 请求对象, 存储在请求对象中的数据, 域范围是一次请求, 请求一旦进行了响应, 就会被销毁. session: (作用域：一次会话，一次会话可能包含多个请求) 会话对象，存储在会话对象中的数据，只有在当前用户会话中可以使用，用户再次访问服务器的时间间隔超过30分钟，session就销毁了。 application: (域范围：一次服务，应用从启动到关闭application一直都在) Servlet上下文对象, 存储在application中的数据, 域范围是最大的. 在应用关闭之前 都可以使用. 7.3 EL表达式 用于将计算的结果输出到网页，也常用于快速的从域对象中取出数据，并输出到网页。 格式：$&#123;表达式&#125; EL表达式用于运算 在JSP中, 可以直接使用el表达式运算一些数据，例如: ${123+123} , 最终网页中显示的效果是: 246 用于取出域对象中的数据 取出数据直接输出：$&#123;域对象中存储的键&#125; 如果取出的数据不存在, 则不输出 (不可能显示null) 取出对象数据的属性值: 格式1： ${对象存储的键.属性名} 格式2： ${对象存储的键[“属性名”]} 格式3(动态取值)： ${对象存储的键[属性存储的键]} 取出集合中的数据 格式: ${集合存储时的key[下标]} 7.4 EL表达式取出数据的流程 四个域对象之间, 有时数据的键可能重复,优先从域范围较小的对象中, 取出数据. 步骤: 先从pageContext中, 寻找数据是否存在. 如果pageContext中数据不存在, 则去request中寻找数据是否存在 如果request 中数据不存在, 则去session中寻找数据是否存在 如果session中数据不存在, 则去application中寻找数据是否存在 如果application中数据不存在,则不输出任何数据. 8. taglib指令用于在JSP文件中，引入标签库文件。 格式： &lt;%@ taglib prefix=&quot;&quot; uri=&quot;&quot; %&gt; prefix: 是引入标签库后，标签库的名称。作用是用于区分引入的多个标签库，在使用标签库中的标签时，标签的写法：&lt;标签库名称:标签名&gt; uri: 每个标签库，都会拥有一个uri，它是用于区分标签库的，我们在引入这个库时，需要匹配uri属性 JSTL(JSP Standard Tag Library): JSP标准标签库 使用时，需要引入jar文件 if 标签，格式：&lt;库名称:if text=”${ booble }”&gt; forEach 标签，格式：&lt;库名称:forEach items=”${ List }” var=”item”&gt; 自定义标签库: 编写一个Java类, 继承SimpleTagSupport类. 重写父类的doTag方法. 在doTag方法中, 通过getJspContext方法, 的到JSP页面的上下文 通过上下文对象, 得到JSP中的out对象, 通过out对象, 向网页中输出内容 编写tld文件 , 描述标签库 以及 标签. 自定义标签库案例: 1234567891011121314public class MyTag1 extends SimpleTagSupport &#123; private static ArrayList&lt;String&gt; data = new ArrayList&lt;&gt;(); static &#123; data.add(&quot;流水在碰到底处时才会释放活力。——歌德&quot;); &#125; @Override public void doTag() throws JspException, IOException &#123; JspContext context = getJspContext(); JspWriter out = context.getOut(); Random r = new Random(); int index = r.nextInt(data.size()); out.println(&quot;&lt;span&gt;&quot;+data.get(index)+&quot;&lt;/span&gt;&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728&lt;taglib xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot;xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-jsptaglibrary_2_0.xsd&quot;version=&quot;2.0&quot;&gt; &lt;!-- 描述标签库 --&gt; &lt;!-- 是对于标签库的介绍 --&gt; &lt;description&gt;我们这个标签库, 是闲的慌 , 所以写的.&lt;/description&gt; &lt;!-- 描述标签库的名称 --&gt; &lt;display-name&gt;xdl&lt;/display-name&gt; &lt;!-- 标签库的版本 --&gt; &lt;tlib-version&gt;11.88&lt;/tlib-version&gt; &lt;!-- 建议的短命名称 --&gt; &lt;short-name&gt;xdl&lt;/short-name&gt; &lt;!-- 标签库的表示, 用于引入时匹配标签库 --&gt; &lt;uri&gt;http://shuidianshuisg.com&lt;/uri&gt; &lt;!-- 开始描述标签 --&gt; &lt;tag&gt; &lt;!-- 对于标签的介绍 --&gt; &lt;description&gt;这个标签用于随机向网页中, 输出一句名言&lt;/description&gt; &lt;!-- 标签名称 --&gt; &lt;name&gt;heiheihei&lt;/name&gt; &lt;!-- 标签所对应的的Java类 --&gt; &lt;tag-class&gt;cn.xdl.tag.MyTag1&lt;/tag-class&gt; &lt;!-- 标签的内容 --&gt; &lt;body-content&gt;empty&lt;/body-content&gt; &lt;/tag&gt;&lt;/taglib&gt; 9. JavaWeb三大组件(Servlet,filter,Lister)9.1 Filter过滤器 请求的过滤器，面向切面编程思想（AOP） 使用步骤： 编写一个类，实现Filter接口 通过注解或web.xml配置过滤器规则 过滤器链： 当多个过滤器，过滤同一个请求地址时，就形成了过滤器链，所有过滤器都放行后，servlet才会处理用户请求 过滤器链执行顺序：（若同时包含注解与web.xml,优先执行web.xml） 注解方式：按照类名的自然顺序先后 web.xml配置方式：按照web.xml配置顺序，先后执行 案例： 123456789101112131415161718192021222324252627282930313233343536373839@WebFilter(&quot;/home.jsp&quot;)public class AdminFilter implements Filter &#123; /** * 当Filter即将销毁时执行 */ @Override public void destroy() &#123; &#125; /** * 有新的请求, 满足了过滤器的过滤规则, 正在过滤 * 参数1. 请求对象 * 参数2. 响应对象 * 参数3. 过滤器链对象 */ @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;过滤管理员登录的过滤器 正在执行&quot;); //1. 从请求中, 得到session HttpServletRequest req = (HttpServletRequest) request; HttpSession session = req.getSession(); //2. 判断session中是否存在username Object username = session.getAttribute(&quot;username&quot;); //3. 如果存在, 且值为admin , 则放行 if(username !=null &amp;&amp; username.equals(&quot;admin&quot;)) &#123; //放行 chain.doFilter(request, response); &#125;else &#123; //4. 否则拦截, 并响应, 提示请先以管理员身份登录 response.getWriter().append(&quot;&lt;script&gt;alert(&#x27;请先以管理员身份登录, 再访问管理页面&#x27;);window.location.href=&#x27;login.jsp&#x27;&lt;/script&gt;&quot;); &#125; &#125; /** * 当Filter初始化时 执行 */ @Override public void init(FilterConfig arg0) throws ServletException &#123; &#125;&#125; web.xml配置方式 12345678&lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;cn.xdl.demo1.EnCodingFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;/home.jsp&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 9.2 Listener监听器 监听服务器的一些状态事件，事件驱动机制。 分为两类状态事件： 服务器中组件的生命周期 一些域对象中数据变化的事件 监听服务器的启动与关闭：ServletContextListener 监听ServletContext中数据的增加,删除,以及替换：ServletContextAttributeListener 监听Session会话的开启与关闭：HttpSessionListener 监听session中数据的增加,删除,以及替换：HttpSessionAttributeListener 10. JSON在Java中的使用 JSON：JavaScript Object Notation GSON.jar，将Java中的对象转换为JSON字符串，将JSON字符串转换为Java中的对象 1234//引入jar文件Gson g = new Gson();String str = g.toJson(Java对象);//转换JSON字符串类型 对象名 = g.fromJson(Json字符串, 类型.class);//转换为Java对象 11. AJAX 一种用于网页异步请求的技术，用于与服务器进行异步交互以及对网页局部刷新操作 Ajax请求的状态（readyState） 0：正在初始化 1：请求正在发送 2：请求发送完毕 3：服务器开始响应 4：响应接收完毕，连接断开 Ajax响应的状态（status） 200：成功 404：找不到资源 500：服务器错误 11.1 GET请求AJAX12345678910var xhr = new XMLHttpRequest();xhr.open(&quot;GET&quot;, &quot;地址?参数列表&quot;);xhr.onreadystatechange = function()&#123; if(xhr.readyState === 4 &amp;&amp; xhr.status === 200)&#123; //通过xhr.responseText接收响应体 &#125;else&#123; //失败处理 &#125;&#125;xhr.send(); 11.2 POST请求AJAX123456789101112var xhr = new XMLHttpRequest();xhr.open(&quot;POST&quot;, &quot;地址&quot;);xhr.onreadystatechange = function()&#123; if(xhr.readyState === 4 &amp;&amp; xhr.status === 200)&#123; //通过xhr.responseText接收响应体 &#125;else&#123; //失败处理 &#125;&#125;//POST请求设置请求头xhr.setRequestHeader(&#x27;Content-Type&#x27;, &#x27;application/x-www-form-urlencoded&#x27;); xhr.send(参数列表); //发送请求参数 11.2 Jquery中的AJAX $.ajax(&#123;url,[settings]&#125;) 1234567891011$.ajax(&#123; url:&quot;请求的网址&quot;, type:&quot;请求方式GET/POST...&quot;, async:&quot;请求是否异步, 默认true&quot;, data:&quot;请求的参数列表, 格式与GET请求?后的格式一致&quot;, dataType:&quot;TEXT或JSON&quot;,//服务器返回的数据类型 success:function(data)&#123;//当服务器响应状态码在200-299之间时, 这里执行 //参数data:就是响应的内容, 当dataType为TEXT时, 类型为string , 当dataType为JSON时, 类型为Object &#125;, error:function()&#123;&#125; //当服务器响应状态码不再200-299之间时, 这里执行&#125;); $.get(url, [data], [callback], [type]) 123$.get(&quot;请求的网址&quot;, &#123; 请求参数键值对 &#125;,function(data)&#123; //data:响应的内容&#125;); $.post(url, [data], [callback], [type]) 123$.post(&quot;请求的网址&quot;, &#123; 请求参数键值对 &#125;,function(data)&#123; //data:响应的内容&#125;, &quot;json&quot;); $.getJSON(url, [data], [callback]) 123$.getJSON(&quot;请求的网址&quot;, &#123; 请求参数键值对 &#125;,function(data)&#123; //data:响应的内容&#125;); jquery对象.load(url, [data], [callback]) 载入远程 HTML 文件代码并插入至 DOM 中，load函数是使用jquery对象来调用.返回的结果无需解析, 直接显示到调用函数的jquery对象中。 123$(&quot;#dom&quot;).load(&quot;请求的网址&quot;, &#123; 请求参数键值对 &#125;,function()&#123; //加载成功&#125;); 11.3 Vue中的AJAX 使用vue的ajax , 除了需要引入vue.js以外, 还需要引入vue-resource.js 不创建Vue对象的情况下, 使用的ajax: Vue.http.get(&quot;请求地址&quot;,[&quot;请求的参数&quot;]).then(success,error); Vue.http.post(&quot;请求地址&quot;,[&quot;请求的参数&quot;],&#123;&quot;emulateJSON&quot;:true&#125;).then(success,error); 创建Vue实例, 使用ajax this.$http.get(&quot;请求地址&quot;,[&quot;请求的参数&quot;]).then(success,error); this.$http.post(&quot;请求地址&quot;,[&quot;请求的参数&quot;],&#123;&quot;emulateJSON&quot;:true&#125;).then(success,error); 1234567891011121314//GET请求: 传递参数列表: &#123; params:&#123; 参数名1:值1, 参数名2:值2 ... &#125; &#125;POST请求: 传递参数列表:&#123; 参数名1:值1, 参数名2:值2 ...&#125; success函数 与 error函数 格式: function(res){} //res , 就是响应对象, 包含了响应的相关信息 响应对象的常用属性: url : 响应的网址 body : 响应的内容 (响应体) , 如果是JSON格式, 则返回对象, 否则返回string ok : boolean值, 响应码在200-299之间时 为 true status : 响应码, 例如: 200,302,404,500 statusText :响应码对应的文字信息, 例如: 状态码为200时, 信息为ok 响应对象的常用函数: text() : 以字符串的形式, 返回响应体 json() : 以对象的形式, 返回响应体 blob() : 以二进制的形式 , 返回响应体. 11.4 AJAX缓存问题 浏览器ajax得到响应结果后, 会缓存起来，当再次访问相同地址时, 会优先使用缓存。 缓存的原理, 是按照网址来缓存的, 我们只要让我们每次请求的网址都不一样, 就可以避免缓存出现。 在请求地址加上随机参数可以比避免缓存，如:&quot;s1.do?time=&quot;+new Date().getTime(); 11.5 AJAX跨域问题 默认编写的Servlet . 不允许其他网站的ajax跨域请求. 我们只需要给servlet的响应头中加入两个键值 , 就可以允许跨域: response.addHeader(&quot;Access-Control-allow-Origin&quot;,&quot;*&quot;); response.addHeader(&quot;Access-Control-allow-Methods&quot;,&quot;GET,POST&quot;);","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】数据结构与算法入门","date":"2017-06-05T10:12:38.000Z","path":"article/20170605.html","text":"数据结构，它是储存数据的一种结构体，在此结构中储存一些数据，而这些数据之间有一定的关系。算法（Algorithm）是对特定问题求解步骤的一种描述，它是指令的有限序列，其中每一条指令表示一个或者多个操作。 Java数据结构 时间复杂度与空间复杂度 算法的基本概念 1.Java数据结构(Data Structure)1.1 数据结构数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。而各数据元素之间的相互关系，又包括三个组成成分，数据的逻辑结构，数据的存储结构和数据运算结构。而一个数据结构的设计过程分成抽象层、数据结构层和实现层。 1.2 Java数据结构 数据结构在Java的语言体系中按数据的逻辑结构可以分为两大类：线性数据结构和非线性数据结构。 线性数据结构：常见的有：一维数组，线性表，栈，队列，双队列，串。 非线性数据结构：常见的有：多维数组，集合，树，图，散列表(hash)。 按数据的存储结构分为：顺序存储结构和链式存储结构 顺序存储结构:用数据元素在存储器中的相对位置来表示数据元素之间的逻辑关系。 链式存储结构：在每一个数据元素中增加一个存放地址的指针，用此指针来表示数据元素之间的逻辑关系。 1.3 线性数据结构常见的线性数据结构有：一维数组，线性表，栈，队列，双队列，串。 1.3.1 一维数组数组是根据下标进行操作的，insert根据下标插入到具体位置，它后面的元素都往后面移动一位。 插入/更新/删除效率比较低，而查询效率非常高; 查询效率时间复杂度是1。 Java中: String [],int [],ArrayList,Vector,CopyOnWriteArrayList等 1.3.2 线性表线性表是有序的储存结构、链式的储存结构。链表的物理储存空间是不连续的，链表的每一个节点都知道上一个节点、或者下一个节点是谁，通常用Node表示。常见的方法有：add(index,element),addFirst(element),addLast(element)。getFirst(),getLast(),get(element)等。 插入效率比较高，插入的时候只需要改变节点的前后节点的连接即可; 而查询效率就比较低。 Java中: LinkedList，LinkedMap等(两个JDK底层也做了N多优化) 1.3.3 栈Stack栈, 最主要的是要实现先进后出，后进先出的逻辑结构。来保证一些场景对逻辑顺序的要求。常用的方法有push(element)压栈，pop()出栈。 Java中: java.util.Stack, Jvm里面的线程栈 1.3.4 队列队列是一种特殊的线性数据结构，队列只能允许在队头，队尾进行添加和查询等相关操作。队列又有单项有序队列，双向队列，阻塞队列等。基本操作方法有：add(E e)加入队列，remove(),poll()等方法。 Java中: 线程池，MQ，连接池等。 1.3.5 串串：也称字符串，是由N个字符组成的优先序列。 在Java里面就是指String, 而String里面是由chat[]来进行储存(KMP算法)。 KMP算法: 一种 字符串的查找匹配算法 算法关键点: 在字符串比对的时候，主串的比较位置不需要回退(利用之前判断过信息，通过一个next数组，保存模式串中前后最长公共子序列的长度，每次回溯时，通过next数组找到，前面匹配过的位置，省去了大量的计算时间) 1.4 非线性数据结构常见的线性数据结构有：多维数组，集合，树，图，散列表(hash)。 1.4.1 多维数组多维数组无非就是String [][],int[][]等 Java里面很少提供这样的工具类，而java里面tree和图底层的native方法用了多维数组来储存。 1.4.2 集合 由一个或多个确定的元素所构成的整体叫做集合。在Java里面可以去广义的去理解为实现了Collection接口的类都叫集合。 1.4.3 树 树的特点: 有且仅有一个根节点; 其他结点有且只有一个直接父节点; 可以有任意多个直接子节点 树的数据结构又分为： 自由树/普通树：对子节点没有任何约束。 二叉树：每个节点最多含有两个子节点的树称为二叉树。 一般二叉树, 完全二叉树, 满二叉树 二叉搜索树/BST：binary search tree,又称二叉排序树、二叉查找树。是有序的。 二叉平衡树，AVL树，红黑树 B-tree：又称B树、B-树, 又叫平衡(balance)多路查找树, 每个节点存储M/2到M个关键字，所有关键字在整颗树中出现，且只出现一次，非叶子节点可以命中； B+tree：又称B+树, 在B树基础上，为叶子节点增加链表指针，所有关键字都在叶子节点中出现(有序)，叶子节点才命中； Btree：又称B树, 在B+树基础上，为非叶子节点也增加兄弟链表指针，将节点的最低利用率从1/2提高到2/3； 红黑树的5条性质： 每个结点要么是红的，要么是黑的。 根结点是黑的。 每个叶结点（叶结点即指树尾端NIL指针或NULL结点）是黑的。 如果一个结点是红的，那么它的俩个儿子都是黑的。 对于任一结点而言，其到叶结点树尾端NIL指针的每一条路径都包含相同数目的黑结点。 B+树的三个特点： 关键字数和子树相同 在 B+ 树中，节点的关键字代表子树的最大值，因此关键字数等于子树数。 非叶子节点仅用作索引，它的关键字和子节点有重复元素 除叶子节点外的所有节点的关键字，都在它的下一级子树中同样存在，最后所有数据都存储在叶子节点中。 根节点的最大关键字其实就表示整个 B+ 树的最大元素。 叶子节点用指针连在一起 叶子节点包含了全部的数据，并且按顺序排列，B+ 树使用一个链表将它们排列起来，这样在查询时效率更快。 1.4.4 Hash Hash，一般翻译做“散列”，也有直接音译为“哈希”的，就是把任意长度的输入（又叫做预映射， pre-image），变换成固定长度的输出，该输出就是散列值。一般通过Hash算法实现。（如：MD5,SHA1,加解密算法等） 简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。 Java中的hashCode： 默认情况就是native方法通过对象的内存的+对象的值然后通过hash散列算法计算出来个int的数字。最大的特性是：不同的对象，不同的值有可能计算出来的hashCode可能是一样的。 Hash表： Hash表综合了数组和链表两种数据结构。如：HashTable,HashMap。哈希表具有较快（常量级）的查询速度，及相对较快的增删速度，所以很适合在海量数据的环境中使用。一般实现哈希表的方法采用“拉链法”，我们可以理解为“链表的数组”。 需要注意的是，相同的内容算出来的hash一定是一样的。既：幂等性。 1.4.4.5 图 图状结构或网状结构：结构中的数据元素之间存在多对多的关系。 2. 时间复杂度与空间复杂度理解了Java数据结构，还必须要掌握一些常见的基本算法。 理解算法之前必须要先理解的几个算法的概念： 空间复杂度：一句来理解就是，此算法在规模为n的情况下额外消耗的储存空间。 时间复杂度：一句来理解就是，此算法在规模为n的情况下，一个算法中的语句执行次数称为语句频度或时间频度。 稳定性：主要是来描述算法，每次执行完，得到的结果都是一样的，但是可以不同的顺序输入，可能消耗的时间复杂度和空间复杂度不一样。 2.1 时间复杂度一个算法花费的时间与算法中语句的执行次数成正比例，哪个算法中语句执行次数多，它花费时间就多。一个算法中的语句执行次数称为语句频度或时间频度。记为T(n) 在刚才提到的时间频度中，n称为问题的规模，当n不断变化时，时间频度T(n)也会不断变化。但有时我们想知道它变化时呈现什么规律。为此，我们引入时间复杂度概念。 一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n),使得当n趋近于无穷大时，T(n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数。记作T(n)=O(f(n)),称O(f(n)) 为算法的渐进时间复杂度，简称时间复杂度。 有时候，算法中基本操作重复执行的次数还随问题的输入数据集不同而不同，如在冒泡排序中，输入数据有序而无序，其结果是不一样的。此时，我们计算平均值。 常见的算法的时间 复杂度之间的关系为：O(1)&lt;O(logn)&lt;O(n)&lt;O(nlog n)&lt;O(n2)&lt;O(2n)&lt;O(n!)&lt;O(nn) 2.2 空间复杂度空间复杂度：算法所需存储空间的度量，记作：S(n)=O( f(n) )，其中 n 为问题的规模。 一个算法在计算机存储器上所占用的存储空间，包括存储算法本身所占用的存储空间，算法的输入输出数据所占用的存储空间和算法在运行过程中临时占用的存储空间这三个方面。如果额外空间相对于输入数据量来说是个常数，则称此算法是原地工作。 算法的输入输出数据所占用的存储空间是由要解决的问题决定的，是通过参数表由调用函数传递而来的，它不随本算法的不同而改变。存储算法本身所占用的存储空间与算法书写的长短成正比，要压缩这方面的存储空间，就必须编写出较短的算法。 3.算法的基本概念 算法: 简单来说就是解决问题的步骤。 算法的五个特征:有穷性，确定性，可行性，有输入，有输出 有穷性：对于任意一组合法输入值，在执行又穷步骤之后一定能结束，即：算法中的每个步骤都能在有限时间内完成。 确定性：在每种情况下所应执行的操作，在算法中都有确切的规定，使算法的执行者或阅读者都能明确其含义及如何执行。并且在任何条件下，算法都只有一条执行路径。 可行性：算法中的所有操作都必须足够基本，都可以通过已经实现的基本操作运算有限次实现之。 有输入：作为算法加工对象的量值，通常体现在算法当中的一组变量。有些输入量需要在算法执行的过程中输入，而有的算法表面上可以没有输入，实际上已被嵌入算法之中。 有输出：它是一组与“输入”有确定关系的量值，是算法进行信息加工后得到的结果，这种确定关系即为算法功能。 算法的设计原则：正确性，可读性，健壮性，高效率与低存储量需求 描述算法的速度必须要和数据项的个数联系起来。 算法的存储量，包括： 程序本身所占空间； 输入数据所占空间； 辅助变量所占空间； 一个算法的效率越高越好，而存储量是越低越好。 4. 常用的查找算法4.1 线性（顺序）查找算法 使用目标元素与样本数列中第一个元素起依次进行比较 若目标元素等于样本元素，则表示查找成功 若目标元素与样本元素比较完毕也不相等，则表示查找失败 4.2 二分查找算法二分查找又称折半查找，优点是比较次数少，查找速度快，平均性能好，占用系统内存较少；其缺点是要求待查表为有序表，且插入删除困难。 普通循环实现二分查找算法 12345678910111213141516171819202122232425262728public static void main(String[] args) &#123; int srcArray[] = &#123;3,5,11,17,21,23,28,30,32,50,64,78,81,95,101&#125;; System.out.println(binSearch(srcArray, 28));&#125;/** * 二分查找普通循环实现 * * @param srcArray 有序数组 * @param key 查找元素 * @return */public static int binSearch(int srcArray[], int key) &#123; int mid = srcArray.length / 2; if (key == srcArray[mid]) return mid; int start = 0; int end = srcArray.length - 1; while (start &lt;= end) &#123; mid = (end - start) / 2 + start; if (key &lt; srcArray[mid]) &#123; end = mid - 1; &#125; else if (key &gt; srcArray[mid]) &#123; start = mid + 1; &#125; else &#123; return mid; &#125; &#125; return -1;&#125; 二分查找算法如果没有用到递归方法的话，只会影响CPU。对内存模型来说影响不大。时间复杂度log2n，2的开方。空间复杂度是2。一定要牢记这个算法。应用的地方也是非常广泛，平衡树里面大量采用。 递归实现二分查找递归实现算法 123456789101112131415161718192021222324252627public static void main(String[] args) &#123; int srcArray[] = &#123;3,5,11,17,21,23,28,30,32,50,64,78,81,95,101&#125;; System.out.println(binSearch(srcArray, 0,15,28));&#125;/** * 二分查找递归实现 * * @param srcArray 有序数组 * @param start 数组低地址下标 * @param end 数组高地址下标 * @param key 查找元素 * @return 查找元素不存在返回-1 */public static int binSearch(int srcArray[], int start, int end, int key) &#123; int mid = (end - start) / 2 + start; if (srcArray[mid] == key) &#123; return mid; &#125; if (start &gt;= end) &#123; return -1; &#125; else if (key &gt; srcArray[mid]) &#123; return binSearch(srcArray, mid + 1, end, key); &#125; else if (key &lt; srcArray[mid]) &#123; return binSearch(srcArray, start, mid - 1, key); &#125; return -1;&#125; 递归不光影响的CPU。JVM里面的线程栈空间也会变大。所以当递归的调用链长的时候需要-Xss设置线程栈的大小。 4. 常用的排序算法 八大排序算法 一、直接插入排序（Insertion Sort） 二、希尔排序（Shell Sort） 三、选择排序（Selection Sort） 四、堆排序（Heap Sort） 五、冒泡排序（Bubble Sort） 六、快速排序（Quick Sort） 七、归并排序（Merging Sort） 八、基数排序（Radix Sort） 4.1 冒泡排序算法冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述： 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤①~③，直到没有任何一对数字需要比较。 代码实现： 123456789101112131415public static void bubbleSort(int[] arr)&#123; for (int i=1; i&lt;arr.length; i++)&#123; boolean flag = true;//声明标志位 for(int j=0; j&lt;arr.length-i; j++)&#123; if(arr[j] &gt; arr[j+1])&#123; int temp = arr[j+1]; arr[j] = arr[j+1]; arr[j++1] = temp; flag = false; &#125; &#125; //若此轮结束flag还是为true,则证明已经有序 if(flag) break; &#125;&#125; 冒泡排序算法复杂度: 平均时间复杂度O(n²)，最好情况O(n)，最坏情况O(n²)，空间复杂度O(1) 冒泡排序是最容易实现的排序, 最坏的情况是每次都需要交换, 共需遍历并交换将近n²/2次, 时间复杂度为O(n²). 最佳的情况是内循环遍历一次后发现排序是对的, 因此退出循环, 时间复杂度为O(n). 平均来讲, 时间复杂度为O(n²). 由于冒泡排序中只有缓存的temp变量需要内存空间, 因此空间复杂度为常量O(1). Tips:由于冒泡排序只在相邻元素大小不符合要求时才调换他们的位置, 它并不改变相同元素之间的相对顺序, 因此它是稳定的排序算法。","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】常用设计模式","date":"2017-05-30T00:34:55.000Z","path":"article/20170530.html","text":"设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性。 常用的设计原则 设计模式分类 单例模式 模板模式 工厂模式 适配器模式 1.常用的设计原则 开闭原则：对扩展开发，对修改方便 里氏代换原则：任何父类出现的的地方，子类一定可以出现（多使用继承和多态） 依赖倒转原则：尽量多依赖于抽象类或接口而不是具体实现类，对子类具有强制性和规范性 接口隔离原则：尽量多依赖小接口而不是大接口 迪米特法则（最少知道原则）：一个实体应当少与其他实体之间发生相互作用，使系统功能模块相对独立。高内聚，低耦合。 合成复用原则：尽量多使用合成/聚合的方式，而不是继承的方式。 2.设计模式分类2.1 基本概念 设计模式是一套被反复使用多数人知晓，经过分类编目，代码设计经验的总结。 设计模式用来解决某些特定场景下的某一类问题–&gt;通用的解决方案。 设计模式可以让代码更容易被理解，确保了复用性、可靠性、可扩展性 2.2 具体分类 创建型模式：用于对象创建的过程 单例模式、工厂方法模式、抽象工厂模式、建造者模式(生成器模式)、原型模式 结构型模式：用于把类或对象通过某种形式结合在一起，构成某种复杂或合理的结构 适配器模式、装饰者模式、代理模式、外观模式、桥接模式、组合模式、享元模式(过滤器/标准模式) 行为型模式：用于解决类或对象之间的交互，更合理的优化类或对象之间的关系 责任链模式、命令模式、迭代子模式(迭代器模式)、观察者模式、中介者模式、解析器模式、状态模式、空对象模式、策略模式、模板模式、访问者模式、备忘录模式、 JEE 设计模式 数据访问对象模式 3.单例模式（Singleton）3.1 实现流程： 私有的构造方法 私有的静态的当前类的对象作为属性 共有的静态方法返回当前对象3.1 实现方式： 饿汉式：立即加载，对象启动时就加载 懒汉式：延迟加载，对象什么时候用到时才会加载 生命周期托管：单例对象交给别人处理 4.模板模式在模板模式中，父抽象类公开几个抽象方法供子类实现。在父抽象类中有另一个方法或几个方法使用抽象方法来实现业务逻辑。 eg: 对于使用不同的软件，我们只需要从抽象类继承并提供详细的实现,模板模式是一种行为模式。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 // 抽象类abstract class Software &#123; abstract void initialize(); abstract void start(); abstract void end(); public final void play()&#123; initialize(); start(); end(); &#125;&#125; // 不同子类以不同方法实现抽象类的的方法class Browser extends Software &#123; @Override void end() &#123; System.out.println(&quot;Browser Finished!&quot;); &#125; @Override void initialize() &#123; System.out.println(&quot;Browser Initialized!.&quot;); &#125; @Override void start() &#123; System.out.println(&quot;Browser Started.&quot;); &#125;&#125;class Editor extends Software &#123; @Override void end() &#123; System.out.println(&quot;Editor Finished!&quot;); &#125; @Override void initialize() &#123; System.out.println(&quot;Editor Initialized!&quot;); &#125; @Override void start() &#123; System.out.println(&quot;Editor Started!&quot;); &#125;&#125;// 使用public class Main &#123; public static void main(String[] args) &#123; Software s1 = new Browser(); s1.play(); s1 = new Editor(); s1.play(); &#125;&#125; 4.1 模式模式优缺点： 优点 模板方法模式通过把不变的行为搬移到超类，去除了子类中的重复代码。子类实现算法的某些细节，有助于算法的扩展。通过一个父类调用子类实现的操作，通过子类扩展增加新的行为，符合“开放-封闭原则”。 缺点 每个不同的实现都需要定义一个子类，这会导致类的个数的增加，设计更加抽象。 适用场景 在某些类的算法中，用了相同的方法，造成代码的重复。控制子类扩展，子类必须遵守算法规则。 5. 工厂模式 简单工厂模式：一个工厂方法，依据传入的参数，生成对应的产品对象； 工厂方法模式：将工厂提取成一个接口或抽象类，具体生产什么产品由子类决定； 抽象工厂模式：为创建一组相关或者是相互依赖的对象提供的一个接口，而不需要指定它们的具体类。 5.1 简单工厂模式的实现：1234567891011121314151617181920212223242526 // 产品接口public interface Fruit &#123; void whatIm(); &#125; // 具体类public class Apple implements Fruit &#123; @Override public void whatIm() &#123; /*苹果*/&#125;&#125;public class Pear implements Fruit &#123; @Override public void whatIm() &#123; /* 梨 */ &#125;&#125; // 工厂public class FruitFactory &#123; public Fruit createFruit(String type) &#123; if (type.equals(&quot;apple&quot;)) &#123;//生产苹果 return new Apple(); &#125; else if (type.equals(&quot;pear&quot;)) &#123;//生产梨 return new Pear(); &#125; return null; &#125;&#125; // 使用FruitFactory mFactory = new FruitFactory();Apple apple = (Apple) mFactory.createFruit(&quot;apple&quot;);//获得苹果Pear pear = (Pear) mFactory.createFruit(&quot;pear&quot;);//获得梨 简单工厂只适合于产品对象较少，且产品固定的需求 5.2 工厂方法模式实现：12345678910111213141516171819202122 // 工厂接口public interface FruitFactory &#123; Fruit createFruit();//生产水果&#125; // 具体工厂public class AppleFactory implements FruitFactory &#123; @Override public Fruit createFruit() &#123; return new Apple(); &#125;&#125;public class PearFactory implements FruitFactory &#123; @Override public Fruit createFruit() &#123; return new Pear(); &#125;&#125; // 使用AppleFactory appleFactory = new AppleFactory();PearFactory pearFactory = new PearFactory();Apple apple = (Apple) appleFactory.createFruit();//获得苹果Pear pear = (Pear) pearFactory.createFruit();//获得梨 工厂方法模式虽然遵循了开闭原则，但如果产品很多的话，需要创建非常多的工厂 5.3 抽象工厂模式实现： 抽象工厂和工厂方法的模式基本一样，区别在于，工厂方法是生产一个具体的产品，而抽象工厂可以用来生产一组相同，有相对关系的产品；重点在于一组，一批，一系列； eg：假如生产小米手机，小米手机有很多系列，小米note、红米note等；假如小米note生产需要的配件有825的处理器，6英寸屏幕，而红米只需要650的处理器和5寸的屏幕就可以了；用抽象工厂来实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 // cpu接口和实现类public interface Cpu &#123; void run(); class Cpu650 implements Cpu &#123; @Override public void run() &#123;/* 625 也厉害 */ &#125; &#125; class Cpu825 implements Cpu &#123; @Override public void run() &#123; /* 825 处理更强劲 */ &#125; &#125;&#125; // 屏幕接口和实现类public interface Screen &#123; void size(); class Screen5 implements Screen &#123; @Override public void size() &#123;/* 5寸 */&#125; &#125; class Screen6 implements Screen &#123; @Override public void size() &#123; /* 6寸 */ &#125; &#125;&#125; // 工厂接口public interface PhoneFactory &#123; Cpu getCpu();//使用的cpu Screen getScreen();//使用的屏幕&#125; // 具体工厂实现类public class XiaoMiFactory implements PhoneFactory &#123; @Override public Cpu getCpu() &#123; return new Cpu.Cpu825();//高性能处理器 &#125; @Override public Screen getScreen() &#123; return new Screen.Screen6();//6寸大屏 &#125;&#125;public class HongMiFactory implements PhoneFactory &#123; @Override public Cpu getCpu() &#123; return new Cpu.Cpu650();//高效处理器 &#125; @Override public Screen getScreen() &#123; return new Screen.Screen5();//小屏手机 &#125;&#125; 对于大批量，多系列的产品，用抽象工厂可以更好的管理和扩展； 5.4 三种工厂方式总结： 对于简单工厂和工厂方法来说，两者的使用方式实际上是一样的，如果对于产品的分类和名称是确定的，数量是相对固定的，推荐使用简单工厂模式； 抽象工厂用来解决相对复杂的问题，适用于一系列、大批量的对象生产； 6.适配器模式（Adapter） 适配器模式Adapter是结构型模式的一种，分为类适配器模式，对象适配器模式，缺省适配器模式。 类的适配器模式把适配的类的API转换成为目标类的API。使用对象继承的方式，是静态的定义方式； 对象的适配器模式把被适配的类的API转换成为目标类的API，与类的适配器模式不同的是，对象的适配器模式不是使用继承关系，而是使用委派关系。一个适配器可以把多种不同的源适配到同一个目标。 适配器模式的缺点过多的使用适配器，会让系统非常零乱，不易整体进行把握。比如，明明看到调用的是A接口，其实内部被适配成了B接口的实现，一个系统如果太多出现这种情况，无异于一场灾难。因此如果不是很有必要，可以不使用适配器，而是直接对系统进行重构。 6.1 缺省适配器模式 缺省适配(Default Adapter)模式为一个接口提供缺省实现，这样子类型可以从这个缺省实现进行扩展，而不必从原有接口进行扩展。作为适配器模式的一个特例，缺省是适配模式在JAVA语言中有着特殊的应用。 缺省适配模式是一种“平庸”化的适配器模式。(实现类不必实现接口所有方法或留空的方法，可以有选择性了) 适配器(通常是一个抽象类)添加某些具体实现(需要缺省的方法内部抛出异常)。","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】网络编程基础","date":"2017-05-22T03:59:46.000Z","path":"article/20170522.html","text":"网络编程是指编写运行在多个设备（计算机）的程序，这些设备都通过网络连接起来。java.net 包中的类和接口，它们提供低层次的通信细节。你可以直接使用这些类和接口，来专注于解决问题，而不用关注通信细节。 网络编程常识 基于tcp协议的编程模型 基于udp协议的编程模型 1. 网络编程常识1.1 七层网络模型为了保证数据传输的可靠和安全，ISO(国际标准委员会组织)将数据的传递从逻辑上划分为以下7层：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层 当发送数据时，需要按照上述七层模型从上到下层层加包再发送出去； 当接收数据时，需要按照上述七层模型从下到上层层拆包再显示出来； 1.2 IP地址 IP地址：是互联网中的唯一地址标识，也就是根据IP地址可以定位到具体某一台设备，IP地址本质上是32位二进制组成的整数叫做IPv4，当然也有128位二进制组成的整数叫做IPv6，目前主流的还是IPv4。 日常生活中采用点分十进制表示法进行IP地址的描述，也就是将每个字节的二进制转换为一个十进制整数，不同的十进制整数之间采用小数点隔开。如：192.168.1.1 1.3 端口号 根据IP地址可以定位到具体某一台设备，而该设备中启动的进程可能很多，此时可以使用端口号来定位该设备中的具体某一个进程。 网络编程需要提供：IP地址 和 端口号 端口号是16位二进制组成的整数，表示范围是：0 ~ 65535，其中0 ~ 1024之间通常被系统占用，因此网络编程需要从1025开始使用。 1.4 tcp协议与udp协议 TCP（Transmission Control Protocol，传输控制协议） 是面向连接的协议，也就是说，在收发数据前，必须和对方建立可靠的连接。一个TCP连接必须要经过三次“握手”才能建立起来。 UDP（User Data Protocol，用户数据报协议） 是一个非连接的协议，传输数据之前源端和终端不建立连接，当它想传送时就简单地去抓取来自应用程序的数据，并尽可能快地把它扔到网络上。在发送端，UDP传送数据的速度仅仅是受应用程序生成数据的速度、计算机的能力和传输带宽的限制；在接收端，UDP把每个消息段放在队列中，应用程序每次从队列中读一个消息段。 tcp协议与udp协议比较： tcp协议 udp协议 传输控制协议，面向连接 用户数据报协议，非面向连接 通信过程全程保持连接 通信过程不需要全程连接 保证了数据传输的可靠性和有序性 不保证数据传输的可靠性和有序性 全双工的字节流的通信方式 全双工的数据报的通信方式 服务器的资源消耗多，压力大，效率低 服务器资源消耗少，压力小，效率高 2. 基于tcp协议的编程模型2.1 编程模型1234567服务器端 客户端创建监听服务等待连接 &lt;----建立连接------ 连接服务器 进行通讯 &lt;----进行通讯-----&gt; 进行通讯关闭连接 关闭连接 服务器： 创建ServerSocket类型的对象并提供端口号； 等待客户端的连接请求，调用accept方法； 使用输入输出流进行通信； 关闭Socket； 客户端： 创建Socket类型的对象并提供服务器的通信地址和端口号； 使用输入输出流进行通信； 关闭Socket； 2.2 ServerSocket类和Socket类 java.net.ServerSocket类主要用于描述服务器套接字信息。 常用方法 ServerSocket(int port) 根据参数指定的端口号来构造对象 Socket accept() 监听并接收到此套接字的连接请求 void close() 用于关闭套接字 java.net.Socket类主要用于描述客户端套接字，是两台机器间通信的端点。 常用方法 Socket(String host, int port) 根据指定主机名和端口号来构造对象 InputStream getInputStream() 用于获取当前套接字的输入流 OutputStream getOutputStream() 用于获取当前套接字的输出流 void close() 用于关闭套接字 3.客户端与服务端通信演示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103//服务端线程public class ServerThread extends Thread &#123; private Socket s; public ServerThread(Socket s) &#123; this.s = s; &#125; @Override public void run() &#123; try &#123; // 3.使用输入输出流进行通信 BufferedReader br = new BufferedReader( new InputStreamReader(s.getInputStream())); PrintStream ps = new PrintStream(s.getOutputStream()); while(true) &#123; // 实现服务器接收到字符串内容后打印出来 // 当客户端没有发送数据时，服务器会在这里阻塞 String str = br.readLine(); //System.out.println(&quot;服务器接收到的数据是：&quot; + str); // 当服务器接收到&quot;bye&quot;后，则聊天结束 if(&quot;bye&quot;.equalsIgnoreCase(str)) &#123; System.out.println(&quot;客户端&quot; + s.getInetAddress() + &quot;已下线！&quot;); break; &#125; System.out.println(&quot;客户端&quot; + s.getInetAddress() + &quot;发来的消息是：&quot; + str); // 当服务器接收到客户端发来的消息后，向客户端回发消息&quot;I received!&quot; ps.println(&quot;I received!&quot;); //System.out.println(&quot;服务器发送数据成功！&quot;); &#125; // 4.关闭Socket ps.close(); br.close(); s.close(); &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;//服务端测试public class ServerStringTest &#123; public static void main(String[] args) &#123; try &#123; // 1.创建ServerSocket类型的对象并提供端口号 ServerSocket ss = new ServerSocket(8888); // 2.等待客户端的连接请求，调用accept方法 while(true) &#123; System.out.println(&quot;等待客户端的连接请求...&quot;); // 当没有客户端连接时，阻塞在accept方法的调用这里 Socket s = ss.accept(); // 获取连接成功的客户端通信地址 System.out.println(&quot;客户端&quot; + s.getInetAddress() + &quot;连接成功！&quot;); // 当有客户端连接成功后，则启动一个新的线程为之服务 new ServerThread(s).start(); &#125; //ss.close(); &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;//客户端测试public class ClientStringTest &#123; public static void main(String[] args) &#123; try &#123; // 1.创建Socket类型的对象并提供服务器的通信地址和端口号 Socket s = new Socket(&quot;XDL-20170621QCO&quot;, 8888); System.out.println(&quot;连接服务器成功！&quot;); // 2.使用输入输出流进行通信 Scanner sc = new Scanner(System.in); PrintStream ps = new PrintStream(s.getOutputStream()); BufferedReader br = new BufferedReader( new InputStreamReader(s.getInputStream())); while(true) &#123; // 希望客户端连接服务器成功后睡眠10秒再发送数据，测试服务器是否阻塞 //Thread.sleep(10000); // 练习：实现客户端向服务器发送的内容由用户从键盘输入 System.out.println(&quot;请输入要发送的内容：&quot;); //String msg = sc.next(); // 读取字符串内容时，遇到空格停止 String msg = sc.nextLine(); // 实现客户端向服务器发送字符串内容&quot;hello&quot; //ps.println(&quot;hello&quot;); ps.println(msg); System.out.println(&quot;客户端发送数据成功！&quot;); // 判断客户端发送的内容是否为&quot;bye&quot;，若是则聊天结束 if(&quot;bye&quot;.equalsIgnoreCase(msg)) &#123; System.out.println(&quot;聊天结束！&quot;); break; &#125; // 实现服务器回发消息的接收 // 当客户端没有发送数据时，服务器会在这里阻塞 String str = br.readLine(); System.out.println(&quot;客户端接收到的数据是：&quot; + str); &#125; // 3.关闭Socket br.close(); sc.close(); ps.close(); s.close(); &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 4. 基于udp协议的编程模型4.1 编程模型 主机A(接收方): 创建DatagramSocket类型的对象，并提供端口号； 创建DatagramPacket类型的对象，用于接收发来的数据； 从Socket中接收数据，调用**receive()**方法； 关闭Socket并释放有关的资源； 主机B(发送方) 创建DatagramSocket类型的对象； 创建DatagramPacket类型的对象，并提供接收方的IP地址和端口号； 通过Socket发送数据，调用**send()**方法； 关闭Socket并释放有关的资源； 4.2 DatagramSocket类 java.net.DatagramSocket类用于描述发送或接受数据报的套接字(邮局点); 常用方法 DatagramSocket() 无参的方式构造对象。 DatagramSocket(int port) 根据参数指定的端口号来构造对象。 void receive(DatagramPacket p) 用于接收数据并存放到参数指定的变量中。 void send(DatagramPacket p) 用于将参数指定的数据发送出去。 void close() 4.3 DatagramPacket类 java.net.DatagramPacket类用于描述数据报信息(信件)； 常用方法 DatagramPacket(byte[] buf, int length) 用于接收数据包并记录到参数变量中； DatagramPacket(byte[] buf, int length, InetAddress address, int port) 用于将参数指定的数据发送到参数指定的位置 InetAddress getAddress() 用于获取发送方或接收方的通信地址信息。 int getPort() 用于获取发送方或接收方的端口信息。 int getLength() 用于获取发送或接收数据的长度。 4.4 InetAddress类 java.net.InetAddress类用于描述互联网协议地址。 常用方法 static InetAddress getLocalHost() 用于获取本地主机的通信地址信息。 static InetAddress getByName(String host) 根据参数指定的主机名来获取通信地址。 String getHostName() 用于获取通信地址中的主机名信息。 String getHostAddress() 用于获取通信地址中的IP地址信息。","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】反射机制","date":"2017-05-18T07:47:15.000Z","path":"article/20170518.html","text":"反射(Reflection)是Java 程序开发语言的特征之一，它允许运行中的 Java 程序获取自身的信息，并且可以操作类或对象的内部属性。多数情况下反射是为了提高程序的灵活性，运行时动态加载需要加载的对象。 基本概念 Class类 Constructor类 Field类 Method类 原始方式与反射方式构造对象实例 注解(Annotation) 1. 基本概念JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为java语言的反射机制。 反射（reflect）就是把java类中的各种成分映射成一个个的Java对象；类是用来描述一组对象，反射机制可以理解为是用来描述一组类 通俗来讲，反射机制就是用于动态创建对象并且动态调用方法的机制；目前主流的框架底层都采用反射机制实现的。 1.1 相关类及描述 Class：用来描述类和接口；该类没有公共构造方法，由虚拟机和类加载器自动构造完成 Package：用来描述类所属的包 Field：用来描述类中的属性 Method：用来描述类中的方法 Constructor：用来描述类中的构造方法 Annotation：用来描述类中的注解 2. Class类java.lang.Class：用来描述类和接口；该类没有公共构造方法，由虚拟机和类加载器自动构造完成 2.1 获取Class类型对象的三种方式1234Class clazz = Class.forName(&quot;包名.类名&quot;);//用的最多，但可能抛出ClassNotFoundException异常Class clazz = 类名.class;//任何类都有一个隐含的静态成员变量classClass clazz = 对象.getClass();//Object类中的方法Class clazz = 包装类.TYPE;//获取对应基本数据类型的class对象 2.2 常用方法 static Class&lt;?&gt; forName(String className) 用于获取参数指定对应的Class对象并返回 T newInstance() 默认调用无参数构造方法创建对象，若类中不存在无参数构造方法抛出异常NoSuchMethodException Constructor getConstructor(Class&lt;?&gt;… parameterTypes) 用于获取此Class对象所表示类型中参数指定的公共构造方法。 Constructor&lt;?&gt;[] getConstructors() 用于获取此Class对象所表示类型中所有的公共构造方法 Field getDeclaredField(String name) 用于获取此Class对象所表示类中参数指定的单个成员变量信息 Field[] fs = getDeclaredFields() 用于获取此Class对象所表示类中所有成员变量信息 Method getMethod(String name, Class&lt;?&gt;… parameterTypes) 用于获取该Class对象所表示类型中名字为name参数为parameterTypes的指定公共成员方法 Method[] getMethods() 用于获取该Class对象表示类中所有公共成员方法。 获取私有相关方法 getDeclaredConstructor(Class&lt;?&gt;… parameterTypes)；获取该类对象表示的类或接口的指定构造函数(包括私有) getDeclaredConstructors()；获取该类对象所表示的类声明的所有构造函数(包括私有) getDeclaredMethod(String name, Class&lt;?&gt;… parameterTypes) 获取一个方法(自己类 公有 私有) getDeclaredMethods(); 获取全部的方法(自己类 公有 私有) 2.3 其他方法 int result = getModifiers(); 获取类的修饰符(权限+特征) 每一个修饰符 用一个整数来进行表示：0–默认不写，1–public，2–private，4–protected，-static， 16–final，32–synchronized，64volatile，128–transient，256–native，512–interface，1024–abstract String name = getName(); 获取类的全名(包名.类名) String name = getSimpleName(); 获取类简单名(只有类名 缺少包) Package p = getPackage(); 获取当前类所属的包 p.getName(); 获取包名(Package类中的方法) Class sclazz = getSuperClass(); 获取超类(父类)对应Class Class[] classes = getInterface(); 获取当前类父亲接口 Class[] classes = getClasses(); 获取类中的内部类 Object obj = newInstance(); 默认调用无参数构造方法创建对象，若类中不存在无参数构造方法抛出异常NoSuchMethodException Field f = getField(“属性名”); 获取类中的属性(公有的 自己类+父类) Field[] fs = getFields(); 获取类中的全部属性(公有的 自己类+父类) getDeclaredField(“属性”); 获取当前类中的属性(公有+私有 自己类) Field[] fs = getDeclaredFields(); 获取当前类中全部的属性(公有+私有 自己类) 3. Constructor类java.lang.reflect.Constructor类主要用于描述获取到的构造方法信息 3.1 Constructor类中的常用方法 T newInstance(Object… initargs) 使用此Constructor对象描述的构造方法来构造Class对象代表类型的新实例；该方法的参数用于给新实例中的成员变量进行初始化操作。 3.2 其他方法 con.getModifiers(); con.getName(); con.getParameterTypes(); con.getExceptionTypes(); 如何操作构造方法 执行一次,创建对象 Object = newInstance(执行构造方法时的所有参数); con.setAccessible(true); 4. Field类java.lang.reflect.Field类主要用于描述获取到的单个成员变量信息。 4.1 Field类中的常用方法 Object get(Object obj) 调用该方法的意义就是获取参数对象obj中此Field对象所表示成员变量的数值。 Object set(Object obj, Object value) 将参数对象obj中此Field对象表示成员变量的数值修改为参数value的数值。 void setAccessible(boolean flag) 当实参传递true时，则反射的对象在使用时应该取消java语言访问检查 4.2 其他方法 int = getModifiers(); 获取属性修饰符(权限+特征) Class = getType(); 获取属性的类型对应的那个class String = getName(); 获取属性的名字 操作属性: set(对象,值); Object = get(对象); 如果是私有属性不能直接操作的，需设置一个使用权setAccessable(true);准入 5. Method类java.lang.reflect.Method类主要用于描述获取到的单个成员方法信息。 5.1 Method类中的常用方法 Object invoke(Object obj, Object… args) 使用对象obj来调用此Method对象所表示的成员方法，实参传递args。 5.2 其他方法 int mm = m.getModifiers(); 获取方法的修饰符(权限+特征) Class mrt = m.getReturnType(); 获取返回值数据类型 String mn = m.getName(); 获取方法的名字 Class[] mpts = m.getParameterTypes(); 获取方法参数列表的类型 Class[] mets = m.getExceptionTypes(); 获取方法抛出异常的类型 如何操作方法 调用方法 让他执行一次 Object result = invoke(对象,执行方法需要传递的所有参数…); 若方法是私有的方法 不允许操作 可以设置setAccessable(true) 设置方法使用权 准入 6. 原始方式与反射方式构造对象实例 使用原始方式来构造对象 123456789 //1.采用无参的方式构造Person对象并打印Person p = new Person();System.out.println(p); //null 0 //2.使用有参方式来构造Person对象Person p2 = new Person(&quot;zhangfei&quot;, 30);System.out.println(p2); //zhangfei 30 //3.修改与获取属性(成员变量)，调用get,set方法p2.setName(&quot;guanyu&quot;);System.out.println(&quot;修改后的姓名是：&quot; + p2.getName()); //guanyu 使用反射机制来构造对象 123456789101112131415161718192021 //1.使用获取到的Class对象来构造Person对象并打印Class c1 = Class.forName(&quot;myproject.Person&quot;);//不可省略包名System.out.println(c1.newInstance());//null 0 //2.使用有参方式来构造对象Class c2 = Class.forName(&quot;myproject.Person&quot;);Constructor ct2 = c2.getConstructor(String.class, int.class);Object obj = ct2.newInstance(&quot;zhangfei&quot;, 30);System.out.println(obj);//zhangfei 30 //3.修改与获取属性(成员变量)Field f2 = c2.getDeclaredField(&quot;name&quot;);f2.setAccessible(true);//暴力反射，设置使用权f2.set(obj, &quot;guanyu&quot;);System.out.println(&quot;修改后的姓名是：&quot; + f2.get(obj)); //guanyu //4.获取成员方法getName，使用获取到的成员方法来获取姓名并打印出来Method m1 = c2.getMethod(&quot;getName&quot;);System.out.println(&quot;获取到的姓名是：&quot; + m1.invoke(obj)); //zhangfei //5.成员方法setName，调用getMethod方法来修改姓名并打印出来Method m2 = c2.getMethod(&quot;setName&quot;, String.class);Object res = m2.invoke(obj, &quot;guanyu&quot;);System.out.println(&quot;方法调用的返回值是：&quot; + res); //nullSystem.out.println(&quot;修改后的姓名是：&quot; + m1.invoke(obj)); //guanyu 7. 注解(Annotation)7.1 注解相关概念 注释 单行注释：// 多行注释：/* */ 文档注释：/** */ 注解的写法 @XXX [(一些信息)] 注解位置 类的上面，属性上面，方法上面，构造方法上面，参数前面 注解的作用 用来充当注释的作用(仅仅是一个文字的说明)，@Deprecated 用来做代码的检测(验证)，@Override *可以携带一些信息(内容)，文件.properties/.xml，注解 常用的注解 @Deprecated：用来说明方法是废弃的 @Override：用来做代码检测 检测此方法是否是一个重写 @SuppressWarnings(String[])：{“”}，如果数组内的元素只有一个长度，可以省略{} unused：变量定义后未被使用 serial：类实现了序列化接口 不添加序列化ID号 rawtypes：集合没有定义泛型 deprecation：方法以废弃 *unchecked：出现了泛型的问题 可以不检测 all：包含了以上所有(不推荐) 注解中可以携带信息，可以不携带；信息不能随意写，信息的类型只能是如下的类型： 基本数据类型 String类型 枚举类型enum 注解类型@ 数组类型[]，数组的内部需要是如上的四种类型 注解的分类 按运行机制分：源码注解，编译时注解，运行时注解 按照来源分：来自JDK的注解，来自第三方的注解，自定义注解 7.2 自定义注解类型的语法要求： 使用@interface关键字定义注解 成员以无参无异常方式声明 可以用default为成员指定一个默认值 成员类型是受限的，合法类型包括原始类型及String,Class,Annotation,Enumeration 如果注解只有一个成员，则成员名必须取名value(),在使用时可以忽略成员名和赋值号(=) 注解类可以没有成员，没有成员的注解称为标识注解 需要元注解来描述说明 @Target：当前注解的放置(CONSTRUCTOR，FIELD，LOCAL_VARIABLE，METHOD，PACKAGE，PARAMETER，TYPE) @Retention：当前注解的生命周期作用域(SOURCE，CLASS，RUNTIME)，源代码文件(SOURCE)—&gt;编译—&gt;字节码文件(CLASS)—&gt;加载—&gt;内存执行(RUNTIME) @Inherited：允许子类继承 @Document：当前注解是否能被文档(javadoc)所记录 123456789@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface Description&#123; String desc(); String author(); int age() default 18;&#125; 7.3 使用自定义注解： @&lt;注解名&gt;(&lt;成员名1&gt;=&lt;成员值1&gt;,&lt;成员名2&gt;=&lt;成员值2&gt;,…) 1234@Description(desc=&quot;I am eyeColor&quot;, author=&quot;Chao&quot;, age=18)public String eyeColor()&#123; return &quot;red&quot;;&#125; 如果自定义注解只有一个value成员，在使用的时候就可以省略方法名，如果方法是两个以上，每一个方法必须写名字 1234567891011121314@Description(&quot;I am class annotation&quot;)public class Child implements Person&#123; @Override @Description(&quot;I am method annotation&quot;) public String name()&#123; return null; &#125; @Override public int age()&#123; return 0; &#125; @Override public void sing()&#123; &#125;&#125; 7.4 解析注解通过反射获取类、函数或成员上的运行时注解信息，从而实现动态控制程序运行的逻辑。 使用类加载器加载类 Class c=Class.forName（&quot;com.ann.test.Child&quot;) 找到类上面的注解 isAnnotationPresent（类类型）：Class对象的方法，判断当前类类型是否存在某个类类型的注解，返回类型为boolean。 拿到注解实例，需要强制类型转换。 Description d=（Description）c.getAnnotation(Description.class); 找到方法上的注解，首先，遍历所有方法，通过方法对象的isAnnotation查看是否有自定义注解。 12345678910111213141516171819202122232425public class ParseAnn&#123; public static void main(String[])&#123; try&#123;//1. 使用类加载器加载类 Class c=Class.forName（&quot;com.ann.test.Child&quot;) //2. 找到类上面的注解 boolean isExist = c.isAnnotationPresent(Description.class); if(isExist)&#123; //3. 拿到注解实例 Description d=（Description）c.getAnnotation(Description.class); System.out.println(d.value()); &#125; //4.找到方法上的注解 Method[] ms = c.getMethods(); for(Method m:ms)&#123; boolean isMExist = m.isAnnotationPresent(Description.class); if(isMExist)&#123; Description md=（Description）c.getAnnotation(Description.class); System.out.println(md.value()); &#125; &#125; &#125;catch(ClassNotFoundException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 另一种解析方法上的注解: 获取这个方法的所有注解，Annotation [] as=m.getAnnotations();然后遍历该注解，如果遍历的注解是Description类型，则把遍历的注解强转为Description类型，并进行输出value()信息。 123456789for(Method m:ms)&#123; Annotation [] as=m.getAnnotations(); for(Annotation a:as)&#123; if(a instanceof Description)&#123; Description md = (Description)a; System.out.println(md.value()); &#125; &#125;&#125; @Inherited:当自定义注解上使用了该注解，如果在父类上标识该注解，解析一个子类，子类也可以获取该注解的信息。","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】多线程","date":"2017-05-12T10:07:41.000Z","path":"article/20170512.html","text":"多线程的存在，不是提高程序的执行速度。其实是为了提高应用程序的使用率。程序的执行其实都是在抢CPU的资源，CPU的执行权。多个进程是在抢这个资源，而其中的某一个进程如果执行路径(线程)比较多，就会有更高的几率抢到CPU的执行权。 基本概念 实现线程的过程 线程常用方法 线程池 线程的主要状态 线程的同步机制 线程的死锁 内存可见性 1. 基本概念 程序：数据结构 + 算法，主要指存放在硬盘上的可执行文件。 进程：主要指运行在内存中的程序；每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含n个线程；(进程是系统进行资源分配和调度的一个独立单位)。 线程：线程是进程的一个实体，同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小；(线程是cpu调度和分派的最小单位)。 多进程是指操作系统能同时运行多个任务（程序）。 多线程是指在同一程序(一个进程)中有多个顺序流在执行。 并行与并发： 并行：多个cpu实例或者多台机器同时执行一段处理逻辑，是真正的同时。 并发：通过cpu调度算法，让用户看上去同时执行，实际上从cpu操作层面不是真正的同时。并发往往在场景中有公用的资源，那么针对这个公用的资源往往产生瓶颈，我们会用TPS或者QPS来反应这个系统的处理能力。 线程和进程一样分为五个阶段：创建、就绪状态、执行状态、等待/挂起/阻塞、终止/异常/消亡。 2. 实现线程的过程java.lang.Thread类主要用于描述线程，Java虚拟机允许应用程序并发地运行多个执行线程。 自定义类继承Thread类并重写run方法，然后创建该类的实例调用start方法。 自定义类实现Runnable接口并重写run方法，然后创建该类的对象作为实参去构造Thread类型的对象，最后使用Thread类对象调用start方法。 2.1 实现方式一：继承Thread类 自己描述一个类 继承父类Thread 重写run方法 new一个线程对象，调用start()方法，让线程进入就绪状态(需要注意的是start方法是Thread类中的) 12345678class MyThread extends Thread&#123; @Override public void run()&#123; //这里编写该线程的执行任务 &#125;&#125;MyThread mt = new MyThread();mt.start(); 2.2 实现方式二：实现Runnable接口 自己描述一个类 实现一个父接口Runnable 重写run方法 new一个线程对象，new一个Thread并传入线程对象，调用start()方法，让线程进入就绪状态 123456789class MyThread implements Runnable&#123; @Override public void run()&#123; //这里编写该线程的执行任务 &#125;&#125;MyThread mt = new MyThread();Thread td = new Thread(mt);td.start(); 2.3 两种方式优缺点： 使用继承Thread方式代码简单，但Java语言只支持单继承，若该类继承Thread类后则无法继承其他类 使用实现Runnable的方式代码复杂，但不影响该类继承其他类，并且支持多实现，适合多个相同程序代码的线程去处理同一个资源，增加程序健壮性，代码可以被多个线程共享，代码和数据独立。 3. 线程常用方法 3.1 相关方法的解析： Thread()：使用无参方式构造对象 Thread(String name)：根据参数指定的名称来构造对象。 Thread(Runnable target)：根据参数指定的Runnable引用来构造对象。 Thread(Runnable target, String name)：根据参数指定的Runnable引用和名称构造对象。 void run()：若使用Runnable对象作为参数构造的对象来调用该方法，则最终调用Runnable对象中的run方法，否则该方法啥也不做。 void start()**：用于启动线程**，除了主方法线程外新启动一个线程同时执行，Java虚拟机会自动调用该线程的run方法。 int getPriority()：用于获取线程的优先级，优先级1-10 void setPriority(int)：更改线程的优先级 3.2 多线程原理分析 执行main方法的线程叫做主线程，而执行run方法的线程叫做子线程。 对于start方法之前的代码来说，由主线程执行一次，当start方法调用成功之后，线程的个数由1个变成了2个，主线程继续向下执行，而新启动的线程去执行run方法的代码，两个线程各自独立运行。 当run方法执行完毕后，则子线程结束；当main方法执行完毕后，则主线程结束。 两个线程执行的先后次序没有明确的规定，由系统的调度算法决定。 3.3 线程的编号和名称 long getId()：用于获取调用对象所表示线程的编号 String getName()：用于获取调用对象所表示线程的名称 void setName()：用于设置线程的名称为参数指定的数值 static Thread currentThread()：获取当前正在执行线程的引用 4. 线程池 为了避免重复的创建线程，线程池的出现可以让线程进行复用。通俗点讲，当有工作来，就会向线程池拿一个线程，当工作完成后，并不是直接关闭线程，而是将这个线程归还给线程池供其他任务使用。 在线程池的编程模式下，任务是提交给整个线程池，而不是直接交给某个线程，线程池在拿到任务后，它就在内部找有无空闲线程，再把任务交给内部某个空闲线程。 一个线程同时只能执行一个任务，但可以同时向一个线程池提交多个任务 接口：Executor,CompletionService,ExecutorService，ScheduledExecutorService 抽象类：AbstractExecutorService 实现类：ExecutorCompletionService，ThreadPoolExecutor，ScheduledThreadPoolExecutor 创建线程的第三种方式是实现Callable接口，主要用于线程池 5. 线程的主要状态 新建状态：使用new关键字创建线程后进入状态，此时线程还没有开始执行 就绪状态：调用start()进入的状态，此时线程还是没有开始执行 运行状态：使用线程调度器调用该线程后进入的状态(获得CPU执行权)，此时线程开始执行，当线程的时间片执行完毕后若没有完成就回到就绪状态，若任务完成进入消亡状态 消亡状态：当线程的任务执行完成之后进入的状态，此时线程已经终止 阻塞状态：当线程执行过程中发生了阻塞事件进入的状态，阻塞解除后再回到就绪状态 5.1 线程的休眠 终止线程：通常使用退出标识，使线程正常退出，也就是当 run() 方法完成后线程终止。 static void **yield()**：当线程让出处理器(离开Running状态)，使用当前线程进入Runnable状态等待。 static void **sleep(times)**：使当前线程从Running放弃处理器进入Block状态，休眠times毫秒，再返回到Runnable如果其他线程打断当前线程的Block(sleep)，就会发生InterruptException。 5.1 线程的等待 void **join()**：等待该线程终止，让多个线程同步执行，变成单个线程 void **join(long millis)**：表示等待参数指定的毫秒数 对象.wait() 和 **对象.notify()/notifyAll()**可以让线程的状态来回切换 sleep()和wait()的区别： sleep()和wait()的区别 sleep() wait() 1.类 Thread类 Object类 2.调用 静态 类名. 对象. 3.理解 调用位置的线程等待 对象调用，访问对象的其他线程等待 4.唤醒 不需要唤醒 需要其他对象调用notify唤醒 5.锁 不会释放锁 等待后会释放锁 5.2 守护线程 boolean **isDeamon()**：用于判断是否为守护线程 void **setDeamon(boolean on)**：用于设置线程为守护线程 Java线程有两类： 用户线程：运行在前台，执行具体任务；程序的主线程、连接网络的子线程等都是用户线程 守护线程：运行在后台，为其他前台线程服务 守护线程特点： 一旦所有线程都结束运行，守护线程会随JVM一起结束工作 守护线程应用： 数据库连接池中检测的线路，JVM虚拟机启动后的监测线程；最常见的是垃圾回收线程。 设置守护线程： 可以通过调用Thread类的setDeamon(true)方法来设置当前的线程为守护线程 6. 线程的同步机制 条件争用：当多个线程同时共享访问同一数据时，每个线程都尝试操作该数据，从而导致数据被破坏(corrupted)，这种现象称为争用条件。 当多个线程同时访问同一种共享资源时，可能会造成数据的覆盖等不一致性问题，此时就需要对多个线程之间进行通信和协调，该机制就叫做线程的同步机制。 Java提供了一种内置的锁机制来支持原子性，使用synchronized关键字来保证线程执行操作的原子性，叫做对象/同步锁机制。 特征修饰符synchronized：表示同步，一个时间点只有一个线程访问 线程安全锁：两种形式是（锁定的永远是对象） 使用同步代码块的方式，将synchronized关键字放在方法体内部123synchronized(对象)&#123; //需同步执行(锁定)的代码&#125; 使用同步方法的方式处理，直接使用synchronized关键字修饰整个方法，锁定的是调用方法的那个对象1public synchronized void 方法名()&#123;&#125; 使用synchronized保证线程同步时应当注意： 多个需要同步的线程在访问该同步块时，看到的应该时同一个锁对象引用 在使用同步块时应当尽量减少同步范围以提高并发的执行效率 无论synchronized关键字加在方法上还是对象上，它取得的锁都是对象，而不是把一段代码或函数当作锁――而且同步方法很可能还会被其他线程的对象访问。 每个对象只有一个锁（lock）与之相关联。 实现同步是要很大的系统开销作为代价的，甚至可能造成死锁，所以尽量避免无谓的同步控制。 7. 线程的死锁Java线程死锁是一个经典的多线程问题，因为不同的线程都在等待那些根本不可能被释放的锁，从而导致所有的工作都无法完成。 123456789101112131415161718/**当两个线程或多个线程之间相互锁定时就形成了死锁**///线程一：public void run() &#123; synchronized(a) &#123; //表示:持有对象锁a,等待对象锁b synchronized(b) &#123; //... &#125; &#125;&#125;//线程二：public void run() &#123; synchronized(b) &#123; //表示:持有对象锁b,等待对象锁a synchronized(a) &#123; //... &#125; &#125;&#125;// 注意：在以后的开发中尽量不要使用同步代码块的嵌套结构。 产生死锁的必要条件：a.互斥条件、b.不可抢占条件、c.占有且申请条件、d.循环等待条件。 隐性死锁：隐性死锁由于不规范的编程方式引起，但不一定每次测试运行时都会出现程序死锁的情形。由于这个原因，一些隐性死锁可能要到应用正式发布之后才会被发现，因此它的危害性比普通死锁更大。 两种导致隐性死锁的情况：加锁次序和占有并等待。 加锁次序：当多个并发的线程分别试图同时占有两个锁时，会出现加锁次序冲突的情形。如果一个线程占有了另一个线程必需的锁，就有可能出现死锁。 占有并等待：如果一个线程获得了一个锁之后还要等待来自另一个线程的通知，可能出现另一种隐性死锁。 7.1 死锁的避免 避免死锁的原则：顺序上锁，反向解锁，不要回头 静态策略：使产生死锁的四个必要条件不能同时具备，从而对进程申请资源的活动加以限制，以保证死锁不会发生。 动态策略：不限制进程有关申请资源的命令，而是对进程所发出的每一个申请资源命令加以动态地检查，并根据检查结果决定是否进行资源分配。具体策略有：安全序列和银行家算法。 8.内存可见性8.1 基本概念 可见性：一个线程对共享变量值的修改，能够及时的被其他线程看到 共享变量：如果一个变量在多个线程的工作内存中都存在副本，那么这个变量就是这几个线程的共享变量 Java内存模型(JMM)： Java Memory Model描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量存储到内存中和从内存中读取变量这样的底层细节。 所有的变量都存储在主内存中 每个线程都有自己的独立的工作内存，里面保存该线程使用到的变量的副本(来自主内存的拷贝) Java内存模型规定： 线程对共享变量的所有操作都必须在自己的工作内存中进行，不能直接从主内存中读写。 不同线程之间无法直接访问其他线程工作内存中的变量，线程间变量值的传递需要通过主内存来完成。 要实现共享变量的可见性，必须保证两点： 线程修改后的共享变量值能够及时从工作内存中刷新到主内存中 其他线程能够及时把共享变量的最新值从主内存更新到自己的工作内存中。 Java语言层面支持的可见性实现方式：Synchronized，volatile 8.2 Synchronized实现可见性 Synchronized能够实现：原子性(同步)、可见性 JMM关于synchronized的两条规定： 线程解锁前，必须把共享变量的最新值刷新到主内存中 线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从内存中重新读取最新的值（注意：加锁与解锁需要是同一把锁） 线程执行互斥代码的过程： 获得互斥锁 清空工作内存 从主内存拷贝变量的最新副本到工作内存 执行代码 将更改后的共享变量的值刷新到主内存 释放互斥锁 重排序：代码的书写顺序与实际的执行顺序不同，指令重排序是编译器或处理器为了性能而做的优化 编译器优化重排序（编译器处理） 指令级并行重排序（处理器优化） 内存系统的重排序（处理器读写缓存的优化） as-is-serial:无论如何重排序，程序执行的结果应该与代码的顺序执行结果一致 单线程中重排序不会带来内存可见性问题 多线程中程序交错执行时，重排序可能造成内存可见性问题 不可见的原因 syschronized解决方案 1.线程的交叉执行 原子性 2.重排序结合线程交叉执行 原子性 3.共享变量未及时更新 可见性 8.3 volatile实现可见性 深入来说：通过加入内存屏障和禁止重排序优化来实现的。 对volatile变量执行写操作时，会在写操作后加入一条store屏蔽指令 对volatile变量执行读操作时，会在读操作前加入一条load屏蔽指令 通俗地讲：volatile变量在每次被线程访问时，都强迫从主内存中重读该变量的值，而当该变量发生变化时，又会强迫线程将最新的值刷新到主内存。这样任何时刻，不同的线程总能看到该变量的最新值。 线程写volatile变量的过程： 改变线程工作内存中volatile变量副本的值 将改变后的副本的值从工作内存刷新到主内存 线程读volatile变量的过程： 从主内存中读取volatile变量的最新值到线程的工作内存中 从工作内存中读取volatile变量的副本 volatile不能保证volatile变量复合操作的原子性 volatile适用场景： 对变量的写操作不依赖其当前值 该变量没有包含在具有其他变量的不变式中 8.4 Synchronized和volatile比较 volatile不需要加锁，比synchronized更轻量级，不会阻塞线程； 从内存可见性角度讲，volatile读相当于加锁，volatile写相当于解锁 synchronized既能保证可见性，又能保证原子性，而volatile只能保证可见性，无法保证原子性 volatile没有synchronized使用广泛。","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】IO机制","date":"2017-05-05T09:31:22.000Z","path":"article/20170505.html","text":"输入输出（I/O）是指程序与外部设备或其他计算机进行交互的操作。几乎所有的程序都具有输入与输出操作，Java把这些输入与输出操作用流来实现，通过统一的接口来表示，从而使程序设计更为简单。 File类 IO流 文件流 字节型文件流 字符型文件流 *缓冲流 转换流 对象流 打印流(PrintStream类) Properties类的使用 1. File类 File与真实硬盘中的文件或文件夹 不是一个东西 File是在内存中的一个对象&lt;—映射—&gt;硬盘上的文件或文件夹 java.io.File类用于文件或目录信息(名称、大小等)的抽象表示方式，不能对文件内容进行访问。 File类中的常用的方法 canRead()，canWrite()，isHidden()，isFile()，isDirectory() length()，获取文件中字节的个数 lastModified()，获取文件最后的修改时间—&gt;毫秒值 *String path = getAbsolutePath()，获取文件的绝对路径 D://test//Test.txt 绝对路径&lt;—-&gt;相对路径 绝对路径可以通过完整的字符串，定位盘符，文件夹，文件 相对路径没有盘符的写法，当前工程(项目)所在的位置找寻 String name = getName()，获取文件的名字 Test.txt *boolean = **createNewFile()**，创建新的文件 *boolean = mkdir ，创建新的文件夹 外层没有 不能创建 *boolean = mkdirs，创建新的文件夹 外层没有 可以自动创建 String pname = getParent()，获取当前file的父亲file名字 *File file = getParentFile()，获取当前file的父亲file对象 String[] names = list()，获取当前file的所有儿子名字 *File[] files = listFiles()，获取当前file的所有儿子对象 *boolean = delete()，删除文件或空的文件夹 不能删除带元素的文件夹 文件夹的路径(找父目录) 1234567//查找当前file的所有父目录File file = new File(&quot;D:\\\\test\\\\bbb\\\\inner\\\\InnerTest.txt&quot;);File pfile = file.getParentFile();while(pfile!=null)&#123; System.out.println(pfile.getAbsolutePath()); pfile = pfile.getParentFile();//再找一遍&#125; 文件夹的遍历—-需要一个递归 123456789101112131415//设计一个方法 用来展示(遍历)文件夹,参数--&gt;file(代表文件或文件夹)public void showFile(File file)&#123; //获取file的子元素 //files==null是个文件 //files!=null是个文件夹 //files.length!=0是一个带元素的文件夹 File[] files = file.listFiles();//test文件夹所有子元素 if(files!=null &amp;&amp; files.length!=0)&#123; for(File f:files)&#123; this.showFile(f); &#125; &#125; //做自己的显示(file是文件或file是一个空的文件夹) System.out.println(file.getAbsolutePath());&#125; 文件夹的删除—-需要一个递归 123456789101112//设计一个方法 删除文件夹,参数 filepublic void deleteFile(File file)&#123; //判断file不是空文件夹 File[] files = file.listFiles(); if(files!=null &amp;&amp; files.length!=0)&#123; for(File f:files)&#123; this.deleteFile(f); &#125; &#125; //删除file (file是个文件或file是一个空文件夹) file.delete();&#125; 2. IO流 流的本质是数据传输，根据数据传输特性将流抽象为各种类，方便更直观的进行数据操作。 流的分类: 根据处理数据类型的不同分为：字符流和字节流 根据数据流向不同分为：输入流in(读取)和输出流out(写入) 操作的目标来区分: 文件流，数组流，字符串流，数据流，对象流，网络流… IO流的框架结构 123456789101112131415161718|——IO流 |————字节流 |————InputStream |————FileInputStream |————DataInputStream |————ObjectInputStream |————OutputStream |————FileOutputStream |————DataOutputStream |————ObjectOutputStream |————PrintStream |————字符流 |————Reader |————BufferedReader |————InputStreamReader |————Writer |————BufferedWriter |————OutputStreamWriter 3. 文件流读取文件中的信息in，将信息写入文件中out；文件流按照读取或写入的单位(字节数)大小来区分 字节型文件流(1字节)：FileInputStream/FileOutputStream 字符型文件流(2字节–1字符)：FileReader/FileWriter 字节流和字符流的区别： 读写单位不同：字节流以字节（8bit）为单位，字符流以字符为单位，根据码表映射字符，一次可能读多个字节。 处理对象不同：字节流能处理所有类型的数据（如图片、avi等），而字符流只能处理字符类型的数据。 结论：只要是处理纯文本数据，就优先考虑使用字符流。 除此之外都使用字节流。 输入流和输出流 对输入流只能进行读操作，对输出流只能进行写操作。 4. 字节型文件流4.1 字节型文件输入流FileInputStream(读) FileInputStream类在java.io包，继承自InputStream类(字节型输入流的父类)。 创建对象 调用一个带File类型的构造方法 调用一个带String类型的构造方法 常用方法 int code = read(); 每次从流管道中读取一个字节，返回字节的code码 *int count = read(byte[] ) 每次从流管道中读取若干个字节，存入数组内 返回有效元素个数 int count = available(); 返回流管道中还有多少缓存的字节数 skip(long n);跳过几个字节 读取 多线程—&gt;利用几个线程同时读取文件 *close() 将流管道关闭—必须要做,最好放在finally里 注意代码的健壮性，判断严谨（eg:非空判断） 4.2 字节型文件输出流FileOutputStream(写) FileOutputStream类在java.io包，继承自OutputStream类(所有字节型输出流的父类)。 创建对象 调用一个带File参数，还有File boolean重载 调用一个带String参数，还有String boolean重载 eg: new FileOutputStream(“D://test//bbb.txt”, true)//第二个参控制每次写入追加还是重载 常用方法 write(int code); 将给定code对应的字符写入文件 ‘=’ write(byte[]); 将数组中的全部字节写入文件 getByte() write(byte[] b, int off, int len); flush(); 将管道内的字节推入(刷新)文件 close(); 注意在finally中关闭 创建的是文件输入流，若文件路径有问题，则抛出异常 FileNotFoundException 创建的是文件输出流，若文件路径有问题，则直接帮我们创建一个新的文件 设计一个文件复制的方法 12345678910111213141516171819202122232425262728293031323334public void copyFile(File file, String path) &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; //创建输入流读取信息 fis = new FileInputStream(file); //创建一个新的File对象 File newFile = new File(path +&quot;\\\\&quot;+ file.getName());//&quot;E:\\\\test\\\\test.txt&quot; //创建一个输出流 fos = new FileOutputStream(newFile); byte[] b = new byte[1024];//通常1kb-8kb之间 int count = fis.read(b); while(count != -1) &#123; fos.write(b, 0, count);//将读取到的有效字节写入 fos.flush(); count = fis.read(b); &#125; System.out.println(&quot;复制完毕！&quot;); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭 if(fis!=null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123;e.printStackTrace();&#125; &#125; if(fos!=null) &#123; try &#123;fos.close();&#125; catch (IOException e) &#123;e.printStackTrace();&#125; &#125; &#125;&#125; 5. 字符型文件流FileReader/FileWriter：只能操作纯文本的文件 .txt / .properties 5.1 字符型文件输入流FileReader(读) FileReader类在java.io包，继承自InputStreamReader，Reader 创建对象 调用一个带File类型的构造方法 调用一个带String类型的构造方法 常用方法 read() read(char[]) close() 12345678910111213141516File file = new File(&quot;F://test//Test.txt&quot;);try &#123; FileReader fr = new FileReader(file); // int code = fr.read(); // System.out.println(code); char[] c = new char[1024]; int count = fr.read(c); while(count!=-1) &#123; System.out.println(new String(c, 0, count)); count = fr.read(c); &#125;&#125; catch (FileNotFoundException e) &#123; e.printStackTrace();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 5.2 字符型文件输出流FileWriter(写) FileWriter类在java.io包，继承自OutputStreamWriter，Writer 构造方法 带file参数，带file,boolean参数 带String参数，带String,boolean参数 常用方法 write(int) write(char[]) write(string) flush()，close() 6. *缓冲流 缓冲流,也叫高效流，是对4个基本的File…流的增强，所以也是4个流，按照数据类型分类： 字节缓冲流：BufferedInputStream，BufferedOutputStream 字符缓冲流：BufferedReader，BufferedWriter 缓冲流的基本原理，是在创建流对象时，会创建一个内置的默认大小的缓冲区数组，通过缓冲区读写，减少系统IO次数，从而提高读写的效率。 缓冲流读写方法与基本的流是一致 6.1 字节缓冲流 BufferedInputStream，BufferedOutputStream 构造方法 public BufferedInputStream(InputStream in) ：创建一个 新的缓冲输入流。 public BufferedOutputStream(OutputStream out)： 创建一个新的缓冲输出流。 1234// 创建字节缓冲输入流BufferedInputStream bis = new BufferedInputStream(new FileInputStream(&quot;bis.txt&quot;));// 创建字节缓冲输出流BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(&quot;bos.txt&quot;)); 6.2 字符缓冲流 BufferedReader，BufferedWriter 构造方法 public BufferedReader(Reader in) ：创建一个 新的缓冲输入流。 public BufferedWriter(Writer out)： 创建一个新的缓冲输出流。 1234// 创建字符缓冲输入流BufferedReader br = new BufferedReader(new FileReader(&quot;br.txt&quot;));// 创建字符缓冲输出流BufferedWriter bw = new BufferedWriter(new FileWriter(&quot;bw.txt&quot;)); 字符缓冲流的基本方法与普通字符流调用方式一致，不再阐述，我们来看它们具备的特有方法。 特有方法: BufferedReader：public String readLine(): 读一行文字。 BufferedWriter：public void newLine(): 写一行行分隔符,由系统属性定义符号。 12345678910111213141516171819202122//设计一个方法，用来用户登录认证public String login(String username, String password) &#123; try &#123; BufferedReader br = new BufferedReader(new FileReader(&quot;F://test//User.txt&quot;)); //User.txt每行存储格式：张三-123 String user = br.readLine();//user表示一行记录，记录账号密码 while(user!=null) &#123; //将user信息拆分，分别与参数比较 String[] value = user.split(&quot;-&quot;);//value[0]账号，value[1]密码 System.out.println(value[0]); if(value[0].equals(username)) &#123; if(value[1].equals(password)) &#123; return &quot;登录成功&quot;; &#125; &#125; user = br.readLine(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return &quot;账号或密码错误！&quot;;&#125; readLine方法演示: 12345678try &#123; BufferedWriter bw &#x3D; new BufferedWriter(new FileWriter(&quot;F:&#x2F;&#x2F;test&#x2F;&#x2F;User.txt&quot;, true)); bw.newLine(); bw.write(&quot;java-888&quot;); bw.flush();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 7. 转换流7.1 字符编码 字符编码Character Encoding : 就是一套自然语言的字符与二进制数之间的对应规则。 字符集 Charset：也叫编码表。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等。 常见字符集: ASCII字符集 ： ASCII（American Standard Code for Information Interchange，美国信息交换标准代码） ISO-8859-1字符集： 拉丁码表，别名Latin-1，用于显示欧洲使用的语言；ISO-8859-1使用单字节编码，兼容ASCII编码。 GBxxx字符集： GB就是国标的意思，是为了显示中文而设计的一套字符集。 GB2312（简体中文码表），GBK（最常用的中文码表），GB18030（最新的中文码表） Unicode字符集 ： Unicode编码系统为表达任意语言的任意字符而设计，是业界的一种标准，也称为统一码、标准万国码。 UTF-8、UTF-16和UTF-32；最为常用的UTF-8编码。 编码引出的问题 在IDEA中，使用FileReader 读取项目中的文本文件。由于IDEA的设置，都是默认的UTF-8编码，所以没有任何问题。但是，当读取Windows系统中创建的文本文件时，由于Windows系统的默认是GBK编码，就会出现乱码。 7.2 InputStreamReader类转换流java.io.InputStreamReader，是Reader的子类，是从字节流到字符流的桥梁。它读取字节，并使用指定的字符集将其解码为字符。它的字符集可以由名称指定，也可以接受平台的默认字符集。 构造方法 InputStreamReader(InputStream in): 创建一个使用默认字符集的字符流。 InputStreamReader(InputStream in, String charsetName): 创建一个指定字符集的字符流。 12InputStreamReader isr = new InputStreamReader(new FileInputStream(&quot;in.txt&quot;));InputStreamReader isr2 = new InputStreamReader(new FileInputStream(&quot;in.txt&quot;) , &quot;GBK&quot;); 指定编码读取: 12345678910111213141516171819202122public class ReaderDemo2 &#123; public static void main(String[] args) throws IOException &#123; // 定义文件路径,文件为gbk编码 String FileName = &quot;E:\\\\file_gbk.txt&quot;; // 创建流对象,默认UTF8编码 InputStreamReader isr = new InputStreamReader(new FileInputStream(FileName)); // 创建流对象,指定GBK编码 InputStreamReader isr2 = new InputStreamReader(new FileInputStream(FileName) , &quot;GBK&quot;);// 定义变量,保存字符 int read; // 使用默认编码字符流读取,乱码 while ((read = isr.read()) != -1) &#123; System.out.print((char)read); // ��Һ� &#125; isr.close(); // 使用指定编码字符流读取,正常解析 while ((read = isr2.read()) != -1) &#123; System.out.print((char)read);// 大家好 &#125; isr2.close(); &#125;&#125; 7.3 OutputStreamWriter类转换流java.io.OutputStreamWriter ，是Writer的子类，是从字符流到字节流的桥梁。使用指定的字符集将字符编码为字节。它的字符集可以由名称指定，也可以接受平台的默认字符集。 构造方法 OutputStreamWriter(OutputStream in): 创建一个使用默认字符集的字符流。 OutputStreamWriter(OutputStream in, String charsetName): 创建一个指定字符集的字符流。 12OutputStreamWriter isr = new OutputStreamWriter(new FileOutputStream(&quot;out.txt&quot;));OutputStreamWriter isr2 = new OutputStreamWriter(new FileOutputStream(&quot;out.txt&quot;) , &quot;GBK&quot;); 指定编码写出 123456789101112131415161718public class OutputDemo &#123; public static void main(String[] args) throws IOException &#123; // 定义文件路径 String FileName = &quot;E:\\\\out.txt&quot;; // 创建流对象,默认UTF8编码 OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(FileName)); // 写出数据 osw.write(&quot;你好&quot;); // 保存为6个字节 osw.close();// 定义文件路径String FileName2 = &quot;E:\\\\out2.txt&quot;; // 创建流对象,指定GBK编码 OutputStreamWriter osw2 = new OutputStreamWriter(new FileOutputStream(FileName2),&quot;GBK&quot;); // 写出数据 osw2.write(&quot;你好&quot;);// 保存为4个字节 osw2.close(); &#125;&#125; 8. 对象流 对象序列化和反序列化 Java 提供了一种对象序列化的机制。用一个字节序列可以表示一个对象，该字节序列包含该对象的数据、对象的类型和对象中存储的属性等信息。字节序列写出到文件之后，相当于文件中持久保存了一个对象的信息。 反之，该字节序列还可以从文件中读取回来，重构对象，对它进行反序列化。对象的数据、对象的类型和对象中存储的数据信息，都可以用来在内存中创建对象 简单来讲 对象的序列化指的是：将一个完整的对象 拆分成字节碎片 记录在文件中 对象的反序列化指的是：将文件中记录的对象随便 反过来组合成一个完整的对象 如果想要将对象序列化到文件中：需要让对象实现Serializable接口，是一个示意性接口；如果想要将对象反序列化：需要给对象提供一个序列化的版本号，private long serialVersionUID = 任意L; 8.1 ObjectOutputStream类 java.io.ObjectOutputStream 类，将Java对象的原始数据类型写出到文件,实现对象的持久存储。 构造方法 public ObjectOutputStream(OutputStream out)： 创建一个指定OutputStream的ObjectOutputStream。 12FileOutputStream fileOut = new FileOutputStream(&quot;employee.txt&quot;);ObjectOutputStream out = new ObjectOutputStream(fileOut); 序列化操作 一个对象要想序列化，必须满足两个条件: 该类必须实现java.io.Serializable 接口，Serializable 是一个标记接口，不实现此接口的类将不会使任何状态序列化或反序列化，会抛出NotSerializableException 。 该类的所有属性必须是可序列化的。如果有一个属性不需要可序列化的，则该属性必须注明是瞬态的，使用transient 关键字修饰。 写出对象方法 public final void writeObject (Object obj) : 将指定的对象写出。1234567891011121314151617181920212223242526272829303132//满足两个条件public class Employee implements java.io.Serializable &#123; public String name; public String address; public transient int age; // transient瞬态修饰成员,不会被序列化 public void addressCheck() &#123; System.out.println(&quot;Address check : &quot; + name + &quot; -- &quot; + address); &#125;&#125;//写出对象方法public class SerializeDemo&#123; public static void main(String [] args) &#123; Employee e = new Employee(); e.name = &quot;zhangsan&quot;; e.address = &quot;beiqinglu&quot;; e.age = 20; try &#123; // 创建序列化流对象 ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(&quot;employee.txt&quot;)); // 写出对象 out.writeObject(e); // 释放资源 out.close(); fileOut.close(); System.out.println(&quot;Serialized data is saved&quot;); // 姓名，地址被序列化，年龄没有被序列化。 &#125; catch(IOException i) &#123; i.printStackTrace(); &#125; &#125;&#125;//输出结果：//Serialized data is saved 8.1 ObjectInputStream类 ObjectInputStream反序列化流，将之前使用ObjectOutputStream序列化的原始数据恢复为对象。 构造方法 public ObjectInputStream(InputStream in)： 创建一个指定InputStream的ObjectInputStream。 反序列化操作1 如果能找到一个对象的class文件，我们可以进行反序列化操作，调用ObjectInputStream读取对象的方法。 对于JVM可以反序列化对象，它必须是能够找到class文件的类。如果找不到该类的class文件，则抛出一个 ClassNotFoundException 异常。 public final Object readObject () : 读取一个对象。 12345678910111213141516171819202122232425262728public class DeserializeDemo &#123; public static void main(String [] args) &#123; Employee e = null; try &#123; // 创建反序列化流 FileInputStream fileIn = new FileInputStream(&quot;employee.txt&quot;); ObjectInputStream in = new ObjectInputStream(fileIn); // 读取一个对象 e = (Employee) in.readObject(); // 释放资源 in.close(); fileIn.close(); &#125;catch(IOException i) &#123; // 捕获其他异常 i.printStackTrace(); return; &#125;catch(ClassNotFoundException c) &#123; // 捕获类找不到异常 System.out.println(&quot;Employee class not found&quot;); c.printStackTrace(); return; &#125; // 无异常,直接打印输出 System.out.println(&quot;Name: &quot; + e.name);// zhangsan System.out.println(&quot;Address: &quot; + e.address); // beiqinglu System.out.println(&quot;age: &quot; + e.age); // 0 &#125;&#125; 反序列化操作2 另外，当JVM反序列化对象时，能找到class文件，但是class文件在序列化对象之后发生了修改，那么反序列化操作也会失败，抛出一个InvalidClassException异常。发生这个异常的原因如下： 该类的序列版本号与从流中读取的类描述符的版本号不匹配 该类包含未知数据类型 该类没有可访问的无参数构造方法 Serializable 接口给需要序列化的类，提供了一个序列版本号。serialVersionUID 该版本号的目的在于验证序列化的对象和对应类是否版本匹配。 123456789101112public class Employee implements java.io.Serializable &#123; // 加入序列版本号 private static final long serialVersionUID = 1L; public String name; public String address; // 添加新的属性 ,重新编译, 可以反序列化,该属性赋为默认值. public int eid; public void addressCheck() &#123; System.out.println(&quot;Address check : &quot; + name + &quot; -- &quot; + address); &#125;&#125; 9. 打印流(PrintStream类) 平时我们在控制台打印输出，是调用print方法和println方法完成的，这两个方法都来自于java.io.PrintStream类，该类能够方便地打印各种数据类型的值，是一种便捷的输出方式。 构造方法 public PrintStream(String fileName); 使用指定的文件名创建一个新的打印流。 1PrintStream ps = new PrintStream(&quot;ps.txt&quot;)； 改变打印流向 System.out就是PrintStream类型的，只不过它的流向是系统规定的，打印在控制台上。不过，我们可以改变它的流向。 123456789101112public class PrintDemo &#123; public static void main(String[] args) throws IOException &#123; // 调用系统的打印流,控制台直接输出97 System.out.println(97); // 创建打印流,指定文件的名称 PrintStream ps = new PrintStream(&quot;ps.txt&quot;); // 设置系统的打印流流向,输出到ps.txt System.setOut(ps); // 调用系统的打印流,ps.txt中输出97 System.out.println(97); &#125;&#125; 10. Properties类的使用 Java.util.Properties，主要用于读取Java的配置文件。 Properties类继承自Hashtable 配置文件：在Java中，其配置文件常为.properties文件，格式为文本文件，文件的内容的格式是“键=值”的格式，文本注释信息可以用”#”来注释。 Properties类的主要方法： getProperty ( String key)，用指定的键在此属性列表中搜索属性。也就是通过参数 key ，得到 key 所对应的 value。 load ( InputStream inStream)，从输入流中读取属性列表（键和元素对）。通过对指定的文件（比如说上面的 test.properties 文件）进行装载来获取该文件中的所有键 - 值对。以供 getProperty ( String key) 来搜索。 setProperty ( String key, String value) ，调用 Hashtable 的方法 put 。他通过调用基类的put方法来设置 键 - 值对。 store ( OutputStream out, String comments)，以适合使用 load 方法加载到 Properties 表中的格式，将此 Properties 表中的属性列表（键和元素对）写入输出流。与 load 方法相反，该方法将键 - 值对写入到指定的文件中去。 clear ()，清除所有装载的 键 - 值对。该方法在基类中提供。","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】异常处理机制","date":"2017-04-25T00:34:31.000Z","path":"article/20170425.html","text":"Java语言提供了完善的异常处理机制。正确运用这套机制，有助于提高程序的健壮性。 基本概念 异常的分支结构 添加处理异常的手段 异常的捕获 异常的抛出 自定义异常 总结 1. 基本概念 异常用于在Java语言中描述运行阶段发生的错误。 在Java中有一个定义好的规则Throwable（可以抛出的） java.lang.Throwable类是所有错误(Error)和异常(Exception)的超类。 Error类主要用于描述比较严重无法编码解决的错误，如：JVM内存资源耗尽等。 Exception类主要用于描述比较轻微可以编码解决的错误，如：文件损坏、非法输入等。 java.lang.Exception类是所有异常的超类，主要分为两大类： RuntimeException - 运行时异常，也叫非检测性异常 IOException和其他异常 - 其他异常也叫做检测性异常 注意：当程序运行过程中发生异常而又没有手动处理时，则由java虚拟机采用默认方式处理，即打印异常名称、原因、发生位置并终止程序。在开发中尽量使用条件判断避免异常的发生。 12345678910Throwable类 |————Exception类 |————RuntimeException异常 |————ArithmeticException类 |————ArrayIndexOutOfBoundsException类 |————NullPointerException类 |————ClassCastException类 |————NumberFormatException类 |————IOException和其他异常 |————Error类 2. 异常的分支结构2.1 运行时异常（非检查异常） Error和RuntimeException都算作运行时异常 javac编译的时候，不会提示和发现的， 在程序编写时不要求必须做处理，如果我们愿意可以添加处理手段(try throws) 要求大家出现这样异常的时候 知道怎么产生及如何修改 InputMisMatchException 输入不匹配 int value = input.nextInt();// abc *NumberFormatException 数字格式化 int value = Integer.parseInt(“123.45”); NegativeArraySizeException 数组长度负数 int[] array = new int[-2]; *ArrayIndexOutOfBoundsException 数组索引越界 int[] array = {1,2,3}; array[5]; *5NullPointerException 空指针异常 int[][] array = new int[3][]; array[0][0] =10; Person p = null; p.getName(); ArithmeticException 数字异常 10/0 整数不允许除以0 Infinity小数除以0会产生无穷 *ClassCastException 造型异常 Person p = new Teacher(); Student s = (Student)p; *StringIndexOutOfBoundsException 字符串越界 String str = “abc”; str.charAt(5); *IndexOutOfBoundsException 集合越界 List家族 ArrayList list = new ArrayList(); list.add(); list.add(); list.add(); list.get(5); IllegalArgumentException 非法参数异常 ArrayList list = new ArrayList(-1); 2.2 编译时异常(检查异常) 除了Error和RuntimeException以外其他的异常 javac编译的时候，强制要求我们必须为这样的异常做处理(try或throws) 因为这样的异常在程序运行过程中极有可能产生问题的 异常产生后后续的所有执行就停止 123456//eg: InterruptExceptiontry&#123; Thread.sleep(5000);&#125;catch(Exception e)&#123; //...&#125; 3. 添加处理异常的手段 处理异常不是 异常消失了 处理异常指的是：处理掉异常之后，后续的代码不会因为此异常而终止执行 两种手段： 异常的捕获：try{}catch(){}[ finally{} ] throws抛出 final，finally，finalize区别 final：特征修饰符，修饰变量，属性，方法，类 修饰变量：基本类型:值不能改变；引用类型:地址不能改变(如果变量没有初值,给一次机会赋值) 修饰属性：特点与修饰变量类似(要求必须给属性赋初始值,否则编译报错) 修饰方法：不能被子类重写 修饰类：不能被其他的子类继承 finally：处理异常手段的一部分 try{}catch(){}后面的一个部分 这个部分可有可无，如果有只能含有一份，且必须执行 finalize：是Object类中的一个protected方法 对象没有任何引用指向的时候 – 会被GC回收 当对象回收的时候 默认调用finalize方法 若想要看到对象回收的效果，可以重写 public void finalize(){} 4. 异常的捕获12345678try&#123; 可能发生异常的代码;&#125;catch(异常类型 引用变量)&#123; 针对该异常的处理代码;&#125;catch ...finally&#123; 无论是否发生异常都要执行的代码;&#125; 处理异常放在方法内部 可能会出现的小问题 如果在方法内部含有返回值，不管返回值return关键字在哪里，finally一定会执行完毕，返回值的具体结果得看情况。 1234567891011public String test() &#123; try &#123; &#x2F;&#x2F;...可能产生异常的的代码 return &quot;值1&quot;;&#x2F;&#x2F;事先约定好 返回值 &#125;catch(Exception e)&#123; e.printStackTrace();&#x2F;&#x2F;打印输出异常的名字 &#125;finally &#123; System.out.println(&quot;finally块执行啦&quot;); &#125; return &quot;值2&quot;;&#125; 上述执行结果：若try中代码块产生异常return返回 值2，若try中无异常则return返回 值1，无论return在哪finally都会执行。 异常捕获的注意事项： 当需要多分catch分子时，切记小类型应该放在大类型的前面； 懒人写法：catch(Exception e){…} finally通常用于善后处理，如：关闭已经打开的文件等。 5. 异常的抛出 当程序中发生异常又不方便直接处理时，可以将异常转移给方法调用者进行处理，这个过程叫做异常的抛出。 语法格式：访问权限 返回值类型 方法名(形参列表) throws 异常类型1,异常类型2,…{} ，如：public void show() throw Exception &#123;&#125; 重写方法的抛出规则： 不抛出异常 抛出父类异常中的子类异常 抛出和父类一样的异常 不能抛出同级不一样的异常 不能抛出更大的异常 6. 自定义异常 可以根据需要自定义异常类。 自定义异常的方式： 继承Exception或者异常的子类。 提供两个构造，无参构造和String做参数的构造。 异常的手段 如果继承是RuntimeException—-&gt;运行时异常(不需要必须添加处理手段) 如果继承是Exception—–&gt;编译时异常(必须添加处理手段) 类中可以写带String参数的构造方法，可以做细致的说明 通过throw关键字，new一个异常的对象 主动产生异常：throw new 异常类型(); 7. 总结 1.在开发中尽量使用条件判断避免异常的发生; 2.若实在避免不了，则进行异常捕获； 3.若实在捕获不了，则进行异常抛出； 4.若需要使用针对性异常，则自定义异常。","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】集合框架","date":"2017-04-17T14:10:57.000Z","path":"article/20170417.html","text":"为了方便对多个对象的操作，对对象进行存储，集合就是存储对象最常用的一种方式。 Collection集合 List集合 泛型机制 Queue集合 *ArrayList类 Vector类 Stack类 *LinkedList类 Set集合 HashSet类 TreeSet类 Map集合 HashMap类 TreeMap类 Lambda表达式 Stream API 1. Collection集合 Collection集合框架，字面意思容器；与数组类似，集合的长度存储之后还能改变，集合框架中包含了一系列不同数据结构的实现类。 数组与集合的比较 数组的特点： 数组本质上就是一段连续的存储单元，用于存放多个类型相同的数据类容； 支持下标访问，实现随机访问非常方便； 增删操作不方便，可能会移动大量元素； 数组一旦声明长度固定无法更改； 数组支持基本数据类型，也支持引用数据类型； 集合的特点： 集合的存储单元可以不连续，数据类容可以不相同； 集合部分支持下标访问，部分不支持； 集合中增删元素可以不移动大量元素； 集合大小可以随时动态调整； 集合中的元素必须是引用数据类型（基本数据类型可用包装类）； 1234567891011121314-Collection接口 |————List接口 |————ArrayList类 |————LinkedList类 |————Stack类 |————Vector类 |————Queue接口 |————LinkedList类 |————Set接口 |————HashSet类 |————TreeSet类-Map接口 |————HashMap类 |————TreeMap类 Collection存储的都是value,其中List有序可重复，Set无序无重复 Map存储的是以key-value形式,key无序无重复 value无序可重复 序 : 顺序–添加进去的元素，取得元素的顺序一致；注意指的不是集合自己的顺序 Collection集合的常用方法 boolean add(E e); 向集合中添加对象 boolean contains(Object o); 判断是否包含指定对象 boolean remove(Object o); 从集合中删除对象 void clear(); 清空集合 int size(); 返回包含对象的个数 boolean isEmpty(); 判断是否为空 1234567Collection c2 = new ArrayList(); //多态boolean b1 = c2.add(new String(&quot;one&quot;)); //trueboolean b2 = c2.add(new Integer(2)); //trueSystem.out.println(&quot;c2 = &quot; + c2); //[one, 2]boolean b3 = c2.contains(new Integer(2));//true//contains方法工作原理：(o==null ? e==null : o.equals(e)); 2. List集合 java.util.List集合是Collection集合的子集合。 List集合中元素有先后放入次序并且元素可以重复；实现类有：ArrayList类、LinkedList类、Stack类以及Vector类。 ArrayList类的底层使用数组进行数据管理，访问元素方便，增删不方便。 LinkedList类的底层使用链表进行数据管理，访问不方便，增删方便。 Stark类的底层使用数组进行数据管理，该类主要描述具有后进先出的特征的数据结构，叫做栈。 Vector类的底层使用数组进行数据管理，与ArrayList类似，与之比线程安全的类，因此效率低。 List类除了继承Collection定义的方法外，还根据线性表的数据结构定义了一系列方法，其中最常用的是基于下标的get()，set()方法。 List类常用方法 void add(int index, E element) 向集合指定位置添加元素 boolean addAll(int index, Collection&lt;?extends E&gt; c) 向集合中添加所有元素 E get(int index) 从集合中获取指定位置的元素 E set(int index, E element) 修改指定位置的元素 E remove(int index) 删除指定位置的元素 int indexOf(Object o) 在集合中检索某个对象，判断逻辑(o==null?get(i)==null:o.equals(get(i))) T[] toArray(T[] a) 将集合中的对象序列化以对象数组的形式返回。 List subList(int fromIndex, int toIndex) 获取List从fromIndex(包括)和 toIndex(不包括)之间的部分视图 3. 泛型机制 集合可以存放不同的对象，本质上都看作Object类型放入，此时从集合中取出也是Object类型，为了表达该元素真实类型需要强制类型转换，而强制类型转换可能发生类型转换异常。 从jdk1.5开始推出泛型机制，在集合名称后面使用&lt;数据类型&gt;的方式明确要求该集合中可以存放的数据类型。如：List&lt;String&gt; lt = new LinkedList&lt;String&gt;();。 从jdk1.7开始可省略后面&lt;&gt;的数据类型，叫做菱形特性，如：List&lt;String&gt; lt = new ArrayList&lt;&gt;();。 泛型本质就是参数化类型，让数据类型作为参数传递，public interface List&lt;E&gt;&#123;&#125;其中E是占位形参，由于实参可以支持各种广泛的类型，因此得名泛型。 泛型可以用在哪里： 泛型类：类定义的时候描述某种数据类型，集合的使用就是这样 泛型接口：与泛型类的使用基本一致，子类实现接口时必须添加泛型 泛型方法：方法调用时传参数，方法的泛型与类无关，带有泛型的方法可以不放在带有泛型的类中 方法参数泛型限制，高级泛型，规范边界，extends，super 4. Queue集合 java.util.Queue集合是Collection集合的子集合。 Queue集合主要描述具有先进先出特性的数据结构，叫做队列(FIFO:First Input First Output)。 Queue集合主要实现类是LinkedList类，因为该类在增删方面有一定优势。 Queue接口中主要方法 boolean offer(E e) 将一个对象添加至队尾，若添加成功则返回true E poll() 从队首删除并返回一个元素 E peek() 返回队首的元素（但并不删除） 12345Queue&lt;Integer&gt; q1 = new LinkedList&lt;Integer&gt;();//将数据11、22、33、44、55依次入队for(int i=1; i&lt;=5; i++) &#123; q1.offer(i*11);&#125; 5. *ArrayList类 底层是利用(动态)数组形式实现，jdk1.5，所属的包 java.util ArrayList特点适合遍历轮询，不适合插入删除 如何构建一个ArrayList对象 无参数构造方法，带默认容量构造方法，带collection参数的构造方法 ArrayList中常用的方法 增删改查：add(E e)，remove(index)，set(index value)，get(index)，size() 类中其他常用的方法 addAll并集，removeAll差集，ratainAll交集; indexOf()，lastIndexOf()，contains()，List=subList(); isEmpty()，clear()，ensureCapacity()，iterator();迭代器 toArray(T[] x)，trimToSize(); 6. Vector类 是ArrayList集合的早期版本，所属的包 java.util Vector底层也是利用(动态)数组的形式存储 Vector是线程同步的(synchronized)，安全性高，效率较低 扩容方式与ArrayList不同 默认是扩容2倍，可以通过构造方法创建对象时修改这一机制 构造方法和常用方法与ArrayList类似 7. Stack类 Stack类，栈，java.util包 构造方法只有一个无参数 除了继承自Vacton类的方法外还有特殊的方法 push(E e)将某一个元素压入栈顶(add()) E = pop()将某一个元素从栈顶取出并删掉(E = remove()) E = peek()查看栈顶的一个元素 不删除(get()) boolean = empty()判断栈内元素是否为空(isEmpty()) int = search()查找给定的元素在占中的位置(indexOf()) 应用场景 中国象棋，悔棋 栈中存储每一次操作的步骤 撤销功能 8. *LinkedList类 LinkedList类，java.util包 底层使用双向链表的数据结构形式来存储 适合于插入或删除 不适合遍历轮询 构建对象 无参数构造方法，带参数的构造方法(collection) 常用的方法 增删改查：add()，remove()，set()，get()，size()，offer，poll，peek 手册中提供的其他常用方法：addAll，addFist，addLast()，clear()，contains()，element()，getFirst()，getLast()，indexOf()，lastIndex() 插入删除的特性是否像想的那样 对比ArrayList Linked 9. Set集合 java.util.Set集合是Collection集合的子集合。 Set集合没有先后放入次序，并且不允许有重复关系，实现类有HashSet类和TreeSet类。 其中HashSet类底层是采用哈希表进行数据管理的。 其中TreeSet类的底层是采用二叉树进行数据管理的。 1234//方法和Collection集合基本一样Set&lt;String&gt; set1 = new HashSet&lt;String&gt;();set1.add(&quot;one&quot;);System.out.println(&quot;s1=&quot;+s1); set集合的无重复特性 HashSet，无重复原则有两个方法同时起作用 equals hashCode 默认比较的是两个对象的地址 若第二个对象地址与之前的一致 不再存入 如果想要改变其比较的规则 可以重写上述两个方法 TreeSet，无重复原则有一个方法起作用 compareTo 上述这个方法不是每一个对象都有的 若想要将某一个对象存入TreeSet集合中，需要让对象所属的类实现接口Comparable 实现接口后将compareTo方法重写，返回值int，负数靠前排布，整数排列靠后 9.1 Set集合的遍历 所有Collection的实现类都实现了其iterator方法，该方法返回Iterator接口类型对象，用于实现对集合元素的迭代遍历。 迭代器Iterator&lt;E&gt; iterator()，主要方法有 boolean hasNext() 判断集合中是否有可以迭代/访问的元素 E next() 用于取出一个元素并指向下一个元素 void remove() 用于删除访问到的最后一个元素 12345678Iterator&lt;String&gt; it = set1.iterator();//获取当前集合的迭代器对象while(it.hasNext()) &#123;//判断是否有可以访问的元素 String temp = it.next();//取出一个并指向下一个 System.out.println( temp ); if(&quot;two&quot;.equals(temp))&#123; it.remove();//删除set1中该元素 &#125;&#125; 增强for循环(for each结构) 语法格式：for(元素类型 变量名:集合/数组)&#123; 循环体; &#125;。 执行流程：不断从集合/数组中取出一个元素赋值给变量名后执行循环体，直到取出所有元素。 123456789//遍历集合for(String ts : s1) &#123; System.out.println(ts);&#125;//遍历数组int[] arr = &#123;11,22,33,44,55&#125;;for(int ti : arr) &#123; System.out.println(ti);&#125; 10. HashSet类 HashSet集合底层采用HashMap（数组+链表–&gt;散列表），java.util包。 它不保证set 的迭代顺序；特别是它不保证该顺序恒久不变。此类允许使用null元素。 创建对象：无参数，有参数 集合容器的基本使用 增删改查：boolean = add(value)，addAll(collection c)，retainAll，removeAll，boolean = remove(Object) 没有修改方法 iterator() 获取一个迭代器对象 size() 无重复的原则 在HashSet中，元素都存到HashMap键值对的Key上面，而Value时有一个统一的值private static final Object PRESENT = new Object();，(定义一个虚拟的Object对象作为HashMap的value，将此对象定义为static final。) 11. TreeSet类 TreeSet类，无序无重复，java.util包。(底层TreeMap 二叉树 利用Node(left item right)) 创建对象： 无参数构造方法 ，带Collection构造方法 基本常用方法：add(E e)，iterator()，remove(E e)，没有修改，size() 二叉树主要指每个节点最多只有两个子节点的树形结构。 满足以下三个特征的二叉树叫做有序二叉树： 左子树中的任意节点元素都小于根节点元素； 右子树中的任意节点元素都大于根节点元素； 左子树和右子树内部也遵守上述规则； 无序无重复：treeSet集合本身有顺序，我们指的无序存入的和取出来的不一致。 元素放入TreeSet集合过程：由于TreeSet集合底层采用有序二叉树进行数据的管理，当有新元素插入到TreeSet集合时，需要使用新元素与集合中已有的元素依次比较来确定存放合理位置，而比较元素大小规则有两种方式： 使用元素的自然排序规则进行比较并排序，让元素类型实现java.lang.Comparable接口； 使用比较器规则进行比较并排序，构造TreeSet集合时传入java.util.Comparable接口； 注意： 1. 自然排序的规则比较单一，而比较强的规则比较多元化，而且比较器优先于自然排序； 2. 可以使用Collections工具类对集合中的元素进行操作； 12. Map集合 java.util.Map&lt;K, V&gt;集合存取元素的基本单位是：单对元素（键值对key-value）。 Map：映射，通过某一个key可以直接定位到一个value值 key无序无重复 value无序可重复 key无序还是一样，指的是存入顺序与取得顺序不一致，key无重复当然指的是，元素不能一致 主要有两个实现类：HashMap类和TreeMap类。 Map基本使用：HashMap，TreeMap，Properties Map集合常用方法： 增改：put(key,value)，删：remove(key)，查：get(key),containsKey(key),containsValue(value) Map集合的遍历方式：a.迭代Key，b.迭代Entry Map集合的性能调优： 加载因子较小时散列查找性能会提高，同时也浪费了散列桶空间容量。0.75是性能和空间相对平衡的结果，在常见散列表时指定合理容量，减少rehash提高性能。（Capacity:容量，Initial capacity:初始容量，Size:数据大小，Load factor:加载因子(size/capacity),默认0.75） 13. HashMap类 包:java.util，底层散列表的形式（数组+链表） 构造方法创建对象 无参数 带默认容量的 带map参数的构造方法 特点:(数组+链表)底层散列表形式存储，key无序无重复,value无序可重复 找寻某一个唯一元素的时候建议使用map，更适合于查找唯一元素，Map$Entry 基本方法： 增 put(key,value)，存放一组映射关系key-value key存储的顺序与取得顺序不同 不同的key可以存储相同的value key若有相同的 则将 原有的value覆盖而不是拒绝存入(跟set刚好相反) 删 E = remove(key); 改 replace(key,newValue)，put(key,value2) 查 E = get(key)； Set = keySet()获取全部的key Set = entrySet(); size(); 12345678Set&lt;Entry&lt;Integer,String&gt;&gt; entrys = map.entrySet();//获取集合中全部的entry对象Iterator&lt;Entry&lt;Integer,String&gt;&gt; it = entrys.iterator();while(it.hasNext())&#123; Entry&lt;Integer,String&gt; entry = it.next();//entry key value Integer key = entry.getKey(); String value = entry.getValue(); System.out.println(key+&quot;--&quot;+value);&#125; 除了上述几个常用的方法外 其他API中提供的方法 clear，containsKey(key)，containsValue(value) getOrDefault(key,defaultValue);如果key存在就返回对应的value 若没有找到则返回默认值 isEmpty() putAll(map) putIfAbsent(key,value);//如果key不存在才向集合内添加 如果key存在就不添加啦 map集合在什么情形下用? 想要存储一组元素 数组 or 集合，如果存储的元素以后长度不变 用数组，如果长度以后不确定 用集合 如果发现长度以后不确定—&gt;集合 list Set Map List家族有序的 Set家族无重复 Map家族k-v 存储有顺序用这个 存储元素希望自动去掉重复元素用这个 通过唯一的k快速找寻v用这个 ArrayList:更适合遍历轮询 HashSet:性能更高 HashMap:性能更高 LinkedList:更适合插入和删除 TreeSet:希望存进去的元素自动去重复,同时还能自动排序 Tree:希望存进去的元素key自动排序 Stack:LIFO - - 14. TreeMap类 java.util包 构造方法：无参数，带map参数 常用方法：put， get，remove，replace，size 底层数据结构的存储：红黑二叉树（层级多余2层可能会左旋或右旋） 自然有序，按照Unicode编码自然有序 ap集合中的key需要可比较的 key的对象需要实现Comparable接口 15. Lambda表达式 java8支持的新的语法格式，Lambda允许把函数作为一个方法的参数(函数作为参数传递进方法中)，使用lambda表达式可以使代码变得更加简洁紧凑。 函数式编程：一种抽象程度很高的编程范式。函数也可以跟变量、对象一样使用，可以作为参数，也可以作为返回值，大大简化了代码的开发。 lambda表达式语法由参数列表、箭头函数-&gt;**和函数体**组成，函数体即可以是一个表达式，也可以是一个语句块。 123(int a, int b) -&gt; a+b() -&gt; 42(String s) -&gt; &#123;System.out.println(s);&#125; 函数式接口：指仅仅只包含一个抽象方法的接口，每一个该类型的lambda表达式大都会被匹配到这个抽象方法。 jdk1.8提供了一个@FunctionalInterface注解来定义函数式接口，如果我们定义的接口不符合函数式的规范便会报错。 15.1 Lambda表达式-方法引用 方法引用：只需要使用方法的名字，而具体调用交给函数式接口，需要和Lambda表达式配合使用。 方法引用和lambda表达式拥有相同的特性，我们并不需要为方法引用提供方法体，我们可以直接通过方法名称引用已有的方法。 16. Stream API Stream(流)借助lambda表达式来进行集合数据处理,分为中间操作和最终操作两种；最终操作返回一特定类型的计算结果，而中间操作返回Stream本身，这样就可以将多个操作依次串起。 虽然大部分情况下stream是容器调用Collection.stream()方法得到的，但stream和collections有以下不同： 无存储。stream不是一种数据结构，它只是某种数据源的一个视图，数据源可以是一个数组，Java容器或I/O channel等。 为函数式编程而生。对stream的任何修改都不会修改背后的数据源，比如对stream执行过滤操作并不会删除被过滤的元素，而是会产生一个不包含被过滤元素的新stream。 惰式执行。stream上的操作并不会立即执行，只有等到用户真正需要结果的时候才会执行。 可消费性。stream只能被“消费”一次，一旦遍历过就会失效，就像容器的迭代器那样，想要再次遍历必须重新生成。 对stream的操作分为为两类，中间操作和结束操作，二者特点是： 中间操作总是会惰式执行，调用中间操作只会生成一个标记了该操作的新stream，仅此而已。 结束操作会触发实际计算，计算发生时会把所有中间操作积攒的操作以pipeline的方式执行，这样可以减少迭代次数。计算完成之后stream就会失效。 16.1 stream方法使用 stream跟函数接口关系非常紧密，没有函数接口stream就无法工作（通常函数接口出现的地方都可以使用Lambda表达式，所以不必记忆函数接口的名字)。 12345// 找出最长的单词Stream&lt;String&gt; stream = Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);Optional&lt;String&gt; longest = stream.reduce((s1, s2) -&gt; s1.length()&gt;=s2.length() ? s1 : s2);//Optional&lt;String&gt; longest = stream.max((s1, s2) -&gt; s1.length()-s2.length());System.out.println(longest.get());","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】核心工具类","date":"2017-04-12T08:42:47.000Z","path":"article/20170412.html","text":"API (Application Programming Interface) 应用程序编程接口，Java中的API，就是JDK提供的各种功能的Java类。 Object类与其常用方法 包装类 数学处理类 Scanner类和System类 日期类 String类 StringBuilder类/StringBuffer类 Optional类 常用的包 java.lang包：是Java最核心的包，JVM(Java虚拟机)启动时自动加载lang包的所有类和接口，无需import。如：System类、String类、Object类、Class类… java.util包：是Java工具包，包括很多工具类和集合。如：Scanner类、Random类… java.io包：是输入输出包，包括读写各种设备。 java.net包：是网络编程的包，包括各种网络编程。 java.sql包：是操作数据库的所有类和接口。 1. Object类与其常用方法1.1 Object类 java.lang.Object类在Java类继承结构中位于顶端(根类)，任何类都是该类的直接或间接子类。 Object定义了“对象”的基本行为，被子类默认继承。 1.2 equals() 和 hashCode() boolean equals()方法用于非空对象的“相等”逻辑，默认比较两个对象的地址，返回布尔值。 equals()方法要求：自反性/对称性/传递性/一致性/非空性。 Java类可以根据需要重写继承自Object的equals()方法。 注意：当equals()方法被重写时，必须重写hashCode方法，以维护hashCode方法的常规协定，该协定声明相等对象必须具有相等的哈希码。 int hashCode():返回对象的哈希码值，对应一个内存。 hashCode规范要求： 一致性，同一对象，若没有改变属性值，多次调用其hashCode应该时一致的 如果两个对象判定相等，它们的hashCode应该时同一个值 如果两个对象不相等，它们的hashCode可以相同，但最好不相同而可以提高哈希表的性能。 hashCode()方法和equals()方法的判断条件必须保持一致，如果重写一个，另一个也必须重写。 1.3 toString() String toString()：用于获取调用对象的字符串形式，返回”包名.类名@hashCode值的16进制”。 Java类可以根据需要重写toString方法返回更有意义的信息。 Java在使用System.out.println()打印对象时或者+连接字符串时，默认调用toString()方法。 2. 包装类2.1 包装类 由于某些特殊场合(集合)中要求所有数据内容都必须是对象，而对于基本数据类型的变量来说不满足该要求，为了使得该变量也能够使用就需要对变量打包处理变成对象，此时就需要借助包装类。 Java语言8种基本类型分别对应了8中“包装类”，每一种包装类都封装了一个对应的基本类型成员变量，还提供了一些针对该数据类型的实用方法。 基本类型 对应包装类 byte java.lang.Byte short java.lang.Short int java.lang.Integer long java.lang.Long float java.lang.Float double java.lang.Double boolean java.lang.Boolean char java.lang.Character 八个包装类都在同一个包下（java.lang包），不需要import导包直接使用 八个包装类中有六个是与数字相关，都默认继承父类Number 八个包装类都实现了Serializable, Comparable 八个包装类都有带自己对应类型参数的构造方法，其中有七个(除了Character)还有构造方法重载，带String类型 八个包装类都提供了各自对应的拆包方法，如intValue,floatValue,将包装类对象拆成基本类型 2.2 Integer类 java.lang.Integer类是int类型的包装类，该类型对象中包含一个int类型的成员变量。该类由final关键字修饰表示不能被继承。 Integer类重写了**equals()**方法（重写后比较的是数值）、hashCode()以及toString()方法。 Integer类的常用方法 Integer(int i) 根据参数指定整数来构造对象 Integer(String s) 根据参数指定的字符串来构造对象 int intValue() 获取调用对象中整数值并返回 static Integer valueOf(int i) 根据参数指定整数值得到Integer类型对象 static int parseInt(String s) 将字符串类型转换为int类型并返回 2.3 装箱和拆箱123456int i = 100;Integer it = Integer.valueOf(i); //实现了int类型到Integer类型的转换，这个过程叫做装箱int ia = it.intValue();//实现了Integer类型到int类型的转换，这个过程叫做拆箱//jdk5增加了自动拆箱和装箱功能（编译器预处理）:Integer i = 100;//自动装箱int ia = i;//自动拆箱 笔试考点： 在Integer类部提供了自动装箱池技术，将**-128~127间的整数已经装箱完毕**，当使用该范围整数时直接取池中的对象即可，从而提高效率。 Integer类加载的时候，自己有一个静态的空间立即加载Integer类型的数组，存储256个Integer对象（-128 ~ 127），当使用该范围整数时，直接取静态区中找对应的对象；如果我们用的对象范围会帮我们创建一个新的Integer对象。 1234567891011121314151617Integer it1 = 128;Integer it2 = 128;Integer it3 = new Integer(128);Integer it4 = new Integer(128);System.out.println(it1.equals(it2));//比较内容 trueSystem.out.println(it1 == it2);//比较地址 falseSystem.out.println(it3.equals(it4));//比较内容 trueSystem.out.println(it3 == it4);//比较地址 falseInteger it5 = 127;Integer it6 = 127;Integer it7 = new Integer(127);Integer it8 = new Integer(127);System.out.println(it5.equals(it6));//比较内容 trueSystem.out.println(it5 == it6);//比较地址 true, 自动装箱池范围-128~127。System.out.println(it7.equals(it8));//比较内容 trueSystem.out.println(it7 == it8);//比较地址 false 3. 数学处理类 java.lang.Math构造方法是私有的，我们不能直接调用创建对象；由于Math中提供的属性及方法都是static 不需要创建对象。 常用的方法 返回值类型 Math.abs() 返回给定数字的绝对值(参数 int long float double) Math.ceil() double 向上取整 Math.floor() double 向下取整 Math.rint() double 临近的整数 如果两边距离一样 则返回偶数 Math.round() int 四舍五入的整数 Math.max(a,b)/min(a,b) (参数int long float double) Math.pow(a,b) double a的b次方 (参数double 返回值double) Math.sqrt(double a) 获取给定参数的平方根 Math.random() double 随机产生一个[0.0–1.0) 0-9之间的随机整数：int value = (int)*(Math.random()10); Math.random()计算小数的时候精确程度可能有些损失 3.1 Random类 java.util.Random，在java.util包中的类，需要import导入，没有任何继承关系 默认继承Object类 常用的方法 Random r = new Random(); r.nextInt(); 随机产生 int取值范围的整数 有正有负(-2^31~2^31-1即正负21亿之间) r.nextInt(int bound); 随机产生一个[0–bound)整数；注意bound必须为正数，否则会出现如下的运行时异常：IllegalArgumentException r.nextFloat() 随机产生一个 [0.0—1.0) r.nextBoolean() 随机产生一个boolean值 true false 3.2 UUID类 java.util.UUID，在java.util包中的类，需要import导入，没有任何继承关系 默认继承Object类 只有有参构造方法，我们通常不会创建对象 UUID uuid = UUID.randomUUID();//通常用于数据库表格主键 primary key 产生一个32位的随机元素 每一个位置是一个16进制的数字 3.3 BigDecimal java.math.BigDecimal类处理大浮点数，需要import导入，继承自Number Java浮点数据类型(float和double)在运算时会有舍入误差，如果希望得到精确运算结果，可以使用java.math.BigDecimal类。 提供的构造方法全部都是带参数的 通常利用带String参数的构造方法创建这个类的对象：BigDecimal bi = new BigDecimal(“1.23”); BigDecimal类的常用方法 BigDecimal(String val) 根据参数指定的字符串来构造对象 BigDecimal setScale(int newScale, RoundingMode roundingMode) 两个参数前面是保留小数点之后的位数，后面参数是设置的模式(向上取整或向下等) BigDecimal add(BigDecimal augend) 用于实现加法运算 BigDecimal subtract(BigDecimal subtrahend) 用于实现减法运算 BigDecimal multiply(BigDecimal multiplicand) 用于实现乘法运算 BigDecimal divide(BigDecimal divisor) 用于实现除法运算，也可传入更多参数设置保留小数点位数和取值模式 123456BigDecimal d3 = new BigDecimal(&quot;3.0&quot;);BigDecimal d4 = new BigDecimal(&quot;2.9&quot;);System.out.println(d3.add(d4));//加：5.9System.out.println(d3.subtract(d4));//减：0.1System.out.println(d3.multiply(d4));//乘：8.70System.out.println(d3.divide(d4, 8, BigDecimal.ROUND_HALF_UP));//除：1.03448276 对于divide方法，通常需要制定精度和舍入模式，否则当遇到无限小数时，除法会一直进行下去直至抛出异常。 3.4 BigInteger java.math.BigInteger类处理大整数，需要import导入，继承自Number java提供的整数类型(int\\long)的存储范围有限，当需要进行很大整数运算时可以使用java.math.BigInteger类，理论上其储值范围只受内存容量限制。 如何创建对象，提供的构造方法全部都是带参数的 通常利用带String参数的构造方法创建这个类的对象：BigInteger bi = new BigInteger(“123”); 和BigDecimal类似，BigInteger也提供add()、substract()、multiply()、divide()等方法。 3.5 DecimalFormat类 所属的包 java.text，import导入才能使用 通过带String参数的构造方法创建一个格式化对象(0:未满会补齐，#：未满不补） 123456789101112 //调用format方法将一个小数格式化成一个字符串DecimalFormat df = new DecimalFormat(&quot;000.000&quot;);System.out.println(df.format(12.45)); //012.450System.out.println(df.format(12345.6789)); //12345.679DecimalFormat df2 = new DecimalFormat(&quot;###.###&quot;);System.out.println(df2.format(12.45)); //12.45System.out.println(df2.format(12345.6789)); //12345.679DecimalFormat df3 = new DecimalFormat(&quot;000.###&quot;);System.out.println(df3.format(12.45)); //012.45System.out.println(df3.format(12345.6789)); //12345.679 4. Scanner类和System类4.1 Scanner类 所属的包java.util包 需要import导包 通过一个带输入流的构造方法创建对象 常用方法 nextInt() nextFloat() next() nextLine() 4.1 System类 所属的包java.lang包 不需要导入 不需要创建对象 通过类名就可以访问 有三个属性及若干的方法 三个属性out in err 方法：gc() exit(0); currentTimeMillis()获取系统当前时间毫秒; 5. 日期类5.1 Date类 java.util.Date类表示特定的瞬间，精确到毫秒。 通常使用无参数的构造方法，或者带long构造方法 Date类中常用的方法 before(); after(); setTime() getTime();—–&gt;long compareTo(); //-1 1 0 Date类大多数用于进行时间分量计算的方法已经被Calender取代。 1234Date date = new Date();//当前日期信息 //Date类重写了toString方法，输出格式如：Sun Jan 06 11:52:55 CST 2019long time = date.getTime();//1970年1月1日距今毫秒数。date.setTime(time + 24\\*60\\*60\\*1000);//通过毫秒数设置时间 5.2 SimpleDateFormat类 java.text.SimpleDateFormat类主要用于实现日期和文本类型之间的转换。是DateFormat(抽象类)的子类 其构造方法 SimpleDateFormat(String pattern) 12345678Date date = new Date();SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);String dateStr = sdf.format(date);// format用于将日期按指定格式转换为字符串String str = &quot;2013-01-06&quot;;SimpleDateFormat sdf2 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);Date date2 = sdf2.parse(str);//如果字符串格式不匹配将抛出异常 常用格式字符串 含义 示例 y 年 yyyy年——2013年；yy——13年 M 月 MM月——01月；M月——1月 d 日 dd日——01日；d日——1日 H 小时(24) HH:mm:ss—12:46:33 h 小时(12) hh(a):mm:ss—12(下午):47:48 m 分钟 – s 秒 – 5.3 Calendar类 java.util.Calendar类是一个抽象类,主要用于取代Date类中过时的方法来描述年月日时分秒信息。 有构造方法，用protected修饰的，通常访问不到，通常会调用默认的getInstance(); 通常使用Calendar的静态方法getInstance获得Calendar对象；getInstance方法将根据系统地域信息返回不同的Calendar类的实现 123Calendar c1 = Calendar.getInstance();c1.set(2008,9-1,20,8,8,8);System.out.println(c1.getTime()); 常用方法 after() before() setTime() getTime()—-&gt;Date getTimeInMillis()—-time getTimeZone()—TimeZone Calendar里面包含一个date属性 可以操作date的某一个局部信息 set get calendar.set(Calendar.YEAR,2015); int year = calendar.get(Calendar.YEAR); TimeZone java.util包 可以通过calendar对象.getTimeZone()获取 或 TimeZone.getDefault(); 常用方法 tz.getID() —-&gt; Asia/Shanghai tz.getDisplayName() —-&gt; 中国标准时间 6. String类6.1 基本概念 String类 —&gt; 引用类型 —&gt; java.lang包 没有任何继承关系，实现三个接口Serializable, CharSequence, Comparable java.lang.String类用于描述字符串数据，java程序中所有的字符串字面值都可以使用String类的实例(对象)加以描述，如”abc”等，任何一个字符对应2字节定长编码。 String类由final关键字修饰表示该类不能被继承，该类描述的字符串内容是常量，一旦创建无法更改，因此可以被共享。对字符串重新赋值不是改变其内容，而是改变引用的指向。 12345678910//如何构建对象String str1 = &quot;abc&quot;; //直接将字符串常量赋值给str (字符串常量池)String str2 = new String();//无参数构造方法创建空的对象String str3 = new String(&quot;abc&quot;);//带string参数的构造方法创建对象byte[] bArr = &#123;97, 98, 99, 100, 101&#125;;//a:97，b:98，c:99，d:100String str4 = new String(bArr);//将数组中的每一个元素转化成对应的char 组合成Stringchar[] cArr = &#123;&#x27;h&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;l&#x27;, &#x27;o&#x27;&#125;;String str5 = new String(cArr);//将数组中的每一个char元素拼接成最终的StringString str6 = String(char[], index, count);//使用char数组中下标从index位置开始的count个字符来构造对象String str7 = String(byte[], index, length);//使用byte数组下标从index位置开始length个字节来构造对象 6.2 字符串常量池 由于String类型对象描述的字符串内容是个常量，若多个相同的内容单独存储会造成时间和空间的浪费。 出于性能考虑，Java虚拟机(JVM)将字符串字面量对象缓存在常量池中；对于重复出现的字符串直接量，JVM会首先在缓存池中查找，如果存在即返回该对象。 12345678910String str1 = &quot;Hello&quot;;String str2 = &quot;Hello&quot;;String str3 = new String(&quot;Hello&quot;);System.out.println(str1.equals(str2));//比较内容 trueSystem.out.println(str1==str2);//比较地址 true，不会重新创建System.out.println(str1.equals(str3));//比较内容 trueSystem.out.println(str1==str3);//比较地址 false，使用new会重新创建新的String对象 //1.下面的代码中创建了几个对象并分别存放在什么位置？String s1 = &quot;hello&quot;; //1个对象，常量池。String s2 = new String(&quot;world&quot;); //2个对象，1个在常量池，1个new后在堆区(内容为常量池里的副本) 6.3 String类常用方法 第一梯队(重写): equals hashCode compareTo toString 第二梯队(常用):charAt()，codePointAt()，indexOf()，lastIndexOf()，substring()，split()，replace()，length()，concat()，contains()， trim()，getBytes()， toCharArray()，matches()。 第三梯队(一般):toUpperCase()，toLowerCase()，startsWith()，endsWith()，isEmpty()。 重写了equals(obj)，hashCode()，toString()方法，compareTo(str)方法实现自Comparable接口 boolean = equals(Object obj); 继承自Object类中的方法，重写后改变了规则，比较字符串中的字面值（==与equals()区别）; int = hashCode(); 继承自Object类中的方法，重写了：31*h+和… int = compareTo(); 实现自Comparable接口，实现方法：结果按照字典排布(unicode编码)顺序，按照两个字符串的长度较小的那个(次数)来进行循环，若每次的字符不一致 则直接返回code之差，若比较之后都一致 则直接返回长度之差 String = toString() Object类中返回类名@hashCode(16进制形式) String类重写后返回的是String对象的字面值 忽略大小写比较：equalsIgnoreCase(), compareToIgnoreCase(); String类的成员方法 char charAt(int index) 返回字符串指定位置 int codePointAt(int index) “abc”0–&gt;97，返回给定index对应位置的那个char所对应的code码 String concat(String) 将给定的字符串拼接在当前字符串之后 int length() 返回字符串序列的长度 注意：区别数组的length是属性，String的length()是方法，集合是size()方法 123456789101112131415String str6 = new String(&quot;hello&quot;);System.out.println(&quot;下标为0的字符是：&quot;+str6.charAt(0));// hSystem.out.println(&quot;字符串长度是：&quot;+str6.length());// 5 //将字符串&quot;12345&quot;转换为整数类型String str = new String(&quot;123456&quot;); //方式一：Integer类中的pareseInt方法int ia = Integer.parseInt(str);System.out.println(&quot;转换出来结果是：&quot;+ ia);//123456 //方式二：利用ASCII数值进行转换&#x27;1&#x27;-&#x27;0&#x27;=1，&#x27;2&#x27;-&#x27;0&#x27;=2，...int res = 0;for(int i=0; i&lt;str.length(); i++)&#123; res = res*10 + (str.charAt(i)-&#x27;0&#x27;);&#125;System.out.println(&quot;转换出来结果是：&quot;+ res);//123456 String类的常用基本方法 boolean contains(CharSequence s) 判断当前字符串是否包含参数指定的内容 String toLowerCase() 返回小写形式 String toUpperCase() 返回大写形式 String trim() 返回去掉前后空格的字符串 boolean startsWith(String prefix) 判断是否以参数字符开头 boolean endsWith(String suffix) 判断是否以参数字符结尾 boolean equals(Object anObject) 比较字符串内容是否相等，String类已重写 boolean equalsIgnoreCase(String anotherString) 同上，并且忽略大小写 int indexOf(String str) 返回第一次出现str位置，找不到返回-1 int indexOf(String str, int fromIndex) 同上，从fromIndex开始检索 String substring(int beginIndex, int endIndex) 截取字符串，beginIndex开始，endIndex结束 String substring(int beginIndex) 截取字符串，beginIndex开始到结尾 6.4 正则相关方法 正则表达式本质就是一个字符串，用于对用户输入数据的格式进行验证。 正则相关方法 boolean matches(String regex) 用于判断是否匹配正则表达式规则。 String[] split(String regx) 以正则为分割符，将字符串拆分成字符串数组 String replaceAll(String regex, String replacement) 正则替换 7. StringBuilder类/StringBuffer类7.1 基本概念 java.lang.StringBuilder类和java.lang.StringBuffer类描述的字符串内容是个可以改变的字符串序列。 StringBuffer和StringBuilder继承AbstractStringBuilder间接继承 Object，实现接口Serializable,CharSequence,Appendable StringBuffer/StringBuilder没有compareTo方法 StringBuffer/StringBuilder含有一个String没有的方法 append();拼接 7.2 特性可变字符串，char[] value; 动态扩容 7.3 对象的构建123456 //无参数构造方法 构建一个默认长度16个空间的对象 char[]StringBuilder builder = new StringBuilder(); //利用给定的参数 构建一个自定义长度空间的对象 char[]StringBuilder builder = new StringBuilder(20); //利用带String参数的构造方法 默认数组长度字符串长度+16个StringBuilder builder = new StringBuilder(&quot;abc&quot;); 7.4 StringBuilder中常用的方法 最主要的方法 append() 频繁的拼接字符串的时候使用此方法 提高性能 ensureCapacity(int minimumCapacity) 确保底层数组容量够用 capacity();//字符串底层char[]的容量 length();//字符串有效元素个数(长度) setLength();//设置字符串的有效元素个数 char = charAt(int index); int = codePointAt(int index); String = substring(int start [,int end]);//注意需要接受返回值 看见截取出来的新字符串效果 StringBuilder = delete(int start [,int end]);//StringBuilder类中独有的方法String类没有，将start到end之间的字符串删掉 不用接受返回值就看到效果啦 StringBuilder = deleteCharAt(int index);//String类中没有的方法，将给定index位置的某一个字符删除掉啦 int = indexOf(String str [,int fromIndex]); int = lastIndexOf(String str [,int fromIndex]);//找寻给定的str在字符串中第一次出现的索引位置 带重载 则从某一个位置开始找 insert(int index,value);//将给定的value插入在index位置之上 replace(int start,int end,String str);//将start和end之间的部分替换成str, builder.replace(2,5,”zzt”); setCharAt(int index,char value);//将index位置的字符改成给定的value toString();//将StringBuilder对象 构建成一个string对象 返回 trimToSize();//将数组中无用的容量去掉 变成length长度的数组 7.5 总结 StringBuilder类不一定需要，是为了避免String频繁拼接修改字符串信息的时候才用的，底层数组是可变的，提高了性能； 常用方法 与String类不同的独有方法：append()，insert()，delete()，deleteCharAt()，reverse()； 与String类相同的方法：length()，charAt()，codePointAt()，indexOf()，lastIndexOf()，substring()，replace()；名字相同 用法不一致 不是很常用的方法：ensureCapacity()，capacity()，setLength()，trimToSize()，setCharAt(); String家族笔试中经常容易考察的知识点 String所属的包 继承关系 实现接口 java.lang 继承Object 接口Serializable,CharSequence,Comparable String构建方式 常量 构造方法 String对象内存结构 字符串常量区 new堆内存对象 == equals()区别 “a”+”b”+”c” String不可变特性 长度及内容 String中的常用方法 concat(); toUpperCase(); String和StringBuilder区别 | String和StringBuffer区别 String不可变字符串 JDK1.0 有一个接口Comparable 不可变体现在长度及内容 有一些方法StringBuilder没有 concat compareTo toUpperCase StringBuilder可变字符串 JDK1.5 有一个接口Appendable 可变字符串 没有final修饰 底层可以进行数组扩容 有一些方法String没有 append() insert() delete() reverse() StringBuffer和StringBuilder的不同 它们方法基本相同 StringBuffer早期版本1.0，早期版本，线程同步，安全性比较高，执行效率相对较低 StringBuilder后来的版本1.5，后期版本，线程非同步，安全性比较低，执行效率相对较高 8. Optional类 可能包含或不包含非空值的容器对象。 如果一个值存在， isPresent()将返回true和get()将返回值。 获取字符串长度： 方式1：if(null==str){return 0;}else{return str.length();} 方式2：return Optional.ofNullable(str).map(String::length).orElse(0); 1234567891011121314// 获取两个字符串长度和String str1 = &quot;zhangsan&quot;;String str2 = null;int str1Length = Optional.ofNullable(str1).map(String::length).orElse(0);int str2Length = Optional.ofNullable(str2).map(String::length).orElse(0);System.out.println(str1Length + str2Length);//8，8+0//步骤分解://构建Optional对象Optional&lt;String&gt; op1 = Optional.ofNullable(str1);//将str1的长度的结果构建成Optional对象Optional&lt;Integer&gt; op2 = op1.map(String::length);//如果长度不为空，则获取长度值，否则返回默认值int len = op2.orElse(0);System.out.println(len);//8","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】面向对象","date":"2017-04-10T10:50:47.000Z","path":"article/20170410.html","text":"面向对象是相对于面向过程而言，过程其实就是函数，对象是将函数和属性进行了封装。Java中的面向对象（Object Oriented）是一种新兴的程序设计方法，或者是一种新的程序设计规范(paradigm)，其基本思想是使用对象、类、继承、封装、多态等基本概念来进行程序设计。 类与对象 构造方法和方法重载 this关键字 方法的传递和递归 封装 static关键字 单例设计模式 继承（extends） 方法的重写（Override） 访问控制 包（Package） final关键字 对象的创建过程 多态 抽象类 接口 内部类 回调模式 抽象方法的笔试考点 枚举类（enum） 内存机制问题 1. 类与对象1.1 类的定义1class 类名 &#123;类体&#125; 类名由多个单词组成时，要求每个单词首字母大写 1.2 成员变量的定义1class 类名 &#123; 数据类型 成员变量名=初始值; ... &#125; 成员变量名由多个单词组成时，要求第二个起每个单词首字母大写 1.3 对象的创建1new 类名(); 当一个类定义完毕后使用new关键字创建/构造该类的对象的过程叫做类的实例化。 1.4 引用12类名 引用变量名;Person p = new Person(); //声明person类型的引用p指向Person类型对象 12引用变量名.成员变量名;p.name = &#x27;zhangsan&#x27;; 在JAVA中，使用引用数据类型声明的变量叫做引用变量，简称‘引用’。 使用引用可以记录对象在堆区中存放的内存地址信息，便于下次访问。 除八种基本类型之外，用类名（接口，数组）声明的变量称为引用类型变量，引用类型变量存的某个对象的地址信息，引用的功能在于访问对象。 1.5 成员方法123class 类名 &#123; 返回值类型 成员方法名(形参列表)&#123;方法体;&#125;&#125; 返回值类型：可以是基本数据类型，也可以是引用，当方法不需要返回数据用void 形参列表：数据类型 形参1, 数据类型 形参2, … 2. 构造方法和方法重载2.1 构造方法1class 类名 &#123; 构造方法名(形参列表)&#123;构造方法体;&#125; &#125; 构造方法名与类名相同且没有返回值 当使用new关键字构造对象时，会自动调用构造方法，实现成员变量的初始化工作。 2.2 默认构造方法 当一个类中没有没有自定义任何构造方法时，编译器会提供一个无参的空构造方法，叫做默认/缺省构造方法。 若类中出现自定义构造方法，则编译器不再提供构造方法。 2.3 方法重载（overload）在Java中，方法名相同，参数列表不同的方法构成重载关系。 体现形式：参数个数，参数顺序，参数类型。（与形参变量名和返回值无关，但最好返回值类型相同） 实际意义：调用者只需要记住一个方法名就可以不同的版本，从而实现不同的效果。 3. this关键字在构造方法中出现this时,this代表当前正在构造的对象；在成员方法中出现this,this代表当前正在调用的对象。 使用方式： 当形参变量和成员变量同名时，在方法体中优先使用形参变量，若希望使用成员变量，则需要加上this，即this.变量名 在构造方法的的第一行，可以调用本类中的其他构造方法。 4. 方法的传递和递归4.1 传参 基本数据类型变量作为参数传递时，型参数值改变不会影响实参变量的数值。 引用类型变量作为参数传递时，形参指向内容的改变会影响实参变量指向的内容。 引用数据类型变量作为参数传递时，形参改变指向后再改变指向内容不会影响实参指向的内容。 4.2 递归的调用 递归是指方法体内部调用自身 必须有递归的规律和退出条件 使用递归必须使得问题简单化而不是复杂化 若递归影响到程序的执行性能时，则用递推取代之 5. 封装面向对象的三大特征：封装，继承，多态。 封装基本概念：封装就是对成员变量的数值进行密封包装处理以及合理性判断 封装基本流程： 私有化成员变量(private) 提供公有的get、set方法，并在set方法体中进行合理性判断 在构方法中调用set方法进行合理值的判断 6. static关键字基本概念：通常情况下成员变量隶属于对象层级，也就是每创建一个对象就会申请一块独立的内存空间来存储就会造成内存空间的浪费。 为了解决上诉问题，Java中使用static关键字修饰该成员变量表达静态的含义，此时成员变量提升到类层级，所有对象共享，随着类的加载准备就绪，与对象创建再无关。 static可以修饰：修饰属性 修饰方法 修饰块 修饰类(内部类) 特点 静态元素在类加载时就初始化，此时还没创建对象，可以通过类名直接访问 静态元素存储在静态元素区，每个类有一个自己的区域，与别的类不冲突 静态元素只加载一次，全部类对象及类本身共享 静态元素区Carbage Collection无法管理，可以粗暴理解为常驻内存 非静态成员和静态成员都可以访问静态成员 静态成员不可以访问非静态成员 静态元素中不可出现this或super关键字，静态元素属于类的 7. 单例设计模式基本概念：当一个类有且只能对外提供一个对象时，这样的类就叫作单例类，而设计单例类的思想和模式，叫做单例设计模式。 12345678910/** * 编程实现Singleton类的封装 */public class Singleton&#123; private static Singleton sin = new Singleton();//2.提供本类的引用指向本类的对象 private Singleton()&#123;&#125; //1.私有化构造方法 public static Singleton getInstance()&#123;//3.提供公有的get方法将上述成员变量的数值返回出去 return sin; &#125;&#125; 实现流程： 私有化构造方法（private） 提供本类类型的引用指向本类类型对象（private static） 提供公有的get方法将上述对象return出去（public static） 实现方式：饿汉式和懒汉式，开发中推荐饿汉式。 8. 继承（extends） 继承就是子类复用父类的代码，关键字extends表示类和类的继承关系 使用继承可以提高代码复用性、扩展性、以及可维护性。 子类不能继承父类的构造方法和私有方法，私有成员变量可以继承但不能直接使用。 无论使用何种方式构造方式构造子类的对象都会自动调用父类的无参构造方法来初始化从父类中继承下来的成员变量，相当于在构造方法的第一行增加super()的效果。 使用继承必须满足逻辑关系：子类 is a 父类，不能滥用继承。 在Java中只能支持单继承，也就是一个一个子类只能有一个父类，但一个父类可以有多个子类。 1234567891011class Cricle extends Shape&#123; int r; Cricle()&#123;&#125; //编译器会加入无参的调用 super()。 Cricle(int x, int y, int r)&#123; super(x, y); //通过super关键字调用父类的构造方法。 setR(r); &#125; public void setR(int r)&#123; this.r = r; &#125;&#125; 9. 方法的重写（Override）概念：从父类继承下来的方法不满足子类的需求时，就需要子类中重新写一个和父类一样的方法，覆盖从父类中继承下来的版本，该方法就叫方法的重写。原则： 要求方法名相同，参数列表相同，返回值类型相同；jdk1.5开始返回子类类型。 要求访问权限不能变小，可以相同或变大 重写的方法不能抛出更大的异常 10. 访问控制 public修饰的内容可以在任意位置使用，private修饰的内容只能在本类中使用， 通常情况下，成员变量都使用private修饰，成员方法都使用pubic修饰 访问控制符 访问权限 本类内部 本类中的包 子类 其他包 public 共有的 Y Y Y Y protected 保护的 Y Y Y N 不写 默认的 Y Y N N private 私有的 Y N N N 11. 包（Package）为了解决命名冲突问题，便于文件的管理 1234package 包名；package 包名1.包名2.包名3...包名n;/* 指定包名时应按照一定的规范，eg: 公司域名反写.项目名称.模块名称.类名 */org.apache.commons.lang.StringUtil; 12. final关键字 final关键字修饰类体现该类不能被继承（防止滥用继承）。 final关键字修饰方法体现在该方法不能被重新，但可以被继承（防止不经意间造成的方法重写）。 final关键字修饰成员变量体现在改成员变量必须初始化且不能更改（防止不经意间造成的数据更改）。 扩展：在开发中很少单独使用static或者final单独修饰成员变量，而是使用**public static final**共同修饰成员变量来表达常量的含义，而常量的命名规范是：所有字母大写，不同单词之间下划线连接。 13. 对象的创建过程 单个对象的创建过程 main方法是程序的入口，若创建对象时没有指定初始值则采用默认初始化方式处理； 若声明成员变量时进行了显示初始化操作，则最终采用显示初始化的初始值处理； 执行构造块中的代码可以对成员变量进行赋值； 执行构造方法体中的代码可以对成员变量进行再次赋值； 此时对象构造完毕，继续向下执行后续的代码； 子类对象的创建过程 main方法是程序的入口，先加载父类的的代码再加载子类的代码； 先执行父类静态代码块，再执行子类的静态代码块； 先执行父类的构造块，再执行父类的构造方法体，此时包含的父类对象构造完毕； 先执行子类的构造块，再执行子类的构造方法体，此时子类对象构造完毕，继续向下执行后续代码。 14. 多态 语法：父类的引用指向子类的对象 123父类类型 引用变量名 = new 子类类型();Person pw = new Worker();pw.show();//再编译阶段调用Person的show()方法，在运行阶段调用Worker的show()方法。 多态的效果： 父类的引用可以直接调用父类独有的方法。 父类的引用不可以直接调用子类独有的方法。 对于父类子类都有的非静态方法来说，编译阶段调用父类的，运行阶段调用子类重写后的。 对于父类子类都有的静态方法来说，只调用父类的。 多态的实际意义：屏蔽不同子类的差异性实现通用的编程，从而带来不同的结果。 多态的表现形式 多态的前提要有继承的关系 使用父类引用指向子类对象 Person p = new Teacher();//向上转型 该引用只能调用父类中定义的属性/方法 执行结果，如果调用属性:执行父类的，如果调用方法:看子类是否重写 若想要调用子类独有的成员，将身份还原回去(向下转型/造型)，若需要转换的类型与真实对象类型不匹配，会产生一个运行时异常ClassCastException 引用数据类型之间的转换 转换必须发生在父子类之间，否则编译报错。 自动类型转换：小到大，子类型向父类型的转换，eg:Person pw = new Worker();。 强制类型转换：大到小，父类型向子类型转换，eg:((Worker) pw).getSalary();//将父类引用强制转换子类型调用子类方法。 为了避免类型转换异常，对象进行强制类型转换时应该用instanceof判断引用变量真正指向的对象是否是要转换的目标类型。 123456/*语法格式：*/ 对象 instanceof 类型 //返回布尔值if(pw instanceof Teacher)&#123; Teacher t = (Teacher) pw;&#125;else&#123; System.out.println(&quot;转换会有异常&quot;);&#125; 多态的使用场合： 1234567// 通过方法的参数传递形成多态。public static void draw(Shape s)&#123;&#125;TestShape.draw(new Rect(1,2,3,4));// 在方法体中直接使用多态的语法格式。TestAbstrat ta = new SubTestAbstract();ta.show(); 15. 抽象类 基本概念 用abstract关键字修饰的类称为抽象类。 抽象类不能实例化，抽象类的意义在于被继承。 抽象类为其子类“抽象”出了公共部分，通常也定义了子类所必须具体实现的抽象方法。 抽象方法：指不能具体实现的方法，没有方法体并使用abstract修饰。 12345public abstract class Shape&#123; //一个类若定义了抽象方法，则必须以abstract关键字声明为抽象类 private int x; private int y; public abstract boolean contains(int x, int y);//用abstract修饰的方法，称之为抽象方法，没有方法体&#125; 注意： 抽象类中可以有成员变量，成员方法，以及构造方法。 抽象类中可以没有抽象方法，也可以有抽象方法。 具有抽象方法的类必须是抽象类，因此其真正意义的抽象类应该是有抽象方法，并且使用abstract修饰。 子类必须实现抽象方法（不同子类可能有不同实现），否则改子类也变抽象。 抽象类对子类具有强制性和规范性，因此叫做模板设计模式。 推荐使用多态的语法格式实现抽象类，若需要更换子类时，该方法中只需要将new关键字后面的类型名称修改而其他位置无需改变就可以立即生效，从而提高了代码的维护性和扩展性。 多态实现抽象类的缺点：若希望调用子类独有的方法时，则需要强制类型转换。 16. 接口 基本概念：接口可以看成是特殊的抽象类。即只包含抽象方法的抽象类。通过interface关键字定义。 1234interface Runner &#123; //-通过interface关键字定义接口 public static final int SEF_SPEED=100;//-接口中不能定义成员变量，只能定义常量 public void run();//-接口中只可以定义没有实现的方法（可以省略public abstract）&#125; 一个类可以通过implements关键字实现接口，一个类可以实现多个接口，并且该类需要实现这些接口中定义的所有方法。 12345678910class American implements Runner,... &#123; //与继承不同，可以实现多个接口 @Override public void run()&#123;//该类需要实现接口中定义的所有方法 System.out.println(&quot;run...&quot;); &#125; public static void main(String[] args) &#123; Runner ra = new American();//接口作为一种类型声明，并且声明的变量可以引用实现类的对象 ra.run();//通过该变量可以调用该接口定义的方法 &#125;&#125; 一个接口可以通过extends关键字继承另一个接口，子接口继承了父接口所有的方法。 1interface Hunter extends Runner&#123;...&#125; 类与接口的关系 类和类使用extends继承，仅支持单继承。 接口和接口使用extends继承，支持多继承。 类使用implements实现接口，支持多实现。 抽象类与接口的关系（笔试题） 定义抽象类:abstract class，而定义接口:interface； 类继承抽象类:extends单继承，而类实现接口:implements多实现； 抽象类可以有构造方法，而接口不能有构造方法； 抽象类可以有成员变量，而接口只能有常量； 抽象类可以有成员方法，而接口只能有抽象方法； 抽象类中增加方法子类可以不用重写，而接口中增加方法子类必须重写； 从jdk1.8开始允许接口中有非抽象方法，但需要default关键字修饰。 17. 内部类 内部类指的是在Java中可以将一个类定义在另一个类定义在另一个类的内部 内部类定义在 类的内部 ，与类成员层次一致 内部类定义在 方法/块内部（与类成员相差一个层次，方法的局部变量一个层次） 成员内部类：将一个类直接定义在类的里面，作为成员，与属性或方法层次一致 局部内部类：将一个类定义在方法/块里面，作为成员的内部结构，与临时的局部变量一个层次 匿名内部类：成员匿名内部类，局部匿名内部类 静态内部类：成员静态内部类 17.1 *成员内部类 将一个类直接定义在类的里面，作为成员，与属性或方法层次一致 成员内部类可以与正常类一样 使用不同的修饰符来修饰 好处1.省略了一个.java文件 好处2.成员内部类中可以访问外部类的所有成员 包括私有的 若想要在内部类中通过对象.调用外部类成员 外部类.this.外部类成员; 内部类存在后 源代码进行编译 产生一个字节码 Demo$InnerDemo.class 17.2 局部内部类 将一个类定义在方法/块里面，作为成员的内部结构，与临时的局部变量一个层次 局部内部类像是一个局部的变量一样，不能用public protected private及static 只能用abstract或final 局部内部类命名规则Demo$1InnerTestMethod Demo$2InnerTestMethod 局部内部类使用的变量只能是final修饰 17.3 *匿名内部类将类直接定义在类中 或者类成员中 成员匿名内部类 局部匿名内部类匿名内部类没有类的所有结构(名字 修饰符) 只有类体通常会在抽象类或接口创建的后面使用，当然具体的类也可以有匿名子类匿名类内部没有构造方法，也不能用任何修饰符来修饰 当接口类型的引用作为方法的形参时，实参的传递方式有两种： 自定义类实现接口并重写抽象方法，然后创建该类的对象作为实参传递。 直接使用匿名内部类的语法格式得到接口类型的引用，再作为实参传递。 12345678910111213141516171819202122232425262728293031public interface A &#123; public abstract void show();&#125;//-方式1：自定义类实现接口并重写抽象方法，然后创建该类的对象作为实参传递public class SubA implements A &#123; @Override public void show() &#123; System.out.println(&quot;这里自定义类实现接口并重写抽象方法！&quot;); &#125;&#125;//测试类public class ATest &#123; public static void test(A a) &#123; a.show(); &#125; public static void main(String[] args) &#123; //ATest.test(new A());//报错，A是接口，不能new对象 //-方式1：接口实现类的对象作为实参传递 ATest.test(new ASub());//接口类型引用指向实现类的对象，形成了多态。 //-方式2：匿名内部类 // 接口/父类类型 引用变量名 = new 接口/父类类型() &#123;方法的重写&#125;; A ta = new A() &#123; @Override public void show() &#123; System.out.println(&quot;这里是匿名内部类&quot;); &#125; &#125;; ATest.test(ta);//得到接口类型的引用，再作为实参传递 &#125;&#125; 匿名内部类定义：如果在一段程序需要创建一个类的对象（通常这个类需要实现某个接口或继承某个类），而且对象创建后这个类的价值就不存在了，这个类不必命名，称之为匿名内部类。 语法格式：接口/父类类型 引用变量名 = new 接口/父类类型() &#123;匿名类类体，这里重写方法&#125;;。 1SuperType obj = new SuperType(...)&#123; ... &#125;; 17.4 静态内部类 成员静态内部类 不需要外部类对象，通过正常的方式直接创建内部类 静态元素不能访问非静态成员(自己类和外部类) 18. 回调模式回调模式是指：如果一个方法的参数是接口类型，则在调用该方法时，需要创建并传递一个实现此接口的对象；而该方法在运行时会调用到参数对象中所实现的方法 123456789101112131415interface Action&#123; public void doSth();&#125;//repeat方法需要一个Action接口类型参数，让其doSth方法重复执行n次public static void repeat(int n, Action ac)&#123; for(int i=0; i&lt;n; i++)&#123; ac.doSth();&#125;&#125;//此处的语义可解释为：通过接口回调传递了一个方法给repeat,让repeat将其执行5次。public static void main(String[] args)&#123; repeat(5, new Action()&#123;//通过匿名内部类传递参数 public void doSth()&#123; System.out.println(&quot;Hello&quot;) &#125; &#125;);&#125; 19. 抽象方法的笔试考点 abstract与哪些关键字不能共存： final关键字；因为final关键字修饰的类不能被继承，方法不能被重写，而abstract关键字修饰的类继承后，该类的方法需要重写，相互冲突。 static关键字；因为static能被实例化可直接调用，而abstract不能被实例化，相互冲突。 private关键字；因为private修饰的私有方法不能被继承，就不能重写，而abstract方法需要重写。 20. 枚举类（enum） 一个类中的对象 认为个数是有限且固定的 可以将每一个对象一一列举出来 创建枚举类型要使用 enum 关键字，隐含了所创建的类型都是 java.lang.Enum 类的子类（java.lang.Enum 是一个抽象类）。枚举类型符合通用模式 Class Enum&lt;E extends Enum&gt;，而 E 表示枚举类型的名称。枚举类型的每一个值都将映射到 protected Enum(String name, int ordinal) 构造函数中，在这里，每个值的名称都被转换成一个字符串，并且序数设置表示了此设置被创建的顺序。 我们自己定义的每一个enum类型 都会默认继承Enum 间接继承Object Enum类型，有两个属性 name—–&gt;枚举对象的名字，name()获取name属性 ordinal—&gt;枚举对象在类中罗列的顺序 类似index 也从0开始 ordinal()获取序号 一些常用的方法 valueOf() 通过给定的name获取对应的枚举对象 values() 获取全部的枚举对象 —&gt; 返回一个数组 Day[] compareTo() 可以比较两个枚举对象 int toString() 由于这个方法没有final修饰 可以覆盖(重写) switch内部判断枚举的应用 我们也可以在enum中描述自己的一些属性或方法 必须在enum类中第一行 描述一下枚举的样子 最后需要分号结束; 可以定义自己的属性 类创建的过程中 帮我们创建枚举类型的对象 需要给枚举类型提供对应样子的构造方法 构造方法只能private修饰 可以重载 1234567891011121314151617181920public enum Day&#123; //描述了七个当前类的对象 monday(&quot;星期一&quot;,1),tuesday(&quot;星期二&quot;,2),wednesday,thursday,friday,saturday,sunday; private String name; private int index; private Day()&#123;&#125; private Day(String name,int index)&#123; this.name=name; this.index=index; &#125; public String getName()&#123; return this.name; &#125; public void setName(String name)&#123; this.name=name; &#125;&#125; 21. 内存机制问题 类创建在哪儿 对象创建在哪里 继承关系 静态成员 方法执行 栈内存—&gt;Person p = new Person();—-&gt;堆内存 方法区—类模板 栈内存—-变量空间,方法临时执行空间（从创建开始执行完毕,立即回收） 堆内存—-new申请对象空间（垃圾回收器GC,对象空间没有任何引用指向视为垃圾） 方法区—-常量 类模板 静态成员（有且只有一份,不回收） Runtime类(是单例模式)之中提供了几个管理内存的方法 maxMemory totalMemory freeMemory 栈内存溢出错误StackOverflowError 堆内存溢出错误OutOfMemoryError Object类中有一个finalize方法 如果重写也能看见对象回收的效果 GC系统提供的一个线程 回收算法","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Java教程】语法入门","date":"2017-04-07T09:33:54.000Z","path":"article/20170407.html","text":"这是Java教程的第一篇，梳理Java基础知识是学习其他专业知识的第一步阶梯；要想精通编程必须通过大量的编程训练，在实践中掌握编程知识，培养编程能力，并逐步理解和掌握程序设计的思想和方法。 数据类型 变量与常量 数据类型之间的转换 运算符 循环 数组 函数 1. 数据类型1.1 分为两大类： 基本数据类型：byte，short，int，long，float，double，boolean，char 引用数据类型：数组，类(抽象类)class，接口interface，枚举enum，标注@interface 注：单个字节表示8位二进制位，最左边表示符号位（0：正，1：负）。 1.2 整数类型（byte，short，int，long） byte： 1个字节，范围-2^7~2^7-1，即-128~127。 short：2个字节，范围-2^15~2^15-1，即-32768~32767。 int: 4个字节，范围-2^31~2^31-1，即正负21亿之间。 long： 8个字节，范围-2^63~2^63-1，比int更大。如果要表示long直接量，需要L或l结尾。 1.3 浮点类型（float，double） float： 4个字节，单精度浮点数，取到小数点后7位有效数字。如果要表示float直接量，需要F或f结尾 double：8个字节，双精度浮点数，取到小数点后15位有效数字。 扩展：浮点运算有时会有误差，为了实现精确运算可使用java.math.BigDecimal类型加以描述。 1.4 布尔类型（boolean） boolean：1个字节（未明确规定），值只有true和false。 1.5 字符类型（char） char：2个字节，表示单个字符的数据类型。事实是一个16位无符号整数，值是对应字符的编码，如：’a’,’1’,’中’ 等。 开发中很少用到char类型，而使用String类型描述的多个字符组成的字符串，使用双引号””引起来。 需记住的ASII码字符：’a’:97，’A’:65，’0’:48，空格:32，换行符:10 常用转义符(逃逸字符)：\\t:制表符，\\n:换行，\\&quot;，\\&#39;，\\\\:反斜杠本身，\\b:回退一格，\\r:回车 字符类型计算 一个字符加一个数字，得到Unicode码表中那个数之后的字符 两个字符相减得到它们在表中的距离 char也可以和int之间相互赋值 2. 变量与常量2.1 常量 常量是一个值，在程序运行的过程中不能再次发生改变 基本类型的值都可以认为是常量 4 3.4 ‘a’ true，String类(引用数据类型)值”abc”视为常量 常量存储在常量缓冲区(常量池)中，有且只有一份,常量池中的值默认空间大小 32bit–int 64bit–double 2.2 变量 变量是在栈内存中开辟的一块内存空间(小容器),程序执行过程中可以改变的 变量空间在创建(变量声明)的时候,必须指定数据类型,变量空间的名字 变量空间 里面只能存储一个内容(值/引用) 空间内的内容的类型与定义时一致 内容可以改变 内存结构与执行过程：类的定义，编译，加载 空间各个区，变量 赋值 2.3 标识符（变量）命名规则 必须字母，数字，下划线以及美元$等组成，且首位非数字。 不能使用Java语言中的关键字，如class，static，void，int等。 区分大小写，长度无限制，但不能过长，尽量见名知意。 2.3 命名的规约 类名字：首字母大写，如果两个以上的单词，所有首字母都大写 变量名：首字母小写，如果两个以上的单词，之后的首字母大写 遵循驼峰式命名规约，所有名字都需要见名知义，为了增强程序的可读性 3. 数据类型之间的转换3.1 基本数据类型之间转换 自动类型转换：从小类型到大类型自动转换 12byte --&gt; short --&gt; int --&gt; long --&gt; float --&gt; double char -----^ 强制类型转换：需在被转换数据前加上类型，会造成精度损失或者溢出 12long big &#x3D; 1024L\\*1024\\*1024;int i &#x3D; (int)big; 3.2其他数据类型之间转换 同种大数据类型之间才能发生转换 类型转换之前，保证大前提：同种大数据类型一致 基本–基本 可以直接转换(自动 强制) 引用–引用 可以直接转化(自动 强制 – 上转型 下转型) 基本–引用 不可以直接进行转化(间接-桥梁-包装类) 保证大数据类型一致的前提下(都是基本类型) 小数据类型一致:整型–&gt;整型 / 浮点–&gt;浮点 比较内存空间的大小 大数据类型空间可以直接接受小数据类型的值(自动转换) 小数据类型空间不可以直接接受大数据类型的值(强制类型转换) 强制类型转换,写法好用,但是需要注意取值范围的问题,丢失一部分数据1234byte a &#x3D; 1; int b &#x3D; a;&#x2F;&#x2F;自动直接转化就可以int a &#x3D; 1; byte b &#x3D; (byte)a;&#x2F;&#x2F;需要强制类型转换float x &#x3D; 3.4F; double y &#x3D; x;&#x2F;&#x2F;自动直接转化double x &#x3D; 3.4; float y &#x3D; (float)x;&#x2F;&#x2F;强制转换 小数据类型不一致:整型–&gt;浮点 比较精确程度 浮点型的精确程度更高 任何一个浮点型空间都可以直接接受一个整型的值 反之需要进行强制类型转换(强行将小数点之后的部分去掉,只保留整数)12int a &#x3D; 1; float b &#x3D; a;&#x2F;&#x2F;自动直接转化float a &#x3D;1.0F; int b &#x3D; (int)a;&#x2F;&#x2F;强制类型转换 整型–&gt;字符 每一个字符都对应这一个Unicode码 a–9712char x &#x3D; &#39;a&#39;; int y &#x3D; x;&#x2F;&#x2F;自动转化 y--97int x &#x3D; 97; char y &#x3D; (char)x;&#x2F;&#x2F;强制的转化 布尔类型很特殊 不能与其他基本类型之间发生转化 4. 运算符 算术运算符： +，-，*，/，% 关系运算符： &gt;，&lt;，&gt;=，&lt;=，==，!= 自增减运算符： ++，-- 逻辑运算符： &amp;&amp;，||，！（短路特性：逻辑与&amp;&amp;运算，若第一个条件为假,跳过第二个条件；逻辑或||运算，若第一个条件为真，跳过第二个条件） 三目运算符： 条件 ? 表达式1 ： 表达式2 赋值运算符：=，+=，-=，*=，= 5. 循环 while循环和for循环完全可以互换。 while循环主要用于明确循环条件，但不明确循环次数的场合 for循环主要用于明确次数或范围的场合 while(true) 等价于 for(;;)，表示无限循环。 6. 数组6.1 一维数组 数组类型[] 数组名 = new 数据类型[长度] ：动态方式 12数组类型[] 数组名 = new 数据类型[长度] //动态方式int[] arr = new int[5]; 123数组类型[] 数组名 = &#123;初始值1, 初始值1, 初始值1, ...&#125; //静态方式int[] arr = &#123;10, 20, 30, 40&#125;;/*特殊方式：*/ int[] arr = new int[]&#123;10, 20, 30&#125;; 6.2 二维数组12数组类型[][] 数组名 = new 数据类型[行数][列数]int[][] arr = new int[5][6]; 12数组类型[][] 数组名 = &#123;&#123;初始值1, 初始值1, 初始值1&#125;,&#123;值2,...&#125; ...&#125;int[] arr = &#123;&#123;10, 20, 30&#125;, &#123;01, 5, 3&#125;, &#123;8, 20, 6&#125;&#125;; 二维数组arr.length表示行数，arr[0].length表示此行的长度 7. 函数 函数是一块代码，接收零个或多个参数，做一件事情，并返回零个或一个值。 123456789public static int sum(int a, int b)&#123; int i; int sum=0; for(i=a; i&lt;=b; i++)&#123; sum += i; &#125; System.out.println(a +&quot;到&quot;+ b +&quot;的和是&quot;+ sum); return sum;&#125; 函数的调用：函数名(参数值); 即使没有参数也需要(),()起到了调用函数的作用，如果有参数，则需要给出正确的数量和顺序 函数的返回：return停止函数的执行，并返回一个值，可以再赋值给变量，传递给另一个函数，甚至可以丢弃，有时候要的是副作用","tags":[{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"},{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"}]},{"title":"【Hexo博客折腾】BlueLake博客主题的详细配置","date":"2016-12-29T03:25:33.000Z","path":"article/20161229.html","text":"2021年2月3日更新：BlueLake主题写了有些年头了，随着Hexo升级到5.X，BlueLake很多配置已经过时，在使用中难免出现问题。 所以，我利用工作之余抽了些时间进行了大改版，主题模板引擎由Jade(Pug)换成了EJS，以landscape为原型进行二次开发；保留BlueLake主题老版本的其他功能，如原生JS实现站内搜索功能，本地分享等；新添加了一些新的功能，如打赏模块，集成新的三方评论等等。 说了这么多不如你亲自体验来的直接，BlueLake主题地址：https://github.com/chaooo/hexo-theme-BlueLake。 在阅读本文之前，假定您已经成功安装了Hexo，并使用 Hexo 提供的命令创建了一个静态博客。Hexo是一个快速、简洁且高效的博客框架。Hexo基于Node.js ，使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 需要特别注意的是Hexo有两个_config.yml配置文件，一份位于站点根目录下，主要包含 Hexo 站点本身的配置，下文中会称为**根_config.yml；另一份位于主题目录下（themes/主题名/_config.yml），这份配置由主题作者提供，主要用于配置主题相关的选项,下文中会称为主题_config.yml**。 1. 安装您可以直接到BlueLake发布页下载，然后解压拷贝到themes目录下，修改配置即可。不过我还是推荐使用GIT来checkout代码，之后也可以通过git pull来快速更新。 1.1 安装主题在根目录下打开终端窗口： git bash1git clone https://github.com/chaooo/hexo-theme-BlueLake.git themes/BlueLake 1.2 安装主题插件在根目录下打开终端窗口： git bash12npm install hexo-generator-feed --savenpm install hexo-generator-sitemap --save 1.3 启用主题打开根_config.yml配置文件，找到theme字段，将其值改为BlueLake(先确认主题文件夹名称是否为BlueLake)。 根_config.yml_config.yml1theme: BlueLake 1.4 验证首先启动 Hexo 本地站点，并开启调试模式： git bash1hexo s --debug 在服务启动的过程，注意观察命令行输出是否有任何异常信息，如果你碰到问题，这些信息将帮助他人更好的定位错误。 当命令行输出中提示出：INFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.此时即可使用浏览器访问 http://localhost:4000，检查站点是否正确运行。 1.5 更新主题今后若主题添加了新功能正是您需要的，您可以直接git pull来更新主题(更新前请先备份主题_config.yml防止被覆盖)。 git bash12cd themes/BlueLakegit pull 2. 配置2.1 配置网站头部显示文字打开根_config.yml，找到： 根_config.yml_config.yml12345title: # 标题，如：秋过冬漫长subtitle: # 副标题，如：没有比脚更长的路，走过去，前面是个天！description: # 网站关键字，如：key, key1, key2, key3keywords:author: title和subtitle分别是网站主标题和副标题，会显示在网站头部； description在网站界面不会显示，内容会加入网站源码的meta标签中，主要用于SEO； keywords在网站界面不会显示，内容会加入网站源码的meta标签中，主要用于SEO； author就填写网站所有者的名字，会在网站底部的Copyright处有所显示。 2.2 设置语言该主题目前有七种语言：简体中文（zh-CN），繁体中文（zh-TW），英语（en），法语（fr-FR），德语（de-DE），韩语 （ko）,西班牙语（es-ES）；例如选用简体中文，在根_config.yml配置如下： 根_config.yml_config.yml1language: zh-CN 2.3 设置菜单打开主题_config.yml，找到： 主题_config.ymlthemes/BlueLake/_config.yml12345678910111213menu: - page: home directory: . icon: fa-home - page: archive directory: archives/ icon: fa-archive # - page: about # directory: about/ # icon: fa-user - page: rss directory: atom.xml icon: fa-rss 主题默认是展示四个菜单，即主页home，归档archive，关于about，订阅RSS；about需要手动添加，若您并不需要，可以直接注释掉。 2.4 添加about页此主题默认page页面是about页的布局，在根目录下打开命令行窗口，生成一个关于我页面： git bash1hexo new page &#x27;about&#x27; 打开主题_config.yml，补全about页面的详细信息： 主题_config.ymlthemes/BlueLake/_config.yml12345678910111213# About pageabout: photo_url: # 头像的链接地址 e.g. http://cdn.chaooo.top/blog/themeauthor.jpg items: - label: email # 个人邮箱 url: ## Your email with mailto: e.g. mailto:zhenggchaoo@gmail.com title: ## Your email e.g. zhenggchaoo@gmail.com - label: github # github url: ## Your github&#x27;url e.g. https://github.com/chaooo title: ## Your github&#x27;name e.g. chaooo - label: weibo # 微博 url: ## Your weibo&#x27;s url e.g. http://weibo.com/zhengchaooo title: ## Your weibo&#x27;s name e.g. 秋过冬漫长 2.5 添加本地搜索本地搜索是用原生JS写的，但还需要HEXO插件创建的JSON数据文件配合。安装插件hexo-generator-json-content来创建JSON数据文件： git bash1npm install hexo-generator-json-content --save 然后在根_config.yml添加配置： 根_config.yml_config.yml123456789101112131415161718jsonContent: meta: false pages: false posts: title: true #文章标题 date: true #发表日期 path: true #路径 text: true #文本字段 raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true #标签 2.6 首页添加文章置顶在根目录下打开命令行窗口安装： git bash12$ npm uninstall hexo-generator-index --save$ npm install hexo-generator-index-pin-top --save 然后在需要置顶的文章的Front-matter中加上top: true即可。 123456---title: BlueLake博客主题的详细配置tags: [hexo,BlueLake]categories: Hexo博客折腾top: true--- 3.其他配置3.1 集成第三方服务目前主题集成多种第三方评论，比如GITALK（申请GitHub Application），Valine评论，畅言云评，Disqus评论、来必力评论、友言评论、网易云跟帖评论等。 配置主题_config.yml： 主题_config.ymlthemes/BlueLake/_config.yml123456789101112131415161718192021222324252627282930313233343536# Gitalk commentgitalk: enable: false owner: ## Your GitHub ID, e.g. username repo: ## The repository to store your comments, make sure you&#x27;re the repo&#x27;s owner, e.g. gitalk.github.io client_id: ## GitHub client ID, e.g. 75752dafe7907a897619 client_secret: ## GitHub client secret, e.g. ec2fb9054972c891289640354993b662f4cccc50 admin: ## Github repo owner and collaborators, only these guys can initialize github issues. language: &#x27;zh-CN&#x27; ## Language pagerDirection: last # Comment sorting direction, available values are last and first.# Valine comment. https://valine.js.orgvaline: enable: false # if you want use valine,please set this value is true appId: # leancloud application app id appKey: # leancloud application app key notify: false # valine mail notify (true/false) https://github.com/xCss/Valine/wiki verify: false # valine verify code (true/false) pageSize: 10 # comment list page size avatar: mm # gravatar style https://valine.js.org/#/avatar lang: zh-cn # i18n: zh-cn/en placeholder: Just go go # valine comment input placeholder(like: Please leave your footprints ) guest_info: nick,mail,link #valine comment header info# Changyan comment. 畅言changyan: enable: false appid: ## changyan appid appkey: ## changyan appkey# Other commentscomment: disqus: ## disqus_shortname livere: ## 来必力(data-uid) uyan: ## 友言(uid) cloudTie: ## 网易云跟帖(productKey) 3.2 站点统计主题_config.ymlthemes/BlueLake/_config.yml1234busuanzi: true # 卜算子阅读次数统计baidu_analytics: # 百度统计google_analytics: # 谷歌统计gauges_analytics: # Gauges 3.2.1 卜算子访问量统计若busuanzi设置为true将计算文章的阅读量，及网站的访问量与访客数，并显示在文章标题下和网站底部。 3.2.2 百度统计 登录百度统计，定位到站点的代码获取页面。 复制//hm.baidu.com/hm.js?后面那串统计脚本id(假设为：8006843039519956000) 配置主题_config.yml:主题_config.ymlthemes/BlueLake/_config.yml1baidu_analytics: 8006843039519956000 注意： baidu_analytics不是你的百度id或者百度统计id如若使用其他统计，配置方法与百度统计类似。","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chaooo.github.io/tags/hexo/"},{"name":"BlueLake","slug":"BlueLake","permalink":"http://chaooo.github.io/tags/BlueLake/"}]},{"title":"【Hexo博客折腾】自定义HEXO站内搜索Javascript+json","date":"2016-11-09T01:24:56.000Z","path":"article/20161109.html","text":"开始之前目前很多Hexo博客都用的Swiftype和Algolia等第三方搜索服务。其实针对无数据库的情况下，Hexo本身也提供了两个插件来生成数据文件作为数据源： hexo-generator-search生成xml格式的数据文件。 hexo-generator-json-content 生成json格式的数据文件。今天的主角是hexo-generator-json-content，对于 Javascript语言来说还是解析 json 更方便，如果需要用 xml 做数据文件也可以使用已有的atom.xml。 1.安装1$ npm install hexo-generator-json-content@2.2.0 --save 然后执行hexo generate时会自动生成content.json文件，若使用默认设置，生成的数据结构如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445meta: &#123; title: hexo.config.title, subtitle: hexo.config.subtitle, description: hexo.config.description, author: hexo.config.author, url: hexo.config.url&#125;,pages: [&#123; //-&gt; all pages title: page.title, slug: page.slug, date: page.date, updated: page.updated, comments: page.comments, permalink: page.permalink, path: page.path, excerpt: page.excerpt, //-&gt; only text ;) keywords: null //-&gt; it needs settings text: page.content, //-&gt; only text minified ;) raw: page.raw, //-&gt; original MD content content: page.content //-&gt; final HTML content&#125;],posts: [&#123; //-&gt; only published posts title: post.title, slug: post.slug, date: post.date, updated: post.updated, comments: post.comments, permalink: post.permalink, path: post.path, excerpt: post.excerpt, //-&gt; only text ;) keywords: null //-&gt; it needs settings text: post.content, //-&gt; only text minified ;) raw: post.raw, //-&gt; original MD content content: post.content, //-&gt; final HTML content categories: [&#123; name: category.name, slug: category.slug, permalink: category.permalink &#125;], tags: [&#123; name: tag.name, slug: tag.slug, permalink: tag.permalink &#125;]&#125;] 2.配置hexo-generator-json-content默认生成的json数据内容非常全，默认配置如下： 123456789101112131415161718192021222324252627282930313233jsonContent: meta: true keywords: false # (english, spanish, polish, german, french, italian, dutch, russian, portuguese, swedish) pages: title: true slug: true date: true updated: true comments: true path: true link: true permalink: true excerpt: true keywords: true # but only if root keywords option language was set text: true raw: false content: false posts: title: true slug: true date: true updated: true comments: true path: true link: true permalink: true excerpt: true keywords: true # but only if root keywords option language was set text: true raw: false content: false categories: true tags: true 因为默认生成了很多我们不需要的数据，所以我们要对其进行配置让它只生成我们想要的内容,在hexo/_config.yml中加入： 123456789101112131415161718jsonContent: meta: false pages: false posts: title: true #文章标题 date: true #发表日期 path: true #路径 text: true #文本字段 raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true #标签 这样，就只生成每篇文章的标题，日期，路径，标签和文本字段，同时也减小了文件的大小。例如： 1234567891011&#123; &quot;title&quot;: &quot;自定义HEXO站内搜索Javascript+json&quot;, &quot;date&quot;: &quot;2016-11-09T01:24:56.000Z&quot;, &quot;path&quot;: &quot;2016/11/09/自定义HEXO站内搜索Javascript-json.html&quot;, &quot;text&quot;: &quot;目前很多Hexo博客都用的Swiftype和Algolia等第三......#这里显示整篇文章的内容&quot;, &quot;tags&quot;: [&#123; &quot;name&quot;: &quot;javascript,hexo&quot;, &quot;slug&quot;: &quot;javascript-hexo&quot;, &quot;permalink&quot;: &quot;http://chaoo.oschina.io/tags/javascript-hexo/&quot; &#125;]&#125; 3.JavaScript实现代码接下来就是用JS实现查询方法并把结果渲染到页面。 3.1 xhr加载数据12345678910111213141516171819202122var searchData;function loadData(success) &#123; if (!searchData) &#123; var xhr = new XMLHttpRequest(); xhr.open(&#x27;GET&#x27;, &#x27;/content.json&#x27;, true); xhr.onload = function() &#123; if (this.status &gt;= 200 &amp;&amp; this.status &lt; 300) &#123; var res = JSON.parse(this.response || this.responseText); searchData = res instanceof Array ? res : res.posts; success(searchData); &#125; else &#123; console.error(this.statusText); &#125; &#125;; xhr.onerror = function() &#123; console.error(this.statusText); &#125;; xhr.send(); &#125; else &#123; success(searchData); &#125;&#125; 3.2 匹配文章内容返回结果12345678910function matcher(post, regExp) &#123; // 匹配优先级：title &gt; tags &gt; text return regtest(post.title, regExp) || post.tags.some(function(tag) &#123; return regtest(tag.name, regExp); &#125;) || regtest(post.text, regExp);&#125;function regtest(raw, regExp) &#123; regExp.lastIndex = 0; return regExp.test(raw);&#125; 3.3 结果渲染到页面123456789101112131415function render(data) &#123; var html = &#x27;&#x27;; if (data.length) &#123; html = data.map(function(post) &#123; return tpl(searchTpl, &#123; title: post.title, path: post.path, date: new Date(post.date).toLocaleDateString(), tags: post.tags.map(function(tag) &#123; return &#x27;&lt;span&gt;&#x27; + tag.name + &#x27;&lt;/span&gt;&#x27;; &#125;).join(&#x27;&#x27;) &#125;); &#125;).join(&#x27;&#x27;); &#125; &#125; 3.3 查询匹配1234567891011function search(key) &#123; // 关键字 =&gt; 正则，空格隔开的看作多个关键字 // a b c =&gt; /a|b|c/gmi var regExp = new RegExp(key.replace(/[ ]/g, &#x27;|&#x27;), &#x27;gmi&#x27;); loadData(function(data) &#123; var result = data.filter(function(post) &#123; return matcher(post, regExp); &#125;); render(result); &#125;);&#125;","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chaooo.github.io/tags/hexo/"},{"name":"BlueLake","slug":"BlueLake","permalink":"http://chaooo.github.io/tags/BlueLake/"},{"name":"Javascript","slug":"Javascript","permalink":"http://chaooo.github.io/tags/Javascript/"}]},{"title":"【工具】好用的Web包管理器-Bower","date":"2016-08-12T07:32:41.000Z","path":"article/20160812.html","text":"Bower是twitter推出的客户端包管理工具，用于命令行操作包的搜索、下载、更新、卸载(如jQuery、Bootstrap、JavaScript、HTML、CSS之类的网络资源)。Bower对包结构没有强制规范，可以很方便获取各种Web模块文件，但bower本身不存储模块文件和模块版本信息，模块发布者通过register方式将模块可访问的公开的git地址记录在bower的数据库中，而所有版本都是通过代码库的tag来决定的。 开始之前在安装bower之前，必须确认你已经安装了Node.js和Git。 1.安装Bower使用npm，打开终端，键入： 1npm install -g bower #全局安装bower 移步这里查看不同平台上安装的问题。 2.使用Bower使用help命令查看帮助。 12345678910111213141516171819202122232425262728293031323334bower helpUsage: bower &lt;command&gt; [&lt;args&gt;] [&lt;options&gt;]Commands: cache Manage bower cache help Display help information about Bower home Opens a package homepage into your favorite browser info Info of a particular package init Interactively create a bower.json file install Install a package locally link Symlink a package folder list List local packages - and possible updates login Authenticate with GitHub and store credentials lookup Look up a package URL by name prune Removes local extraneous packages register Register a package search Search for a package by name update Update a local package uninstall Remove a local package unregister Remove a package from the registry version Bump a package versionOptions: -f, --force Makes various commands more forceful -j, --json Output consumable JSON -l, --loglevel What level of logs to report -o, --offline Do not hit the network -q, --quiet Only output important information -s, --silent Do not output anything, besides errors -V, --verbose Makes output more verbose --allow-root Allows running commands as root -v, --version Output Bower version --no-color Disable colorsSee &#x27;bower help &lt;command&gt;&#x27; for more information on a specific command. 3.安装包到本地通过命令bower install安装软件包默认到bower_components/目录。 1bower install &lt;package&gt; #package为包名 想要下载的包可以是GitHub上的短链接（如jquery/jquery）、.git 、一个URL或者其它. 12345bower install # 通过 bower.json 文件安装bower install jquery # 通过在github上注册的包名安装bower install desandro/masonry # GitHub短链接bower install git://github.com/user/package.git # Github上的 .gitbower install http://example.com/script.js # URL 安装选项 12345-F, --force-latest: Force latest version on conflict-p, --production: Do not install project devDependencies-S, --save: Save installed packages into the project’s bower.json dependencies-D, --save-dev: Save installed packages into the project’s bower.json devDependencies-E, --save-exact: Configure installed packages with an exact version rather than semver 4.用bower.json文件来管理依赖发布项目的时候没有必要把所有依赖的库发布上去，只需在根目录生成一个bower.json文件即可，别人使用时在根目录执行bower install就可根据bower.json来安装依赖的包。在项目中执行 1bower init 会提示你输入一些基本信息，根据提示按回车或者空格即可，然后会生成一个bower.json文件，用来保存该项目的配置.如果想保存依赖信息(dependencies)到你的bower.json文件，安装包时，命令后面跟上--save即可。 5.使用下载好的包对于已经下载下来的包，默认在当前目录的bower_components文件夹。你可以直接在项目里引用。例如： 12&lt;link rel=&quot;stylesheet&quot; href=&quot;bower_components/bootstrap/dist/css/bootstrap.min.css&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;bower_components/jquery/dist/jquery.min.js&quot;&gt;&lt;/script&gt; 6.更新包若下载的包升级了，只需执行update命令即可更新，例如： 1bower update jquery 这样就可以自动升级到最新版的jquery了。更新选项 1234-F, --force-latest: Force latest version on conflict-p, --production: Do not install project devDependencies-S, --save: Update dependencies in bower.json-D, --save-dev: Update devDependencies in bower.json 7.搜索包12bower search #搜索所有包bower search &lt;packageName&gt; #搜索指定名称的包 或者可以在这里:https://bower.io/search/搜索喜欢的包. 8.卸载包1bower uninstall &lt;name&gt; [&lt;name&gt; ..] [&lt;options&gt;] 卸载选项 12-S, --save: Remove uninstalled packages from the project’s bower.json dependencies-D, --save-dev: Remove uninstalled packages from the project’s bower.json devDependencies","tags":[{"name":"bower","slug":"bower","permalink":"http://chaooo.github.io/tags/bower/"},{"name":"工具","slug":"tool","permalink":"http://chaooo.github.io/tags/tool/"}]},{"title":"【数据库】MongoDB学习笔记","date":"2016-07-30T10:20:16.000Z","path":"article/20160730.html","text":"part1 安装配置一、安装：在mongodb官网下载对应自己电脑系统的安装包，地址为： http://www.mongodb.org/downloads。 1、以Windows64bit为例，下载.msi文件双击安装。2、安装过程中，点击 “Custom(自定义)” 按钮来设置安装目录(D:\\MongoDB\\bin)。3、创建数据目录(D:\\MongoDB\\data\\db),MongoDB默认数据目录\\data\\db。4、连接数据库(命令行win+r cmd,到D:\\MongoDB\\bin目录下，执行代码：mongod –dbpath D:\\MongoDB\\data\\db) 123D:cd D:\\MongoDB\\binmongod --dbpath D:\\MongoDB\\data\\db 5、启动 MongoDB JavaScript 工具(D:\\MongoDB\\bin目录下,打开mongo,会看到：) 12MongoDB shell version: 3.2.4 //mongodb版本connecting to: test //默认shell连接的是本机localhost 上面的test库 此时就可以操作数据库了。 二、将MongoDB服务器作为Windows服务运行1、在D:\\MongoDB目录下创建mongodb.config,写入如下： 12345## 数据库文件目录dbpath=D:/MongoDB/data## 日志目录logpath=D:/MongoDB/log/mongo.logdiaglog=3 2、常规命令(cmd管理员): 123D:cd D:\\MongoDB\\binmongod --config D:\\MongoDB\\mongodb.config 3、若常规方式失败，则sc方式(cmd管理员)： 123D:cd D:\\MongoDB\\binsc create mongodb binPath&#x3D; &quot;D:\\MongoDB\\bin\\mongod.exe --service --config&#x3D;D:\\mongoDB\\mongodb.config&quot; 访问地址：localhost:27017测试是否启动成功 part2 CRUD操作(Creat,Read,Update,Delete)一、基础：1、document(文档) MongoDB把所有数据存放在类似于JSON数据结构的文档内： 1&#123; &quot;item&quot;: &quot;pencil&quot;, &quot;qty&quot;: 500, &quot;type&quot;: &quot;no.2&quot; &#125; 2、collection(集合) 集合是一组相关的文档，MongoDB存储所有的文档在集合里,他们拥有一套共享的通用索引。 123&#123; &quot;item&quot;: &quot;pencil&quot;, &quot;qty&quot;: 500, &quot;type&quot;: &quot;no.1&quot; &#125;&#123; &quot;item&quot;: &quot;pencil2&quot;, &quot;qty&quot;: 550, &quot;type&quot;: &quot;no.2&quot; &#125;&#123; &quot;item&quot;: &quot;pencil3&quot;, &quot;qty&quot;: 800, &quot;type&quot;: &quot;no.3&quot; &#125; 3、database(数据库) MongoDB的默认数据库为”db”，该数据库存储在data目录中。一个mongodb中可以建立多个数据库。 二、数据库操作：连接及运行mongoDB“show dbs“命令可以显示所有的数据的列表“db“命令可以显示当前数据库对象或集合“use“命令可以连接到一个指定的数据库数据库也通过名字来标识。数据库名可以是满足以下条件的任意UTF-8字符串。 1.不能是空字符串（””)。 2.不得含有’ ‘（空格)、.、$、/、\\和\\0 (空宇符)。 3.应全部小写。 4.最多64字节。 1、创建数据库：use Database_Name 1use test ##创建名为test的数据库 2、删除当前数据库： 1db.dropDatabase() 三、文档操作（以 Collection_Name = col 为例）1、插入：12345678910db.col.insert(Document) ##插入一条或多组数据db.col.insertOne(Document) ##插入一条数据db.col.insertMany(Document) ##插入多条数据##例如： db.col.insertOne(&#123; &quot;item&quot;: &quot;pencil&quot;, &quot;type&quot;: &quot;no.1&quot; &#125;) db.col.insertMany([ &#123; &quot;item&quot;: &quot;dog&quot;, &quot;type&quot;: &quot;no.2&quot; &#125;, &#123; &quot;item&quot;: &quot;apple&quot;, &quot;type&quot;: &quot;no.3&quot; &#125;, &#123; &quot;item&quot;: &quot;orange&quot;, &quot;type&quot;: &quot;no.4&quot; &#125; ]) 2、删除：123456789db.col.remove(&#123;&#125;) ##删除所有数据db.col.remove(query &lt;,options&gt;) # query: 查询条件(数据索引或名字) # ptions:两个可选参数 # &#123;justOne: &lt;boolean&gt;, //默认false，删除所有匹配到的。 # writeConcern: &lt;document&gt;//抛出异常的级别。 # &#125;db.col.deleteOne(query &lt;,options&gt;) ##同上，无justOne参数，只删除第一条db.col.deleteMany(query &lt;,options&gt;) ##同上，无justOne参数，只删除多条 3、更新：123456789101112131415161718db.col.update(query, update &lt;,options&gt;) # query: 查询条件(数据索引或名字) # update: 更新的内容，语法：&#123;$set:query&#125; # options:三个可选参数 # &#123;upsert: &lt;boolean&gt;, //如果不存在update的记录，是否插入新数据，默认:false。 # multi: &lt;boolean&gt;, //只更新找到的第一条记录，默认是false,如果为true,多条记录全部更新。 # writeConcern: &lt;document&gt;//#抛出异常的级别。 # &#125;##例如： db.col.update( &#123;&quot;type&quot;: &quot;no.1&quot;&#125;, &#123;$set: &#123;&quot;item&quot;: &quot;human&quot;&#125;&#125;, &#123;upsert: true, multi: true&#125; )db.col.updateOne() ##同上，无multi参数，只更新第一条db.col.updateMany() ##同上，无multi参数db.col.replaceOne() ##同updateOnedb.col.save(document &lt;,writeConcern&gt;) ##通过传入的文档整个替换 insert 与 save的区别如果插入的数据的_id相同,save将会更新该文档,而insert将会报错 update常用操作符1234567891011$set ##当文档中包含该字段的时候,更新该字段,如果该文档中没有该字段,则为本文档添加一个字段.$unset ##删除文档中的一个字段.$rename ##重命名某个列$inc ##增长某个列$setOnInsert ##当upsert为true时,并且发生了insert操作时,可以补充的字段$push ##将一个数字存入一个数组,分为三种情况,如果该字段存在,则直接将数字存入数组.如果该字段不存在,创建字段并且将数字插入该数组.如果更新的字段不是数组,会报错的.$pushAll ##将多个数值一次存入数组.上面的push只能一个一个的存入$addToSet ##与$push功能相同将一个数字存入数组,不同的是如果数组中有这个数字,将不会插入,只会插入新的数据,同样也会有三种情况,与$push相同.$pop ##删除数组最后一个元素$pull ##删除数组中的指定的元素,如果删除的字段不是数组,会报错$pullAll ##删除数组中的多个值,跟pushAll与push的关系类似. 4、查询12345db.col.find(&#123;&#125;) ##查询所有文档db.col.find().pretty() ##以易读的方式来读取数据db.collection.find(query, projection) # query：查询条件(数据索引或名字) # projection：可选。指定返回的字段。 4.1、深入查询表达式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647db.col.find()##查询所有db.col.find(&#123;filed: value&#125;) ##等值查询db.col.find(&#123;filed: &#123;$ne: value&#125;&#125;) ##不等于 $nedb.col.find(&#123;filed: &#123;$nin: [value1, value2, ...]&#125;&#125;) ##不能包含给定的值 $nindb.col.find(&#123;filed: &#123;$all: [value1, value2, ...]&#125;&#125;) ##必须包含所有给定的值 $alldb.col.find(&#123;filed: &#123;$in: [value1, value2, ...]&#125;&#125;) ##只要包含一个或多个给定的值 $indb.col.find(&#123;filed: &#123;$exists:1&#125;&#125;) ##存在filed字段的db.col.find(&#123;filed: &#123;$exists:0&#125;&#125;) ##不存在filed字段的db.col.find(&#123;filed: &#123;$mod:[3,1]&#125;&#125;) ##模三余一，$mod(取模操作)db.col.find(&#123;$or: [&#123;filed1: vulue1&#125;, &#123;filed2: vulue2&#125;]&#125;) ##或 $ordb.col.find(&#123;$nor: [&#123;filed1: vulue1&#125;, &#123;filed2: vulue2&#125;]&#125;)##排除 $nordb.col.find(&#123;filed: &#123;$size: 3&#125;&#125;) ##返回值得数组是给定的长度(3) $sizedb.col.find(&#123;$where: function()&#123;return ...&#125;&#125;) ##回调，隐式迭代，符合条件才返回db.col.find(&#123;$where: &#x27;...&#x27;&#125;&#125;) ##同上db.col.find(&#123;age: &#123;$lt: 5&#125;&#125;).limit(3) ##查询age的值小于5，限制3条 #范围查询： # $lt （小于） # $gt （大于） # $lte （小于等于） # $gte （大于等于） # limit（限制显示）db.col.find().skip(2).limit(3) ##跳过前两个文档查询后面三个 # skip(num):表示跳过前面num个文档db.col.find().sort(&#123;age: 1&#125;) ##查询后以age升序排列显示 # sort():排序，这里 1 代表升序, -1 代表降序.db.col.find(&#123;filed: /user.*/i&#125;) ##正则，查询filed以user开头不区分大小写（正则效率低）db.col.find(&#123;filed: &#123;$type: 1&#125;&#125;) ##查找filed为双精度的文档 # 根据数据类型查询 $type # |类型 |编号| # |双精度 |1 | # |字符串 |2 | # |对象 |3 | # |数组 |4 | # |二进制数据 |5 | # |对象ID |7 | # |布尔值 |8 | # |日期 |9 | # |空 |10 | # |正则表达式 |11 | # |JavaScript |13 | # |符号 |14 | # |JavaScript(带范围)|15 | # |32位整数 |16 | # |时间戳 |17 | # |64位整数 |18 | # |最小键 |255 | # |最大键 |127 | 4.2、group分组查询group做的聚合有些复杂。先选定分组所依据的键，此后MongoDB就会将集合依据选定键值的不同分成若干组。然后可以通过聚合每一组内的文档，产生一个结果文档。 12345group(&#123; key:&#123;字段:1&#125;, initial:&#123;变量:初始值&#125;, $reduce:function(doc,prev)&#123;函数代码&#125;&#125;) 其中key下的字段代表,需要按哪个字段分组.initial下的变量表示这一个分组中会使用的变量,并且给一个初始值.可以在后面的$reduce函数中使用.$reduce的两个参数,分别代表当前的文档和上个文档执行完函数后的结果. 栗子：如下我们按年龄分组,同级不同年龄的用户的多少: 123456789101112131415161718192021222324252627282930313233db.user.find() &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 2 &#125;db.user.group(&#123; key:&#123;age:1&#125;, initial:&#123;count:0&#125;, $reduce:function(doc,prev)&#123; prev.count++ &#125;&#125;); [ &#123;&quot;age&quot;: 0, &quot;count&quot;: 1&#125;, &#123;&quot;age&quot;: 1, &quot;count&quot;: 3&#125;, &#123;&quot;age&quot;: 2, &quot;count&quot;: 2&#125; ]db.user.group(&#123; key:&#123;age:1&#125;, initial:&#123;users:[]&#125;, reduce:function(doc,prev)&#123; prev.users.push(doc.name) &#125;&#125;); [ &#123;&quot;age&quot;: 0, &quot;users&quot;: [&quot;user0&quot;]&#125;, &#123;&quot;age&quot;: 1, &quot;users&quot;: [&quot;user1&quot;, &quot;user3&quot;, &quot;user4&quot;]&#125;, &#123;&quot;age&quot;: 2, &quot;users&quot;: [&quot;user2&quot;, &quot;user5&quot;]&#125; ] 另外本函数还有两个可选参数 condition 和 finalizecondition就是分组的条件筛选类似mysql中的having 123456789101112db.user.group(&#123; key:&#123;age:1&#125;, initial:&#123;users:[]&#125;, $reduce:function(doc,prev)&#123; prev.users.push(doc.name) &#125;, condition:&#123;age:&#123;$gt:0&#125;&#125;&#125;) ##筛选出age大于0的:[ &#123;&quot;age&quot;: 1, &quot;users&quot;: [&quot;user1&quot;, &quot;user3&quot;, &quot;user4&quot;]&#125;, &#123;&quot;age&quot;: 2, &quot;users&quot;: [&quot;user2&quot;, &quot;user5&quot;]&#125;] 4.3、count统计12db.goods.count() ##统计该集合总数db.goods.count(&#123;cat_id: 3&#125;) ##统计cat_id=3的总数 4.4、distinct排重12345678910db.user.find() &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 2 &#125; db.user.distinct(&quot;age&quot;) ## 特殊,传入的参数直接是字符串,而不是对象; [0, 1, 2] 4.5、子文档查询$elemMatchelemMatch投影操作符将限制查询返回的数组字段的内容只包含匹配elemMatch条件的数组元素。注意：(1)数组中元素是内嵌文档。(2)如果多个元素匹配$elemMatch条件，操作符返回数组中第一个匹配条件的元素。假设集合school有如下数据： 1234567891011121314151617181920212223242526272829303132&#123; _id: 1, zipcode: 63109, students: [ &#123; name: &quot;john&quot;, school: 102, age: 10 &#125;, &#123; name: &quot;jess&quot;, school: 102, age: 11 &#125;, &#123; name: &quot;jeff&quot;, school: 108, age: 15 &#125; ]&#125;&#123; _id: 2, zipcode: 63110, students: [ &#123; name: &quot;ajax&quot;, school: 100, age: 7 &#125;, &#123; name: &quot;achilles&quot;, school: 100, age: 8 &#125;, ]&#125;&#123; _id: 3, zipcode: 63109, students: [ &#123; name: &quot;ajax&quot;, school: 100, age: 7 &#125;, &#123; name: &quot;achilles&quot;, school: 100, age: 8 &#125;, ]&#125;&#123; _id: 4, zipcode: 63109, students: [ &#123; name: &quot;barney&quot;, school: 102, age: 7 &#125;, ]&#125; 下面的操作将查询邮政编码键值是63109的所有文档。 $elemMatch操作符将返回 students数组中的第一个匹配条件（内嵌文档的school键且值为102）的元素。 12345db.school.find(&#123;zipcode: 63109 &#125;,&#123; students: &#123; $elemMatch: &#123; school: 102 &#125; &#125; &#125; );&#123;&quot;_id&quot;: 1, &quot;students&quot;: [&#123;&quot;name&quot;:&quot;john&quot;, &quot;school&quot;:102, &quot;age&quot;:10&#125;]&#125;&#123;&quot;_id&quot;: 3&#125;&#123;&quot;_id&quot;: 4, &quot;students&quot;: [&#123;&quot;name&quot;:&quot;barney&quot;, &quot;school&quot;:102, &quot;age&quot;:7&#125;]&#125; 查询结果说明：_id为1的文档，students数组包含多个元素中存在school键且值为102的元素，$elemMatch只返回一个匹配条件的元素。_id为3的文档，因为students数组中元素无法匹配$elemMatch条件，所以查询结果不包含”students”字段。 $elemMatch可以指定多个字段的限定条件，下面的操作将查询邮政编码键值是63109的所有文档。 $elemMatch操作符将返回 students数组中的第一个匹配条件（内嵌文档的school键且值为102且age键值大于10）的元素。 12345db.school.find( &#123; zipcode: 63109 &#125;,&#123; students: &#123; $elemMatch: &#123; school: 102, age: &#123; $gt: 10&#125; &#125; &#125; &#125; ); &#123;&quot;_id&quot;: 1, &quot;students&quot;: [&#123;&quot;name&quot;:&quot;jess&quot;, &quot;school&quot;:102, &quot;age&quot;:11&#125;]&#125; &#123;&quot;_id&quot;: 3&#125; &#123;&quot;_id&quot;: 4&#125;","tags":[{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"},{"name":"mongodb","slug":"mongodb","permalink":"http://chaooo.github.io/tags/mongodb/"}]},{"title":"【工具】sublime text3个人习惯配置","date":"2016-06-29T01:41:48.000Z","path":"article/20160629.html","text":"1、安装分别在官网下载并安装 nodejs 和 sublime text3。 2、sublime text3注册： 点击菜单【help】－&gt;【Enter License】，粘贴注册码。 3、安装package control组件，用于管理所有插件按ctrl + ~调出控制台(或点击菜单栏的【View】-&gt;【Show Console】)，在Console窗口中输入以下代码，按回车键： 1import urllib.request,os,hashlib; h = &#x27;2915d1851351e5ee549c20394736b442&#x27; + &#x27;8bc59f460fa1548d1514676163dafc88&#x27;; pf = &#x27;Package Control.sublime-package&#x27;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &#x27;http://packagecontrol.io/&#x27; + pf.replace(&#x27; &#x27;, &#x27;%20&#x27;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&#x27;Error validating download (got %s instead of %s), please try manual install&#x27; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &#x27;wb&#x27; ).write(by) 等待安装完毕，重启Sublime Text3。按快捷键：Ctrl+Shift+P，调出界面，在其中输入：install，第一个选项即是Package Control： 4、用Package Control安装插件按快捷键：Ctrl+Shift+P，调出界面，按照个人习惯安装插件（以下是我使用的插件）Material ThemeEmmetCSS FormatCSScombjsFormatAutoFileNameAutoprefixerDoc BlockrSublimeLinterSublimeLinter-jshintSublimeLinter-csslintColor HighlighterBracketHighlighter 5、配置nodejs方法1(1)下载sublime Text的nodejs插件(2)下载后解压:直接改名为nodejs 放到 Preferences–&gt;浏览程序包Browse Packages所在的文件夹(3)修改配置:打开Nodejs文件夹，找到文件“Nodejs.sublime-build”， 拖拽到sublime，显示： 12345678910111213141516171819&#123; &quot;cmd&quot;: [&quot;node&quot;, &quot;$file&quot;], &quot;file_regex&quot;: &quot;^[ ]*File \\&quot;(...*?)\\&quot;, line ([0-9]*)&quot;, &quot;selector&quot;: &quot;source.js&quot;, &quot;shell&quot;:true, &quot;encoding&quot;: &quot;cp1252&quot;, &quot;windows&quot;: &#123; &quot;cmd&quot;: [&quot;taskkill /F /IM node.exe &amp; node&quot;, &quot;$file&quot;] &#125;, &quot;linux&quot;: &#123; &quot;cmd&quot;: [&quot;killall node; node&quot;, &quot;$file&quot;] &#125;, &quot;osx&quot;: &#123; &quot;cmd&quot;: [&quot;killall node; node $file&quot;] &#125;&#125; (4)修改为： 1234567891011121314151617181920&#123; &quot;cmd&quot;: [&quot;node&quot;, &quot;$file&quot;], &quot;file_regex&quot;: &quot;^[ ]*File \\&quot;(...*?)\\&quot;, line ([0-9]*)&quot;, &quot;selector&quot;: &quot;source.js&quot;, &quot;shell&quot;:true, &quot;encoding&quot;: &quot;utf-8&quot;, &quot;windows&quot;: &#123; &quot;cmd&quot;: [&quot;taskkill /F /IM node.exe&quot;, &quot;&quot;], &quot;cmd&quot;: [&quot;node&quot;, &quot;$file&quot;] &#125;, &quot;linux&quot;: &#123; &quot;cmd&quot;: [&quot;killall node; node&quot;, &quot;$file&quot;] &#125;, &quot;osx&quot;: &#123; &quot;cmd&quot;: [&quot;killall node; node $file&quot;] &#125;&#125; (5)完成:随便写一段nodejs代码，ctrl+B运行(6)注意：在手动解压sublime Text插件后，需要在preference-&gt;package settings-&gt;package control的user setting下添加installed packages中的“Nodejs”，不然重启sublime Text 会被删除Nodejs插件。 方法2首先需要先安装nodejs。(1)运行Sublime,菜单上找到Tools —&gt; Build System —&gt; new Build System(2)输入：{ “cmd”: [“node”, “$file”], “file_regex”: “^[ ]File &quot;(…?)&quot;, line ([0-9]*)”, “selector”: “source.js”, “shell”:true, “encoding”: “utf-8”, “windows”: { “cmd”: [“taskkill /F /IM node.exe”, “”], “cmd”: [“node”, “$file”] }}(3)保存文件为NodeJs.sublime-build(4)菜单上找到Tools —&gt; Build System —&gt;选择 NodeJs(5)安装sublime插件 JavaScript &amp; NodeJs Snippets(6)新建test.js文件，输入 console.log(‘Hello Node.js’); 按快捷键 Ctrl + B 运行，成功输出","tags":[{"name":"工具","slug":"tool","permalink":"http://chaooo.github.io/tags/tool/"},{"name":"sublime","slug":"sublime","permalink":"http://chaooo.github.io/tags/sublime/"}]},{"title":"【Hexo博客折腾】Hexo3基于github搭建静态博客","date":"2016-05-23T03:16:51.000Z","path":"article/20160523.html","text":"开始之前在安装hexo之前，必须确认你已经安装了Node.js和Git。 1.创建GitHub仓库注册GitHub账号，创建一个以”用户名.github.io”命名的仓库，如我的用户名为chaooo,那我的仓库名为：chaooo.github.io，仓库默有master分支，用于托管生成的静态文件，再新建一个develop(名字自定)分支，用于托管后台文件，方便以后换电脑时后台文件不会丢失。 2.配置Git设置Git的用户名和邮件地址（邮箱就是你注册Github时候的邮箱），打开Git Bash,键入： 12$ git config --global user.name &quot;username&quot;$ git config --global user.email &quot;email@example.com&quot; 3.本地Git与GitHub建立联系这里介绍SSH的配置，先检查电脑是否已经有SSH 1$ ls -al ~/.ssh 如果不存在就没有关系，如果存在的话，直接删除.ssh文件夹里面所有文件。输入以下指令后，一路回车就好： 1$ ssh-keygen -t rsa -C &quot;emailt@example.com&quot; 然后键入以下指令： 12$ ssh-agent -s$ ssh-add ~/.ssh/id_rsa 如果出现这个错误:Could not open a connection to your authentication agent，则先执行如下命令即可： 1$ ssh-agent bash 再重新输入指令： 1$ ssh-add ~/.ssh/id_rsa 到了这一步，就可以添加SSH key到你的Github账户了。键入以下指令，拷贝Key（先拷贝了，等一下可以直接粘贴）： 1$ clip &lt; ~/.ssh/id_rsa.pub 在github上点击你的头像–&gt;Your profile–&gt;Edit profile–&gt;SSH and GPG keys–&gt;New SSH keyTitle自己随便取，然后这个Key就是刚刚拷贝的，你直接粘贴就好（也可以文本打开id_rsa.pub复制其内容），最后Add SSH key。最后还是测试一下吧，键入以下命令： 1$ ssh -T git@github.com 你可能会看到有警告，没事，输入“yes”就好。 4.初始化hexo文件夹到GitHub的chaooo.github.io仓库下，点击Clone or download,复制里面的HTTPS地址。在E盘或是你喜爱的文件夹下，右键Git Bash Here: 键入git clone -b develop &lt;刚复制的地址&gt; 12$ git clone -b develop https://github.com/chaooo/chaooo.github.io.git$ mkdir Hexo-admin Hexo安装配置1.Hexo初始化进入Hexo-admin文件夹 1$ cd Hexo-admin 接下来只需要使用 npm 即可完成 Hexo 的安装: 1$ npm install -g hexo-cli 安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件: 12$ hexo init$ npm install 接下来也可以本地预览博客，执行下列命令,然后到浏览器输入localhost:4000看看。 12$ hexo generate$ hexo server 输入Ctrl+C停止服务。 2.Hexo配置用编辑器打开 Hexo-admin/ 下的配置文件_config.yml找到： 12345# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: repository: 到GitHub的chaooo.github.io仓库下，点击Clone or download,复制里面的HTTPS地址到repository:，添加branch: master。 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/chaooo/chaooo.github.io.git branch: master 3.完成部署最后一步，快要成功了，键入指令： 123$ npm install hexo-deployer-git --save$ hexo generate$ hexo deploy 输入弹出框的用户名与密码(首次使用git会弹出)。OK，我们的博客就已经完全搭建起来了，在浏览器输入（当然，是你的Repository名），例如我的：chaooo.github.io/每次修改本地文件后，需要键入hexo generate才能保存，再键入hexo deploy上传文件。成功之后命令行最后两句大概是这样： 123To https://github.com/chaooo/chaooo.github.io.git 7f3b50a..128a10d HEAD -&gt; masterINFO Deploy done: git 当然，不要忘了回退到父文件夹提交网站相关的文件以备今后迁移，依次执行git add .、git commit -m “…”、git push origin develop。 日常操作1.写文章执行new命令，生成指定名称的文章至 Admin-blog\\source_posts\\文章标题.md 。 1$ hexo new [layout] &quot;文章标题&quot; #新建文章 然后用编辑器打开“文章标题.md”按照Markdown语法书写文章。 其中layout是可选参数，默认值为post。到 scaffolds 目录下查看现有的layout。当然你可以添加自己的layout， 同时你也可以编辑现有的layout，比如post的layout默认是 hexo\\scaffolds\\post.md 1234title: &#123; &#123; title &#125; &#125;date: &#123; &#123; date &#125; &#125;tags:--- 我想添加categories，以免每次手工输入，只需要修改这个文件添加一行，如下： 12345title: &#123; &#123; title &#125; &#125;date: &#123; &#123; date &#125; &#125;categories:tags:--- 文件标题也是md文件的名字，同时也出现在你文章的URL中，postName如果包含空格，必须用”将其包围。请注意，大括号与大括号之间我多加了个空格，否则会被转义，不能正常显示；所有文件&quot;：&quot;后面都必须有个空格，不然会报错。 2.提交每次在本地对博客进行修改后，先执行下列命令提交网站相关的文件。 123$ git add .$ git commit -m &quot;...&quot;$ git push origin develop 然后才执行hexo generate -d发布网站到master分支上。 1$ hexo generate -d 3.本地仓库丢失当你想在其他电脑工作，或电脑重装系统后，安装Git与Node.js后，可以使用下列步骤： 3.1拷贝仓库1$ git clone -b develop https://github.com/chaooo/chaooo.github.io.git 3.2配置Hexo在本地新拷贝的chaooo.github.io文件夹下通过Git bash依次执行下列指令: 1234$ npm install -g hexo-cli$ npm install hexo$ npm install$ npm install hexo-deployer-git --save 小Tips:hexo 命令1234567891011121314hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&#x27;ctrl + c&#x27;关闭server）hexo deploy #将.deploy目录部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本hexo deploy -g #生成加部署hexo server -g #生成加预览#命令的简写hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chaooo.github.io/tags/hexo/"}]},{"title":"Hello World","date":"2015-12-31T15:59:59.000Z","path":"article/20151231.html","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chaooo.github.io/tags/hexo/"}]}]