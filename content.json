[{"title":"「数据结构与算法」单模式字符串匹配算法（BF、RK、KMP、BM）","date":"2022-06-25T13:15:00.000Z","path":"2022/06/25/data-structure-text-pattern-algorithm.html","text":"介绍字符串匹配算法之前，先定义几个概念： 主串Text: 长度记作 n； 模式串Pattern: 长度记作 m，并且 m&lt;=n。 有效位移s（Valid Shift）：即模式串在主串中出现，并且位置移动 s 次。 1. BF 算法BF（Brute Force）算法，中文叫作暴力匹配算法，也叫朴素匹配算法。 从主串的首或尾开始逐个匹配字母（比较顺序没有限制）。BF 算法的思想可以用一句话来概括：在主串中，检查从起始位置 0 开始到 n-m 位置且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的。如下图： BF 算法从名字可以看出，这种算法的字符串匹配方式很“暴力”，当然也就会比较简单、好懂，但相应的性能也不高。我们每次都比对 m 个字符，要比对 n-m+1 次，所以，这种算法的最坏情况时间复杂度是 O(n*m)。 尽管理论上，BF 算法的时间复杂度很高，是 O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。原因有两点。 第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把 m 个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是 O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。 第二，朴素字符串匹配算法思想简单，代码实现也非常简单。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。 1.1 BF 算法总结BF 算法（Brute Force）是最简单、粗暴的字符串匹配算法，它的实现思路是，拿模式串与主串中是所有子串匹配，看是否有能匹配的子串。尽管理论上的最坏情况时间复杂度很高，是 O(n*m)（n、m 表示主串和模式串的长度）。但在实际的开发中，它却是一个比较常用的字符串匹配算法。因为这种算法实现简单，对于处理小规模的字符串匹配很好用。 1.2 Java代码实现123456789101112131415161718192021222324252627/** * 字符串暴力匹配算法 * @param text 主串 * @param pattern 模式串 */public static int bf(String text, String pattern) &#123; int n = text.length(); int m = pattern.length(); if (n &lt; m) &#123;return -1;&#125; // i表示主串与模式串对齐的第一个字符 int i = 0; while (i &lt;= n - m) &#123; int j; // 模式串从后往前匹配 for (j = 0; j &lt;= m-1; ++j) &#123; if (text.charAt(i+j) != pattern.charAt(j))&#123; break; // j为已匹配的字符串下一个字符的下标 &#125; &#125; if (j == m) &#123; // 匹配成功，返回主串与模式串第一个匹配的字符的位置 return i; &#125; i += 1; &#125; return -1;&#125; 2. RK 算法RK（Rabin-Karp）算法，是由它的两位发明者 Rabin 和 Karp 的名字来命名的。这个算法算是 BF 算法的升级版。 BF 算法中，如果主串长度为 n，模式串长度为 m，那在主串中，就会有 n-m+1 个长度为 m 的子串与模式串匹配。但是，每次检查主串与子串是否匹配，需要依次比对每个字符，所以 BF 算法的时间复杂度就比较高，是 O(n*m)。 RK 算法的思路：通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。 整个RK 算法包含两部分：第一部分，计算子串哈希值；通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是 O(n)。第二部分，模式串哈希值与每个子串哈希值之间的比较的时间复杂度是 O(1)，总共需要比较 n-m+1 个子串的哈希值，所以，这部分的时间复杂度也是 O(n)。所以，RK 算法整体的时间复杂度就是 O(n)。但是，当存在哈希冲突的时候，有可能存在子串和模式串的哈希值虽然是相同的，但是两者本身并不匹配。极端情况下，如果存在大量的冲突，每次都要再对比子串和模式串本身，那时间复杂度就会退化成 O(n*m)。但也不要太悲观，一般情况下，冲突不会很多，RK 算法的效率还是比 BF 算法高的。 2.1 RK 算法总结RK 算法（Rabin-Karp）是对每个子串分别求哈希值，然后拿子串的哈希值与模式串的哈希值比较，减少了比较的时间。理想情况下，RK 算法的时间复杂度是 O(n)。不过这样的效率取决于哈希算法的设计方法，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为 O(n*m)。 2.2 Java代码实现123456789101112131415161718192021222324252627/** * Rabin-Karp算法 * @param text 主串 * @param pattern 模式串 */public static int rk(String text, String pattern) &#123; int n = text.length(); int m = pattern.length(); if (n &lt; m) &#123; return -1; &#125; // 对模式串与第一个字串求哈希值 Integer patternCode = pattern.hashCode(); String sub = text.substring(0, m); int subCode = sub.hashCode(); // 对主串中的n-m+1个子串分别求哈希值 int i = 0; while (i &lt; n - m + 1) &#123; // 哈希值相同，才进一步确认 if (patternCode.equals(subCode) &amp;&amp; sub.equals(pattern)) &#123; return i; &#125; // 继续下一个子串 i++; sub = text.substring(i, i + m); subCode = sub.hashCode(); &#125; return -1;&#125; 3. KMP 算法KMP（Knuth Morris Pratt）算法，是最常用的之一。它以三个发明者命名，很多时候，提到字符串匹配，我们首先想到的就是KMP 算法。 KMP 算法的核心思想：在模式串与主串匹配的过程中，当遇到不可匹配的字符的时候，我们希望找到一些规律，可以将模式串往后多滑动几位，跳过那些肯定不会匹配的情况。 先了解几个概念： 前缀：指除了最后一个字符以外，一个字符串的全部头部组合； 后缀：指除了第一个字符以外，一个字符串的全部尾部组合； 部分匹配值：”前缀”和”后缀”的最长的共有元素的长度。 以模式串”ABCDABD”为例： 子串 前缀 后缀 共有 长度 A 空 空 - 0 AB [A] [B] - 0 ABC [A, AB] [BC, C] - 0 ABCD [A, AB, ABC] [BCD, CD, D] - 0 ABCDA [A, AB, ABC, ABCD] [BCDA, CDA, DA, A] A 1 ABCDAB [A, AB, ABC, ABCD, ABCDA] [BCDAB, CDAB, DAB, AB, B] AB 2 ABCDABD [A, AB, ABC, ABCD, ABCDA, ABCDAB] [BCDABD, CDABD, DABD, ABD, BD, D] - 0 所以模式串”ABCDABD”的《部分匹配表》是： 模式串 A B C D A B D 部分匹配表 0 0 0 0 1 2 0 “部分匹配”的实质是，有时候，字符串头部和尾部会有重复。比如，”ABCDAB”之中有两个”AB”，那么它的”部分匹配值”就是2（”AB”的长度）。搜索词移动的时候，第一个”AB”向后移动4位（字符串长度-部分匹配值），就可以来到第二个”AB”的位置。所以KMP 算法的后移规律： 移动位数 &#x3D; 已匹配的字符数 - 对应的部分匹配值 根据这个规律，我们以模式串”ABCDABD”来匹配字符串”BBC ABCDAB ABCDABCDABDE”。KMP 算法的匹配顺序是按照模式串下标从小到大。 逐位比较，不匹配就移一位；直到主串有一个字符与模式串的第一个字符相同为止。 已知空格与”D”不匹配时，前面6个字符”ABCDAB”是匹配的。查表可知，最后一个匹配字符”B”对应的”部分匹配值”为2，因此按照规律移动位数 = 已匹配的字符数 - 对应的部分匹配值算出向后移动的位数：6 - 2 等于4，所以将搜索词向后移动4位。 因为空格与”C”不匹配，搜索词还要继续往后移。这时，已匹配的字符数为2（”AB”），对应的”部分匹配值”为0。所以，移动位数 = 2 - 0，于是将搜索词向后移2位。按照规律移动位数 = 已匹配的字符数 - 对应的部分匹配值算出向后移动的位数：2 - 0 等于2，所以将搜索词向后移动2位。继续逐位比较，因为空格与”A”不匹配，继续后移1位。 逐位比较，直到发现”C”与”D”不匹配。于是，移动位数 = 6 - 2，继续将搜索词向后移动4位。逐位比较，直到搜索词的最后一位，发现完全匹配，于是搜索完成。 3.1 KMP 算法总结KMP 算法（Knuth Morris Pratt）的核心是利用匹配失败后的信息，尽量减少模式串与主串的匹配次数以达到快速匹配的目的。具体实现就是通过一个next()函数实现，函数本身包含了模式串的局部匹配信息。KMP 算法的时间复杂度O(m+n)。 3.2 KMP 算法的JAVA 代码实现部分匹配值是模式串中子串”前缀”和”后缀”的最长的共有元素的长度。《部分匹配表》在代码中定义为 next 数组，很多书中也叫失效函数（failure function）。 123456789101112131415161718192021222324252627282930313233/** * 实现next()函数 * @param pattern 模式串 */private static int[] kmpNext(String pattern)&#123; int m = pattern.length(); // 声明部分匹配表数组 next，用于存储部分匹配值 int[] next = new int[m]; // 第一个字符没有前后缀，最长匹配值为 0 next[0] = 0; // 循环模式串，计算部分匹配表，i初始化为 1 // j用于记录部分匹配值 for(int i = 1,j = 0; i &lt; m; i++)&#123; // ④ 在0~i中，j&gt;0 并且 pattern.charAt(j) != pattern.charAt(i)时， // 表示上一轮比较，前后缀部分匹配值为j。 // 上一轮比较时，下标[0~j-1]是前后缀最长相同字符串。 // 下标[j-1]的部分匹配值表示为next[j-1]，即j=next[j-1]; while(j &gt; 0 &amp;&amp; pattern.charAt(j) != pattern.charAt(i))&#123; j = next[j - 1]; &#125; // ① 在0~i中，j=0 并且 pattern.charAt(j) != pattern.charAt(i)时， // 表示前后缀字符串集合中没有相同字符串，next[i]=j（同next[i]=0） // ② 在0~i中，j=0 并且 pattern.charAt(j) == pattern.charAt(i)时， // 表示前后缀字符串集合中有 1 个相同字符串，j++;next[i]=j;（同next[i]=1） // ③ 在0~i中，j&gt;0 并且 pattern.charAt(j) == pattern.charAt(i)时， // 表示上一轮比较，前后缀部分匹配值为j。这轮为j+1。因此j++;next[i]=j; if(pattern.charAt(i) == pattern.charAt(j))&#123; j++; &#125; next[i] = j; &#125; return next;&#125; KMP 算法的框架代码，比较模式串和主串。 12345678910111213141516171819202122232425262728public static int kmp(String text, String pattern)&#123; int n = text.length(); int m = pattern.length(); if (n &lt; m) &#123; return -1; &#125; // 计算出部分匹配表 int[] next = kmpNext(pattern); // i表示主串与模式串对齐的第一个字符 int i = 0; while (i &lt;= n - m) &#123; int j; // 模式串从后往前匹配 for (j = 0; j &lt;= m-1; ++j) &#123; if (text.charAt(i+j) != pattern.charAt(j))&#123; break; // j为已匹配的字符串下一个字符的下标 &#125; &#125; if (j == m) &#123; return i; // 匹配成功，返回主串与模式串第一个匹配的字符的位置 &#125; int s = 1; if (j &gt; 0) &#123; // 后移位数 = 已匹配的字符数j - 对应的部分匹配值next[j-1] s = j - next[j-1]; &#125; i += s; &#125; return -1;&#125; 4. BM 算法BM（Boyer-Moore）算法，也是以两位发明者名字命名的字符串匹配算法。BM 算法不仅效率高，而且构思巧妙，容易理解。 BM 算法的核心思想：我们把模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后多滑动几位。 BM 算法包含两部分，分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift）。 4.1 坏字符规则BM 算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，倒着匹配的。当发现某个字符没法匹配的时候，我们把这个没有匹配的字符叫作”坏字符“（bad character）。 如上图，从尾部开始比较，发现”S”与”E”不匹配，所以”S”就是”坏字符”。并且，”S”不包含在模式串”EXAMPLE”之中，也就是说，可以把模式串直接移到”S”的后一位。 如上图，依然从尾部开始比较，发现”P”与”E”不匹配，所以”P”是”坏字符”。但是，”P”包含在模式串”EXAMPLE”之中。所以，将模式串后移两位，两个”P”对齐。 由此我们总结出坏字符规则： 后移位数 &#x3D; 坏字符(对应模式串)的位置 - 模式串中的上一次出现位置 （如果”坏字符”不包含在模式串中，则上一次出现位置为 -1） 如上图，”P”作为”坏字符”为例，出现在模式串的第6位（从0开始编号），在模式串中的上一次出现位置为4，所以后移6 - 4 = 2 位。再以前面”坏字符”的”S”为例，它出现在第6位，上一次出现位置是 -1（即未出现），则整个模式串后移 6 - (-1) = 7 位。 4.2 好后缀规则好后缀规则实际上跟坏字符规则的思路很类似。从尾部开始比较，我们把所有尾部匹配的字符串称为”好后缀“（good suffix）。 如上图，从尾部开始比较，”E”与”E”匹配；”LE”与”LE”匹配；”PLE”与”PLE”匹配；”MPLE”与”MPLE”匹配。所以，”MPLE”、”PLE”、”LE”、”E”都是好后缀。比较前一位，发现”I”与”A”不匹配。所以，”I”是”坏字符”。 好后缀规则： 后移位数 &#x3D; 好后缀的位置 - 模式串中的上一次出现位置 举例来说，如果模式串”ABCDAB”的后一个”AB”是”好后缀”。那么好后缀的位置是 5（取最后的”B”的值），模式串中的上一次出现位置是 1（第一个”B”的位置），所以后移5 - 1 = 4位。如果模式串”ABCDEF”的”EF”是好后缀，则好后缀的位置是5 ，上一次出现的位置是-1（即未出现），所以后移5 - (-1) = 6位。 好后缀规则三个注意点： “好后缀”的位置以最后一个字符为准。 假定”ABCDEF”的”EF”是好后缀，则它的位置以”F”为准，即5。 如果”好后缀”在搜索词中只出现一次，则它的上一次出现位置为 -1。 比如，”EF”在”ABCDEF”之中只出现一次，则它的上一次出现位置为-1（即未出现）。 如果”好后缀”有多个，则除了最长的那个”好后缀”，其他”好后缀”的上一次出现位置必须在头部。 比如，假定”BABCDAB”的”好后缀”是”DAB”、”AB”、”B”，此时计算上一次出现位置是第0位，采用好后缀”B”计算。这个规则也可以这样表达：如果最长的那个”好后缀”只出现一次，则可以把搜索词改写成如下形式进行位置计算”(DA)BABCDAB”，即虚拟加入最前面的”DA”。 回到上图的例子。此时，所有的”好后缀”（MPLE、PLE、LE、E）之中，只有”E”在”EXAMPLE”还出现在头部，所以好后缀规则是后移6 - 0 = 6位。而”I”是”坏字符”，坏字符规则后移2 - (-1) = 3位。可以看到，”坏字符规则”只能移3位，”好后缀规则”可以移6位。所以，BM 算法的基本思想是，每次后移这两个规则之中的较大值。 更巧妙的是，这两个规则的移动位数，只与搜索词有关，与原字符串无关。因此，可以预先计算生成《坏字符规则表》和《好后缀规则表》。使用时，只要查表比较一下就可以了。 继续从尾部开始比较，”P”与”E”不匹配，因此”P”是”坏字符”。根据”坏字符规则”，后移 6 - 4 = 2位；从尾部开始逐位比较，发现全部匹配，于是搜索结束。 4.3 BM 算法总结BM 算法（Boyer-Moore）核心思想是，利用模式串本身的特点，在模式串中某个字符与主串不能匹配的时候，将模式串往后多滑动几位，以此来减少不必要的字符比较，提高匹配的效率。BM 算法构建的规则有两类，坏字符规则和好后缀规则。 4.4 BM 算法的JAVA 代码实现BM 算法的框架代码: 12345678910111213141516171819202122232425262728293031public static int bm(String text, String pattern)&#123; int n = text.length(); int m = pattern.length(); if (n &lt; m) &#123; return -1; &#125; // 计算 坏字符规则表，记录模式串中每个字符最后出现的位置 Map&lt;Character, Integer&gt; badCharRules = generateBadCharRules(pattern); // 计算 好后缀规则表，记录好后缀位置对应后移的位数 int[] goodSuffixRules = generateGoodSuffix(pattern.toCharArray()); // i表示主串与模式串对齐的第一个字符 int i = 0; while (i &lt;= n - m) &#123; int j; // 模式串从后往前匹配 for (j = m - 1; j &gt;= 0; --j) &#123; if (text.charAt(i+j) != pattern.charAt(j))&#123; break; // 坏字符对应模式串中的下标是j &#125; &#125; if (j &lt; 0) &#123; // 匹配成功，返回主串与模式串第一个匹配的字符的位置 return i; &#125; // 后移位数 = 坏字符(对应模式串)的位置 - 坏字符在模式串中最后出现位置 int s1 = j - getBadCharPosition(badCharRules, text.charAt(i + j)); // 好后缀后移位数 int s2 = goodSuffixRules[j]; //后移这两个规则之中的较大值 i += Math.max(s1, s2); &#125; return -1;&#125; 4.4.1 坏字符规则表计算模式串中每个字符最后出现的位置。 1234567891011121314151617181920/** * 坏字符规则表 * 记录模式串中每个字符最后出现的位置 */private static Map&lt;Character, Integer&gt; generateBadCharRules(String pattern) &#123; Map&lt;Character, Integer&gt; bmBc = new HashMap&lt;&gt;(pattern.length()); // 选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过 for (int j = 0; j &lt; pattern.length(); j++) &#123; bmBc.put(pattern.charAt(j), j); &#125; return bmBc;&#125;/** * 获取坏字符在模式串中最后出现位置 * 未出现返回 -1 */private static int getBadCharPosition(Map&lt;Character, Integer&gt; badChar, Character c)&#123; Integer a = badChar.get(c); return a == null ? -1 : a;&#125; 4.4.2 好后缀规则表好后缀规则表，用来记录好后缀位置对应后移的位数。为了实现好后缀规则，需要定义一个suffix数组，用来记录模式串中匹配上好后缀的子串长度。其中，suffix[i] = s满足pattern[i-s,i] == pattern[m-1-s,m-1]，m是模式串的长度。如下图：pattern[i-4,i]和pattern[m-5,m-1]字符相同。 1234567891011121314151617181920/** * 计算后缀长度数组 * @param pattern 模式串字符 */private static int[] getSuffix(char[] pattern)&#123; // 初始化 int m = pattern.length; int[] suffix = new int[m]; // 计算 suffix[m-1] = m; for (int i = m-2; i &gt;= 0; --i) &#123; int q = i; while (q &gt;= 0 &amp;&amp; pattern[q] == pattern[q+m-1-i])&#123; q--; &#125; suffix[i] = i-q; &#125; // 返回后缀长度数组 return suffix;&#125; 有了suffix数组，就可以定义好后缀忽略映射bmgs数组。suffix[i]表示模式串中匹配上好后缀的子串长度（i表示子串的末位置）；bmgs[j]表示好后缀位置对应后移的位数（j表示好后缀前一个字符的位置，即坏字符的位置）。根据好后缀规则：后移位数 = 好后缀的位置 - 模式串中的上一次出现位置，构建bmgs数组分为三种情况： ①模式串没有子串匹配上好后缀，也没有最大前缀。 后移位数为m-1-(-1)，即bmgs[j]=m。 ②模式串没有子串匹配上好后缀，但有最大前缀。 后移位数为m-1-i，即bmgs[j] = m-1-i。 ③模式串有子串匹配上好后缀。 后移位数为m-1-i，即bmgs[j] = m-1-i; j=m-1-suffix[i]。 12345678910111213141516171819202122232425262728293031/** * 好后缀忽略映射（后移位数数组） * @param pattern 模式串字符 */private static int[] generateGoodSuffix(char[] pattern)&#123; // 初始化 int m = pattern.length; int[] bmgs = new int[m]; // 获取后缀长度数组 int[] suffix = getSuffix(pattern); // 赋值默认值 for (int i = 0; j &lt; m; ++j) &#123; bmgs[j] = m; &#125; // 模式串没有没有子串匹配上好后缀，但有最大前缀 for (int i = m-1, j = 0; i &gt;= 0; --i) &#123; if (suffix[i] == i+1)&#123; for (; j &lt; m-1-i; ++j) &#123; if(bmgs[j] == m)&#123; bmgs[j] = m-1-i; &#125; &#125; &#125; &#125; // 模式串有子串匹配上好后缀（多个取最左） for (int i = 0; i &lt; m-1; ++i) &#123; bmgs[m-1-suffix[i]] = m-1-i; &#125; // 返回忽略数组 return bmgs;&#125;","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」二分查找","date":"2022-06-02T10:02:00.000Z","path":"2022/06/02/data-structure-binary-search.html","text":"二分查找（Binary Search）又称折半查找、二分搜索、折半搜索等，查找思想有点类似分治思想，对应的时间复杂度为O(logn)。二分查找算法仅适用于有序且使用顺序存储结构的序列（比如有序数组）。核心思想是：不断地缩小搜索区域，降低查找目标元素的难度。（每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。） 以在升序序列中查找目标元素为例，二分查找算法的实现思路是： 初始状态下，将整个序列作为搜索区域（假设为 [B, E]）； 找到搜索区域内的中间元素（假设所在位置为 M），和目标元素进行比对。如果相等，则搜索成功；如果中间元素大于目标元素，将左侧 [B, M-1] 作为新的搜素区域；反之，若中间元素小于目标元素，将右侧 [M+1, E] 作为新的搜素区域； 重复执行第二步，直至找到目标元素。如果搜索区域无法再缩小，且区域内不包含任何元素，表明整个序列中没有目标元素，查找失败。 1. 标准二分查找二分查找递归实现： 1234567891011121314151617181920212223/** * 二分查找 * 如果存在，返回其索引，否则返回 -1 */public int binarySearch(int[] nums, int target) &#123; return searchRecursion(nums, 0, nums.length - 1, target);&#125;/*** 递归实现*/private int searchRecursion(int[] nums, int left, int right, int target) &#123; if (left &gt; right) return -1; // mid = left + (right - left) / 2; int mid = left + ((right - left) &gt;&gt; 1); if (nums[mid] == target) &#123; return mid; &#125; else if (nums[mid] &lt; target) &#123; return searchRecursion(nums, mid+1, right, target); &#125; else &#123; return searchRecursion(nums, left, mid-1, target); &#125;&#125; 二分查找循环实现： 1234567891011121314151617181920/** * 二分查找循环实现 */public int binarySearch(int[] arr, int value) &#123; int low = 0; int high = arr.length - 1; while (low &lt;= high) &#123; int mid = low + ((high - low) &gt;&gt; 1); if (arr[mid] == target) &#123; return mid; &#125; else if (arr[mid] &lt; target) &#123; low = mid + 1; &#125; else &#123; high = mid - 1; &#125; &#125; return -1;&#125; 二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。 2. 二分查找左边界查找第一个值等于给定值的元素 12345678910111213141516171819public int leftSearch(int[] nums, int target) &#123; int left = 0; int right = nums.length - 1; while (left &lt;= right) &#123; int mid = left + ((right - left) &gt;&gt; 1); if (nums[mid] == target) &#123; if ((mid == 0) || (nums[mid - 1] != target)) return mid; // 即使我们找到了nums[mid] == target, 这个mid的位置也不一定就是最左侧的那个边界， // nums[mid - 1] == target，继续收缩右边界 else right = mid - 1; &#125; else if (nums[mid] &lt; target) &#123; left = mid + 1; &#125; else if (nums[mid] &gt; target) &#123; right = mid - 1; &#125; &#125; return -1;&#125; 3. 二分查找右边界查找最后一个值等于给定值的元素 12345678910111213141516171819public int rightSearch(int[] nums, int target) &#123; int left = 0; int right = nums.length - 1; while (left &lt;= right) &#123; int mid = left + ((right - left) &gt;&gt; 1); if (nums[mid] == target) &#123; if ((mid == nums.length - 1) || (nums[mid + 1] != target)) return mid; // 即使我们找到了nums[mid] == target, 这个mid的位置也不一定就是最右侧的那个边界， // nums[mid + 1] == target，继续收缩左边界 else left = mid + 1; &#125; else if (nums[mid] &lt; target) &#123; left = mid + 1; &#125; else if (nums[mid] &gt; target) &#123; right = mid - 1; &#125; &#125; return -1;&#125;","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」排序算法","date":"2022-05-25T12:02:00.000Z","path":"2022/05/25/data-structure-sort.html","text":"排序算法可以分为内部排序和外部排序，内部排序是数据记录在内存中进行排序，而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。常见的内部排序算法有：插入排序、希尔排序、选择排序、冒泡排序、归并排序、快速排序、堆排序、基数排序等。 用一张图概括： 名词解释： n：数据规模 k：”桶”的个数 In-place：占用常数内存，不占用额外内存 Out-place：占用额外内存 稳定性：排序后 2 个相等键值的顺序和排序之前它们的顺序相同 内部排序 描述 冒泡排序 （无序区，有序区） 从无序区通过交换找出最大元素放到有序区最前端。 选择排序 （无序区，有序区） 从无序区选择一个最小的元素放到有序区的末尾。比较多，交换少 插入排序 （无序区，有序区） 把无序区的第一个元素插入到有序区合适的位置。比较少，交换多 希尔排序 通过比较相距一定间隔的元素来进行（每一定间隔取一个元素组成子序列），各趟比较所用的距离随着算法的进行而减小，直到只比较相邻元素的最后一趟排序为止。 归并排序 把数组从中间分成前后两部分，分别排序，再将排好序的两部分合并。 快速排序 （小数，基准，大数） 随机一个元素作为基准数，将小于基准的元素放在基准之前，大于基准的放在基准之后，再分别对小数区与大数区进行排序。 堆排序 （大顶堆，有序区） 取出堆顶元素放在有序区，再恢复堆。 计数排序 统计小于等于该元素值的元素个数i,于是该元素就放在目标数组的索引i位（i&gt;&#x3D;0）。 桶排序 将值为i的元素放在i号桶，最后依次把桶里元素倒出来。 基数排序 一种多关键字的排序算法，可用桶排序实现。 1. 冒泡排序（Bubble Sort）冒泡排序（Bubble Sort）是一种简单直观的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。 冒泡排序步骤： 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 Java代码实现： 1234567891011121314151617181920212223/** * 冒泡排序 */public void bubbleSort(int[] arr, int len)&#123; if (len &lt;= 1) return; for (int i = 0; i &lt; len; i++) &#123; // 若已有序，提前退出标志位 boolean flag = false; for (int j = 0; j &lt; len-i; j++) &#123; if(arr[j] &gt; arr[j+1])&#123; int temp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = temp; // 发生交换 flag = true; &#125; &#125; if (!flag)&#123; // 若已有序,未发生交换，提前退出循环 break; &#125; &#125;&#125; 2. 选择排序（Selection sort）选择排序（Selection sort）是一种简单直观的排序算法。 它的工作原理是：第一次从待排序的数据元素中选出最小（或最大）的一个元素，存放在序列的起始位置，然后再从剩余的未排序元素中寻找到最小（大）元素，然后放到已排序的序列的末尾。 以此类推，直到全部待排序的数据元素的个数为零。 选择排序是不稳定的排序方法。 冒泡排序步骤： 首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。 再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。 重复第二步，直到所有元素均排序完毕。 Java代码实现： 1234567891011121314151617181920/** * 选择排序 */public void selectionSort(int[] arr, int len)&#123; if (len &lt;= 1) return; for (int i = 0; i &lt; len-1; i++) &#123; int min=i; for (int j = i+1; j &lt; len; j++) &#123; if(arr[j] &lt; arr[min])&#123; min = j; &#125; &#125; if (i != min)&#123; // 交换 int temp = arr[i]; arr[i] = arr[min]; arr[min] = temp; &#125; &#125;&#125; 3. 插入排序（Insertion Sort）插入排序（Insertion Sort）是一种最简单直观的排序算法，一般也被称为直接插入排序。通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 插入排序步骤： 将第一待排序序列第一个元素看做一个有序序列，把第二个元素到最后一个元素当成是未排序序列。 从头到尾依次扫描未排序序列，将扫描到的每个元素插入有序序列的适当位置。（如果待插入的元素与有序序列中的某个元素相等，则将待插入元素插入到相等元素的后面。） Java代码实现： 123456789101112131415161718/** * 插入排序 */public void insertionSort(int[] arr, int len)&#123; if (len &lt;= 1 ) return; for (int i = 1; i &lt; len; i++) &#123; int temp = arr[i]; int j = i-1; for (; j &gt;=0; j--) &#123; if (arr[j] &gt; temp)&#123; arr[j+1] = arr[j]; &#125; else &#123; break; &#125; &#125; arr[j+1] = temp; &#125;&#125; 4. 希尔排序（Shell Sort）希尔排序（Shell Sort）是插入排序的一种更高效的改进版本；又称缩小增量排序，因 DL.Shell 于 1959 年提出而得名。通过比较相距一定间隔的元素来进行（每一定间隔取一个元素组成子序列），各趟比较所用的距离随着算法的进行而减小，直到只比较相邻元素的最后一趟排序为止。 希尔排序步骤： 将 len 个元素的数组序列 分割为间隔 gap (gap &#x3D; len&#x2F;2) 的子序列（每间隔 gap 取一个元素） 在每一个子序列中分别采用直接插入排序。 然后缩小间隔 gap，将 gap &#x3D; gap&#x2F;2。 重复上述的子序列划分和排序工作，随着序列减少最后变为一个，也就完成了整个排序。 Java代码实现： 1234567891011121314151617181920/** * 希尔排序 */public void shellSort(int[] arr, int len)&#123; if (len &lt;= 1) return; for (int gap = len/2; gap &gt; 0; gap = gap/2) &#123; for (int i = gap; i &lt; len; i++) &#123; int temp = arr[i]; int j = i-gap; for (; j &gt;=0; j = j-gap) &#123; if (arr[j] &gt; temp)&#123; arr[j+gap] = arr[j]; &#125; else &#123; break; &#125; &#125; arr[j+gap] = temp; &#125; &#125;&#125; 5. 归并排序（Merge sort）归并排序（Merge sort）是建立在归并操作上的一种有效、稳定的排序算法，该算法是采用分治法(Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并。归并排序使用的就是分治思想。分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。 归并排序步骤： 先把数组从中间分成前后两部分子序列，然后对前后两部分分别排序，然后再合并。 前后两部分子序列递归重复上述步骤。 Java代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * 归并排序（稳定的排序算法） */public void mergeSort(int[] arr, int len) &#123; mergeSortRecursion(arr, 0, len-1);&#125;/** * 归并排序递归调用函数 * 给下标从 p 到 r 之间的数组排序。我们将这个排序问题转化为了两个子问题， * 分割点下标 q 等于 p 和 r 的中间位置，也就是 (p+r)/2。当下标从 p 到 q 和从 q+1 到 r 这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从 p 到 r 之间的数据就也排好序了。 */private void mergeSortRecursion(int[] arr, int p, int r) &#123; if (p == r) return; // 分割点下标 int q = p + (r-p)/2; // 递归左边排序 mergeSortRecursion(arr, p, q); // 递归右边排序 mergeSortRecursion(arr, q+1, r); // 合并左右 mergeResult(arr, p, q, r);&#125;/** * 合并 * 将已经有序的 A[p...q]和 A[q+1....r]合并成一个有序的数组，并且放入 A[p....r]。 * 具体我们申请一个临时数组 tmp，大小与 A[p...r]相同。我们用两个游标 i 和 j，分别指向 A[p...q]和 A[q+1...r]的第一个元素。比较这两个元素 A[i]和 A[j]，如果 A[i]&lt;=A[j]，我们就把 A[i]放入到临时数组 tmp，并且 i 后移一位，否则将 A[j]放入到数组 tmp，j 后移一位。 * 继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中剩余的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组 tmp 中的数据拷贝到原数组 A[p...r]中。 */private void mergeResult(int[] arr, int p, int q, int r) &#123; int[] temp = new int[r-p+1]; // i是arr前半部分下标 int i = p; // j是arr后半部分下标 int j = q +1; // k是临时新数组temp下标 int k = 0; // 当arr数组两边都还有数时,依次从小到大拷贝到临时新数组temp while (i &lt;= q &amp;&amp; j &lt;= r) &#123; if (arr[i] &lt;= arr[j]) &#123; temp[k] = arr[i]; i++; &#125; else &#123; temp[k] = arr[j]; j++; &#125; k++; &#125; // 判断哪个子数组中有剩余的数据 int start = i, end = q; if (j &lt;= r) &#123; // arr后半部分还有剩余(初始化时默认前半部分还有剩余) start = j; end = r; &#125; // 将剩余的数据拷贝到临时数组temp while (start &lt;= end) &#123; temp[k++] = arr[start++]; &#125; //将排好序的temp数组拷贝回arr[p...r] System.arraycopy(temp, 0, arr, p, temp.length);&#125; 6. 快速排序（Quick Sort）快速排序使用分治法（Divide and conquer）策略来把一个序列（list）分为两个子序列（sub-lists）。快速排序又是一种分而治之思想在排序算法上的典型应用。本质上来看，快速排序应该算是在冒泡排序基础上的递归分治法。 通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 排序算法的思想非常简单，在待排序的数列中，我们首先要找一个数字作为基准数。把小于基准数的元素移动到待排序的数列的左边，把大于基准数的元素移动到待排序的数列的右边。这时，左右两个分区的元素就相对有序了；接着把两个分区的元素分别按照上面两种方法继续对每个分区找出基准数，然后移动，直到各个分区只有一个数时为止。 快速排序步骤： 从数列中挑出一个元素，称为 “基准”（pivot）。 将小于基准的元素放在基准之前，大于基准的放在基准之后，再分别对小数区与大数区进行快速排序。 小数区与大数区子序列递归地重复上述步骤（直到分区子序列的大小是0或1）。 Java代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 快速排序 */public void quickSort(int[] arr, int len) &#123; quickSortRecursion(arr, 0, len-1);&#125;/** * 快速排序递归函数，p,r为下标 * 根据分治、递归的处理思想，我们可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了。 */private void quickSortRecursion(int[] arr, int p, int r) &#123; if (p &gt;= r) return; // 获取分区点下标 int q = getPartitionIndex(arr, p, r); System.out.println(&quot;pivotIndex=: &quot; + q + &quot;,pivot=&quot; + arr[q]); System.out.println(Arrays.toString(Arrays.copyOfRange(arr, p, r+1))); // 用递归排序下标从 p 到 q-1 之间的数据 quickSortRecursion(arr, p, q-1); // 用递归排序下标从 q+1 到 r 之间的数据 quickSortRecursion(arr, q+1, r);&#125;/** * 获取分区点 * 随机选择一个元素作为分区点 pivot（一般情况下，可以选择 p 到 r 区间的最后一个元素），然后对 A[p...r]分区，返回 pivot 的下标。 * 遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。 */private int getPartitionIndex(int[] arr, int p, int r) &#123; int pivot = arr[r]; int i = p; for (int j = p; j &lt; r; j++) &#123; if (arr[j] &lt; pivot) &#123; swapArr(arr, i, j); i++; &#125; &#125; swapArr(arr, i, r); return i;&#125;/** * 交换 */private void swapArr(int[] arr, int a, int b) &#123; int temp = arr[a]; arr[a] = arr[b]; arr[b] = temp;&#125; 7. 堆排序（Heap Sort）堆排序（Heap Sort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。堆排序可以说是一种利用堆的概念来排序的选择排序。分为两种方法： 大顶堆：每个节点的值都大于或等于其子节点的值，在堆排序算法中用于升序排列； 小顶堆：每个节点的值都小于或等于其子节点的值，在堆排序算法中用于降序排列； 堆排序步骤： 创建一个堆（建堆后数组中的第一个元素就是堆顶，大顶堆的堆顶也就是最大的元素）。 堆顶（最大值）和堆尾互换（最大元素就放到了下标为 n 的位置）。 剩下 n-1 个元素重复这个过程(堆化-&gt;取堆顶)。 直到最后堆中只剩下标为 1 的一个元素，排序完成。 Java代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 堆排序 * len 表示数据的个数，数组a中的数据从下标1到n的位置。 */public void heapSort(int[] arr, int len) &#123; buildHeap(arr, len); int k = len; while (k &gt; 1) &#123; swapArr(arr, 1, k); --k; heapify(arr, k, 1); &#125;&#125;/** * 构建堆 */private void buildHeap(int[] arr, int len) &#123; for (int i = len/2; i &gt;= 1; --i) &#123; heapify(arr, len, i); &#125;&#125;/** * 堆化 */private void heapify(int[] arr, int len, int i) &#123; while (true) &#123; int maxPos = i; if (i*2 &lt;= len &amp;&amp; arr[i] &lt; arr[i*2]) maxPos = i*2; if (i*2+1 &lt;= len &amp;&amp; arr[maxPos] &lt; arr[i*2+1]) maxPos = i*2+1; if (maxPos == i) break; swapArr(arr, i, maxPos); i = maxPos; &#125;&#125;/** * 交换 */private void swapArr(int[] arr, int a, int b) &#123; int temp = arr[a]; arr[a] = arr[b]; arr[b] = temp;&#125; 8. 计数排序（Counting Sort）计数排序（Counting Sort）是一种牺牲内存空间来换取低时间复杂度的排序算法，同时它也是一种不基于比较的算法。它的优势在于在对一定范围内的整数排序时，它的复杂度为Ο(n+k)（其中k是整数的范围），快于任何比较排序算法。它适合于最大值和最小值的差值不是不是很大的排序。 基本思想：就是把数组元素作为数组的下标，然后用一个临时数组统计该元素出现的次数，例如 temp[i] &#x3D; m, 表示元素 i 一共出现了 m 次。最后再把临时数组统计的数据从小到大汇总起来，此时汇总起来是数据是有序的。 计数排序步骤： 找出待排序数组中最大值 max 和最小值 min； 创建临时数组，大小为 (max - min + 1)； 统计待排序数组中每个值为i的元素出现的次数，存入临时数组的第i项； 反向填充目标数组：依次从临时数组中取出放入新数组。 Java代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 计数排序 */public void countingSort(int[] arr, int len) &#123; if (len &lt;= 1 ) return; // 寻找数组的最大值与最小值 int min = getMin(arr); int max = getMax(arr); // 创建大小为(max - min + 1)的临时数组 int range = max - min + 1; int[] temp = new int[range]; // 统计元素i出现的次数 for (int i = 0; i &lt; len; i++) &#123; temp[arr[i] - min]++; &#125; // 把临时数组统计好的数据汇总到原数组 int k = 0; for (int i = 0; i &lt; range; i++) &#123; for (int j = temp[i]; j &gt; 0; j--) &#123; arr[k++] = i + min; &#125; &#125;&#125;/** * 寻找数组的最大值 */private int getMax(int[] arr) &#123; int max = arr[0]; for (int value : arr) &#123; if (max &lt; value) &#123; max = value; &#125; &#125; return max;&#125;/** * 寻找数组的最小值 */private int getMin(int[] arr) &#123; int min = arr[0]; for (int value : arr) &#123; if (min &gt; value) &#123; min = value; &#125; &#125; return min;&#125; 9. 桶排序（Bucket Sort）桶排序（Bucket sort）是计数排序的升级版。核心思想是将要排序的数据分到几个有序的“桶”里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。桶排序的表现取决于数据的分布。当数据在各个桶之间的分布是比较均匀的，排序效率非常高；在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。 Java代码实现： 1234567891011121314151617181920212223242526272829303132/** * 桶排序 */public void bucketSort(int[] arr, int len) &#123; if (len &lt;= 1 ) return; // 寻找数组的最大值与最小值 int min = getMin(arr); int max = getMax(arr); // 创建 (max - min) / 5 + 1 个桶，第 i 桶存放 5*i ~ 5*i+5-1范围的数 int range = max - min; int bucketNum = range / 5 + 1; ArrayList&lt;LinkedList&lt;Integer&gt;&gt; bucketList = new ArrayList&lt;&gt;(bucketNum); // 初始化桶 for (int i = 0; i &lt; bucketNum; i++) &#123; bucketList.add(new LinkedList&lt;Integer&gt;()); &#125; // 遍历原数组，将每个元素放入桶中 for (int a : arr) &#123; bucketList.get((a - min) / range).add(a - min); &#125; //对桶内的元素进行排序，我这里采用系统自带的排序工具 for (int i = 0; i &lt; bucketNum; i++) &#123; Collections.sort(bucketList.get(i)); &#125; //把每个桶排序好的数据进行合并汇总放回原数组 int k = 0; for (int i = 0; i &lt; bucketNum; i++) &#123; for (Integer t : bucketList.get(i)) &#123; arr[k++] = t + min; &#125; &#125;&#125; 10. 基数排序（Radix Sort）基数排序（Radix Sort）要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。基数排序的方式可以采用最低位优先LSD（Least sgnificant digital）法或最高位优先MSD（Most sgnificant digital）法，LSD的排序方式由键值的最右边开始，而MSD则相反，由键值的最左边开始。 基数排序步骤： 将所有待比较数值统一为同样的数位长度，数位较短的数前面补零。 然后，从最低位开始，依次进行一次排序。 这样从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列。 Java代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 基数排序 */public void radixSort(int[] arr, int len) &#123; if (len &lt;= 1 ) return; // 计算最大值是几位数 int num = getMaxNum(arr); // 创建10个桶 ArrayList&lt;LinkedList&lt;Integer&gt;&gt; bucketList = new ArrayList&lt;&gt;(10); // 初始化桶 for (int i = 0; i &lt; 10; i++) &#123; bucketList.add(new LinkedList&lt;Integer&gt;()); &#125; // 进行每一趟的排序，从个位数开始排 for (int i = 1; i &lt;= num; i++) &#123; for (int j = 0; j &lt; len; j++) &#123; // 获取每个数最后第 i 位是数组 int radio = (arr[j] / (int)Math.pow(10,i-1)) % 10; // 放进对应的桶里 bucketList.get(radio).add(arr[j]); &#125; // 合并放回原数组 int k = 0; for (int j = 0; j &lt; 10; j++) &#123; for (Integer t : bucketList.get(j)) &#123; arr[k++] = t; &#125; // 取出来合并了之后把桶清光数据 bucketList.get(j).clear(); &#125; &#125;&#125;/** * 计算最大值是几位数 */private int getMaxNum(int[] arr) &#123; int max = arr[0]; // 寻找数组的最大值 for (int value : arr) &#123; if (max &lt; value) &#123; max = value; &#125; &#125; // 计算最大值是几位数 int num = 1; while (max / 10 &gt; 0) &#123; num++; max = max / 10; &#125; return num;&#125;","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」图（Graph）","date":"2022-05-10T11:02:00.000Z","path":"2022/05/10/data-structure-graph.html","text":"图（Graph）。和树比起来，这是一种更加复杂的非线性表结构，由顶点和连接每对顶点的边所构成的抽象网络就是图。 图的定义：图是由顶点的有穷非空集合和顶点之间边的集合组成，通常表示为：G(V,E)，其中，G表示一个图，V是顶点的集合，E是边的集合。 图中的元素叫做顶点（vertex）。顶点与其他顶点建立的连接关系叫做边（edge）。跟顶点相连接的边的条数叫做顶点的度（degree）。 如果图中任意两个顶点之间的边都是无向边（边没有方向），则称该图为无向图（Undirected graphs）。以此类推，把边有方向的图称为有向图（Directed graphs）。 完全图： ①无向完全图：在无向图中，如果任意两个顶点之间都存在边，则称该图为无向完全图。 ②有向完全图：在有向图中，如果任意两个顶点之间都存在方向互为相反的两条弧，则称该图为有向完全图。 当一个图接近完全图时，则称它为稠密图（Dense Graph），而当一个图含有较少的边时，则称它为稀疏图（Spare Graph）。 1. 图的存储方式图的两个主要的存储方式：邻接矩阵和邻接表。邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。 邻接矩阵（Adjacency Matrix）邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 A[i][j]和 A[j][i]标记为 1；对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 A[i][j]标记为 1；对于带权图，数组中就存储相应的权重。 用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间。 邻接表（Adjacency List）图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶点有边相连的顶点。 邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储起来比较节省空间，但是使用起来就比较耗时间。 可以将邻接表中的链表改成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表、有序动态数组等。 2. 深度和广度优先搜索算法是作用于具体数据结构之上的，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。这是因为，图这种数据结构的表达能力很强，大部分涉及搜索的场景都可以抽象成“图”。 2.1 广度优先搜索（BFS）广度优先搜索（Breadth-First-Search），我们平常都简称 BFS。直观地讲，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。 尽管广度优先搜索的原理挺简单，但代码实现还是稍微有点复杂度。 代码里面，bfs() 函数就是基于之前定义的，图的广度优先搜索的代码实现。其中 s 表示起始顶点，t 表示终止顶点。我们搜索一条从 s 到 t 的路径。实际上，这样求得的路径就是从 s 到 t 的最短路径。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 广度优先搜索 * @param int s 起始顶点 * @param int t 终止顶点 */public void bfs(int s, int t) &#123; if (s == t) return; // visited 是用来记录已经被访问的顶点，用来避免顶点被重复访问。 // 如果顶点 q 被访问，那相应的 visited[q]会被设置为 true boolean[] visited = new boolean[v]; visited[s]=true; // queue 是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。 // 因为广度优先搜索是逐层访问的，也就是说，我们只有把第 k 层的顶点都访问完成之后，才能访问第 k+1 层的顶点。 // 当我们访问到第 k 层的顶点的时候，我们需要把第 k 层的顶点记录下来，稍后才能通过第 k 层的顶点来找第 k+1 层的顶点。 // 所以，我们用这个队列来实现记录的功能。 Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); queue.add(s); // prev 用来记录搜索路径。 // 当我们从顶点 s 开始，广度优先搜索到顶点 t 后，prev 数组中存储的就是搜索的路径。 // 不过，这个路径是反向存储的。prev[w]存储的是，顶点 w 是从哪个前驱顶点遍历过来的。 // 比如，我们通过顶点 2 的邻接表访问到顶点 3，那 prev[3]就等于 2。 // 为了正向打印出路径，我们需要递归地来打印。 int[] prev = new int[v]; for (int i = 0; i &lt; v; ++i) &#123; prev[i] = -1; &#125; while (queue.size() != 0) &#123; int w = queue.poll(); for (int i = 0; i &lt; adj[w].size(); ++i) &#123; int q = adj[w].get(i); if (!visited[q]) &#123; prev[q] = w; if (q == t) &#123; print(prev, s, t); return; &#125; visited[q] = true; queue.add(q); &#125; &#125; &#125;&#125;/** * 递归打印s-&gt;t的路径 */private void print(int[] prev, int s, int t) &#123; if (prev[t] != -1 &amp;&amp; t != s) &#123; print(prev, s, prev[t]); &#125; System.out.print(t + &quot; &quot;);&#125; 广度优先搜索的分解图： 最坏情况下，终止顶点 t 离起始顶点 s 很远，需要遍历完整个图才能找到。这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以，广度优先搜索的时间复杂度是 O(V+E)，其中，V 表示顶点的个数，E 表示边的个数。当然，对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，E 肯定要大于等于 V-1，所以，广度优先搜索的时间复杂度也可以简写为 O(E)。 广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、prev 数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是 O(V)。 2.2 深度优先搜索（DFS）深度优先搜索（Depth-First-Search），简称 DFS。最直观的例子就是“走迷宫”。假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。 如下图。用深度递归算法搜索一条从 s 到 t 的路径，实线箭头表示遍历，虚线箭头表示回退。从图中我们可以看出，深度优先搜索找出来的路径，并不是顶点 s 到顶点 t 的最短路径。 深度优先搜索用的是一种比较著名的算法思想，回溯思想。这种思想解决问题的过程，非常适合用递归来实现。 深度优先搜索代码实现： 123456789101112131415161718192021222324252627282930boolean found = false; // 全局变量或者类成员变量public void dfs(int s, int t) &#123; // found，的作用是，当我们已经找到终止顶点 t 之后，我们就不再递归地继续查找了。 found = false; boolean[] visited = new boolean[v]; int[] prev = new int[v]; for (int i = 0; i &lt; v; ++i) &#123; prev[i] = -1; &#125; recurDfs(s, t, visited, prev); print(prev, s, t);&#125;private void recurDfs(int w, int t, boolean[] visited, int[] prev) &#123; if (found == true) return; visited[w] = true; if (w == t) &#123; found = true; return; &#125; for (int i = 0; i &lt; adj[w].size(); ++i) &#123; int q = adj[w].get(i); if (!visited[q]) &#123; prev[q] = w; recurDfs(q, t, visited, prev); &#125; &#125;&#125; 从上图可以看出，每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是 O(E)，E 表示边的个数。深度优先搜索算法的消耗内存主要是 visited、prev 数组和递归调用栈。visited、prev 数组的大小跟顶点的个数 V 成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是 O(V)。 2.3 深度和广度优先搜索总结广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法。广度优先搜索（BFS）和 深度优先搜索（DFS）搜索统称暴力搜索算法，适用于图不大的搜索。广度优先搜索，通俗的理解就是，地毯式层层推进，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。在执行效率方面，深度优先和广度优先搜索的**时间复杂度都是 O(E)，空间复杂度是 O(V)**。","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」堆（Heap）","date":"2022-05-02T14:10:01.000Z","path":"2022/05/02/data-structure-heap.html","text":"堆的两点要求： 堆是一个完全二叉树； 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。 对于每个节点的值都大于等于子树中每个节点值的堆，我们叫做大顶堆。 对于每个节点的值都小于等于子树中每个节点值的堆，我们叫做小顶堆。 完全二叉树：除最后一层外，其他层的节点都满；并且最后一层的节点从左到右是连续排列，中间没有断开，空位都在右边。 在构建完全二叉树的时候，新加入的节点在最后一层从左往右依次排列，直到排满为止。 有两种方法存储一棵二叉树，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。堆是一种完全二叉树，最常用的存储方式就是基于数组的顺序存储法。 1. 基于数组的顺序存储法。如下图，我们把根节点存储在数组下标 i=1 的位置，那左子节点存储在下标 2*i = 2 的位置，右子节点存储在 2*i+1 = 3 的位置。以此类推，B节点的左子节点存储在 2*i = 2*2 = 4 的位置，右子节点存储在 2*i+1 = 2*2+1 = 5 的位置。 如果节点X存储在数组中下标为 i 的位置，下标为 2*i 的位置存储的就是左子节点，下标为 2*i+1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（通常根节点会存储在下标为1的位置），这样就可以通过下标计算，把整棵树都串起来。 2. 堆化（heapify）往堆中插入或者删除一个元素后，需要继续满足堆的两个特性，而这个重新维持堆特性的过程称为堆化（heapify）。 堆化非常简单，就是顺着节点所在的路径，向上或者向下，比较，然后交换。堆化实际上有两种：从下往上、从上往下。 插入元素时涉及的是从下往上的堆化方法。 删除堆顶元素涉及的是从上往下的堆化方法。 3. 往堆中插入一个元素把新插入的元素放到堆的最后，需要继续满足堆的两个特性。 从下往上的堆化：让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。 往堆中插入数据的过程，翻译成代码： 12345678910111213141516171819202122public class Heap &#123; private int[] a; // 数组，从下标1开始存储数据 private int n; // 堆可以存储的最大数据个数 private int count; // 堆中已经存储的数据个数 public Heap(int capacity) &#123; a = new int[capacity + 1]; n = capacity; count = 0; &#125; public void insert(int data) &#123; if (count &gt;= n) return; // 堆满了 ++count; a[count] = data; int i = count; while (i/2 &gt; 0 &amp;&amp; a[i] &gt; a[i/2]) &#123; // 自下往上堆化 swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素 i = i/2; &#125; &#125; &#125; 4. 删除堆顶元素从堆的特性可以看出，堆顶元素存储的就是堆中数据的最大值或者最小值。从上往下的堆化：当我们删除堆顶元素之后，把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。 堆顶元素删除的过程，翻译成代码： 1234567891011121314151617public void removeMax() &#123; if (count == 0) return -1; // 堆中没有数据 a[1] = a[count]; --count; heapify(a, count, 1);&#125;private void heapify(int[] a, int n, int i) &#123; // 自上往下堆化 while (true) &#123; int maxPos = i; if (i*2 &lt;= n &amp;&amp; a[i] &lt; a[i*2]) maxPos = i*2; if (i*2+1 &lt;= n &amp;&amp; a[maxPos] &lt; a[i*2+1]) maxPos = i*2+1; if (maxPos == i) break; swap(a, i, maxPos); i = maxPos; &#125;&#125; 2. 堆化的时间复杂度一个包含 n 个节点的完全二叉树，树的高度不会超过 log2​n。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是 O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn)。 3. 堆排序（heap sort）堆排序（heap sort）是基于堆的一种排序，堆排序的过程大致分为两大步骤：建堆和排序。 大顶堆：每个节点的值都大于等于子树中每个节点值。小顶堆：每个节点的值都小于等于子树中每个节点值。求升序用大顶堆，求降序用小顶堆。 3.1 堆的构建这里提供一种构建思路：从最后一个非叶子节点开始从后往前处理数组，并且每个数据都是从上往下堆化。 包含 n 个节点的完全二叉树来说，下标从 2/n​+1 到 n 的节点都是叶子节点；我们对下标从 2/n​ 开始到 1 的节点（非叶子节点）开始处理数组。 如下图，9个节点，从数组下标4开始处理数组，构建大顶堆。 实现代码： 123456789101112131415private static void buildHeap(int[] a, int n) &#123; for (int i = n/2; i &gt;= 1; --i) &#123; heapify(a, n, i); &#125;&#125;private static void heapify(int[] a, int n, int i) &#123; while (true) &#123; int maxPos = i; if (i*2 &lt;= n &amp;&amp; a[i] &lt; a[i*2]) maxPos = i*2; if (i*2+1 &lt;= n &amp;&amp; a[maxPos] &lt; a[i*2+1]) maxPos = i*2+1; if (maxPos == i) break; swap(a, i, maxPos); i = maxPos; &#125;&#125; 堆排序的建堆过程的时间复杂度是 O(n)。 每个节点堆化的时间复杂度是 O(logn)，因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度成正比。通过将每个非叶子节点的高度求和，最终求得建堆的时间复杂度就是 O(n)。 3.2 排序建堆结束之后，数组中的第一个元素就是堆顶，大顶堆的堆顶也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。剩下 n-1 个元素重复这个过程(堆化-&gt;取堆顶)。一直重复，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。 堆排序的过程代码： 12345678910// n表示数据的个数，数组a中的数据从下标1到n的位置。public static void sort(int[] a, int n) &#123; buildHeap(a, n); int k = n; while (k &gt; 1) &#123; swap(a, 1, k); --k; heapify(a, k, 1); &#125;&#125; 整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlogn)，所以，堆排序整体的时间复杂度是 O(nlogn)。堆排序不是稳定的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。 在实际开发中，为什么快速排序要比堆排序性能好？ 第一点，堆排序数据访问的方式没有快速排序友好。 对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的（堆化）。 CPU对于顺序访问的数据更加友好，可以做缓存。 第二点，对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。 堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。 4. 堆的应用4.1 优先级队列在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。 用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。 优先级队列的应用广泛，如赫夫曼编码，图的最短路径，做小生成树的算法等。 4.2 利用堆求Top K 针对静态数据： 可以维护一个大小为k的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果堆顶元素大，就将堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前k大数据了。 遍历数据需要O(n)的时间复杂度，一次堆化操作需要O(logk)的时间复杂度，最坏情况下，n个元素都入堆一次，时间复杂度就是O(nlogk)。 针对动态数据：求得Topk就是实时Topk。 一个数据集合有两个操作，一个是添加数据，另一个询问当前的前k大数据。 可以维护一直都维护一个k大小的小顶堆，当有数据被添加到集合时，就那它与堆顶的元素对对比。如果比堆顶元素大，就把堆顶元素删除，并将这个元素插入到堆中，如果比堆顶元素小，这不处理。这样，无论任何时候需要查询当前的前k大数据，就都可以 立刻返回给他。 4.2 利用堆求中位数 对于一组静态数据，中位数是固定的，可以先排序，第n/2个数据就是中位数。 对于动态数据集合，就无法先排序了，需要借助堆这种数据结构，我们不用排序，就可以非常高效的实现求中位数操作。 实现思路： 需要维护两个堆，大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。 如果新加入的数据小于等于大顶堆的堆顶元素，就将这个数据插入到大顶堆；否则就插入小顶堆。 当两个堆中的数据量不服和中位数的约定时，就从一个堆中不停的将堆顶的元素移动到另一个堆，重新让两个堆中数据满足上面的约定。 可以利用两个堆实现动态数据集合中求中位数的操作，插入数据因为涉及堆化，所以时间复杂度变成了O(logn)，但求中位数只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是O(1)。","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」红黑树（Red-Black Tree）","date":"2022-04-25T11:05:10.000Z","path":"2022/04/25/data-structure-redblacktree.html","text":"平衡二叉查找树其实有很多，比如，红黑树（Red-Black Tree，简称 R-B Tree）、伸展树（Splay Tree）、树堆（Treap）等，但是我们提到平衡二叉查找树，听到的基本都是红黑树，它是一种不严格的平衡二叉查找树。红黑树是一种含有红黑节点并能自平衡的二叉查找树。它必须满足下面性质： 每个节点要么是红色，要么是黑色； 根节点是黑色的； 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据； 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的； 任意一节点到每个叶子节点的路径，都包含相同数目的黑色节点； 1. 红黑树的平衡特征红黑树与AVL树类似，都在添加和删除的时候通过旋转操作保持二叉树的平衡，以求更高效的查询性能。 红黑树牺牲了部分平衡性，以换取插入&#x2F;删除操作时较少的旋转操作，整体来说性能要优于AVL树。 红黑树的黑色属性（即性质第五点）：任意一节点到每个叶子节点的路径，都包含相同数目的黑色节点。黑色属性，可以理解为平衡特征， 如果满足不了平衡特征，就要进行平衡操作（变色、左旋、右旋）。 红黑树的平衡条件，是以黑色节点的高度来约束的，所以称红黑树这种平衡为黑色完美平衡。 如下图，去掉红色节点，会得到一个四叉树， 从根节点到每一个叶子节点高度相同，就是红黑树的根节点到叶子的黑色路径长度。 2. 红黑树的平衡调整三种操作：左旋（rotate left）、右旋（rotate right）和 变色。 左旋，即是逆时针旋转，以某个节点作为旋转点，其右子节点变为旋转节点的父节点，右子节点的左子节点变为旋转节点的右子节点，左子节点保持不变； 右旋，即是顺时针旋转，以某个节点作为旋转点，其左子节点变为旋转节点的父节点，左子节点的右子节点变为旋转节点的左子节点，右子节点保持不变。 变色，节点的颜色由红变黑或由黑变红； 3. 插入操作的平衡调整红黑树规定，插入的节点一定是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。 红黑树的平衡调整过程是一个迭代的过程。我们把正在处理的节点叫做关注节点。关注节点会随着不停地迭代处理，而不断发生变化。最开始的关注节点就是新插入的节点。 新插入节点的父节点是黑色的，红黑树不需要调整。 新插入节点是根节点，根节点变黑即可。 新插入节点的父节点和它的叔叔都是红色，红黑树只需要变色，不需要旋转。 新插入节点的父节点是红色，但它叔叔是黑色（可能为null的黑色空节点），红黑树需要变色+旋转。 需要红黑树变色+旋转（父节点红色，叔节点黑色）的四种情形： 一、LL型（左左插），即父节点和插入的节点都是左节点： ①父节点变黑色，爷节点变红色 ②爷节点右旋 二、LR型（左右插），即父节点是左节点，插入节点是右节点： ①父节点左旋，父子交换变成了【情形一·LL】 ②跳到【LL型】的调整 三、RR型（右右插），即父节点和插入的节点都是右节点： ①父节点变黑色，爷节点变红色 ②爷节点左旋 二、RL型（右左插），即父节点是右节点，插入的节点是左节点： ①父节点右旋，父子交换变成了【情形三·RR】 ②跳到【RR型】的调整 4. 删除操作的平衡调整红黑树的删除情况相对插入会复杂一些，不过原理都是类似的。 4.1 二叉查找树的删除操作 待删除节点没有子节点，将父节点中，指向要删除节点的指针置为null。 待删除节点只有一个子节点，更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点。 待删除节点有两个子节点，用后继节点或者前继节点替换删除节点，然后再删除掉替换节点。 后继节点：删除节点的右子树中的最小节点，即右子树中最左节点。 前继节点：删除节点的左子树中最大节点，即左子树中最右节点。 4.2 红黑树的删除动作红黑树和二叉查找树的删除类似，只不过加上颜色属性（这里的子节点均指非黑色叶子NULL节点）： 无子节点时，删除节点可能为红色或者黑色； 1.1 如果为红色，直接删除即可，不会影响黑色节点的数量； 1.2 如果为黑色，则需要进行删除平衡的操作（如果是根节点，无需平衡操作）； 只有一个子节点时，删除节点只能是黑色，其子节点是红色（否则无法满足红黑树的性质）。 此时用删除节点的子节点接到父节点，且将子节点颜色涂黑，保证黑色数量。 有两个子节点时，与二叉搜索树一样，使用后继节点作为替换的删除节点，情形转至为【1】或【2】处理。 删除情形【3】总是会转换为情形【1】或【2】的，而情形【1.2】（删除节点无子节点且是黑色）还需要额外的平衡调整；因为一旦该节点被拿掉，红黑树中通过该节点的路径黑色节点数量将会减1，而且无法像情形【2】那样将子节点涂黑来达到平衡。此时只能自底向上进行平衡操作。 4.3 红黑树删除后的平衡操作为了简化描述，先约定关注节点及相关节点的名称： 【情形 1.1】： 兄黑（S 黑），兄子全黑（L 和 R 全黑），父红（P 红）；即 兄弟节点为黑色，兄弟节点的子节点全部黑色，父节点为红色。 【情形 1.2】： 兄黑（S 黑），兄子全黑（L 和 R 全黑），父黑（P 黑）；即 兄弟节点为黑色，兄弟节点的子节点全部黑色，父节点为黑色。 【情形 2.1】： 兄黑（S 黑），兄在左（S 左），兄左子红（L 红）；即 兄弟节点为黑色，且兄弟节点在左边，兄弟节点的左子节点为红色。 【情形 2.2】： 兄黑（S 黑），兄在左（S 左），兄左子黑（L 黑）；即 兄弟节点为黑色，且兄弟节点在左边，兄弟节点的左子节点为黑色。 【情形 2.3】： 兄黑（S 黑），兄在右（S 右），兄右子红（R 红）；即 兄弟节点为黑色，且兄弟节点在右边，兄弟节点的右子节点为红色。 【情形 2.4】： 兄黑（S 黑），兄在右（S 右），兄右子黑（R 黑）；即 兄弟节点为黑色，且兄弟节点在右边，兄弟节点的右子节点为黑色。 【情形 3.1】： 兄红（S 红），兄在左（S 左）；即 兄弟节点为红色，且兄弟节点在左边。 【情形 3.2】： 兄红（S 红），兄在右（S 右）；即 兄弟节点为红色，且兄弟节点在右边。 4.4 红黑树删除总结删除动作（移除节点）之后，若待平衡节点是黑色的叶子节点：平衡调整情形，主要根据 [兄节点的位置&#x2F;颜色]、[兄的子节点的颜色]、[父节点颜色] 进行分类： 5. 红黑树的应用场景二叉查找树的时间复杂度会受到其树深度的影响，而红黑树可以保证在最坏情况下的时间复杂度仍为O(lgn)。当数据量多到一定程度时，使用红黑树比二叉查找树的效率要高。 平衡二叉树的时间复杂度是O(logn)，红黑树的时间复杂度为O(lgn)，两者都表示的都是时间复杂度为对数关系（lg 函数为底是 10 的对数）。 同平衡二叉树相比较，红黑树没有像平衡二叉树对平衡性要求的那么苛刻，虽然两者的时间复杂度相同，但是红黑树在实际测算中的速度要更胜一筹！ 红黑树的使用非常广泛， 如Java的TreeMap和TreeSet都是基于红黑树实现的（相对与HashMap优势，内部key保持有序，且支持自定义排序比较器）； 而JDK8中HashMap当链表长度大于8时也会转化为红黑树（时间复杂度从O(n)--&gt;O(lgn) ，且自旋开销比其他树低）。 其他：如epoll在内核中的实现，用红黑树管理事件块（文件描述符）；linux进程调度Completely Fair Scheduler，用红黑树管理进程控制块；nginx中，用红黑树管理timer等。","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」AVL树","date":"2022-04-15T12:05:10.000Z","path":"2022/04/15/data-structure-avltree.html","text":"AVL树（得名于发明者G. M. Adelson-Velsky 和 E. M. Landis）本质上是一棵带有平衡条件的二叉搜索树。AVL树具有以下2个性质： 左子树和右子树的深度之差的绝对值不超过1； 左子树和右子树全都是 AVL树。 其中为了度量左右子树的深度之差，我们引入平衡因子(BF)的概念。 平衡因子： 某个节点的左子树的高度减去右子树的高度得到的差值。 对于一棵 AVL树，里面的所有节点的平衡因子只能取值于-1、0、1，否则，AVL树将是不平衡的并且需要平衡调整。 1. AVL 树平衡调整二叉树的平衡化有两大基础操作： 左旋（rotate left）和右旋（rotate right）。 左旋，即是逆时针旋转，以某个节点作为旋转点，其右子节点变为旋转节点的父节点，右子节点的左子节点变为旋转节点的右子节点，左子节点保持不变； 右旋，即是顺时针旋转，以某个节点作为旋转点，其左子节点变为旋转节点的父节点，左子节点的右子节点变为旋转节点的左子节点，右子节点保持不变。 这种旋转在整个平衡化过程中可能进行一次或多次，这两种操作都是从失去平衡的最小子树根节点开始的(即离插入节点最近且平衡因子超过1的祖节点)。 造成失衡一共有 4 种情况：LL型、LR型、RL型、RR型，如下图所示。 LL型平衡调整：对节点C右旋即可。 LR型平衡调整：先对A进行一次左旋再对C进行一次右旋。 RL型平衡调整：先对C进行一次右旋再对A进行一次左旋。 RR型平衡调整：对节点A左旋即可。 2. 模拟建 AVL 树按照整数序列 {4,5,7,2,1,3,6} 依次插入AVL树。 由于AVL树必须保证左右子树平衡(左子树和右子树的深度之差的绝对值不超过1)， 所以在插入的时候很容易出现不平衡的情况，一旦这样，就需要进行旋转以求达到平衡。 正是由于这种严格的平衡条件，导致AVL需要花大量时间在调整上，故AVL树一般适用于查询场景， 而不适用于增删频繁的场景。","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」二叉树","date":"2022-04-08T10:08:18.000Z","path":"2022/04/08/data-structure-binarytree.html","text":"树：树是一种非线性的数据结构，一棵树是n（n&gt;=0）个节点的集合。用来连接相邻节点之间的关系，我们叫做“父子关系”。我们把没有父节点的节点叫做根节点，节点的上一层节点是其父节点，下一层节点是其子节点，拥有相同父节点的子节点之间互称为兄弟节点。 树的三个比较相似的概念：高度（Height）、深度（Depth）、层（Level）。 节点的高度：节点到叶子节点的最长路径（边数） 节点的深度：根节点到这个节点所经历的路径（边数） 节点的层数：节点的深度 +1 树的高度：根节点的高度。 1. 二叉树二叉树：每个节点最多有两个子节点，分别是左子节点和右子节点。二叉树的递归定义为：二叉树是一棵空树，或者是一棵由一个根节点和两棵互不相交的，分别称作根的左子树和右子树组成的非空树；左子树和右子树又同样都是二叉树。 两个比较特殊的二叉树： 满二叉树：树中每个分支节点（非叶节点）都有两棵非空子树，并且所有的叶节点都在同一层。 完全二叉树：除最后一层外，其他层的节点都满；并且最后一层的节点从左到右是连续排列，中间没有断开，空位都在右边。 有两种方法存储一棵二叉树，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。 2. 存储二叉树基于基于指针的链式存储法。每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。从根节点开始，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。 基于数组的顺序存储法。如下图，我们把根节点存储在数组下标 i=1 的位置，那左子节点存储在下标 2*i = 2 的位置，右子节点存储在 2*i+1 = 3 的位置。以此类推，B节点的左子节点存储在 2*i = 2*2 = 4 的位置，右子节点存储在 2*i+1 = 2*2+1 = 5 的位置。 如果节点X存储在数组中下标为 i 的位置，下标为 2*i 的位置存储的就是左子节点，下标为 2*i+1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（通常根节点会存储在下标为1的位置），这样就可以通过下标计算，把整棵树都串起来。 堆其实就是一种完全二叉树，最常用的存储方式就是数组。 3. 遍历二叉树经典的方法有三种，前序遍历、中序遍历和后序遍历。 前序遍历：对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。 中序遍历：对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。 后序遍历：对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。 实际上，二叉树的前、中、后序遍历就是一个递归的过程。比如，前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树。 4. 二叉查找树（Binary Search Tree）二叉查找树，也叫二叉搜索树。顾名思义，二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。 二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值都比该节点的值小，而右子树节点的值都比该节点的值大。 4.1 二叉查找树的查找操作先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。 4.2 二叉查找树的插入操作二叉查找树的插入操作二叉查找树的插入过程有点类似查找操作。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。 4.3 二叉查找树的删除操作分三种情况来处理。 如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为null。(比如图中的删除节点55。) 如果要删除的节点只有一个子节点，我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。(比如图中的删除节点13。) 如果要删除的节点有两个子节点，可以用后继节点或者前继节点替换删除节点，然后再删除掉替换节点。（比如图中的删除节点18。) 后继节点：删除节点的右子树中的最小节点，即右子树中最左节点。 前继节点：删除节点的左子树中最大节点，即左子树中最右节点。 4.2 二叉查找树的其他操作 除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点。 中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效。因此，二叉查找树也叫作二叉排序树。 支持重复数据的二叉查找树 通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。 每个节点仍然只存储一个数据。新插入值相同的数据当作大于这个节点的值来处理。查找时，遇到值相同的节点不停止，而是继续在右子树中查找，直到遇到叶子节点才停止。这样就可以把键值等于要查找值的所有节点都找出来。删除时，也需要先查找到每个要删除的节点，依次删除。 5. 平衡二叉树二叉查找树支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，也就是O(height)，完全二叉树的高度height&lt;=log2n；理想情况下，时间复杂度是O(logn)。不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于log2n的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到O(n)。 为了解决二叉查找树因为动态更新导致的性能退化问题，发明了平衡二叉查找树这类数据结构。平衡二叉查找树的高度接近logn，所以插入、删除、查找操作的时间复杂度也比较稳定，是O(logn)。在工程中，很多用到平衡二叉查找树的地方都会用红黑树。红黑树是一种特殊的平衡二叉查找树。 平衡二叉树的严格定义：**二叉树中任意一个节点的左右子树的高度相差不能大于1**。 从这个定义来看，完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。 但是很多平衡二叉查找树其实并没有符合严格的定义，比如红黑树，它从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。 所以，平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」哈希算法","date":"2022-03-25T13:40:12.000Z","path":"2022/03/25/data-structure-hashalgorithm.html","text":"哈希算法（Hash）又称摘要算法（Digest），它的作用是：对任意一组输入数据进行计算，得到一个固定长度的输出摘要。 哈希算法最重要的特点就是：相同的输入一定得到相同的输出；不同的输入大概率得到不同的输出。 哈希算法的目的就是为了验证原始数据是否被篡改。 1234// Java字符串的hashCode()就是一个哈希算法，它的输入是任意字符串，输出是固定的4字节int整数：&quot;hello&quot;.hashCode(); // 0x5e918d2&quot;hello, java&quot;.hashCode(); // 0x7a9d88e8&quot;hello, bob&quot;.hashCode(); // 0xa0dbae2f 哈希碰撞是指，两个不同的输入得到了相同的输出。一个安全的哈希算法必须满足：碰撞概率低，不能猜测输出。 12&quot;AaAaAa&quot;.hashCode(); // 0x7460e8c0&quot;BBAaBB&quot;.hashCode(); // 0x7460e8c0 1. 常用的哈希算法 算法 输出长度（位） 输出长度（字节） MD5 128 bits 16 bytes SHA-1 160 bits 20 bytes RipeMD-160 160 bits 20 bytes SHA-256 256 bits 32 bytes SHA-512 512 bits 64 bytes 根据碰撞概率，哈希算法的输出长度越长，就越难产生碰撞，也就越安全。 2. 哈希算法的应用哈希算法的应用非常非常多，这里介绍最常见的七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。 2.1 安全加密最常用于加密的哈希算法是 MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法）和 SHA（Secure Hash Algorithm，安全散列算法）。除了这两个之外，当然还有很多其他加密算法，比如计算机网络的SSL/TLS协议里面的对称加密（AES）、非对称加密（RSA）。 安全加密的关键点：①很难根据哈希值反向推导出原始数据；②散列冲突的概率要很小。 不管是什么哈希算法，只能尽量减少碰撞冲突的概率，理论上是无法做到完全不冲突的。一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。 没有绝对安全的加密。任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长。所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。 2.2 唯一标识哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。 比如URL字段或者图片字段要求不能重复，这个时候就可以通过哈希算法对这个数据取唯一标识，或者说信息摘要（比如MD5处理）。此外，还可以对文件之类的二进制数据做MD5处理，作为唯一标识，这样判定重复文件的时候更快捷。 2.3 数据校验网络传输数据不安全，传输数据的过程中，数据可能被宿主机篡改，或数据丢失，所以需要校验文件的安全，正确，完整性。 因为哈希算法对数据很敏感，只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同；所以在文件块下载完后，使用相同的哈希算法，对于文件计算，判断是否相同。 2.4 散列函数散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。 散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中。除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。 2.5 负载均衡负载均衡算法有很多，比如轮询、随机、加权轮询等。利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。 我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上。 2.6 数据分片针对海量数据的处理问题，通过哈希算法对处理的海量数据进行分片，多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。 2.7 分布式存储在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。 2.7.1 一致性哈希算法一致性哈希算法是对2^32进行取模运算，是一个固定的值。即0~(2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。如下图： 一致性哈希要进行的两步哈希： 对存储节点进行哈希计算，也就是对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希； 当对数据进行存储或访问时，对数据进行哈希映射； 所以，一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上。 如下图，已经有3个节点经过哈希计算，映射到了哈希环上相应的位置，有三个数据进行哈希映射到节点上；①首先，对 key 进行哈希计算，确定此 key 在环上的位置；②然后，从这个位置沿着顺时针方向走，遇到的第一节点就是存储 key 的节点。 假设节点数量从 3 增加到了 4，新的节点 D 经过哈希计算后映射到了下图中的位置；可以看到，key-1、key-3 都不受影响，只有 key-2 需要被迁移节点 D。 假设节点数量从 3 减少到了 2，比如将节点 A 移除；你可以看到，key-2 和 key-3 不会受到影响，只有 key-1 需要被迁移节点 B。 在一致性哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响。 2.7.2 虚拟节点一致性哈希算法虽然减少了数据迁移量，但是并不保证节点能够在哈希环上分布均匀，这样就会带来一个问题，会有大量的请求集中在一个节点上。在这种情况下进行容灾与扩容时，容易出现雪崩的连锁反应。如下图中 3 个节点的映射位置都在哈希环的右半边： 要想解决节点能在哈希环上分配不均匀的问题，就是要有大量的节点，节点数越多，哈希环上的节点分布的就越均匀。这个时候我们就加入虚拟节点，也就是对一个真实节点做多个副本。具体做法是，不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。 比如对每个节点分别设置 3 个虚拟节点，引入虚拟节点后，原本哈希环上只有 3 个节点的情况，就会变成有 9 个虚拟节点映射到哈希环上，哈希环上的节点数量多了 3 倍。节点数量多了后，节点在哈希环上的分布就相对均匀了。这时候，如果有访问请求寻址到「A-01」这个虚拟节点，接着再通过「A-01」虚拟节点找到真实节点 A，这样请求就能访问到真实节点 A 了。 虚拟节点除了会提高节点的均衡度，还会提高系统的稳定性。当节点变化时，会有不同的节点共同分担系统的变化，因此稳定性更高。 有了虚拟节点后，还可以为硬件配置更好的节点增加权重，比如对权重更高的节点增加更多的虚拟机节点即可。 因此，带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景。 2.7.3 分布式存储的哈希算法总结一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响。 但是一致性哈希算法不能够均匀的分布节点，会出现大量请求都集中在一个节点的情况，在这种情况下进行容灾与扩容时，容易出现雪崩的连锁反应。 为了解决一致性哈希算法不能够均匀的分布节点的问题，就需要引入虚拟节点，对一个真实节点做多个副本。不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。 引入虚拟节点后，可以会提高节点的均衡度，还会提高系统的稳定性。所以，带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景。","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」散列表（Hash Table）","date":"2022-03-18T09:46:12.000Z","path":"2022/03/18/data-structure-hashtable.html","text":"散列表（Hash Table），也叫“哈希表”或者“Hash表”。是能够通过给定的关键字的值直接访问到具体对应的值的一个数据结构。 通常，我们把这个关键字称为 Key，把对应的记录称为 Value，所以也可以说是通过 Key 访问一个映射表来得到 Value 的地址。而这个映射表，也叫作散列函数或者哈希函数，存放记录的数组叫作散列表。 散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。 Hash，一般翻译做“散列”&#x2F;“哈希”，就是把任意长度的输入，通过散列算法，变换成固定长度的输出，该输出就是散列值。(这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，这种现象叫作碰撞（Collision）。)简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。 对散列表中的键(key)进行Hash的函数就是散列函数（Hash(key)）。函数的计算结果就是散列值,就是Hash(key)的值。 1. 几种常见哈希函数： 直接寻址法：取关键字或关键字的某个线性函数值为散列地址；如Hash(key)=key或者Hash(key)=a*key+b，a和b都为常数。 数字分析法：如果关键字由多位字符或者数字组成，就可以考虑抽取其中的2位或者多位作为该关键字对应的散列地址，在取法上尽量选择变化较多的位，避免冲突发生。 平方取中法：对关键字做平方操作，取平方值的中间几位作为散列地址。此方法也是比较常用的构造哈希函数的方法。计算平方之后的中间几位和关键字中的每一位都相关，所以不同的关键字会以较高的概率产生不同的散列地址。 折叠法：将关键字分割成位数相同的几部分（最后一部分的位数可以不同），然后取这几部分的叠加和（舍去进位）作为哈希地址。此方法适合关键字位数较多的情况。 取随机数法：使用一个随机函数，取关键字的随机值作为散列地址，这种方式通常用于关键字长度不同的场合。 除留余数法：若已知整个哈希表的最大长度m，可以取一个不大于m的数p，然后对该关键字key做取余运算，即：Hash(key)=key%p。 随机数法：取关键字的一个随机函数值作为它的哈希地址，即：Hash(key)=random(key)，此方法适用于关键字长度不等的情况。 构建哈希函数，需要根据实际的查找表的情况采取适当的方法。通常考虑的因素有以下几方面： 关键字的长度。如果长度不等，就选用随机数法。如果关键字位数较多，就选用折叠法或者数字分析法；反之如果位数较短，可以考虑平方取中法； 哈希表的大小。如果大小已知，可以选用除留余数法； 关键字的分布情况； 查找表的查找频率； 计算哈希函数所需的时间（包括硬件指令的因素） ２. 几种哈希冲突的处理方式： 开放寻址法：Hash(key) = (Hash(key) + d) MOD m；（其中m为哈希表的表长，d为一个增量）。当得出的哈希地址产生冲突时，选取一种探测方法获取d的值，然后继续计算，直到计算出的哈希地址不在冲突为止。三种探测方法： 线性探测法：d&#x3D;1，2，3，…，m-1 （每次 +1，向右探测，直到有空闲的位置为止） 二次探测法：d&#x3D;12，-12，22，-22，32，… （按照 +12，-12，+22，…如此探测，直到有空闲的位置） 伪随机数探测法：d&#x3D;伪随机数 （每次加上一个随机数，直到探测到空闲位置结束） 再哈希法：当通过哈希函数求得的哈希地址同其他关键字产生冲突时，使用另一个哈希函数计算，直到冲突不再发生。 链地址法：链地址法其实就是对Key通过哈希之后落在同一个地址上的值，做一个链表。 建立一个公共溢出区：建立两张表，一张为基本表，另一张为溢出表。基本表存储没有发生冲突的数据，当关键字由哈希函数生成的哈希地址产生冲突时，就将数据填入溢出表。 ３. 散列表的特点 访问速度很快 由于散列表有散列函数，可以将指定的 Key 都映射到一个地址上，所以在访问一个 Key（键）对应的 Value（值）时，根本不需要一个一个地进行查找，可以直接跳到那个地址。所以我们在对散列表进行添加、删除、修改、查找等任何操作时，速度都很快。 需要额外的空间 首先，散列表实际上是存不满的，如果一个散列表刚好能够存满，那么肯定是个巧合。而且当散列表中元素的使用率越来越高时，性能会下降，所以一般会选择扩容来解决这个问题。另外，如果有冲突的话，则也是需要额外的空间去存储的，比如链地址法，不但需要额外的空间，甚至需要使用其他数据结构。 这个特点有个很常用的词可以表达，叫作“空间换时间”，在大多数时候，对于算法的实现，为了能够有更好的性能，往往会考虑牺牲些空间，让算法能够更快些。 无序 散列表还有一个非常明显的特点，那就是无序。为了能够更快地访问元素，散列表是根据散列函数直接找到存储地址的，这样我们的访问速度就能够更快，但是对于有序访问却没有办法应对。 可能会产生碰撞 没有完美的散列函数，无论如何总会产生冲突，这时就需要采用冲突解决方案，这也使散列表更加复杂。通常在不同的高级语言的实现中，对于冲突的解决方案不一定一样。","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」跳表（Skip List）","date":"2022-03-12T12:08:10.000Z","path":"2022/03/12/data-structure-skiplist.html","text":"跳表（Skip List）是一个动态数据结构（链表加多级索引），可以支持快速地插入、删除、查找操作的有序链表。跳表在原有的有序链表上面增加了多级索引，通过索引来实现快速查找。 1. 跳表的演化跳表的原始链表是一个有序单链表，如果要想在其中查找某个数据，只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是 O(n)。 从链表中每两个结点提取一个结点到上一级，我们把抽出来的那一级叫做索引或索引层。如下图。图中的 down 表示 down 指针，指向下一级结点。 加来一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了。跟前面建立第一级索引的方式相似，我们在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引。查找一个结点需要遍历的结点数量又减少了。这就是跳表的思想，用“空间换时间”，通过给链表建立索引，提高了查找的效率。 当元素数量较多时，建立多级索引，索引提高的效率比较大，近似于二分查找。 2. 跳表查找的时间复杂度跳表查找元素的过程是从最高级索引开始，一层一层遍历最后下沉到原始链表。所以，时间复杂度 = 索引的高度 * 每层索引遍历元素的个数。 如果每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是n/2，第二级索引的结点个数大约就是n/4，第三级索引的结点个数大约就是n/8，依次类推，那第k级索引结点的个数就是n/(2^k)。 最高级索引一般有2个元素，即：最高级索引h满足2=n/(2^h)，从而求得h=log2n-1，如果包含原始链表这一层，整个跳表的高度就是log2n。 按照前面这种索引结构，我们每一级索引都最多只需要遍历3个结点，根据时间复杂度 = 索引的高度 * 每层索引遍历元素的个数，即跳表中查找一个元素的时间复杂度为O(3*log2n)，省略常数即：O(logn)。 3. 跳表的空间复杂度跳表通过建立索引，来提高查找元素的效率，就是典型的“空间换时间”的思想，所以在空间上做了一些牺牲。 前面提到，如果每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是n/2，第二级索引的结点个数大约就是n/4，以此类推，每上升一级就减少一半，直到剩下2个结点。如果我们把每层索引的结点数写出来，就是一个等比数列。 1n/2, n/4, n/8, ..., 8, 4, 2 这几级索引的结点总和就是n/2+n/4+n/8…+8+4+2=n-2。所以，跳表的空间复杂度是O(n)。也就是说，如果将包含n个结点的单链表构造成跳表，我们需要额外再用接近n个结点的存储空间。 我们前面都是每两个结点抽一个结点到上级索引，如果我们每三个结点或五个结点，抽一个结点到上级索引，第一级索引需要大约n/3个结点，第二级索引需要大约n/9个结点，以此类推，每往上一级，索引结点个数都除以3。通过等比数列求和公式，总的索引结点大约就是n/3+n/9+n/27+...+9+3+1=n/2。尽管空间复杂度还是O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。 在软件工程中，对象数据所占的空间要远大于指针，指针大小可以忽略。 4. 跳表的插入和删除跳表这个动态数据结构，插入、删除操作的时间复杂度也是O(logn)。 4.1 插入数据跳表的插入首先查找某个数据应该插入的位置（时间复杂度是O(logn)），当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。 需要某种手段来维护索引与原始链表大小之间的平衡。当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第K级索引中。随机函数简化跳跃链表建立层级索引的复杂度，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。 如下图所示，插入数据6到跳表中，随机函数生成K=2: 4.2 删除数据跳表删除数据时，要把索引中对应节点也要删掉。删除元素的过程跟查找元素的过程类似，只不过在查找的路径上如果发现了要删除的元素x，则执行删除操作。 如下图所示，如果要删除元素9，需要把原始链表中的9和第一级索引的9都删除掉。 5. 总结跳表是查询效率近似于二分查找的有序链表；每个元素插入时随机生成它的索引层级level；最底层原始链表包含所有的元素；如果一个元素出现在x层索引，那么它肯定出现在x以下的索引层中；跳表查询、插入、删除的时间复杂度为O(logn)，与平衡二叉树接近；","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」栈（Stack）与 队列（Queue）","date":"2022-03-05T07:06:28.000Z","path":"2022/03/05/data-structure-stack-queue.html","text":"栈（Stack）和队列（Queue），严格意义上来说，也属于线性表，是一种操作受限的线性表数据结构。使用栈结构存储数据，讲究“先进后出”，即最先进栈的数据，最后出栈；使用队列存储数据，讲究“先进先出”，即最先进队列的数据，也最先出队列。 1. 栈栈（Stack）是一种只能从一端存取数据且遵循“先进后出”原则（First In Last Out，简称FILO）的线性存储结构。通常，栈的开口端被称为栈顶；相应地，封口端被称为栈底。栈只支持两个基本操作：入栈push()和出栈pop()。 1.1 栈的实现栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。 1.2 栈的应用 栈在函数调用中的应用 操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构，用来存储函数调用时的临时变量。每进入一个函数，就会将其中的临时变量作为栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。 栈在表达式求值中的应用（比如：34+13*9+44-12/3） 利用两个栈，其中一个用来保存操作数，另一个用来保存运算符。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较，若比运算符栈顶元素优先级高，就将当前运算符压入栈，若比运算符栈顶元素的优先级低或者相同，从运算符栈中取出栈顶运算符，从操作数栈顶取出2个操作数，然后进行计算，把计算完的结果压入操作数栈，继续比较。 栈在括号匹配中的应用（比如：&#123;&#125;&#123;[()]()&#125;） 用栈保存为匹配的左括号，从左到右一次扫描字符串，当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号，如果能匹配上，则继续扫描剩下的字符串。如果扫描过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。 当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明未匹配的左括号为非法格式。 实现浏览器的前进后退功能 我们使用两个栈X和Y，我们把首次浏览的页面依次压如栈X，当点击后退按钮时，再依次从栈X中出栈，并将出栈的数据一次放入Y栈。当点击前进按钮时，我们依次从栈Y中取出数据，放入栈X中。当栈X中没有数据时，说明没有页面可以继续后退浏览了。当Y栈没有数据，那就说明没有页面可以点击前进浏览了。 1.3 为什么函数调用要用“栈”来保存临时变量？ 因为函数调用的执行顺序符合后进者先出，先进者后出的特点。比如函数中的局部变量的生命周期的长短是先定义的生命周期长，后定义的生命周期短；还有函数中调用函数也是这样，先开始执行的函数只有等到内部调用的其他函数执行完毕，该函数才能执行结束。 正是由于函数调用的这些特点，根据数据结构是特定应用场景的抽象的原则，我们优先考虑栈结构。 2. 队列队列（queue）是一种遵循“先进先出”原则（First In First Out，简称FIFO）的线性存储结构。与栈结构不同的是，队列的两端都“开口”，要求数据只能从一端进，从另一端出；通常，称进数据的一端为“队尾”，出数据的一端为“队头”。最基本的操作也是两个：入队enqueue()，放一个数据到队列尾部；出队dequeue()，从队列头部取一个元素。所以，队列跟栈一样，也是一种操作受限的线性表数据结构。 2.1 队列的实现 队列存储结构的实现有以下两种方式： 顺序队列：在顺序表的基础上实现的队列结构。 链队列：在链表的基础上实现的队列结构。 跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的队列叫作顺序队列，用链表实现的队列叫作链式队列。 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。 而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。 2.2 队列的应用 循环队列 循环队列还是基于数组实现的。原本数组是有头有尾的，是一条直线。我们把首尾相连，扳成了一个环，形成逻辑上的环状空间。 阻塞队列 在队列的基础上增加阻塞操作，就成了阻塞队列。 阻塞队列就是在队列为空的时候，从队头取数据会被阻塞，因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后在返回。 从上面的定义可以看出这就是一个“生产者-消费者模型”。这种基于阻塞队列实现的“生产者-消费者模型”可以有效地协调生产和消费的速度。当“生产者”生产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了，这时生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续生产。不仅如此，基于阻塞队列，我们还可以通过协调“生产者”和“消费者”的个数，来提高数据处理效率，比如配置几个消费者，来应对一个生产者。 并发队列 在多线程的情况下，会有多个线程同时操作队列，这时就会存在线程安全问题。能够有效解决线程安全问题的队列就称为并发队列。 并发队列简单的实现就是在enqueue()、dequeue()方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或取操作。 实际上，基于数组的循环队列利用CAS原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。 线程池资源枯竭是的处理 在资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」数组与链表","date":"2022-02-28T13:05:24.000Z","path":"2022/02/28/data-structure-array-linked-list.html","text":"线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。比如数组，链表、队列、栈等也是线性表结构。而与它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。 1. 数组数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。 数组寻址 1a[i]_address = base_address + i * data_type_size 以长度为10的int数组（int[] a = new int[10]）为例：分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000，int类型的data_type_size为4个字节，即a[0]地址为1000，a[1]地址为1004… 容器是否完全替代数组？ 容器的优势：对于Java语言，容器封装了数组插入、删除等操作的细节，并且支持动态扩容。 对于Java，一些更适合用数组的场景： Java的ArrayList无法存储基本类型，需要进行装箱操作，而装箱与拆箱操作都会有一定的性能消耗，如果特别注意性能，或者希望使用基本类型，就可以选用数组。 若数组大小事先已知，并且对数组只有非常简单的操作，不需要使用到ArrayList提供的大部分方法，则可以直接使用数组。 多维数组时，使用数组会更加直观。 2. 链表链表（Linked list）也是一种线性表数据结构。它的内存结构是不连续的内存空间，是将一组零散的内存块串联起来，从而进行数据存储。链表中每一个内存块称为节点（Node），节点除了存储数据外，还需存储下一个节点的地址。 单链表：每个节点只包含一个指针，即后继指针（next）；首节点地址表示整条链表，尾节点的后继指针指向空地址null。 循环链表：尾节点的后继指针指向首节点，其他与单链表一致。 双向链表：每个节点包含两个指针，前驱指针（prev）和后继指针（next），首节点的前驱指针和尾节点后继指针都指向空地址null。 双向循环链表：首节点前驱指针指向尾节点，尾节点后继指针指向首节点，其他与双链表一致。 3. 数组vs链表 数组中的元素存在一个连续的内存空间中，若数组申请空间足够但不连续也会失败；而链表中的元素可以存在于不连续的内存空间，不过需要额外的内存空间存储指针信息。 数组支持随机访问，根据下标随机访问的时间复杂度是O(1)；链表随机访问的时间复杂度是O(n)。 链表适合插入、删除操作，时间复杂度为O(1)；数组插入、删除操作，时间复杂度为O(n)。 数组大小固定，若存储空间不足，需要进行扩容，扩容就需要数据复制，这非常耗时；链表进行频繁的插入删除操作会导致内存频繁的内存申请和释放，容易造成内存碎片，Java环境还可能造成频繁的GC(自动垃圾回收)操作。 数组在实现上使用连续的内存空间，可以借助CPU的缓冲机制预读数组中的数据，所以访问效率更高，而链表在内存中并不是连续存储，所以对CPU缓存不友好，没办法预读。 附：双向链表的Java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170/** * 双向链表 */public class LinkedList&lt;E&gt; &#123; int size = 0; /** * 头节点指针 */ Node&lt;E&gt; first; /** * 尾节点指针 */ Node&lt;E&gt; last; /** * 构造函数 */ public LinkedList() &#123;&#125; /** * 结点类（私有静态内部类） */ private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; /** * 返回size */ public int size() &#123; return size; &#125; /** * 添加元素（默认尾部添加） */ public boolean add(E e) &#123; linkLast(e); return true; &#125; /** * 添加为头元素 */ public boolean addFirst(E e) &#123; linkFirst(e); return true; &#125; /** * 删除元素 */ public boolean remove(Object o) &#123; if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); return true; &#125; &#125; &#125; return false; &#125; /** * 根据索引获取元素 */ public E get(int index) &#123; checkElementIndex(index); return node(index).item; &#125; /** * 根据索引设置元素 */ public E set(int index, E element) &#123; checkElementIndex(index); Node&lt;E&gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal; &#125; /** * e元素链接为尾部 */ void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;E&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; &#125; /** * e元素链接为头部 */ private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first; final Node&lt;E&gt; newNode = new Node&lt;E&gt;(null, e, f); first = newNode; if (f == null) last = newNode; else f.prev = newNode; size++; &#125; /** * 取消非空节点 x 的链接 */ void unlink(Node&lt;E&gt; x) &#123; // assert x != null; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; &#125; /** * 根据指定索引检查非空 */ private void checkElementIndex(int index) &#123; if (!(index &gt;= 0 &amp;&amp; index &lt; size)) throw new IndexOutOfBoundsException(&quot;Index: &quot;+index+&quot;, Size: &quot;+size); &#125; /** * 返回指定索引的（非空）节点。 */ Node&lt;E&gt; node(int index) &#123; Node&lt;E&gt; x; if (index &lt; (size &gt;&gt; 1)) &#123; x = first; for (int i = 0; i &lt; index; i++) x = x.next; &#125; else &#123; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; &#125; return x; &#125;&#125;","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「数据结构与算法」复杂度分析","date":"2022-02-26T10:12:38.000Z","path":"2022/02/26/data-structure-complexity.html","text":"数据结构与算法解决的是：如何让计算机更快时间、更省空间的解决问题。因此需要从执行时间和占用空间两个维度来评估数据结构和算法的性能，二者统称为复杂度。复杂度描述的是算法执行时间或占用系统空间与数据规模的增长关系。 和性能测试相比，复杂度分析有不依赖执行环境、成本低、效率高、易操作、指导性强的特点。掌握复杂度分析，将能编写出性能更优的代码，有利于降低系统开发和维护成本。 1. 时间复杂度与空间复杂度大O表示法：所有代码的执行时间T(n)与每行代码的执行次数f(n)成正比，用T(n) = O(f(n))表示，其中n表示数据规模的大小 T(n) = O(f(n))表示算法的执行时间与数据规模之间的增长关系，所以也叫渐进时间复杂度（asymptotic time complexity），简称时间复杂度。 类比一下，S(n)&#x3D;O(f(n))表示空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 2. 复杂度量级 多项式量级：随着数据规模的增长，算法的执行时间和空间占用，按照多项式的比例增长。这类有： 常量阶O(1) 对数阶O(logn) 线性阶O(n) 线性对数阶O(nlogn) 平方阶O(n^2)、立方阶O(n^3)、···、k次方阶O(n^k) 非多项式量级NP（NP：Non-Deterministic Polynomial，非确定多项式）：随着数据规模的增长，算法的执行时间和空间占用暴增，这类算法性能极差。非多项式量级只有两个：指数阶O(2^n) 和 阶乘阶O(n!)。 复杂度分析法则 单段代码看高频：比如循环。 多段代码取最大：比如一段代码中有单循环和多重循环，那么取多重循环的复杂度。 嵌套代码求乘积：比如递归、多重循环等 多个规模求加法：比如方法有两个参数控制两个循环的次数，那么这时就取二者复杂度相加。 3.复杂度分析的4个概念代码复杂度在不同情况下出现量级差别时，为了更全面，更准确的描述代码的时间复杂度，所以引入这4个概念。大多数情况下，是不需要区别分析它们的。 最好时间复杂度（best case time complexity）：代码在最理想情况下执行的时间复杂度。 最坏时间复杂度（worst case time complexity）：代码在最坏情况下执行的时间复杂度。 平均时间复杂度（average case time complexity）：用代码在所有情况下执行的次数的加权平均值表示。 分析：代码在不同情况下复杂度出现量级差别，则用代码所有可能情况下执行次数的加权平均值表示。 均摊时间复杂度（amortized time complexity）：在代码执行的所有复杂度情况中绝大部分是低级别的复杂度，个别情况是高级别复杂度且发生具有时序关系时，可以将个别高级别复杂度均摊到低级别复杂度上。均摊结果一般都等于低级别复杂度。 分析：两个条件满足时使用：①代码在绝大多数情况下是低级别复杂度，只有极少数情况是高级别复杂度；②低级别和高级别复杂度出现具有时序规律。均摊结果一般都等于低级别复杂度。","tags":[{"name":"算法","slug":"algorithm","permalink":"http://chaooo.github.io/tags/algorithm/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「Spring Security」前后端分离权限控制-指令级权限","date":"2021-12-30T04:28:00.000Z","path":"2021/12/30/spring-security-permission.html","text":"实现按钮级别的权限控制，基于上一篇Spring Secuirty（六）前后端分离菜单权限控制-前端动态路由的扩展。前端部分还是基于vue-element-admin模板来演示。 这里实现按钮级别的权限判断的逻辑：每个按钮对应一个权限标识，后台根据用户角色计算出当前用户可访问的权限标识列表，前端登录后得到权限标识列表存入全局，通过单个按钮的权限标识去匹配列表里的。来实现按钮级别的权限判断。 1. 数据库添加权限表123456789101112131415161718-- 系统权限表 DROP TABLE IF EXISTS `system_permission`; CREATE TABLE `system_permission` ( `id` BIGINT NOT NULL AUTO_INCREMENT COMMENT &#x27;ID&#x27;, `menu_id` BIGINT NOT NULL COMMENT &#x27;菜单ID&#x27;, `name` VARCHAR(100) DEFAULT NULL COMMENT &#x27;权限标识&#x27;, `title` VARCHAR(100) NOT NULL COMMENT &#x27;权限名称&#x27;, PRIMARY KEY (`id`) ) ENGINE=INNODB DEFAULT CHARSET=UTF8MB4 COMMENT=&#x27;系统权限表&#x27;; -- 权限&amp;角色 关联表DROP TABLE IF EXISTS `system_role_permission`;CREATE TABLE `system_role_permission` ( `id` BIGINT NOT NULL AUTO_INCREMENT, `role_id` BIGINT DEFAULT NULL COMMENT &#x27;角色ID&#x27;, `permission_id` BIGINT DEFAULT NULL COMMENT &#x27;权限ID&#x27;, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=UTF8MB4 COMMENT=&#x27;系统角色权限关联表&#x27;; 2. 前端代码改造登录后通过接口直接返回用户可访问指令级权限列表： vue-element-admin模板已经封装了一个通过角色来判断的指令权限：v-permission。 这里需要修改其逻辑： 2.1 全局state中添加permissions列表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081// ---------------- state 中添加 permissions/** * src\\store\\modules\\user.js */ // ... other codeconst state = &#123; token: getToken(), username: &#x27;&#x27;, roles: [], menus: [], permissions: [], // ... other code&#125;const mutations = &#123; SET_TOKEN: (state, token) =&gt; &#123; state.token = token &#125;, SET_PERMISSIONS: (state, permissions) =&gt; &#123; state.permissions = permissions &#125;, // ... other code&#125;const actions = &#123; // user login login(&#123; commit &#125;, userInfo) &#123; // ... other code &#125;, // get user info getInfo(&#123; commit, state &#125;) &#123; return new Promise((resolve, reject) =&gt; &#123; getInfo() .then(response =&gt; &#123; const &#123; data &#125; = response if (!data) &#123; reject(&#x27;Verification failed, please Login again.&#x27;) &#125; const &#123; roles, menus, permissions, id, username, avatar, roleDesc, fullName, phone &#125; = data // roles must be a non-empty array if (!roles || roles.length &lt;= 0) &#123; reject(&#x27;getInfo: roles must be a non-null array!&#x27;) &#125; commit(&#x27;SET_ROLES&#x27;, roles) commit(&#x27;SET_MENUS&#x27;, menus) commit(&#x27;SET_PERMISSIONS&#x27;, permissions) commit(&#x27;SET_USERID&#x27;, id) commit(&#x27;SET_USERNAME&#x27;, username) commit(&#x27;SET_AVATAR&#x27;, avatar) commit(&#x27;SET_ROLEDESC&#x27;, roleDesc) commit(&#x27;SET_FULLNAME&#x27;, fullName) commit(&#x27;SET_PHONE&#x27;, phone) resolve(data) &#125;) .catch(error =&gt; &#123; reject(error) &#125;) &#125;) &#125;, // user logout logout(&#123; commit, state, dispatch &#125;) &#123; // ... other code &#125;, // ... other code&#125;export default &#123; namespaced: true, state, mutations, actions&#125;// ---------------- getters 中添加 permissions/** * src\\store\\getters.js */const getters = &#123; // ... other code permissions: state =&gt; state.user.permissions, // ... other code&#125;export default getters 2.2 重写全局v-permission指令逻辑1234567891011121314151617181920212223242526272829/** * src\\directive\\permission\\permission.js */import store from &#x27;@/store&#x27;function checkPermission(el, binding) &#123; const &#123; value &#125; = binding if (value &amp;&amp; value.length &gt; 0) &#123; const permissions = store.getters &amp;&amp; store.getters.permissions const permissionButton = value const hasPermission = permissions.length &gt; 0 &amp;&amp; permissions.some(permission =&gt; &#123; return permission === permissionButton &#125;) if (!hasPermission) &#123; el.parentNode &amp;&amp; el.parentNode.removeChild(el) &#125; &#125; else &#123; console.error(`need permissions! Like v-permission=&quot;sys:menu:edit&quot;`) return false &#125;&#125;export default &#123; inserted(el, binding) &#123; checkPermission(el, binding) &#125;, update(el, binding) &#123; checkPermission(el, binding) &#125;&#125; 2.3 重新checkPermission权限判断函数逻辑1234567891011121314151617181920212223/** * src\\utils\\permission.js */import store from &#x27;@/store&#x27;/** * @param &#123;String&#125; value * @returns &#123;Boolean&#125; * @example see @/views/permission/directive.vue */export default function checkPermission(value) &#123; if (value &amp;&amp; value.length &gt; 0) &#123; const permissions = store.getters &amp;&amp; store.getters.permissions const permissionButton = value const hasPermission = permissions.length &gt; 0 &amp;&amp; permissions.some(permission =&gt; &#123; return permission === permissionButton &#125;) return hasPermission &#125; else &#123; console.error(`need permissions! Like v-permission=&quot;sys:menu:edit&quot;`) return false &#125;&#125; 3. 使用指令级权限使用v-permission指令 12345678910111213&lt;template&gt; &lt;!-- menu delete permission can see this --&gt; &lt;el-tag v-permission=&quot;&#x27;sys:menu:del&#x27;&quot;&gt;delete&lt;/el-tag&gt; &lt;!-- menu edit permission can see this --&gt; &lt;el-tag v-permission=&quot;&#x27;sys:menu:edit&#x27;&quot;&gt;edit&lt;/el-tag&gt;&lt;/template&gt;&lt;script&gt;import permission from &#x27;@/directive/permission/index.js&#x27;export default&#123; directives: &#123; permission &#125;&#125;&lt;/script&gt; 使用checkPermission函数 123456789101112131415&lt;template&gt; &lt;!-- menu delete permission can see this --&gt; &lt;el-tag v-if=&quot;checkPermission(&#x27;sys:menu:del&#x27;)&quot;&gt;delete&lt;/el-tag&gt; &lt;!-- menu edit permission can see this --&gt; &lt;el-tag v-if=&quot;checkPermission(&#x27;sys:menu:edit&#x27;)&quot;&gt;edit&lt;/el-tag&gt;&lt;/template&gt;&lt;script&gt;import checkPermission from &#x27;@/utils/permission&#x27;export default&#123; methods: &#123; checkPermission &#125;&#125;&lt;/script&gt; 4. 添加按钮权限，与菜单&#x2F;按钮对角色授权演示菜单中添加按钮的权限演示 菜单&#x2F;按钮对角色授权演示 完整代码请查看源码： 源码地址：https://github.com/chaooo/spring-security-jwt.git,这里我将本文的前后端分离后台菜单权限控制放在github源码tag的V5.0中，防止后续修改后代码对不上。 附：当前实例完整SQL123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105-- 系统用户表DROP TABLE IF EXISTS `sys_user`;CREATE TABLE `sys_user` ( `id` bigint NOT NULL AUTO_INCREMENT COMMENT &#x27;用户ID&#x27;, `username` varchar(255) NOT NULL COMMENT &#x27;用户名&#x27;, `password` varchar(255) NOT NULL COMMENT &#x27;密码&#x27;, `avatar` varchar(255) DEFAULT &#x27;https://wpimg.wallstcn.com/f778738c-e4f8-4870-b634-56703b4acafe.gif&#x27; COMMENT &#x27;头像&#x27;, `fullName` varchar(100) NOT NULL COMMENT &#x27;姓名&#x27;, `phone` varchar(20) NOT NULL COMMENT &#x27;电话&#x27;, `login_flag` char(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;是否阻止登录：0否，其他是&#x27;, `create_time` datetime DEFAULT NULL COMMENT &#x27;创建时间&#x27;, `update_time` datetime DEFAULT NULL COMMENT &#x27;更新时间&#x27;, `del_flag` char(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;删除标记：0未删，其他删除&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `username` (`username`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;系统用户表&#x27;;-- 系统角色表DROP TABLE IF EXISTS `sys_role`;CREATE TABLE `sys_role` ( `id` bigint NOT NULL AUTO_INCREMENT COMMENT &#x27;角色ID&#x27;, `role_name` varchar(50) NOT NULL COMMENT &#x27;角色名称&#x27;, `role_desc` varchar(255) DEFAULT NULL COMMENT &#x27;描述&#x27;, `create_time` datetime DEFAULT NULL COMMENT &#x27;创建时间&#x27;, `update_time` datetime DEFAULT NULL COMMENT &#x27;更新时间&#x27;, `del_flag` char(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;删除标记：0未删，其他删除&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `role_name` (`role_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;系统角色表&#x27;;-- 系统菜单表DROP TABLE IF EXISTS `sys_menu`;CREATE TABLE `sys_menu` ( `id` bigint NOT NULL AUTO_INCREMENT COMMENT &#x27;菜单ID&#x27;, `title` varchar(100) NOT NULL COMMENT &#x27;菜单名称&#x27;, `name` varchar(100) NOT NULL COMMENT &#x27;路由名称(前端匹配路由用)&#x27;, `icon` varchar(50) DEFAULT NULL COMMENT &#x27;图标&#x27;, `parent_id` bigint NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;父级菜单Id&#x27;, `hidden` char(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;隐藏状态：0显示，1隐藏&#x27;, `status` char(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;状态：0启用，1停用&#x27;, `sort` int NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;排序&#x27;, `create_time` datetime DEFAULT NULL COMMENT &#x27;创建时间&#x27;, `update_time` datetime DEFAULT NULL COMMENT &#x27;更新时间&#x27;, `del_flag` char(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;删除标记：0未删，其他删除&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;系统菜单表&#x27;;-- 系统权限表DROP TABLE IF EXISTS `sys_permission`;CREATE TABLE `sys_permission` ( `id` bigint NOT NULL AUTO_INCREMENT COMMENT &#x27;ID&#x27;, `menu_id` bigint NOT NULL COMMENT &#x27;菜单ID&#x27;, `name` varchar(100) DEFAULT NULL COMMENT &#x27;权限标识&#x27;, `title` varchar(100) NOT NULL COMMENT &#x27;权限名称&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT=&#x27;系统权限表&#x27;; -- 权限&amp;角色 关联表DROP TABLE IF EXISTS `sys_role_permission`;CREATE TABLE `sys_role_permission` ( `id` bigint NOT NULL AUTO_INCREMENT, `role_id` bigint DEFAULT NULL COMMENT &#x27;角色ID&#x27;, `permission_id` bigint DEFAULT NULL COMMENT &#x27;权限ID&#x27;, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=UTF8MB4 COMMENT=&#x27;系统角色权限关联表&#x27;;-- 用户&amp;角色 关联表DROP TABLE IF EXISTS `sys_role_user`;CREATE TABLE `sys_role_user` ( `id` bigint NOT NULL AUTO_INCREMENT, `role_id` bigint DEFAULT NULL COMMENT &#x27;角色ID&#x27;, `user_id` bigint DEFAULT NULL COMMENT &#x27;用户ID&#x27;, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=UTF8MB4 COMMENT=&#x27;系统用户角色关联表&#x27;;-- 菜单&amp;角色 关联表DROP TABLE IF EXISTS `sys_role_menu`;CREATE TABLE `sys_role_menu` ( `id` bigint NOT NULL AUTO_INCREMENT, `role_id` bigint DEFAULT NULL COMMENT &#x27;角色ID&#x27;, `menu_id` bigint DEFAULT NULL COMMENT &#x27;菜单ID&#x27;, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=UTF8MB4 COMMENT=&#x27;系统角色菜单关联表&#x27;;-- 初始数据INSERT INTO `sys_user`(`id`,`username`,`password`,`create_time`) VALUES (1,&#x27;sysadmin&#x27;,&#x27;$2a$10$Ml/uEJ5BnUSdKspYM4vkneUHM0piXyAVU0aueWya/v7FBauz6XWE6&#x27;,NOW());INSERT INTO `sys_role`(`id`,`role_name`,`role_desc`,`create_time`) VALUES (1,&#x27;admin&#x27;,&#x27;管理员&#x27;,NOW());INSERT INTO `sys_menu` (`id`, `title`, `name`, `icon`, `parent_id`, `hidden`, `create_time`)VALUES (&#x27;1&#x27;,&#x27;系统设置&#x27;,&#x27;SysSetting&#x27;,&#x27;el-icon-s-tools&#x27;,&#x27;0&#x27;,&#x27;0&#x27;,NOW()), (&#x27;2&#x27;,&#x27;菜单管理&#x27;,&#x27;SysMenus&#x27;,&#x27;el-icon-menu&#x27;,&#x27;1&#x27;,&#x27;0&#x27;,NOW()), (&#x27;3&#x27;,&#x27;角色管理&#x27;,&#x27;SysRoles&#x27;,&#x27;peoples&#x27;,&#x27;1&#x27;,&#x27;0&#x27;,NOW()), (&#x27;4&#x27;,&#x27;用户管理&#x27;,&#x27;SysUsers&#x27;,&#x27;user&#x27;,&#x27;1&#x27;,&#x27;0&#x27;,NOW()), (&#x27;5&#x27;,&#x27;系统图标&#x27;,&#x27;SysIcons&#x27;,&#x27;el-icon-picture&#x27;,&#x27;1&#x27;,&#x27;0&#x27;,NOW()), (&#x27;6&#x27;,&#x27;菜单列表&#x27;,&#x27;SysMenuList&#x27;,&#x27;&#x27;,&#x27;2&#x27;,&#x27;1&#x27;,NOW()), (&#x27;7&#x27;,&#x27;菜单编辑&#x27;,&#x27;SysMenuEdit&#x27;,&#x27;&#x27;,&#x27;2&#x27;,&#x27;1&#x27;,NOW()), (&#x27;8&#x27;,&#x27;角色列表&#x27;,&#x27;SysRoleList&#x27;,&#x27;&#x27;,&#x27;3&#x27;,&#x27;1&#x27;,NOW()), (&#x27;9&#x27;,&#x27;角色编辑&#x27;,&#x27;SysRoleEdit&#x27;,&#x27;&#x27;,&#x27;3&#x27;,&#x27;1&#x27;,NOW()), (&#x27;10&#x27;,&#x27;用户列表&#x27;,&#x27;SysUserList&#x27;,&#x27;&#x27;,&#x27;4&#x27;,&#x27;1&#x27;,NOW()), (&#x27;11&#x27;,&#x27;用户编辑&#x27;,&#x27;SysUserEdit&#x27;,&#x27;&#x27;,&#x27;4&#x27;,&#x27;1&#x27;,NOW()), (&#x27;12&#x27;,&#x27;其他菜单&#x27;,&#x27;Other&#x27;,&#x27;bug&#x27;,0,&#x27;0&#x27;,NOW());INSERT INTO `sys_permission`(`id`,`menu_id`,`name`,`title`) VALUES (1,6,&#x27;sys:menu:edit&#x27;,&#x27;编辑&#x27;),(2,6,&#x27;sys:menu:del&#x27;,&#x27;删除&#x27;),(3,8,&#x27;sys:role:edit&#x27;,&#x27;编辑&#x27;),(4,8,&#x27;sys:role:del&#x27;,&#x27;删除&#x27;),(5,10,&#x27;sys:user:edit&#x27;,&#x27;编辑&#x27;),(6,10,&#x27;sys:user:del&#x27;,&#x27;删除&#x27;);INSERT INTO `sys_role_user`(`user_id`, `role_id`) VALUES (1, 1);INSERT INTO `sys_role_menu`(`role_id`, `menu_id`) VALUES (1, 1),(1, 2),(1, 3),(1, 4),(1, 5),(1, 6),(1, 7),(1, 8),(1, 9),(1, 10),(1, 11),(1, 12);INSERT INTO `sys_role_permission`(`id`,`role_id`,`permission_id`) VALUES (1,1,1),(2,1,2),(3,1,3),(4,1,4),(5,1,5),(6,1,6);","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"},{"name":"SpringSecurity","slug":"SpringSecurity","permalink":"http://chaooo.github.io/tags/SpringSecurity/"},{"name":"Permissions","slug":"Permissions","permalink":"http://chaooo.github.io/tags/Permissions/"}]},{"title":"「Spring Security」前后端分离菜单权限控制-前端动态路由","date":"2021-12-27T09:35:00.000Z","path":"2021/12/27/spring-security-vue.html","text":"前端部分，这里基于vue-element-admin模板来演示，vue-element-admin是一个后台前端解决方案，它基于vue和element-ui实现。 1. 安装 vue-element-admin1234567891011# 克隆项目git clone https://github.com/PanJiaChen/vue-element-admin.git# 进入项目目录cd vue-element-admin# 安装依赖， 建议不要用 cnpm 安装 会有各种诡异的bug 可以通过如下操作解决 npm 下载速度慢的问题npm install --registry=https://registry.npm.taobao.org# 本地开发 启动项目npm run dev 2. 改造前端路由挂载方式vue-element-admin中权限的实现方式是：通过获取当前用户的权限去比对路由表，生成当前用户具有的权限可访问的路由表，通过router.addRoutes动态挂载到router上。 这里改造得更灵活一点，后台根据用户计算出可访问得菜单列表，直接返回用户可访问得菜单列表，前端也需要保存一份全的路由表，用户登录后得到可访问菜单，匹配前端保存的路由表然后动态挂载。 用户登录成功之后，在全局钩子router.beforeEach中拦截路由，判断是否已获得token，在获得token之后我们就要去获取用户的基本信息及可访问菜单，然后动态挂载路由。 12345678910111213141516/** * src/permission.js */ // router.beforeEachconst hasRoles = store.getters.roles &amp;&amp; store.getters.roles.length &gt; 0if (hasRoles) &#123;next()&#125; else &#123; // get user info const &#123; menus &#125; = await store.dispatch(&#x27;user/getInfo&#x27;) // generate accessible routes map based on menus const accessRoutes = await store.dispatch(&#x27;permission/generateRoutes&#x27;, menus) // dynamically add accessible routes router.addRoutes(accessRoutes) // ... other code&#125; 2.1 根据接口返回的菜单列表menus动态挂载路由接口返回菜单数据： 动态挂载路由： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * src\\store\\modules\\permission.js */import &#123; constantRoutes, asyncRoutes, afterRoutes &#125; from &#x27;@/router&#x27;/** * 返回当前路由名称对应的菜单 * @param menus 菜单列表 * @param name 路由名称 */function filterMeun(menus, name) &#123; if (name) &#123; for (let i = 0; i &lt; menus.length; i++) &#123; const menu = menus[i] if (name === menu.name) &#123; return menu &#125; &#125; &#125; return null&#125;/** * 通过后台请求的菜单列表递归过滤路由表 * @param routes asyncRoutes * @param menus 接口返回的菜单 */export function filterAsyncRoutes(routes, menus) &#123; const res = [] routes.forEach(route =&gt; &#123; const tmp = &#123; ...route &#125; const meun = filterMeun(menus, tmp.name) if (meun != null &amp;&amp; meun.title) &#123; tmp.hidden = meun.hidden !== 0 // 显示的菜单替换后台设置的标题 if (!tmp.hidden) &#123; tmp.meta.title = meun.title tmp.sort = meun.sort &#125; if (meun.icon) &#123; tmp.meta.icon = meun.icon &#125; if (tmp.children) &#123; tmp.children = filterAsyncRoutes(tmp.children, menus) &#125; res.push(tmp) &#125; &#125;) return res&#125;/** * 对菜单进行排序 */function sortRouters(accessedRouters) &#123; for (let i = 0; i &lt; accessedRouters.length; i++) &#123; const router = accessedRouters[i] if (router.children &amp;&amp; router.children.length &gt; 0) &#123; router.children.sort(compare(&#x27;sort&#x27;)) &#125; &#125; accessedRouters.sort(compare(&#x27;sort&#x27;))&#125;/** * 升序比较函数 */function compare(p) &#123; return (m, n) =&gt; &#123; const a = m[p] const b = n[p] return a - b &#125;&#125;const state = &#123; routes: [], addRoutes: []&#125;const mutations = &#123; SET_ROUTES: (state, routes) =&gt; &#123; state.addRoutes = routes state.routes = constantRoutes.concat(routes) &#125;&#125;const actions = &#123; generateRoutes(&#123; commit &#125;, menus) &#123; return new Promise(resolve =&gt; &#123; // 通过后台请求的菜单列表递归过滤路由表 const roleAsyncRoutes = filterAsyncRoutes(asyncRoutes, menus) // 对可访问菜单进行排序 sortRouters(roleAsyncRoutes) // 拼接尾部公共菜单 const accessedRoutes = roleAsyncRoutes.concat(afterRoutes) commit(&#x27;SET_ROUTES&#x27;, accessedRoutes) resolve(accessedRoutes) &#125;) &#125;&#125;export default &#123; namespaced: true, state, mutations, actions&#125; 2.2 前端保存的全路径路由表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214/** * src\\router\\index.js */import Vue from &#x27;vue&#x27;import Router from &#x27;vue-router&#x27;Vue.use(Router)import Layout from &#x27;@/layout&#x27;/** * 没有权限要求的基本路由 */export const constantRoutes = [ &#123; path: &#x27;/redirect&#x27;, component: Layout, hidden: true, children: [ &#123; path: &#x27;/redirect/:path(.*)&#x27;, component: () =&gt; import(&#x27;@/views/redirect/index&#x27;) &#125; ] &#125;, &#123; path: &#x27;/login&#x27;, component: () =&gt; import(&#x27;@/views/login/index&#x27;), hidden: true &#125;, &#123; path: &#x27;/404&#x27;, component: () =&gt; import(&#x27;@/views/error-page/404&#x27;), hidden: true &#125;, &#123; path: &#x27;/&#x27;, component: Layout, redirect: &#x27;/dashboard&#x27;, children: [ &#123; path: &#x27;dashboard&#x27;, component: () =&gt; import(&#x27;@/views/dashboard/index&#x27;), name: &#x27;Dashboard&#x27;, meta: &#123; title: &#x27;后台首页&#x27;, icon: &#x27;dashboard&#x27;, affix: true &#125; &#125; ] &#125;, &#123; path: &#x27;/profile&#x27;, component: Layout, redirect: &#x27;/profile/index&#x27;, hidden: true, children: [ &#123; path: &#x27;index&#x27;, component: () =&gt; import(&#x27;@/views/profile/index&#x27;), name: &#x27;Profile&#x27;, meta: &#123; title: &#x27;个人中心&#x27;, icon: &#x27;user&#x27;, noCache: true &#125; &#125; ] &#125;]/** * 动态加载的路由 */export const asyncRoutes = [ &#123; path: &#x27;/sys&#x27;, component: Layout, redirect: &#x27;/sys/menus&#x27;, alwaysShow: true, // will always show the root menu name: &#x27;SysSetting&#x27;, // name必须和后台配置一致，不然匹配不到 meta: &#123; title: &#x27;系统设置&#x27;, icon: &#x27;el-icon-s-tools&#x27; &#125;, children: [ &#123; path: &#x27;menus&#x27;, component: () =&gt; import(&#x27;@/views/sys/menus/index&#x27;), redirect: &#x27;/sys/menus/list&#x27;, name: &#x27;SysMenus&#x27;, meta: &#123; title: &#x27;菜单管理&#x27;, icon: &#x27;el-icon-menu&#x27; &#125;, children: [ &#123; path: &#x27;list&#x27;, hidden: true, component: () =&gt; import(&#x27;@/views/sys/menus/list.vue&#x27;), name: &#x27;SysMenuList&#x27;, meta: &#123; title: &#x27;菜单列表&#x27; &#125; &#125;, &#123; path: &#x27;edit&#x27;, hidden: true, component: () =&gt; import(&#x27;@/views/sys/menus/form.vue&#x27;), name: &#x27;SysMenuEdit&#x27;, meta: &#123; title: &#x27;编辑菜单&#x27; &#125; &#125;, &#123; path: &#x27;add&#x27;, hidden: true, component: () =&gt; import(&#x27;@/views/sys/menus/form.vue&#x27;), name: &#x27;SysMenuEdit&#x27;, meta: &#123; title: &#x27;添加菜单&#x27; &#125; &#125; ] &#125;, &#123; path: &#x27;roles&#x27;, component: () =&gt; import(&#x27;@/views/sys/roles/index&#x27;), redirect: &#x27;/sys/roles/list&#x27;, name: &#x27;SysRoles&#x27;, meta: &#123; title: &#x27;角色管理&#x27;, icon: &#x27;lock&#x27; &#125;, children: [ &#123; path: &#x27;list&#x27;, hidden: true, component: () =&gt; import(&#x27;@/views/sys/roles/list.vue&#x27;), name: &#x27;SysRoleList&#x27;, meta: &#123; title: &#x27;角色列表&#x27; &#125; &#125;, &#123; path: &#x27;edit&#x27;, hidden: true, component: () =&gt; import(&#x27;@/views/sys/roles/form.vue&#x27;), name: &#x27;SysRoleEdit&#x27;, meta: &#123; title: &#x27;编辑角色&#x27; &#125; &#125;, &#123; path: &#x27;add&#x27;, hidden: true, component: () =&gt; import(&#x27;@/views/sys/roles/form.vue&#x27;), name: &#x27;SysRoleEdit&#x27;, meta: &#123; title: &#x27;添加角色&#x27; &#125; &#125; ] &#125;, &#123; path: &#x27;users&#x27;, component: () =&gt; import(&#x27;@/views/sys/users/index&#x27;), redirect: &#x27;/sys/users/list&#x27;, name: &#x27;SysUsers&#x27;, meta: &#123; title: &#x27;用户管理&#x27;, icon: &#x27;user&#x27; &#125;, children: [ &#123; path: &#x27;list&#x27;, hidden: true, component: () =&gt; import(&#x27;@/views/sys/users/list.vue&#x27;), name: &#x27;SysUserList&#x27;, meta: &#123; title: &#x27;用户列表&#x27; &#125; &#125;, &#123; path: &#x27;add&#x27;, hidden: true, component: () =&gt; import(&#x27;@/views/sys/users/form.vue&#x27;), name: &#x27;SysUserEdit&#x27;, meta: &#123; title: &#x27;添加用户&#x27; &#125; &#125;, &#123; path: &#x27;edit&#x27;, hidden: true, component: () =&gt; import(&#x27;@/views/sys/users/form.vue&#x27;), name: &#x27;SysUserEdit&#x27;, meta: &#123; title: &#x27;编辑用户&#x27; &#125; &#125; ] &#125;, &#123; path: &#x27;icons&#x27;, component: () =&gt; import(&#x27;@/views/sys/icons/index&#x27;), name: &#x27;SysIcons&#x27;, meta: &#123; title: &#x27;系统图标&#x27;, icon: &#x27;el-icon-picture&#x27;, noCache: true &#125; &#125; ] &#125;, /** when your routing map is too long, you can split it into small modules **/ // componentsRouter, // chartsRouter, // nestedRouter, // tableRouter,]/** * 没有权限要求的底部基本路由 */export const afterRoutes = [ &#123; path: &#x27;external-link&#x27;, component: Layout, children: [ &#123; path: &#x27;https://www.test.com/&#x27;, meta: &#123; title: &#x27;友情链接&#x27;, icon: &#x27;link&#x27; &#125; &#125; ] &#125;, // 404 page must be placed at the end !!! &#123; path: &#x27;*&#x27;, redirect: &#x27;/404&#x27;, hidden: true &#125;]const createRouter = () =&gt; new Router(&#123; // mode: &#x27;history&#x27;, // require service support scrollBehavior: () =&gt; (&#123; y: 0 &#125;), routes: constantRoutes &#125;)const router = createRouter()// Detail see: https://github.com/vuejs/vue-router/issues/1234#issuecomment-357941465export function resetRouter() &#123; const newRouter = createRouter() router.matcher = newRouter.matcher // reset router&#125;export default router 源码地址：https://github.com/chaooo/spring-security-jwt.git,这里我将本文的前后端分离后台菜单权限控制放在github源码tag的V4.0中，防止后续修改后代码对不上。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"},{"name":"SpringSecurity","slug":"SpringSecurity","permalink":"http://chaooo.github.io/tags/SpringSecurity/"},{"name":"Vue","slug":"Vue","permalink":"http://chaooo.github.io/tags/Vue/"}]},{"title":"「Spring Security」前后端分离后台菜单权限控制","date":"2021-12-13T06:35:00.000Z","path":"2021/12/13/spring-security-rbac.html","text":"1. RBAC权限控制模型RBAC（Role-based access control）是一种以角色为基础的访问控制（Role-based access control，RBAC），它是一种较新且广为使用的权限控制机制，这种机制不是直接给用户赋予权限，而是将权限赋予角色。 RBAC 权限模型将用户按角色进行归类，通过用户的角色来确定用户对某项资源是否具备操作权限。RBAC 简化了用户与权限的管理，它将用户与角色关联、角色与权限关联、权限与资源关联，这种模式使得用户的授权管理变得非常简单和易于维护。 2. 数据库设计 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061-- 用户表DROP TABLE IF EXISTS `sys_user`;CREATE TABLE `sys_user` ( `id` BIGINT NOT NULL AUTO_INCREMENT COMMENT &#x27;用户ID&#x27;, `username` VARCHAR(255) DEFAULT NULL COMMENT &#x27;用户名&#x27;, `password` VARCHAR(255) DEFAULT NULL COMMENT &#x27;密码&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;用户表&#x27;;-- 角色表DROP TABLE IF EXISTS `sys_role`;CREATE TABLE `sys_role` ( `id` BIGINT NOT NULL AUTO_INCREMENT COMMENT &#x27;角色ID&#x27;, `role_name` VARCHAR(50) DEFAULT NULL COMMENT &#x27;角色名称&#x27;, `role_desc` VARCHAR(255) DEFAULT NULL COMMENT &#x27;描述&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;角色表&#x27;;-- 菜单表DROP TABLE IF EXISTS `sys_menu`;CREATE TABLE `sys_menu` ( `id` BIGINT NOT NULL AUTO_INCREMENT COMMENT &#x27;菜单ID&#x27;, `menu_name` VARCHAR(100) DEFAULT NULL COMMENT &#x27;菜单名称&#x27;, `menu_path` VARCHAR(255) DEFAULT NULL COMMENT &#x27;菜单路径&#x27;, `menu_type` char DEFAULT NULL COMMENT &#x27;菜单类型(1:一级菜单，2:子菜单，3:按钮)&#x27;, `menu_parent_id` BIGINT DEFAULT NULL COMMENT &#x27;父级菜单Id&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;菜单表&#x27;;-- 用户&amp;角色 关联表DROP TABLE IF EXISTS `sys_role_user`;CREATE TABLE `sys_role_user` ( `id` BIGINT NOT NULL AUTO_INCREMENT, `role_id` BIGINT DEFAULT NULL COMMENT &#x27;角色ID&#x27;, `user_id` BIGINT DEFAULT NULL COMMENT &#x27;用户ID&#x27;, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=UTF8MB4 COMMENT=&#x27;系统用户角色关联表&#x27;;-- 菜单&amp;角色 关联表DROP TABLE IF EXISTS `sys_role_menu`;CREATE TABLE `sys_role_menu` ( `id` BIGINT NOT NULL AUTO_INCREMENT, `role_id` BIGINT DEFAULT NULL COMMENT &#x27;角色ID&#x27;, `menu_id` BIGINT DEFAULT NULL COMMENT &#x27;菜单ID&#x27;, PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=UTF8MB4 COMMENT=&#x27;系统角色菜单关联表&#x27;;-- 初始数据：-- 管理员拥有所有菜单权限-- 普通用户拥有查看权限INSERT INTO `sys_role`(`id`, `role_name`, `role_desc`) VALUES (1, &#x27;admin&#x27;, &#x27;管理员&#x27;),(2, &#x27;user&#x27;, &#x27;普通用户&#x27;);INSERT INTO `sys_menu`(`id`, `menu_name`,`menu_path`,`menu_type`,`menu_parent_id`) VALUES (1, &#x27;用户管理&#x27;, &#x27;/user&#x27;, 1, null), (2, &#x27;用户列表&#x27;, &#x27;/user/list&#x27;, 2, 1), (3, &#x27;新增用户&#x27;, &#x27;/user/add&#x27;, 2, 1), (4, &#x27;修改用户&#x27;, &#x27;/user/update&#x27;, 2, 1), (5, &#x27;删除用户&#x27;, &#x27;/user/delete&#x27;, 3, 1);INSERT INTO `sys_role_user`(`user_id`, `role_id`) VALUES (1, 1);INSERT INTO `sys_role_menu`(`role_id`, `menu_id`) VALUES (1, 1),(1, 2),(1, 3),(1, 4),(1, 5), (2, 1),(2, 2); 3. 代码进化 修改注册逻辑，注册时添加用户权限 1234567891011121314151617181920212223public ResponseJson&lt;SysUser&gt; register(SysUser sysUser) &#123; if (StringUtils.hasLength(sysUser.getUsername()) &amp;&amp; StringUtils.hasLength(sysUser.getPassword())) &#123; // 密码加密 String encodePassword = passwordEncoder.encode(sysUser.getPassword()); sysUser.setPassword(encodePassword); // 新增用户 sysUserDao.insertSysUser(sysUser); // 角色Ids，用&quot;,&quot;隔开 String roleIds = sysUser.getRoleIds(); if (StringUtils.hasLength(roleIds)) &#123; // 设置用户角色 String[] split = roleIds.split(&quot;,&quot;); for (String s : split) &#123; if (StringUtils.hasLength(s)) &#123; // 保存用户角色关系 sysUserDao.insertUserRoleRelation(sysUser.getId(), Long.valueOf(s)); &#125; &#125; &#125; return ResponseJson.success(&quot;注册成功&quot;, sysUser); &#125; return ResponseJson.error(&quot;用户名或密码不能为空&quot;, null);&#125; 封装JWT服务工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153@Slf4j@Servicepublic class JwtService &#123; @Resource private RedisService redisService; /** * 生成token * @param username 用户名 * @param roleList 角色列表 */ public String createToken(String username, List&lt;String&gt; roleList) &#123; Calendar calendar = Calendar.getInstance(); // 设置签发时间 calendar.setTime(new Date()); Date now = calendar.getTime(); // 设置过期时间 calendar.add(Calendar.MINUTE, ConstantKey.TOKEN_EXPIRE); Date time = calendar.getTime(); String token = Jwts.builder() .setSubject(username + &quot;-&quot; + roleList) // 签发时间 .setIssuedAt(now) // 过期时间 .setExpiration(time) // 自定义算法与签名：这里算法采用HS512，常量中定义签名key .signWith(SignatureAlgorithm.HS512, ConstantKey.SIGNING_KEY) .compact(); // 将token存入redis,并设置超时时间为token过期时间 long expire = time.getTime() - now.getTime(); redisService.set(token, token, expire); return token; &#125; /** * 解析Token */ public String parseToken(HttpServletRequest request) &#123; String userinfo = null; String token = request.getHeader(ConstantKey.TOKEN_NAME); if (StringUtils.hasLength(token)) &#123; String cacheToken = String.valueOf(redisService.get(token)); if (StringUtils.hasLength(cacheToken) &amp;&amp; !&quot;null&quot;.equals(cacheToken)) &#123; try &#123; Claims claims = Jwts.parser() // 设置生成token的签名key .setSigningKey(ConstantKey.SIGNING_KEY) // 解析token .parseClaimsJws(cacheToken).getBody(); // 取出用户信息 userinfo = claims.getSubject(); // 重设Redis超时时间 resetRedisExpire(token, claims); &#125; catch (ExpiredJwtException e) &#123; log.info(&quot;Token过期续签，ExpiredJwtException=&#123;&#125;&quot;, e.getMessage()); Claims claims = e.getClaims(); // 取出用户信息 userinfo = claims.getSubject(); // 刷新Token refreshToken(token, claims); &#125; catch (UnsupportedJwtException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，UnsupportedJwtException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (MalformedJwtException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，MalformedJwtException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (SignatureException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，SignatureException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (IllegalArgumentException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，IllegalArgumentException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; &#125; &#125; return userinfo; &#125; /** * 解析Token,取出用户名（Token过期仍取出用户名） */ public String getUsername(HttpServletRequest request)&#123; String username = null; String token = request.getHeader(ConstantKey.TOKEN_NAME); if (StringUtils.hasLength(token)) &#123; String userinfo = null; try &#123; Claims claims = Jwts.parser() // 设置生成token的签名key .setSigningKey(ConstantKey.SIGNING_KEY) // 解析token .parseClaimsJws(token).getBody(); // 取出用户信息 userinfo = claims.getSubject(); &#125; catch (ExpiredJwtException e) &#123; Claims claims = e.getClaims(); // 取出用户信息 userinfo = claims.getSubject(); &#125; catch (Exception ignored)&#123;&#125; if (StringUtils.hasLength(userinfo))&#123; username = userinfo.split(&quot;-&quot;)[0]; &#125; &#125; return username; &#125; /** * 重设Redis超时时间 * 当前时间 + (`cacheToken`过期时间 - `cacheToken`签发时间) */ private void resetRedisExpire(String token, Claims claims) &#123; // 当前时间 long current = System.currentTimeMillis(); // token签发时间 long issuedAt = claims.getIssuedAt().getTime(); // token过期时间 long expiration = claims.getExpiration().getTime(); // 当前时间 + (`cacheToken`过期时间 - `cacheToken`签发时间) long expireAt = current + (expiration - issuedAt); // 重设Redis超时时间 redisService.expire(token, expireAt); &#125; /** * 刷新Token * 刷新Token的时机： 当cacheToken已过期 并且Redis在有效期内 * 重新生成Token并覆盖Redis的v值(这时候k、v值不一样了)，然后设置Redis过期时间为：新Token过期时间 */ private void refreshToken(String token, Claims claims) &#123; // 当前时间 long current = System.currentTimeMillis(); /* * 重新生成token */ Calendar calendar = Calendar.getInstance(); // 设置签发时间 calendar.setTime(new Date()); Date now = calendar.getTime(); // 设置过期时间: TOKEN_EXPIRE分钟 calendar.add(Calendar.MINUTE, ConstantKey.TOKEN_EXPIRE); Date time = calendar.getTime(); String refreshToken = Jwts.builder() .setSubject(claims.getSubject()) // 签发时间 .setIssuedAt(now) // 过期时间 .setExpiration(time) // 算法与签名(同生成token)：这里算法采用HS512，常量中定义签名key .signWith(SignatureAlgorithm.HS512, ConstantKey.SIGNING_KEY) .compact(); // 将refreshToken覆盖Redis的v值,并设置超时时间为refreshToken过期时间 long expire = time.getTime() - now.getTime(); redisService.set(token, token, expire); // 打印日志 log.info(&quot;刷新token执行时间: &#123;&#125;&quot;, (System.currentTimeMillis() - current) + &quot; 毫秒&quot;); &#125;&#125; 编写获取用户可访问菜单接口（用户登录后，携带Token去获取用户角色，根据角色计算出用户可访问菜单） 12345@GetMapping(&quot;/menu&quot;)public ResponseJson&lt;List&lt;SysMenu&gt;&gt; menuList(HttpServletRequest request) &#123; String username = jwtService.getUsername(request); return sysUserService.menuList(username);&#125; 12345678910111213public ResponseJson&lt;List&lt;SysMenu&gt;&gt; menuList(String username) &#123; if (!StringUtils.hasLength(username)) &#123; return ResponseJson.error(&quot;用户信息异常&quot;, null); &#125; // 获取用户角色Id List&lt;Long&gt; roleIds = sysUserDao.getRoleIdsByUserId(username); List&lt;SysMenu&gt; menus = null; if (!CollectionUtils.isEmpty(roleIds)) &#123; // 根据角色Id获取菜单列表 menus = sysUserDao.getMenuListByRoleIds(roleIds); &#125; return ResponseJson.success(menus);&#125; 1234567891011121314&lt;select id=&quot;getRoleIdsByUserId&quot; resultType=&quot;java.lang.Long&quot;&gt; SELECT DISTINCT ru.role_id FROM sys_role_user ru LEFT JOIN sys_user u ON ru.user_id = u.id WHERE u.username=#&#123;username&#125;&lt;/select&gt;&lt;select id=&quot;getMenuListByRoleIds&quot; resultType=&quot;com.example.jwt.entity.SysMenu&quot;&gt; SELECT m.id, m.menu_name AS menuName, m.menu_path AS menuPath, m.menu_type AS menuType, m.menu_parent_id AS parentId FROM sys_menu m LEFT JOIN sys_role_menu rm ON m.id = rm.menu_id WHERE rm.role_id IN &lt;foreach item=&quot;roleId&quot; collection=&quot;roleIds&quot; open=&quot;(&quot; separator=&quot;,&quot; close=&quot;)&quot;&gt; #&#123;roleId&#125; &lt;/foreach&gt;&lt;/select&gt; 4.测试 普通用户可访问菜单： 管理员可访问菜单： 源码地址：https://github.com/chaooo/spring-security-jwt.git,这里我将本文的前后端分离后台菜单权限控制放在github源码tag的V3.0中，防止后续修改后代码对不上。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"},{"name":"SpringSecurity","slug":"SpringSecurity","permalink":"http://chaooo.github.io/tags/SpringSecurity/"},{"name":"RBAC","slug":"RBAC","permalink":"http://chaooo.github.io/tags/RBAC/"}]},{"title":"「Spring Security」基于Redis的Token自动续签优化","date":"2021-12-10T03:35:00.000Z","path":"2021/12/10/spring-security-token.html","text":"本文基于上一篇文章：《Spring Security（三）整合 JWT 实现无状态登录示例》。 在 SpringSecurity 整合 JWT 实现无状态登录示例中，我们在 JwtAuthenticationFilter (自定义JWT认证过滤器) 解析 Token 成功后，提供了续签逻辑： 1234567891011121314151617181920212223242526272829303132333435363738/** * 刷新Token的时机： * 1. 当前时间 &lt; token过期时间 * 2. 当前时间 &gt; (签发时间 + (token过期时间 - token签发时间)/2) */private void refreshToken(HttpServletResponse response, Claims claims) &#123; // 当前时间 long current = System.currentTimeMillis(); // token签发时间 long issuedAt = claims.getIssuedAt().getTime(); // token过期时间 long expiration = claims.getExpiration().getTime(); // (当前时间 &lt; token过期时间) &amp;&amp; (当前时间 &gt; (签发时间 + (token过期时间 - token签发时间)/2)) if ((current &lt; expiration) &amp;&amp; (current &gt; (issuedAt + ((expiration - issuedAt) / 2)))) &#123; /* * 重新生成token */ Calendar calendar = Calendar.getInstance(); // 设置签发时间 calendar.setTime(new Date()); Date now = calendar.getTime(); // 设置过期时间: 5分钟 calendar.add(Calendar.MINUTE, 5); Date time = calendar.getTime(); String refreshToken = Jwts.builder() .setSubject(claims.getSubject()) // 签发时间 .setIssuedAt(now) // 过期时间 .setExpiration(time) // 算法与签名(同生成token)：这里算法采用HS512，常量中定义签名key .signWith(SignatureAlgorithm.HS512, ConstantKey.SIGNING_KEY) .compact(); // 主动刷新token，并返回给前端 response.addHeader(&quot;refreshToken&quot;, refreshToken); log.info(&quot;刷新token执行时间: &#123;&#125;&quot;, (System.currentTimeMillis() - current) + &quot; 毫秒&quot;); &#125;&#125; 这里的逻辑是：Token 未过期并且当前时间已经超过 Token 有效时间的一半，重新生成一个 refreshToken，并返回给前端，前端需要用 refreshToken 替换之前旧的 Token。 Token续签优化方案预期效果：前端不需要手动替换 Token，每次用 Token 请求资源时自动续期。 实现方案：引入 Redis，实现逻辑： 登录成功后将 Token 存储到 Redis 里面(k,v都为 Token 的值)，并设置 Redis 过期时间为： Token 过期时间。 用户发起请求时，每次都根据k为Token的键去换取 Redis 的值，这里命名为 cacheToken： 当 cacheToken 在有效期内，重设 Redis 过期时间为：当前时间 + (cacheToken过期时间 - cacheToken签发时间)。 当 cacheToken 已过期（Redis 在有效期内），则 JWT 重新生成 Token 并覆盖v值(这时候k、v值不一样了)，然后设置 Redis 过期时间为： cacheToken 过期时间。 若 Redis 也过期，取不到 cacheToken，则拒绝访问或返回错误信息，需要重新登录。 具体实现1. 在 pom.xml 中引入 Redis 依赖：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 2. 在 application.yml 配置文件中配置 Redis：123456789spring: redis: host: 127.0.0.1 port: 6379 password: 123456 # Redis数据库索引（默认为0） database: 0 # 连接超时时间（毫秒） timeout: 5000 3. 简单的 RedisService 封装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Servicepublic class RedisService &#123; @Resource private RedisTemplate&lt;Serializable, Object&gt; redisTemplate; /** * 读取缓存 */ public Object get(String key) &#123; ValueOperations&lt;Serializable, Object&gt; operations = redisTemplate.opsForValue(); return operations.get(key); &#125; /** * 判断缓存中是否存在 */ public boolean exists(String key) &#123; return StringUtils.hasLength(key) &amp;&amp; Boolean.TRUE.equals(redisTemplate.hasKey(key)); &#125; /** * 删除缓存 */ public void remove(String key) &#123; if (exists(key)) &#123; redisTemplate.delete(key); &#125; &#125; /** * 写入缓存 */ public boolean set(String key, Object value) &#123; boolean result = false; try &#123; ValueOperations&lt;Serializable, Object&gt; operations = redisTemplate.opsForValue(); operations.set(key, value); result = true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * 写入缓存 并 加上过期时间 */ public boolean set(String key, Object value, Date date) &#123; boolean result = false; try &#123; ValueOperations&lt;Serializable, Object&gt; operations = redisTemplate.opsForValue(); operations.set(key, value); redisTemplate.expireAt(key, date); result = true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return result; &#125; /** * 写入过期时间（毫秒） */ public boolean expire(String key, Long expireTimeMillis) &#123; boolean result = false; try &#123; redisTemplate.expire(key, expireTimeMillis, TimeUnit.MILLISECONDS); result = true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return result; &#125;&#125; 4. 修改JWT登录过滤器 JwtLoginFilter，构造方法中加入 RedisService，并生成 Token 后存入 Redis:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182@Slf4jpublic class JwtLoginFilter extends UsernamePasswordAuthenticationFilter &#123; private final AuthenticationManager authenticationManager; private final RedisService redisService; public JwtLoginFilter(AuthenticationManager authenticationManager, RedisService redisService) &#123; this.authenticationManager = authenticationManager; this.redisService = redisService; &#125; /** * 尝试身份认证(接收并解析用户凭证) */ @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; String username = request.getParameter(&quot;username&quot;); String password = request.getParameter(&quot;password&quot;); return authenticationManager.authenticate( new UsernamePasswordAuthenticationToken(username, password, new ArrayList&lt;&gt;()) ); &#125; /** * 认证成功(用户成功登录后，这个方法会被调用，我们在这个方法里生成token) */ @Override protected void successfulAuthentication(HttpServletRequest request, HttpServletResponse response, FilterChain chain, Authentication auth) &#123; try &#123; Collection&lt;? extends GrantedAuthority&gt; authorities = auth.getAuthorities(); // 定义存放角色集合的对象 List&lt;String&gt; roleList = new ArrayList&lt;&gt;(); for (GrantedAuthority grantedAuthority : authorities) &#123; roleList.add(grantedAuthority.getAuthority()); &#125; /* * 生成token */ Calendar calendar = Calendar.getInstance(); // 设置签发时间 calendar.setTime(new Date()); Date now = calendar.getTime(); // 设置过期时间: 5分钟 calendar.add(Calendar.MINUTE, 5); Date time = calendar.getTime(); String token = Jwts.builder() .setSubject(auth.getName() + &quot;-&quot; + roleList) // 签发时间 .setIssuedAt(now) // 过期时间 .setExpiration(time) // 自定义算法与签名：这里算法采用HS512，常量中定义签名key .signWith(SignatureAlgorithm.HS512, ConstantKey.SIGNING_KEY) .compact(); // 将token存入redis,并设置超时时间为token过期时间 redisService.set(token, token, time); /* * 返回token */ log.info(&quot;用户登录成功，生成token=&#123;&#125;&quot;, token); // 登录成功后，返回token到header里面 response.addHeader(&quot;Authorization&quot;, token); // 登录成功后，返回token到body里面 ResponseJson&lt;String&gt; result = ResponseJson.success(&quot;登录成功&quot;, token); response.setCharacterEncoding(&quot;UTF-8&quot;); response.getWriter().write(JSON.toJSONString(result)); &#125; catch (IOException e) &#123; log.error(&quot;IOException:&quot;, e); &#125; &#125; /** * 认证失败调用 */ @Override protected void unsuccessfulAuthentication(HttpServletRequest request, HttpServletResponse response, AuthenticationException exception) throws IOException &#123; log.warn(&quot;登录失败[&#123;&#125;]，AuthenticationException=&#123;&#125;&quot;, request.getRequestURI(), exception.getMessage()); // 登录失败，返回错误信息 ResponseJson&lt;Void&gt; result = ResponseJson.error(exception.getMessage(), null); response.setCharacterEncoding(&quot;UTF-8&quot;); response.getWriter().write(JSON.toJSONString(result)); &#125;&#125; 5. 修改JWT认证过滤器 JwtAuthenticationFilter，构造方法中加入 RedisService，并添加 Token 续签逻辑:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117@Slf4jpublic class JwtAuthenticationFilter extends BasicAuthenticationFilter &#123; private final RedisService redisService; public JwtAuthenticationFilter(AuthenticationManager authenticationManager, RedisService redisService) &#123; super(authenticationManager); this.redisService = redisService; &#125; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; UsernamePasswordAuthenticationToken authentication = getAuthentication(request, response); SecurityContextHolder.getContext().setAuthentication(authentication); chain.doFilter(request, response); &#125; private UsernamePasswordAuthenticationToken getAuthentication(HttpServletRequest request, HttpServletResponse response) &#123; /* * 解析token */ String token = request.getHeader(&quot;Authorization&quot;); if (StringUtils.hasLength(token)) &#123; String cacheToken = String.valueOf(redisService.get(token)); if (StringUtils.hasLength(token) &amp;&amp; !&quot;null&quot;.equals(cacheToken)) &#123; String user = null; try &#123; Claims claims = Jwts.parser() // 设置生成token的签名key .setSigningKey(ConstantKey.SIGNING_KEY) // 解析token .parseClaimsJws(cacheToken).getBody(); // 取出用户信息 user = claims.getSubject(); // 重设Redis超时时间 resetRedisExpire(token, claims); &#125; catch (ExpiredJwtException e) &#123; log.info(&quot;Token过期续签，ExpiredJwtException=&#123;&#125;&quot;, e.getMessage()); Claims claims = e.getClaims(); // 取出用户信息 user = claims.getSubject(); // 刷新Token refreshToken(token, claims); &#125; catch (UnsupportedJwtException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，UnsupportedJwtException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (MalformedJwtException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，MalformedJwtException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (SignatureException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，SignatureException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (IllegalArgumentException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，IllegalArgumentException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; if (user != null) &#123; // 获取用户权限和角色 String[] split = user.split(&quot;-&quot;)[1].split(&quot;,&quot;); ArrayList&lt;GrantedAuthority&gt; authorities = new ArrayList&lt;&gt;(); for (String s : split) &#123; authorities.add(new GrantedAuthorityImpl(s)); &#125; // 返回Authentication return new UsernamePasswordAuthenticationToken(user, null, authorities); &#125; &#125; &#125; log.warn(&quot;访问[&#123;&#125;]失败，需要身份认证&quot;, request.getRequestURI()); return null; &#125; /** * 重设Redis超时时间 * 当前时间 + (`cacheToken`过期时间 - `cacheToken`签发时间) */ private void resetRedisExpire(String token, Claims claims) &#123; // 当前时间 long current = System.currentTimeMillis(); // token签发时间 long issuedAt = claims.getIssuedAt().getTime(); // token过期时间 long expiration = claims.getExpiration().getTime(); // 当前时间 + (`cacheToken`过期时间 - `cacheToken`签发时间) long expireAt = current + (expiration - issuedAt); // 重设Redis超时时间 redisService.expire(token, expireAt); &#125; /** * 刷新Token * 刷新Token的时机： 当cacheToken已过期 并且Redis在有效期内 * 重新生成Token并覆盖Redis的v值(这时候k、v值不一样了)，然后设置Redis过期时间为：新Token过期时间 */ private void refreshToken(String token, Claims claims) &#123; // 当前时间 long current = System.currentTimeMillis(); /* * 重新生成token */ Calendar calendar = Calendar.getInstance(); // 设置签发时间 calendar.setTime(new Date()); Date now = calendar.getTime(); // 设置过期时间: 5分钟 calendar.add(Calendar.MINUTE, 5); Date time = calendar.getTime(); String refreshToken = Jwts.builder() .setSubject(claims.getSubject()) // 签发时间 .setIssuedAt(now) // 过期时间 .setExpiration(time) // 算法与签名(同生成token)：这里算法采用HS512，常量中定义签名key .signWith(SignatureAlgorithm.HS512, ConstantKey.SIGNING_KEY) .compact(); // 将refreshToken覆盖Redis的v值,并设置超时时间为refreshToken过期时间 redisService.set(token, refreshToken, time); // 打印日志 log.info(&quot;刷新token执行时间: &#123;&#125;&quot;, (System.currentTimeMillis() - current) + &quot; 毫秒&quot;); &#125;&#125; 6. 修改 SpringSecurity 配置类，注入 RedisService：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(securedEnabled = true)public class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Resource private RedisService redisService; @Resource private UserDetailsService userDetailsService; @Resource private BCryptPasswordEncoder bCryptPasswordEncoder; /** * 全局请求忽略规则配置 */ @Override public void configure(WebSecurity web) &#123; // 需要放行的URL web.ignoring().antMatchers(&quot;/register&quot;, &quot;/hello&quot;); &#125; /** * 自定义认证策略：登录的时候会进入 */ @Override public void configure(AuthenticationManagerBuilder auth) &#123; // 2. 通过实现 AuthenticationProvider 自定义身份认证验证组件 auth.authenticationProvider(new AuthenticationProviderImpl(userDetailsService, bCryptPasswordEncoder)); &#125; /** * 自定义 HTTP 验证规则 */ @Override protected void configure(HttpSecurity http) throws Exception &#123; http // 关闭Session .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) // 所有请求需要身份认证 .and().authorizeRequests().anyRequest().authenticated() .and() // 自定义JWT登录过滤器 .addFilter(new JwtLoginFilter(authenticationManager(), redisService)) // 自定义JWT认证过滤器 .addFilter(new JwtAuthenticationFilter(authenticationManager(), redisService)) // 自定义认证拦截器，也可以直接使用内置实现类Http403ForbiddenEntryPoint .exceptionHandling().authenticationEntryPoint(new AuthenticationEntryPointImpl()) // 允许跨域 .and().cors() // 禁用跨站伪造 .and().csrf().disable(); &#125;&#125; 源码地址：https://github.com/chaooo/spring-security-jwt.git,这里我将本文的基于Redis的Token自动续签优化放在github源码tag的V2.0中，防止后续修改后代码对不上。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"},{"name":"SpringSecurity","slug":"SpringSecurity","permalink":"http://chaooo.github.io/tags/SpringSecurity/"},{"name":"Token","slug":"Token","permalink":"http://chaooo.github.io/tags/Token/"}]},{"title":"「Spring Security」整合 JWT 实现无状态登录示例","date":"2021-12-09T08:35:00.000Z","path":"2021/12/09/spring-security-jwt.html","text":"JSON Web Token（缩写 JWT）基于JSON格式信息一种Token令牌，是目前最流行的跨域认证解决方案。 JWT 的原理是，服务器认证以后，生成一个 JSON 对象，发回给用户。 此后，用户与服务端通信的时候，都要发回这个 JSON 对象。服务器完全只靠这个对象认定用户身份。为了防止用户篡改数据，服务器在生成这个对象的时候，会加上签名。 服务器就不保存任何 session 数据了，也就是说，服务器变成无状态了，从而比较容易实现扩展。 1. 依赖与配置文件 在 pom.xml 中引入依赖： 123456789101112131415161718192021222324252627282930313233&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.9.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.78&lt;/version&gt;&lt;/dependency&gt; 用户信息从数据库中获取，在 application.yml 配置文件中配置： 1234567891011server: port: 8080spring: datasource: url: jdbc:mysql://192.168.2.100:3306/security?characterEncoding=UTF8&amp;serverTimezone=Asia/Shanghai username: developer password: 05bZ/OxTB:X+yd%1mybatis: mapper-locations: classpath:mapper/*.xml 2. 自定义Security策略2.1 通过继承 WebSecurityConfigurerAdapter 实现自定义Security策略12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 1. 通过继承 WebSecurityConfigurerAdapter 实现自定义Security策略 * @Configuration：声明当前类是一个配置类 * @EnableWebSecurity：开启WebSecurity模式 * @EnableGlobalMethodSecurity(securedEnabled=true)：开启注解，支持方法级别的权限控制 */@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(securedEnabled = true)public class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Resource private UserDetailsService userDetailsService; @Resource private BCryptPasswordEncoder bCryptPasswordEncoder; /** * 全局请求忽略规则配置 */ @Override public void configure(WebSecurity web) &#123; // 需要放行的URL web.ignoring().antMatchers(&quot;/register&quot;, &quot;/hello&quot;); &#125; /** * 自定义认证策略：登录的时候会进入 */ @Override public void configure(AuthenticationManagerBuilder auth) &#123; // 2.通过实现 AuthenticationProvider 自定义身份认证验证组件 auth.authenticationProvider(new AuthenticationProviderImpl(userDetailsService, bCryptPasswordEncoder)); &#125; /** * HTTP 验证规则 */ @Override protected void configure(HttpSecurity http) throws Exception &#123; http // 关闭Session .sessionManagement().sessionCreationPolicy(SessionCreationPolicy.STATELESS) // 所有请求需要身份认证 .and().authorizeRequests().anyRequest().authenticated() .and() // 3.自定义JWT登录过滤器 .addFilter(new JwtLoginFilter(authenticationManager())) // 4.自定义JWT认证过滤器 .addFilter(new JwtAuthenticationFilter(authenticationManager())) // 5.自定义认证拦截器，也可以直接使用内置实现类Http403ForbiddenEntryPoint .exceptionHandling().authenticationEntryPoint(new AuthenticationEntryPointImpl()) // 允许跨域 .and().cors() // 禁用跨站伪造 .and().csrf().disable(); &#125;&#125; BCryptPasswordEncoder 解析器注入到容器 1234567@Configurationpublic class WebConfig &#123; @Bean public BCryptPasswordEncoder bCryptPasswordEncoder() &#123; return new BCryptPasswordEncoder(); &#125;&#125; 实现 UserDetailsService 接口，自定义逻辑 1234567891011121314@Servicepublic class UserDetailsServiceImpl implements UserDetailsService &#123; @Resource private SysUserDao sysUserDao; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; // 数据库中查找用户 SysUser user = sysUserDao.findByUsername(username); if(user == null)&#123; throw new UsernameNotFoundException(&quot;用户&quot; + username + &quot;不存在!&quot;); &#125; return new User(user.getUsername(), user.getPassword(), emptyList()); &#125;&#125; 2.2 通过实现 AuthenticationProvider 自定义身份认证验证组件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Slf4jpublic class AuthenticationProviderImpl implements AuthenticationProvider &#123; private final UserDetailsService userDetailsService; private final BCryptPasswordEncoder bCryptPasswordEncoder; public AuthenticationProviderImpl(UserDetailsService userDetailsService, BCryptPasswordEncoder bCryptPasswordEncoder)&#123; this.userDetailsService = userDetailsService; this.bCryptPasswordEncoder = bCryptPasswordEncoder; &#125; /** * 用来验证用户身份 （对传递的Authentication对象的身份验证） * * @param authentication 传递的Authentication对象 * @return 包含凭证的经过完全认证的对象 * @throws AuthenticationException 份验证失败异常 */ @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; // 获取认证的用户名 &amp; 密码 String name = authentication.getName(); String password = authentication.getCredentials().toString(); // 认证逻辑 UserDetails userDetails = userDetailsService.loadUserByUsername(name); if (bCryptPasswordEncoder.matches(password, userDetails.getPassword())) &#123; log.info(&quot;用户登录成功，username=&#123;&#125;&quot;, name); // 这里设置权限和角色 ArrayList&lt;GrantedAuthority&gt; authorities = new ArrayList&lt;&gt;(); authorities.add( new GrantedAuthorityImpl(&quot;ROLE_ADMIN&quot;)); authorities.add( new GrantedAuthorityImpl(&quot;AUTH_WRITE&quot;)); // 生成令牌 这里令牌里面存入了:name,password,authorities, 当然你也可以放其他内容 return new UsernamePasswordAuthenticationToken(name, password, authorities); &#125; else &#123; throw new BadCredentialsException(&quot;密码错误&quot;); &#125; &#125; /** * 判断当前的AuthenticationProvider 是否支持对应的Authentication对象 * @param authentication Authentication对象 * @return boolean */ @Override public boolean supports(Class&lt;?&gt; authentication) &#123; return authentication.equals(UsernamePasswordAuthenticationToken.class); &#125;&#125; 实现 GrantedAuthority 存储权限和角色 12345678910111213141516/** * 权限类型，负责存储权限和角色 */public class GrantedAuthorityImpl implements GrantedAuthority &#123; private String authority; public GrantedAuthorityImpl(String authority) &#123; this.authority = authority; &#125; public void setAuthority(String authority) &#123; this.authority = authority; &#125; @Override public String getAuthority() &#123; return this.authority; &#125;&#125; 2.3 自定义JWT登录过滤器，继承 UsernamePasswordAuthenticationFilter重写了其中的3个方法 attemptAuthentication：接收并解析用户凭证。 successfulAuthentication：用户成功登录后被调用，我们在这个方法里生成token。 unsuccessfulAuthentication：认证失败后被调用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879@Slf4jpublic class JwtLoginFilter extends UsernamePasswordAuthenticationFilter &#123; private final AuthenticationManager authenticationManager; public JwtLoginFilter(AuthenticationManager authenticationManager) &#123; this.authenticationManager = authenticationManager; &#125; /** * 尝试身份认证(接收并解析用户凭证) */ @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; String username = request.getParameter(&quot;username&quot;); String password = request.getParameter(&quot;password&quot;); return authenticationManager.authenticate( new UsernamePasswordAuthenticationToken(username, password, new ArrayList&lt;&gt;()) ); &#125; /** * 认证成功(用户成功登录后，这个方法会被调用，我们在这个方法里生成token) */ @Override protected void successfulAuthentication(HttpServletRequest request, HttpServletResponse response, FilterChain chain, Authentication auth) &#123; try &#123; Collection&lt;? extends GrantedAuthority&gt; authorities = auth.getAuthorities(); // 定义存放角色集合的对象 List&lt;String&gt; roleList = new ArrayList&lt;&gt;(); for (GrantedAuthority grantedAuthority : authorities) &#123; roleList.add(grantedAuthority.getAuthority()); &#125; /* * 生成token */ Calendar calendar = Calendar.getInstance(); // 设置签发时间 calendar.setTime(new Date()); Date now = calendar.getTime(); // 设置过期时间: 5分钟 calendar.add(Calendar.MINUTE, 5); Date time = calendar.getTime(); String token = Jwts.builder() .setSubject(auth.getName() + &quot;-&quot; + roleList) // 签发时间 .setIssuedAt(now) // 过期时间 .setExpiration(time) // 自定义算法与签名：这里算法采用HS512，常量中定义签名key .signWith(SignatureAlgorithm.HS512, ConstantKey.SIGNING_KEY) .compact(); /* * 返回token */ log.info(&quot;用户登录成功，生成token=&#123;&#125;&quot;, token); // 登录成功后，返回token到header里面 response.addHeader(&quot;Authorization&quot;, token); // 登录成功后，返回token到body里面 ResponseJson&lt;String&gt; result = ResponseJson.success(token); response.setCharacterEncoding(&quot;UTF-8&quot;); response.getWriter().write(JSON.toJSONString(result)); &#125; catch (IOException e) &#123; log.error(&quot;IOException:&quot;, e); &#125; &#125; /** * 认证失败调用 */ @Override protected void unsuccessfulAuthentication(HttpServletRequest request, HttpServletResponse response, AuthenticationException exception) throws IOException &#123; log.warn(&quot;登录失败[&#123;&#125;]，AuthenticationException=&#123;&#125;&quot;, request.getRequestURI(), exception.getMessage()); // 登录失败，返回错误信息 ResponseJson&lt;Void&gt; result = ResponseJson.error(exception.getMessage(), null); response.setCharacterEncoding(&quot;UTF-8&quot;); response.getWriter().write(JSON.toJSONString(result)); &#125;&#125; 自定义加密的签名key 123456/** * 自定义签名key常量 */public class ConstantKey &#123; public static final String SIGNING_KEY = &quot;Charles@Jwt!&amp;Secret^#&quot;;&#125; 自定义全局API返回JSON数据对象 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Datapublic class ResponseJson&lt;T&gt; implements Serializable &#123; /** 自定义状态码 */ private int code; /** 提示信息 */ private String msg; /** 返回数据 */ private T data; private ResponseJson(int code, String msg, T data) &#123; this.code = code; this.msg = msg; this.data = data; &#125; public static ResponseJson&lt;Void&gt; success() &#123; return new ResponseJson&lt;&gt;(0, &quot;操作成功&quot;, null); &#125; public static&lt;T&gt; ResponseJson&lt;T&gt; success(T data) &#123; return new ResponseJson&lt;&gt;(0, &quot;操作成功&quot;, data); &#125; public static&lt;T&gt; ResponseJson&lt;T&gt; success(String msg, T data) &#123; return new ResponseJson&lt;&gt;(0, msg, data); &#125; public static&lt;T&gt; ResponseJson&lt;T&gt; success(int code, String msg, T data) &#123; return new ResponseJson&lt;&gt;(code, msg, data); &#125; public static ResponseJson&lt;Void&gt; error() &#123; return new ResponseJson&lt;&gt;(-1, &quot;操作失败&quot;, null); &#125; public static ResponseJson&lt;Void&gt; error(String msg) &#123; return new ResponseJson&lt;&gt;(-1, msg, null); &#125; public static ResponseJson&lt;Void&gt; error(int code, String msg) &#123; return new ResponseJson&lt;&gt;(code, msg, null); &#125; public static&lt;T&gt; ResponseJson&lt;T&gt; error(String msg, T data) &#123; return new ResponseJson&lt;&gt;(-1, msg, data); &#125; public static&lt;T&gt; ResponseJson&lt;T&gt; error(int code, String msg, T data) &#123; return new ResponseJson&lt;&gt;(code, msg, data); &#125; @Override public String toString() &#123; return &quot;ResponseJson&#123;&quot; + &quot;code=&quot; + code + &quot;, msg=&#x27;&quot; + msg + &#x27;\\&#x27;&#x27; + &quot;, data=&quot; + data + &#x27;&#125;&#x27;; &#125; private static final long serialVersionUID = 1L;&#125; 2.4 自定义JWT认证过滤器，继承 BasicAuthenticationFilter，重写doFilterInternal方法从http头的Authorization 项读取token数据，然后用Jwts包提供的方法校验token的合法性。如果校验通过，就认为这是一个取得授权的合法请求。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394@Slf4jpublic class JwtAuthenticationFilter extends BasicAuthenticationFilter &#123; public JwtAuthenticationFilter(AuthenticationManager authenticationManager) &#123; super(authenticationManager); &#125; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; UsernamePasswordAuthenticationToken authentication = getAuthentication(request, response); SecurityContextHolder.getContext().setAuthentication(authentication); chain.doFilter(request, response); &#125; private UsernamePasswordAuthenticationToken getAuthentication(HttpServletRequest request, HttpServletResponse response) &#123; /* * 解析token */ String token = request.getHeader(&quot;Authorization&quot;); if (!ObjectUtils.isEmpty(token)) &#123; try &#123; Claims claims = Jwts.parser() // 设置生成token的签名key .setSigningKey(ConstantKey.SIGNING_KEY) // 解析token .parseClaimsJws(token).getBody(); String user = claims.getSubject(); if (user != null) &#123; String[] split = user.split(&quot;-&quot;)[1].split(&quot;,&quot;); ArrayList&lt;GrantedAuthority&gt; authorities = new ArrayList&lt;&gt;(); for (String s : split) &#123; authorities.add(new GrantedAuthorityImpl(s)); &#125; // 刷新Token refreshToken(response, claims); // 返回Authentication return new UsernamePasswordAuthenticationToken(user, null, authorities); &#125; &#125; catch (ExpiredJwtException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，ExpiredJwtException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (UnsupportedJwtException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，UnsupportedJwtException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (MalformedJwtException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，MalformedJwtException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (SignatureException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，SignatureException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; catch (IllegalArgumentException e) &#123; log.warn(&quot;访问[&#123;&#125;]失败，IllegalArgumentException=&#123;&#125;&quot;, request.getRequestURI(), e.getMessage()); &#125; &#125; log.warn(&quot;访问[&#123;&#125;]失败，需要身份认证&quot;, request.getRequestURI()); return null; &#125; /** * 刷新Token * 刷新Token的时机： * 1. 当前时间 &lt; token过期时间 * 2. 当前时间 &gt; (签发时间 + (token过期时间 - token签发时间)/2) */ private void refreshToken(HttpServletResponse response, Claims claims) &#123; // 当前时间 long current = System.currentTimeMillis(); // token签发时间 long issuedAt = claims.getIssuedAt().getTime(); // token过期时间 long expiration = claims.getExpiration().getTime(); // (当前时间 &lt; token过期时间) &amp;&amp; (当前时间 &gt; (签发时间 + (token过期时间 - token签发时间)/2)) if ((current &lt; expiration) &amp;&amp; (current &gt; (issuedAt + ((expiration - issuedAt) / 2)))) &#123; /* * 重新生成token */ Calendar calendar = Calendar.getInstance(); // 设置签发时间 calendar.setTime(new Date()); Date now = calendar.getTime(); // 设置过期时间: 5分钟 calendar.add(Calendar.MINUTE, 5); Date time = calendar.getTime(); String refreshToken = Jwts.builder() .setSubject(claims.getSubject()) // 签发时间 .setIssuedAt(now) // 过期时间 .setExpiration(time) // 算法与签名(同生成token)：这里算法采用HS512，常量中定义签名key .signWith(SignatureAlgorithm.HS512, ConstantKey.SIGNING_KEY) .compact(); // 主动刷新token，并返回给前端 response.addHeader(&quot;refreshToken&quot;, refreshToken); log.info(&quot;刷新token执行时间: &#123;&#125;&quot;, (System.currentTimeMillis() - current) + &quot; 毫秒&quot;); &#125; &#125;&#125; 2.5 通过实现 AuthenticationEntryPoint 自定义认证拦截器1234567891011121314@Slf4jpublic class AuthenticationEntryPointImpl implements AuthenticationEntryPoint &#123; public AuthenticationEntryPointImpl() &#123;&#125; /** * @param request 遇到了认证异常authException用户请求 * @param response 将要返回给客户的相应 */ @Override public void commence(HttpServletRequest request, HttpServletResponse response, AuthenticationException exception) throws IOException &#123; log.debug(&quot;预认证入口被调用。拒绝访问，AuthenticationException=&#123;&#125;&quot;, exception.getMessage()); // 没有权限，返回403 response.sendError(403, &quot;Access Denied&quot;); &#125;&#125; 3. 示例DEMOsql 1234567DROP TABLE IF EXISTS `sys_user`;CREATE TABLE `sys_user` ( `id` bigint NOT NULL AUTO_INCREMENT, `username` varchar(255) DEFAULT NULL COMMENT &#x27;用户名&#x27;, `password` varchar(255) DEFAULT NULL COMMENT &#x27;密码&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;系统用户表&#x27;; Java代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 接口 */@RestControllerpublic class SysUserApi &#123; @Resource private SysUserService sysUserService; @PostMapping(&quot;/register&quot;) public ResponseJson&lt;SysUser&gt; register(SysUser sysUser) &#123; return sysUserService.register(sysUser); &#125; @GetMapping(&quot;/hello&quot;) public ResponseJson&lt;Void&gt; hello() &#123; return ResponseJson.success(&quot;访问成功！公开接口：/hello&quot;,null); &#125; @GetMapping(&quot;/private&quot;) public ResponseJson&lt;Void&gt; hello2() &#123; return ResponseJson.success(&quot;访问成功！非公开接口：/private&quot;, null); &#125;&#125;/** * 用户实体类 */@Datapublic class SysUser &#123; private Long id; private String username; private String password;&#125;/** * 服务层 */@Servicepublic class SysUserService &#123; @Resource private SysUserDao sysUserDao; @Resource private BCryptPasswordEncoder passwordEncoder; public ResponseJson&lt;SysUser&gt; register(SysUser sysUser) &#123; if (StringUtils.isEmpty(sysUser.getUsername()) || StringUtils.isEmpty(sysUser.getPassword()))&#123; return ResponseJson.error(&quot;用户名或密码不能为空&quot;, null); &#125; String encodePassword = passwordEncoder.encode(sysUser.getPassword()); sysUser.setPassword(encodePassword); sysUserDao.insertSysUser(sysUser); return ResponseJson.success(&quot;注册成功&quot;, sysUser); &#125;&#125;/** * DAO层 */@Mapperpublic interface SysUserDao &#123; SysUser findByUsername(String username); void insertSysUser(SysUser sysUser);&#125; 123456&lt;insert id=&quot;insertSysUser&quot; keyProperty=&quot;id&quot; keyColumn=&quot;id&quot; useGeneratedKeys=&quot;true&quot;&gt;INSERT INTO sys_user(username, password) VALUES(#&#123;username&#125;, #&#123;password&#125;)&lt;/insert&gt;&lt;select id=&quot;findByUsername&quot; resultType=&quot;com.example.jwt.entity.SysUser&quot;&gt;SELECT id,username,PASSWORD FROM sys_user WHERE username=#&#123;username&#125;&lt;/select&gt; 4. 测试注册一个用户： 登录，返回Token： 访问公开接口： 访问需要认证的接口，无权限返回403： 访问需要认证的接口，通过有效Token访问： 源码地址：https://github.com/chaooo/spring-security-jwt.git,这里我将本文的登录认证逻辑放在github源码tag的V1.0中，防止后续修改后对不上。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"},{"name":"SpringSecurity","slug":"SpringSecurity","permalink":"http://chaooo.github.io/tags/SpringSecurity/"},{"name":"JWT","slug":"JWT","permalink":"http://chaooo.github.io/tags/JWT/"}]},{"title":"「Spring Security」安全架构与认证鉴权原理","date":"2021-11-29T06:01:00.000Z","path":"2021/11/29/spring-security-filter.html","text":"1. Spring Security Servlet 安全架构Spring Security 设计的 Servlet 安全从架构上分为三个层次，分别是「认证」、「鉴权」、「入侵防护」。通过过滤器机制将安全逻辑应用到 Servlet 项目。 请求的接收和处理是通过一个一个的过滤器顺序执行实现的，过滤器是 Servlet 项目处理请求的基础。 Spring 将自己体系内的过滤器交由「过滤器代理FilterChainProxy」管理，FilterChainProxy 同样也是一个过滤器，被封装在 Spring 的「过滤器委托代理DelegatingFilterProxy」中。Spring Security 在 FilterChainProxy 中加入了「安全过滤器链SecurityFilterChain」实现安全保护功能。 其过程如图： 安全过滤器链（SecurityFilterChain）的特点： 为所有 Spring Security 支持的 Servlet 指明了起点； 对于一些后台操作，可以提升执行效率； 在 Servlet 容器中，过滤器的选择是由 URL 决定的，如此便可针对不同 URL 指定相互独立的安全策略。 1.1 安全过滤器 FilterSpring Security 内置了 33 种安全过滤器，每个过滤器有固定的顺序及应用场景；内置过滤器的参数设置通过 HttpSecurity 类相应的配置方法完成。 在认证与授权中关键的三个过滤器： UsernamePasswordAuthenticationFilter：该过滤器用于拦截我们表单提交的请求（默认为&#x2F;login），进行用户的认证过程。 FilterSecurityInterceptor：该过滤器主要用来进行授权判断。 ExceptionTranslationFilter：该过滤器主要用来捕获处理spring security抛出的异常，异常主要来源于FilterSecurityInterceptor。 Spring Security 的认证、授权异常在过滤器校验过程中产生，并在 ExceptionTranslationFilter 中接收并进行处理， ExceptionTranslationFilter 过滤器首先像其他过滤器一样，调用过滤器链的执行方法 FilterChain.doFilter(request, response) 启动过滤处理； 如果当前的用户没有通过认证或者因为其他原因在执行过程中抛出了 AuthenticationException 异常，此时将开启「认证流程」： 清空 SecurityContextHolder 对象； 并将原始请求信息「request」保存到 RequestCache 对象中； 使用 AuthenticationEntryPoint 对象存储的认证地址，向客户端索要身份证明。例如，使用浏览器登录的用户，将浏览器地址重定向到 &#x2F;login 或者回传一个 WWW-Authenticate 认证请求头。 如果当前用户身份信息已确认，但是没有访问权限，则会产生 AccessDeniedException 异常，然后访问被拒绝。继续执行拒绝处理 AccessDeniedHandler。 1.2 自定义过滤器 Filter在 HttpSecurity 对象中增加自定义 Filter 可用于实现认证方式的扩展等场景，扩展 Filter 需要实现 javax.servlet.Filter 接口；并且需要指定新过滤器的位置。 例如，扩展自定义接口 SimpleFilter。 自定义接口类 123456public class SimpleFilter implements Filter &#123; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(&quot;In SimpleFilter&quot;); &#125;&#125; 加入到指定位置，比如加在 UsernamePasswordAuthenticationFilter 之前 1http.addFilterBefore(new SimpleFilter(), UsernamePasswordAuthenticationFilter.class); 2. Spring Security 认证2.1 Spring Security 基本认证组件 组别组件名简述 存储单元Authentication维护用户用于认证的信息 GrantedAuthority认证用户的权限信息比如角色、范围等等 SecurityContextHolder用于维护 SpringContext SecurityContext用来存储当前认证用户的信息 认证管理AuthenticationManagerSpringSecurity 向外提供的用于认证的 API 集合 ProviderManagerAuthenticationManager 的常见实现类 AuthenticationProvider用于 ProviderManager 提供认证实现 AuthenticationEntryPoint用于获取用户认证信息 流程管理AbstractAuthenticationProcessingFilter是认证过滤器的基础，用于组合认证流程 2.2 存储单元 SecurityContextHolder 对象是整个 Spring Security 体系的核心，它维护着 SecurityContext 对象。它是唯一的。 SecurityContext 对象用于衔接 SecurityContextHolder 和 Authentication 对象，是对 Authentication 的外层封装。 Authentication 是用户的认证信息。 Authentication对象有三个核心属性： principal：用户的身份信息； credentials：用户的认证凭据，比如密码，通常情况下，当用户完成认证后，此项内容就会被清空； authorities：用户的权限，用于更高层次的鉴权功能，通常包括角色、使用范围等信息。该属性基本由 GrantedAuthority 实现。GrantedAuthority 是在前述 Authentication 对象中所指的权限信息。在开发过程中，可以通过 Authentication.getAuthorities() 方法获取。权限信息通常包括角色、范围，或者其他扩展内容。 Authentication 两个主要作用： 为 AuthenticationManager 对象提供用于认证的信息载体； 用于获取某个用户的基本信息。 2.3 认证管理 AuthenticationManager 为 Spring 过滤器提供认证支持 API。AuthenticationManager 的实现形式并没有严格限制，通常情况下使用 ProviderManager。 ProviderManager 是 AuthenticationManager 的最常用的实现类，它包含了一系列的 AuthenticationProvider 对象，用以判断认证流程是否完成、认证结构是否成功。 AuthenticationProvider：每个 ProviderManager 可以包含多个 AuthenticationProvider ，每个 AuthenticationProvider 提供一种认证类型，例如：DaoAuthenticationProvider 可以完成「用户名 &#x2F; 密码」的认证，JwtAuthenticationProvider 用于完成 JWT 方式的认证。 AuthenticationEntryPoint 在当一个请求包含的认证信息不全时，比如未认证终端访问受保护资源时发挥作用，如跳转到登录页面、返回认证要求等。 2.4 流程管理AbstractAuthenticationProcessingFilter 是所有认证过滤器的基类。 当用户提交认证信息，AbstractAuthenticationProcessingFilter 首先从请求信息（例如用户名、密码）中创建 Authentication 对象； 将 Authentication 对象传递给 AuthenticationManager 对象，用于后续认证； 如果认证失败，则执行失败流程： 清空 SecurityContextHolder 对象； 触发 RememberMeServices.loginFail 方法； 触发 AuthenticationFailureHandler。 如果认证成功，则执行成功流程： SessionAuthenticationStrategy 登记新的登录； 将 Authentication 对象设置到 SecurityContextHolder 对象中，并将 SecurityContext 对象保持到 Session 中； 调用 RememberMeServices.loginSuccess 方法； ApplicationEventPublisher 发起事件 InteractiveAuthenticationSuccessEvent 3. Spring Security 鉴权Spring Security 包含确认身份和确认身份的可执行操作两部分，前者为认证(Authentication)，后者即为鉴权(Authorization)； 3.1 权限Spring Security 的权限默认是以字符串形式存储的权限信息，比如角色名称、功能名称等； 在用户身份信息得到确认后，Authentication 中会存储一系列的 GrantedAuthority 对象，这些对象用来判断用户可以使用哪些资源。 GrantedAuthority 对象通过 AuthenticationManager 插入到 Authentication 对象中，并被 AccessDecisionManager 使用，判断其权限。 GrantedAuthority 是一个接口，其仅包含一个 getAuthority() 方法，返回一个字符串值，该值作为权限的描述，当权限较为复杂，该方法需要返回 null，此时 AccessDecisionManager 会根据 getAuthority() 返回值情况判断是否要进行特殊处理。 SimpleGrantedAuthority 是 GrantedAuthority 的一个基础实现类，可以满足一般的业务需求。 3.2 鉴权 前置鉴权：由 AccessDecisionManager 对象判断其是否允许继续执行； 权限判断发生在方法被调用前，或者 WEB 请求之前。不满足抛出 AccessDeniedException 异常。 后置鉴权：通过 AfterInvocationManager 进行管理；后置鉴权在资源被访问后，根据权限的判定来修改返回的内容，或者返回 AccessDeniedException。 前置鉴权 AccessDecisionManager 对象由 AbstractSecurityInterceptor 发起调用，其职责是给出资源是否能被访问的最终结果； AccessDecisionManager 包含三个主要方法： boolean supports(ConfigAttribute attribute);：判断配置属性是否可被访问； boolean supports(Class clazz);：判断安全对象的类型是否支持被访问； void decide(Authentication authentication, Object secureObject,Collection&lt;ConfigAttribute&gt; attrs) throws AccessDeniedException;：通过认证信息、安全对象、权限信息综合判断安全对象是否允许被访问。 Spring Security 内置了以「投票」为判定方法的鉴权策略。Spring Security 的鉴权策略可以由用户自己实现。 投票策略下，AccessDecisionManager 控制着一系列的 AccessDecisionVoter 实例，判断权限是否满足，如果不满足抛出 AccessDeniedException 异常。 AccessDecisionVoter 也包含三个方法： boolean supports(ConfigAttribute attribute);：判断配置属性是否支持； boolean supports(Class clazz);：判断类型是否支持； int vote(Authentication authentication, Object object, Collection&lt;ConfigAttribute&gt; attrs);：根据认证信息对安全资源进行投票。 投票鉴权分为三类： 基于角色的投票：RoleVoter； 基于认证信息的投票：AuthenticatedVoter，主要区分认证用户、匿名用户等； 自定义投票策略。 3.3 Servlet 请求鉴权流程Servlet 鉴权主要围绕着 FilterSecurityInterceptor 类展开，该类作为一个安全过滤器，被放置在 FilterChainProxy 中。 具体流程如下： FilterSecurityInterceptor 从 SecurityContextHolder 中获取 Authentication 对象； FilterSecurityInterceptor 从 HttpServletRequest、HttpServletREsponse、 FilterChain 中创建 FilterInvocation 对象； 将创建的 FilterInvocation 对象传递给 SecurityMetadataSource 用来获取 ConfigAttribute 对象集合； 最后，将 Authentication、FilterInvocation 和 ConfigAttribute 对象传递给 AccessDecisionManager 实例验证权限： 如果验证失败，将抛出 AccessDeniedException 异常，并由 ExceptionTranslationFilter 接收并处理； 如果验证通过，FilterSecurityInterceptor 将控制权交还给 FilterChain，使程序继续执行。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"},{"name":"SpringSecurity","slug":"SpringSecurity","permalink":"http://chaooo.github.io/tags/SpringSecurity/"}]},{"title":"「Spring Security」Spring Security 基础入门示例","date":"2021-11-25T07:01:00.000Z","path":"2021/11/25/spring-security-base.html","text":"Spring Security 是一个功能强大且高度可定制的身份验证和访问控制的安全框架。它是 Spring 应用程序在安全框架方面的公认标准。 其核心特性包括：认证和授权、常规攻击防范、与 Servlet 接口集成、与 Spring MVC 集成等。 常规攻击防范在 Spring Security 安全框架中是默认开启的，常见的威胁抵御方式有：防止伪造跨站请求（CSRF），安全响应头（HTTP Response headers），HTTP通讯安全等 1. 入门示例新建 SpringBoot 项目，在 pom.xml 中增加 Spring Security 依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 只要加入依赖，项目的所有接口都会被自动保护起来。 创建一个 Controller： 1234567@RestControllerpublic class HelloApi &#123; @GetMapping(&quot;/hello&quot;) public String hello() &#123; return &quot;hello&quot;; &#125;&#125; 导入spring-boot-starter-security启动后，Spring Security已经生效，默认拦截全部请求，如果用户没有登录，跳转到内置登录页面。 访问/hello接口 ，需要登录之后才能访问。 默认配置下，会自动生成一个 user 用户，并分配其随机密码，密码可以从控制台的日志信息中找到： 2. Spring Security默认配置项Spring Boot 引入 Spring Security 启动后，将会自动开启如下配置项： 默认开启一系列基于 springSecurityFilterChain 的 Servlet 过滤器，包含了几乎所有的安全功能，例如：保护系统 URL、验证用户名、密码表单、重定向到登录界面等； 创建 UserDetailsService 实例，并生成随机密码，用于获取登录用户的信息详情； 将安全过滤器应用到每一个请求上。 除此之外，Spring Security 还有一些其他可配置的功能： 限制所有访问必须首先通过认证； 生成默认登录表单； 创建用户名为 user 的可以通过表单认证的用户，并为其初始化密码； 使用 BCrypt 方式加密密码； 提供登出的能力； 保护系统不受 CSRF 攻击； 会话固定保护； 集成安全消息头； 提供一些默认的 Servlet 接口，如：「getRemoteUser」、「getUserPrincipal」、「isUserInRole」、「login」和「logout」。 3. 配置文件定义用户信息也可以直接在 application.yml 配置文件中配置用户的基本信息： 12345spring: security: user: name: user password: 123 配置完成后，重启项目，就可以使用这里配置的用户名&#x2F;密码登录了。 4. Java代码定义用户信息如果需要自定义逻辑时，只需要实现UserDetailsService接口即可。 123456789101112131415161718@Servicepublic class UserDetailsServiceImpl implements UserDetailsService &#123; @Resource private PasswordEncoder encoder; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; // 判断用户名是否存在(用户名也可查询数据库)，如果不存在抛出UsernameNotFoundException if(!username.equals(&quot;admin&quot;))&#123; throw new UsernameNotFoundException(&quot;用户名不存在&quot;); &#125; // 把查询出来的密码进行解析,或直接把password放到构造方法中。 // 理解:password就是数据库中查询出来的密码，查询出来的内容不是123 String password = encoder.encode(&quot;123&quot;); return new User(username,password, AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin&quot;)); &#125;&#125; 返回值 UserDetails 是一个接口，要想返回 UserDetails 的实例就只能返回接口的实现类。这里使用 Spring Security 中提供的 org.springframework.security.core.userdetails.User。 Spring Security 要求：当进行自定义登录逻辑时容器内必须有 PasswordEncoder 实例。 客户端密码和数据库密码是否匹配是由 Spring Security 去完成的，但 Security 中没有默认密码解析器。所以当自定义登录逻辑时要求必须给容器注入PaswordEncoder的bean对象。 12345678910111213@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Bean public PasswordEncoder getPasswordEncoder()&#123; /* * BCryptPasswordEncoder是spring security官方推荐的解析器， * 它是对bcrypt强散列方法的具体实现，基于Hash算法实现的单向加密。 * 可以通过strength控制加密强度，默认为10，长度越长安全性越高。 */ return new BCryptPasswordEncoder(); &#125;&#125; 重启项目后，在浏览器中输入账号：admin，密码：123，登录后就可以访问接口了。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"},{"name":"SpringSecurity","slug":"SpringSecurity","permalink":"http://chaooo.github.io/tags/SpringSecurity/"}]},{"title":"「SpringCloud」OpenFeign整合Sentinel实现熔断降级","date":"2021-11-23T09:01:00.000Z","path":"2021/11/23/spring-cloud-sentinel.html","text":"1. Sentinel简介Sentinel 是阿里开源的项目，提供了流量控制、熔断降级、系统负载保护等多个维度来保障服务之间的稳定性。 Sentinel 分为两个部分: 核心库（Java 客户端）不依赖任何框架&#x2F;库，能够运行于所有 Java 运行时环境，同时对 Dubbo / Spring Cloud 等框架也有较好的支持。 控制台（Dashboard）基于 Spring Boot 开发，打包后可以直接运行，不需要额外的 Tomcat 等应用容器。 这里仅介绍Sentinel核心库 与 Spring Cloud OpenFeign整合使用。 2. Sentinel与OpenFeign整合基于上一篇OpenFeign服务间调用 2.1 在pom.xml文件中添加依赖12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;version&gt;2021.1&lt;/version&gt;&lt;/dependency&gt; 2.2 在YMAL配置文件中添加如下配置1234# 配置文件打开 Sentinel 对 Feign 的支持feign: sentinel: enabled: true Feign 对应的接口中的资源名策略定义：httpmethod:protocol://requesturl。@FeignClient 注解中的所有属性，Sentinel 都做了兼容。如：ToolsFeign 接口中方法 getSendSms 对应的资源名为 POST:http://XXX-CLOUD-TOOLS/tools/sms/send。 2.3 修改OpenFeign调用远程服务，@FeignClient属性中配置降级回调类@FeignClient属性中的fallback和fallbackFactory fallback：定义容错的处理类，当调用远程接口失败或超时时，会调用对应接口的容错逻辑，fallback指定的类必须实现@FeignClient标记的接口。 fallbackFactory：工厂类，用于生成fallback类示例，通过这个属性我们可以实现每个接口通用的容错逻辑，减少重复的代码。 注：同一个@FeignClient里，fallback 和 fallbackFactory 不能同时使用。 2.3.1 使用fallback示例 按照Feign的规则定义接口，使用fallback属性： 12345678910111213/** * value是远程调用微服务在注册中心的服务名。 * fallback：定义容错的类，当远程调用的接口失败或者超时的时候，会调用对应接口的容错逻辑， * fallback指定的类必须实现@FeignClient标记的接口。 */@FeignClient(value = &quot;XXX-CLOUD-TOOLS&quot;, fallback = ToolsFeignFallback.class)public interface ToolsFeign &#123; /** * 远程调用TOOLS服务接口“/tools/sms/send”--请求发短信 */ @PostMapping(&quot;/tools/sms/send&quot;) String getSendSms(@RequestParam String mobile, @RequestParam String content);&#125; 编写降级回调类ToolsFeignFallback.calss 1234567@Componentpublic class ToolsFeignFallback implements ToolsFeign&#123; @Override public String getSendSms(String mobile, String content) &#123; return &quot;接口容错-fallback&quot;; &#125;&#125; 2.3.2 使用fallbackFactory示例 按照Feign的规则定义接口，使用fallbackFactory属性： 12345678910111213/** * value是远程调用微服务在注册中心的服务名。 * fallbackFactory：工厂类，用于生成fallback类实例， * 通过此属性可以实现每个接口通用的容错逻辑，以达到减少重复的代码。 */@FeignClient(value = &quot;XXX-CLOUD-TOOLS&quot;, fallbackFactory = ToolsFeignFallbackFactory.class)public interface ToolsFeign &#123; /** * 远程调用TOOLS服务接口“/tools/sms/send”--请求发短信 */ @PostMapping(&quot;/tools/sms/send&quot;) String getSendSms(@RequestParam String mobile, @RequestParam String content);&#125; 编写降级回调工厂类ToolsFeignFallbackFactory.calss 1234567@Componentpublic class ToolsFeignFallbackFactory implements FallbackFactory&lt;ToolsFeignFallback&gt; &#123; @Override public ToolsFeignFallback create(Throwable throwable) &#123; return new ToolsFeignFallback(throwable); &#125;&#125; 编写降级回调类ToolsFeignFallback.calss 12345678910111213141516/** * sentinel 降级处理 */public class ToolsFeignFallback implements ToolsFeign&#123; private Throwable throwable; ToolsFeignFallback(Throwable throwable) &#123; this.throwable = throwable; &#125; /** * 调用服务提供方的接口 */ @Override public String getSendSms(String mobile, String content) &#123; return &quot;接口容错-fallback&quot; + throwable.getMessage(); &#125;&#125; 3. Sentinel 熔断降级3.1 Sentinel 熔断策略 慢调用比例 (SLOW_REQUEST_RATIO)：选择以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大的响应时间），请求的响应时间大于该值则统计为慢调用。当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且慢调用的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断。 异常比例 (ERROR_RATIO)：当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%。 异常数 (ERROR_COUNT)：当单位统计时长内的异常数目超过阈值之后会自动进行熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。 注意异常降级仅针对业务异常，对 Sentinel 限流降级本身的异常（BlockException）不生效。为了统计异常比例或异常数，需要通过 Tracer.trace(ex) 记录业务异常。开源整合模块，如 Sentinel Dubbo Adapter, Sentinel Web Servlet Filter 或 @SentinelResource 注解会自动统计业务异常，无需手动调用。 3.2 Sentinel熔断降级规则说明熔断降级规则（DegradeRule）包含下面几个重要的属性： Field 说明 默认值 resource 资源名，即规则的作用对象 – grade 熔断策略，支持 慢调用比例&#x2F;异常比例&#x2F;异常数策略 慢调用比例 count 慢调用比例模式下为慢调用临界 RT（超出该值计为慢调用）；异常比例&#x2F;异常数模式下为对应的阈值 – timeWindow 熔断时长，单位为 s – minRequestAmount 熔断触发的最小请求数，请求数小于该值时即使异常比率超出阈值也不会熔断（1.7.0 引入） 5 statIntervalMs 统计时长（单位为 ms），如 60*1000 代表分钟级（1.8.0 引入） 1000 ms slowRatioThreshold 慢调用比例阈值，仅慢调用比例模式有效（1.8.0 引入） – 3.3 熔断器事件监听Sentinel 支持注册自定义的事件监听器监听熔断器状态变换事件（state change event）。示例： 12345678EventObserverRegistry.getInstance().addStateChangeObserver(&quot;logging&quot;, (prevState, newState, rule, snapshotValue) -&gt; &#123; if (newState == State.OPEN) &#123; // 变换至 OPEN state 时会携带触发时的值 System.err.println(String.format(&quot;%s -&gt; OPEN at %d, snapshotValue=%.2f&quot;, prevState.name(), TimeUtil.currentTimeMillis(), snapshotValue)); &#125; else &#123; System.err.println(String.format(&quot;%s -&gt; %s at %d&quot;, prevState.name(), newState.name(), TimeUtil.currentTimeMillis())); &#125;&#125;);","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://chaooo.github.io/tags/SpringCloud/"},{"name":"Sentinel","slug":"Sentinel","permalink":"http://chaooo.github.io/tags/Sentinel/"}]},{"title":"「SpringCloud」OpenFeign服务间调用","date":"2021-11-12T08:01:00.000Z","path":"2021/11/12/spring-cloud-feign.html","text":"1. Spring Cloud OpenFeign简介OpenFeign是SpringCloud提供的一个声明式的伪Http客户端，它使得调用远程服务就像调用本地服务一样简单，只需要创建一个接口并添加一个注解即可。OpenFeign是SpringCloud在Feign的基础上支持了Spring MVC的注解，并通过动态代理的方式产生实现类来做负载均衡并进行调用其他服务。 1.1 OpenFeign使用流程： 引入Spring Cloud OpenFeign的依赖 启动类上添加注解@EnableFeignCleints 按照Feign的规则定义接口并添加@FeignClient注解 在需要使用Feign接口的类里注入，直接调用接口方法 2. OpenFeign的使用 在pom.xml文件中添加依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 在启动类上，加上@EnableFeignCleints注解： 123456789// basePackages 是Feign接口定义的路径@EnableFeignClients(basePackages = &#123;&quot;com.XXX.feign&quot;&#125;)@EnableDiscoveryClient@SpringBootApplicationpublic class UserApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(UserApplication.class, args); &#125;&#125; 按照Feign的规则定义接口： 123456789// 括号内是远程调用微服务在注册中心的服务名@FeignClient(&quot;XXX-CLOUD-TOOLS&quot;)public interface ToolsFeign &#123; /** * 远程调用TOOLS服务接口“/tools/sms/send”--请求发短信 */ @PostMapping(&quot;/tools/sms/send&quot;) String getSendSms(@RequestParam String mobile, @RequestParam String content);&#125; 调用Feign接口方法 1234567891011121314151617@Slf4j@Servicepublic class RemoteCallServiceImpl implements RemoteCallService &#123; /** * 在需要使用`Feign`接口的类里注入ToolsFeign */ @Resource private ToolsFeign toolsFeign; /** * 调用 ToolsFeign 发送短信 */ @Override public String remoteSendSms(String mobile, String content) &#123; return toolsFeign.getSendSms(mobile, content); &#125;&#125; 远程服务接口： 1234567891011121314151617181920212223@RestController@RequiredArgsConstructor@RequestMapping(&quot;/tools&quot;)public class SmsApi &#123; /** * 发送短信 * * @param smsDto &#123; * mobile 手机号 * content 短信内容 * &#125; */ @PostMapping(&quot;/sms/send&quot;) public ResponseJson&lt;String&gt; sendSms(SmsDto smsDto)&#123; String result = SmsUtil.sendSms(smsDto.getMobile(), smsDto.getContent()); JSONObject json = (JSONObject) JSONObject.parse(result); if (null != json &amp;&amp; json.getInteger(&quot;code&quot;) == 0) &#123; return ResponseJson.success(0, &quot;发送成功&quot;,null); &#125; else &#123; return ResponseJson.error(0, &quot;发送失败&quot;, result); &#125; &#125;&#125; 3. OpenFeign的核心工作原理： 通过@EnableFeignCleints触发Spring应用程序对classpath中@FeignClient修饰类的扫描 解析到@FeignClient修饰类后，Feign框架通过扩展SpringBeanDeifinition的注册逻辑，最终注册一个FeignClientFacotoryBean进入Spring容器 Spring容器在初始化其他用到@FeignClient接口的类时，获得的是FeignClientFacotryBean产生的一个代理对象Proxy. 基于java原生的动态代理机制，针对Proxy的调用，都会被统一转发给Feign框架所定义的一个InvocationHandler，由该Handler完成后续的HTTP转换，发送，接收，翻译HTTP响应的工作 4. OpenFeign日志Feign 和 RestTemplate 不一样 ，对请求细节封装的更加彻底，不管是请求还是请求的参数，还是响应的状态都看不到，想要看到请求的细节需要通过Feign的日志，我们可以通过配置来调整日志级别，从而了解OpenFeign中Http请求的细节。即对OpenFeign远程接口调用的情况进行监控和日志输出。 4.1 日志级别 NONE：默认级别，不显示日志 BASIC：仅记录请求方法、URL、响应状态及执行时间 HEADERS：除了BASIC中定义的信息之外，还有请求和响应头信息 FULL：除了HEADERS中定义的信息之外，还有请求和响应正文及元数据信息 4.2 配置日志bean1234567@Configurationpublic class OpenFeignConfig &#123; @Bean Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125;&#125; 4.3 开启日志在YMAL配置文件中中指定监控的接口，以及日志级别 123logging: level: com.XXX.feign.ToolsFeign: debug # 以什么级别监控哪个接口","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://chaooo.github.io/tags/SpringCloud/"},{"name":"OpenFeign","slug":"OpenFeign","permalink":"http://chaooo.github.io/tags/OpenFeign/"}]},{"title":"「SpringCloud」Config配置中心","date":"2021-11-09T10:01:00.000Z","path":"2021/11/09/spring-cloud-config.html","text":"1. Spring Cloud Config简介Spring Cloud Config可以为微服务架构中的应用提供集中化的外部配置支持，它分为服务端和客户端两个部分。服务端被称为分布式配置中心，它是个独立的应用，可以从配置仓库获取配置信息并提供给客户端使用。客户端可以通过配置中心来获取配置信息，在启动时加载配置。Spring Cloud Config默认采用Git来存储配置信息，所以天然就支持配置信息的版本管理，并且可以使用Git客户端来方便地管理和访问配置信息。 2. 准备工作因为config server是需要到git上拉取配置文件的，所以还需要在远程的git上新建一个存放配置文件的仓库，如下仓库中存放客户端配置文件： 123application-beta.ymlapplication-dev.ymlapplication-prod.yml 3. 配置中心服务端搭建 搭建服务端配置中心（config-server），在pom.xml文件中添加依赖： 12345678910&lt;!--配置服务--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--注册到注册中心--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 在服务端编辑application.yml配置文件内容如下： 123456789101112131415161718192021222324server: port: 8009# 指定当前服务的名称，这个名称会注册到注册中心spring: application: name: config-server cloud: config: server: git: #配置存储配置信息的Git仓库 uri: http://git.xxx.com/config-files.git # 远程git仓库的地址 username: username # git账户名 password: password # git密码 clone-on-start: true # 开启启动时直接从git获取配置 search-paths: /** # 指定搜索根路径下的目录，若有多个路径使用逗号隔开# 指定服务注册中心的地址eureka: instance: prefer-ip-address: true # 是否使用 ip 地址注册 instance-id: $&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; # ip:port client: service-url: # 设置服务注册中心地址 defaultZone: http://localhost:18000/eureka/ 在启动类上，加上@EnableConfigServer注解，声明这是一个config-server。代码如下： 123456789101112/** * `@EnableDiscoveryClient`: 声明一个可以被发现的客户端 * `@EnableConfigServer`: 声明一个Config配置服务 */@EnableConfigServer@EnableDiscoveryClient@SpringBootApplicationpublic class ConfigApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigApplication.class, args); &#125;&#125; 启动项目，访问http://localhost:8009/master/application-dev.yml，可以看到能够访问到客户端配置文件的内容。 4. 客户端获取配置 在pom.xml文件中添加依赖： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-bootstrap&lt;/artifactId&gt;&lt;/dependency&gt; 编辑bootstrap.yml配置文件内容如下： 12345678910server: port: 18001spring: cloud: config: #Config客户端配置 profile: dev #启用配置后缀名称 label: master #分支名称 uri: http://localhost:8009 #配置中心地址 name: application #配置文件名称 启动服务，配置中心读取的配置生效。 5. 获取配置信息的基本规则1234# 获取配置信息/&#123;label&#125;/&#123;name&#125;‐&#123;profile&#125;# 获取配置文件信息/&#123;label&#125;/&#123;name&#125;‐&#123;profile&#125;.yml name : 文件名，一般以服务名(spring.application.name)来命名，如果配置了spring.cloud.config.name，则为该名称. profiles : 一般作为环境标识，对应配置文件中的spring.cloud.config.profile lable : 分支（branch），指定访问某分支下的配置文件，对应配置文件中的spring.cloud.config.label，默认值是在服务器上设置的(对于基于git的服务器，通常是“master”) 5.1 Maven的Profile管理Maven提供了Profile切换功能(多环境dev,beta,prod)， 如下pom.xml： 12345678910111213141516171819202122232425&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;!-- 环境标识，需要与配置文件的名称相对应 --&gt; &lt;activatedProperties&gt;dev&lt;/activatedProperties&gt; &lt;/properties&gt; &lt;activation&gt; &lt;!-- 默认环境 --&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;beta&lt;/id&gt; &lt;properties&gt; &lt;activatedProperties&gt;beta&lt;/activatedProperties&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;prod&lt;/id&gt; &lt;properties&gt; &lt;activatedProperties&gt;prod&lt;/activatedProperties&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; 配置文件bootstrap.yml： 1234567spring: cloud: config: # Config客户端配置 profile: @activatedProperties@ # 启用配置后缀名称 label: master # 分支名称 uri: http://localhost:8009 # 配置中心地址 name: application # 配置文件名称 SpringBoot一把情况下会遵从你选的环境将@activatedProperties@替换掉。 但SpringCloud比较特殊，使用配置中心后客户端不会再使用application.yml，而是使用bootstrap.yml。但是Maven不认bootstrap.yml里的@activatedProperties@。 解决：在pom.xml的build标签里添加如下代码，用于过滤yml文件： 1234567891011121314151617181920&lt;build&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;!--可以在此配置过滤文件 --&gt; &lt;includes&gt; &lt;include&gt;**/*.yml&lt;/include&gt; &lt;/includes&gt; &lt;!--开启filtering功能 --&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 5.2 将不同服务的配置文件放到以服务名命名的目录下因为config server默认情况下只会搜索git仓库根路径下的配置文件，所以我们还需要加上一个配置项：search-paths，该配置项用于指定config server搜索哪些路径下的配置文件，需要注意的是这个路径是相对于git仓库的，并非是项目的路径。 1234567spring: cloud: config: server: git: ... search-paths: /** # 指定搜索根路径下的所有目录，若有多个路径使用逗号隔开","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://chaooo.github.io/tags/SpringCloud/"},{"name":"Config","slug":"Config","permalink":"http://chaooo.github.io/tags/Config/"}]},{"title":"「SpringCloud」Gateway网关","date":"2021-10-29T07:26:23.000Z","path":"2021/10/29/spring-cloud-gateway.html","text":"1. Gateway简介Spring Cloud Gateway 是基于 Spring5.0、SpringBoot2.0 和 Project Reactor 开发的网关，旨在提供一种简单而有效的方式来对API进行路由，基于过滤器链的方式提供：安全，监控&#x2F;埋点，和限流。 Spring Cloud Gateway 基于 Spring Boot2.x、Spring WebFlux 和 Project Reactor构建，属于异步非阻塞模型。 1.1 核心概念路由（Route）：路由是网关最基础的部分，路由信息由ID、目标URI、一组断言和一组过滤器组成。如果断言路由为真，则说明请求的 URI 和配置匹配。 断言（Predicate）：Java8 中的断言函数。Spring Cloud Gateway 中的断言函数输入类型是 Spring 5.0 框架中的 ServerWebExchange。Spring Cloud Gateway 中的断言函数允许开发者去定义匹配来自于 Http Request 中的任何信息，比如请求头和参数等。 过滤器（Filter）：使用特定工厂构建的 Spring Framework GatewayFilter 实例。过滤器将会对请求和响应进行处理。 1.2 工作流程 客户端向 Spring Cloud Gateway 发出请求。 由网关处理程序 Gateway Handler Mapping 映射确定请求与路由匹配，则将其发送到网关 Web 处理程序 Gateway Web Handler。 Web 处理程序通过指定的过滤器链将请求发送到我们实际的服务执行业务逻辑，然后返回。 过滤器被虚线分隔的原因是过滤器可以在发送代理请求之前和之后运行逻辑。执行所有 pre 过滤器逻辑，然后发出代理请求；发出代理请求后，将运行 post 过滤器逻辑。 2. Gateway网关搭建 在Spring Cloud父工程中创建module 导入依赖 1234567891011&lt;!-- 引入spring cloud gateway依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 引入SpringCloud Eureka client依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 启动类上加注解@EnableDiscoveryClient注册到Eureka 12345678910// @EnableEurekaClient: 声明一个Eureka客户端，只能注册到Eureka Server// @EnableDiscoveryClient: 声明一个可以被发现的客户端，可以是其他注册中心@EnableDiscoveryClient@SpringBootApplicationpublic class UserApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(UserApplication.class, args); &#125;&#125; 配置文件application.yml 1234567891011121314151617181920212223242526272829303132server: port: 18001# 指定当前服务的名称，这个名称会注册到注册中心spring: application: name: @artifactId@ cloud: # spring cloud gateway 路由配置方式 gateway: discovery: # 是否与服务发现组件进行结合，通过 serviceId 转发到具体的服务实例。 locator: # 默认为false，设为true便开启通过服务中心的自动根据 serviceId 创建路由的功能。 enabled: true lowerCaseServiceId: true # 将请求路径的服务名配置改成小写 routes: - id: user-server # 自定义的路由 ID，保持唯一性 uri: lb://XXXX-cloud-user # 从注册中心获取服务，且以lb(load-balance)负载均衡方式转发 predicates: - Path=/user/** # 将以/user/开头的请求转发到uri为lb://XXXX-cloud-user的地址上 - id: product-server uri: lb://XXXX-cloud-product predicates: - Path=/product/** # 将以/product/开头的请求转发到uri为lb://XXXX-cloud-product的地址上# 指定服务注册中心的地址eureka: instance: prefer-ip-address: true # 是否使用 ip 地址注册 instance-id: $&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; # ip:port client: service-url: # 设置服务注册中心地址 defaultZone: http://localhost:18000/eureka/ 3. 路由配置3.1 路由配置方式 基础URI路由配置 123456789spring: cloud: gateway: routes: - id: product-service # 自定义的路由 ID，保持唯一 uri: http://localhost:9004 # 目标服务地址 predicates: # 断言（判断条件）：Predicate接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。 - Path=/product/** 单个URI的地址的schema协议，一般为http或者https协议。 和注册中心相结合的路由配置的schema协议部分为自定义的lb:类型，表示从微服务注册中心（如Eureka）订阅服务，并且进行服务的路由。 基于代码的路由配置 123456789101112131415161718192021import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.gateway.route.RouteLocator;import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;import org.springframework.context.annotation.Bean; @SpringBootApplicationpublic class GatewayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GatewayApplication.class, args); &#125; @Bean public RouteLocator customRouteLocator(RouteLocatorBuilder builder) &#123; return builder.routes() // 参数分别为：路由 ID，断言（判断条件），目标服务地址 .route(&quot;product-service&quot;, r -&gt; r.path(&quot;/product/**&quot;).uri(&quot;http://localhost:9004&quot;)) .build(); &#125;&#125; 3.2 路由匹配规则Spring Cloud Gateway 是通过 Spring WebFlux 的 HandlerMapping 做为底层支持来匹配到转发路由，Spring Cloud Gateway 内置了很多 Predicates 工厂，这些 Predicates 工厂通过不同的 HTTP 请求参数来匹配，多个 Predicates 工厂可以组合使用。 路由谓词工厂（Route Predicate Factories） 类型路由谓词路由谓词工厂描述 时间相关AfterAfterRoutePredicateFactory在某个时间之后的请求才会被转发，如：`- After=2017-01-20T17:42:47.789-07:00[America/Denver]` BeforeBeforeRoutePredicateFactory在某个时间之前的请求才会被转发，如：`- Before=2017-01-20T17:42:47.789-07:00[America/Denver]` BetweenBetweenRoutePredicateFactory在某个时间段之间的才会被转发，如：`- Between=2017-01-20T17:42:47.789-07:00[America/Denver], 2017-01-21T17:42:47.789-07:00[America/Denver]` Cookie相关CookieCookieRoutePredicateFactory`- Cookie=chocolate, ch.p`名为chocolate的表单或者满足正则ch.p的表单才会被匹配到进行请求转发 Header相关HeaderHeaderRoutePredicateFactory`- Header=X-Request-Id, \\d+`携带参数X-Request-Id或者满足\\d+的请求头才会匹配 HostHostRoutePredicateFactory`- Host=**.somehost.org,**.anotherhost.org`当主机名为somehost.org或anotherhost.org的时候才会被转发 请求相关MethodMethodRoutePredicateFactory`- Method=GET,POST`只有GET和POST方法才会匹配转发请求 PathPathRoutePredicateFactory`- Path=/red/{segment},/blue/{segment}`当请求的路径为/red/、/blue/开头的时才会被转发 QueryQueryRoutePredicateFactory`- Query=green`只要请求中包含green参数即可 RemoteAddrRemoteAddrRoutePredicateFactory`- RemoteAddr=192.168.1.1/24`主机IP WeightWeightRoutePredicateFactory`- Weight=group1, 2`权重是按组计算的, 两个参数：group 和 weight（int） 123456789101112spring: cloud: gateway: routes: - id: predicate_route_test uri: https://example.org predicates: - Between=2017-01-20T17:42:47.789-07:00[America/Denver], 2017-01-21T17:42:47.789-07:00[America/Denver] - Method=GET,POST - Host=**.somehost.org,**.anotherhost.org - Header=X-Request-Id, \\d+ 4. 过滤器规则Spring Cloud Gateway 除了具备请求路由功能之外，也支持对请求的过滤。 Spring Cloud Gateway的Filter的生命周期：”pre”和”post”。 PRE：这种过滤器在请求被路由之前调用。我们可以利用这种过滤器实现身份认证、在集群中选择请求的微服务、记录调试信息等。 POST：这种过滤器在路由到微服务以后执行。这种过滤器可以用来为响应添加标准的HTTP Header、收集统计信息和指标、将响应从微服务发送给客户端等等。 Spring Cloud Gateway的Filter从作用范围可以分为两种：局部过滤器（GatewayFilter）和 全局过滤器（GlobalFilter）。 GatewayFilter：应用到当个路由或者一个分组的路由上。 GlobalFilter：应用到所有的路由上。 4.1 局部过滤器局部过滤器（GatewayFilter），是针对单个路由的过滤器。可以对访问的URL过滤，进行切面处理。Spring Cloud Gateway 包含许多内置的 GatewayFilter 工厂。 过滤器工厂 作用 参数 AddRequestHeader 为原始请求添加Header Header的名称及值 AddRequestParameter 为原始请求添加请求参数 参数名称及值 AddResponseHeader 为原始响应添加Header Header的名称及值 DedupeResponseHeader 剔除响应头中重复的值 需要去重的Header名称及去重策略 CircuitBreaker 为路由引入CircuitBreaker的断路器保护 CircuitBreakerCommand的名称 MapRequestHeader 将fromHeader的值更新到toHeader fromHeader名称, toHeader名称 FallbackHeaders 为fallbackUri的请求头中添加具体的异常信息 Header的名称 PrefixPath 为原始请求路径添加前缀 前缀路径 PreserveHostHeader 为请求添加一个 preserveHostHeader&#x3D;true的属 性，路由过滤器会检查该属性以 决定是否要发送原始的Host 无 RequestRateLimiter 用于对请求限流，限流算法为令 牌桶 keyResolver、 rateLimiter、 statusCode、 denyEmptyKey、 emptyKeyStatus Redirect 将原始请求重定向到指定的URL http状态码及重定向的 url RemoveRequestHeader 为原始请求删除某个Header Header名称 RemoveResponseHeader 为原始响应删除某个Header Header名称 RemoveRequestParameter 为原始请求删除请求参数 参数名称 RewritePath 重写原始的请求路径 原始路径正则表达式以 及重写后路径的正则表 达式 RewriteLocationResponseHeader 重写响应头中 Location 的值 输入四个参数：stripVersionMode、locationHeaderName、hostValue、protocolsRegex RewriteResponseHeader 重写原始响应中的某个Header Header名称，值的正 则表达式，重写后的值 SaveSession 在转发请求之前，强制执行 WebSession::save操作 无 SecureHeaders 为原始响应添加一系列起安全作 用的响应头 无，支持修改这些安全 响应头的值 SetPath 修改原始的请求路径 修改后的路径 SetRequestHeader 重置请求头的值 Header的名称及值 SetResponseHeader 修改原始响应中某个Header的值 Header名称，修改后 的值 SetStatus 修改原始响应的状态码 HTTP 状态码，可以是 数字，也可以是字符串 StripPrefix 用于截断原始请求的路径 使用数字表示要截断的 路径的数量 Retry 针对不同的响应进行重试 retries、statuses、 methods、series RequestSize 设置允许接最大请求包的大小。如果请求包大小超过设置的 值，则返回 413 Payload Too Large 请求包大小，单位为字 节，默认值为5M SetRequestHost 用指定的值替换现有的host header 指定的Host ModifyRequestBody 在转发请求之前修改原始请求体内容 修改后的请求体内容 ModifyResponseBody 修改原始响应体的内容 修改后的响应体内容 每个过滤器工厂都对应一个实体类，并且这些类的名称必须以GatewayFilterFactory结尾，这是Spring Cloud Gateway的一个约定，例如AddRequestHeader对一个的实体类为AddRequestHeaderGatewayFilterFactory。 4.1.1 限流过滤器RequestRateLimiter Spring Cloud Gateway官方提供了基于令牌桶的限流支持。 基于其内置的过滤器工厂RequestRateLimiterGatewayFilterFactory实现。 在过滤器工厂中是通过Redis和Lua脚本结合的方式进行流量控制。 首先引入Redis依赖： 123456&lt;!-- reactive redis依赖包（包含Lettuce客户端） --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis-reactive&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件 123456789101112131415161718192021spring: application: name: api-gateway-server cloud: gateway: discovery: locator: enabled: true # 开启从注册中心动态创建路由的功能，利用微服务名进行路由 lower-case-service-id: true # 微服务名称以小写形式呈现 routes: # 配置路由： 路由id，路由到微服务的uri,断言（判断条件） - id: product-service # 路由id uri: lb://service-product # 路由到微服务的uri。 lb://xxx，lb代表从注册中心获取服务列表，xxx代表需要转发的微服务的名称 predicates: # 断言（判断条件） - Path=/product-service/** filters: # 配置路由过滤器 - name: RequestRateLimiter # 使用的限流过滤器是Spring Cloud Gateway提供的 args: key-resolver: &#x27;#&#123;@pathKeyResolver&#125;&#x27; # 使用SpEL从容器中获取对象 redis-rate-limiter.replenishRate: 1 # 令牌桶每秒填充平均速率，允许用户每秒处理多少个请求 redis-rate-limiter.burstCapacity: 3 # 令牌桶的上限，令牌桶的容量，允许在一秒钟内完成的最大请求数 配置Redis中key的解析器 12345678910111213141516171819202122232425262728293031323334import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import reactor.core.publisher.Mono;import java.util.Objects;@Configurationpublic class KeyResolverConfig &#123; /** * 基于请求路径的限流 */ //@Bean public KeyResolver pathKeyResolver() &#123; return exchange -&gt; Mono.just(exchange.getRequest().getPath().toString()); &#125; /** * 基于请求参数的限流 * 请求/abc?userId=1 */ //@Bean public KeyResolver useKeyResolver() &#123; return exchange -&gt; Mono.just(Objects.requireNonNull(exchange.getRequest().getQueryParams().getFirst(&quot;userId&quot;))); &#125; /** * 基于请求IP地址的限流 */ @Bean public KeyResolver ipKeyResolver() &#123; return exchange -&gt; Mono.just(Objects.requireNonNull(exchange.getRequest().getHeaders().getFirst(&quot;x-Forwarded-For&quot;))); &#125;&#125; Spring Cloud Gateway目前提供的限流还是比较简单的，在实际开发中我们的限流策略会有很多种情况， 比如：对不同接口的限流，被限流后的友好提示，这些可以通过自定义RedisRateLimiter来实现自己的限流策略。 4.2 全局过滤器全局过滤器（GlobalFilter）作用于所有路由，Spring Cloud Gateway定义了Global Filter接口，用户可以自定义实现自己的Global Filter。 通过全局过滤器可以实现对权限的统一校验，安全性校验等功能。 过滤器工厂 描述 ForwardRoutingFilter 它会从exchange.getRequiredAttribute(GATEWAY_REQUEST_URL_ATTR);获取路由配置的URI，如果这个URI是forward模式，过滤器会将请求转发到DispatcherHandler，然后匹配到网关本的请求路径之中，原来请求的URI将被forward的URI覆盖，原始的请求URI被存储到exchange的ServerWebExchangeUtils.GATEWAY_ORIGINAL_REQUEST_URL_ATTR属性之中。 LoadBalancerClientFilter 它是用来处理负载均衡的过滤器。在网关后面的服务可以启动多个服务实例，这个过滤器就是把请求根据均衡规则路由到某台服务实例上面。它从exchange的ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR属性中获取URI，如果这个URI的scheme是“lb”，如：lb:&#x2F;&#x2F;myserivce，它会使用spring cloud 的LoadBalancerClient解析myservice服务名，获取一个服务实例的host和port，并替换原来的客户端请求。原来请求的url会存储在exchange的ServerWebExchangeUtils.GATEWAY_ORIGINAL_REQUEST_URL_ATTR属性中。这个过滤器也会从exchange中获取ServerWebExchangeUtils.GATEWAY_SCHEME_PREFIX_ATTR属性值，如果它的值也是“lb”，也会使用相同的规则路由。 NettyRoutingFilter 这是一个优先级最低的过滤器，如果从exchange的ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR获取的URL的scheme是https或http，它将使用Netty的HttpClient创建向下执行的请求代理，请求返回的结果将存储在exchange的ServerWebExchangeUtils.CLIENT_RESPONSE_ATTR属性中，过滤器链后面的过滤器可以从中获取返回的结果。（还有一个测试使用的过滤器，WebClientHttpRoutingFilter，它和NettyRoutingFilter的功能一样，但是不使用netty）。 NettyWriteResponseFilter 它的优先级是最高的，它是“post”类型的过滤器。如果在exchange中ServerWebExchangeUtils.CLIENT_RESPONSE_ATTR的属性存在HttpClientResponse，它会在所有的其它的过滤器执行完成之后运行，将响应的数据发送给网关的客户端。 RouteToRequestUrlFilter 它的作用是把浏览器的URL请求的Path路径添加到路由的URI之中，比如浏览器请求网关的URL是：http://localhost:8080/app-a/app/balance，路由的URI配置是：uri: lb://app-a，那么添加之后的路由的URI是：lb://app-a/app/balance，并将它存储在exchange的ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR属性之中。 WebsocketRoutingFilter 它是用来路由WebScoket请求，在exchange的ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR的URI中，如果scheme是ws或wss，它会使用Spring Web Socket 模块转发WebSocket请求。WebSockets可以使用路由进行负载均衡，比如：lb:ws:&#x2F;&#x2F;serviceid。 GatewayMetricsFilter 它用来统计一些网关的性能指标。需要添加spring-boot-starter-actuator的项目依赖。 在网关路由 ServerWebExchange 后，它将通过在 exchange 添加一个 gatewayAlreadyRouted 属性，从而将exchange标记为 routed 。一旦请求被标记为 routed ，其他路由过滤器将不会再次路由请求，而是直接跳过，防止重复的路由操。可以使用便捷方法将 exchange 标记为 routed ，或检查 exchange 是否是 routed。 123ServerWebExchangeUtils.isAlreadyRouted //检查是否已被路由ServerWebExchangeUtils.setAlreadyRouted //设置routed状态 4.3 自定义全局过滤器鉴权鉴权逻辑： ①当客户端第一次请求服务的时候，服务端对用户进行信息认证（登录）。 ②认证通过，将用户信息进行加密形成token，返回给客户端，作为登录凭证。 ③以后每次请求，客户端都携带认证的token。 ④服务端对token进行解密，判断是否有效。 对于验证用户是否已经登录授权的过程可以在网关层统一校验。校验的标准就是请求中是否携带token凭证以及token的正确性。 这里代码实现仅判断是否携带token凭证：TokenFilter.java 1234567891011121314151617181920212223242526272829303132333435363738394041import org.apache.commons.lang.StringUtils;import org.springframework.cloud.gateway.filter.GatewayFilterChain;import org.springframework.cloud.gateway.filter.GlobalFilter;import org.springframework.core.Ordered;import org.springframework.http.HttpStatus;import org.springframework.http.server.reactive.ServerHttpRequest;import org.springframework.stereotype.Component;import org.springframework.web.server.ServerWebExchange;import reactor.core.publisher.Mono;@Componentpublic class TokenFilter implements GlobalFilter, Ordered &#123; /** * 是否携带token凭证 * 对请求参数中的access-token进行判断 */ @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; ServerHttpRequest request = exchange.getRequest(); String token = request.getHeaders().getFirst(&quot;access-token&quot;); //如果token为空 if (StringUtils.isEmpty(token)) &#123; //设置Http的状态码 exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED); //请求结束 return exchange.getResponse().setComplete(); &#125; //如果token存在，继续执行 return chain.filter(exchange); &#125; /** * 指定过滤器的执行顺序，返回值越小，优先级越高 */ @Override public int getOrder() &#123; return 0; &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://chaooo.github.io/tags/SpringCloud/"},{"name":"Gateway","slug":"Gateway","permalink":"http://chaooo.github.io/tags/Gateway/"}]},{"title":"「SpringCloud」Eureka注册中心","date":"2021-10-26T03:11:23.000Z","path":"2021/10/26/spring-cloud-eureka.html","text":"1. Eureka简介Eureka是一种RESTful服务，主要用于AWS云中间层服务器的发现、负载平衡和故障转移。 Eureka包含两个组件：服务注册中心Eureka Server 和 服务客户端Eureka Client。 1.1 注册中心 Eureka ServerEureka Server提供注册服务，各个节点启动后，会在Eureka Server中进行注册，这样Eureka Server中的服务注册表中将会存储所有可用服务节点的信息。 Eureka Server通过Register、Get、Renew等接口提供服务的注册、发现和心跳检测等服务。 1.2 服务客户端 Eureka ClientEureka Client是一个java客户端，用于简化与Eureka Server的交互，客户端同时也具备一个内置的、使用轮询(round-robin)负载算法的负载均衡器。 在应用启动后，将会向Eureka Server发送心跳,默认周期为30秒，如果Eureka Server在多个心跳周期内没有收到某个节点的心跳，Eureka Server将会从服务注册表中把这个服务节点移除(默认90秒)。 Eureka Client分为两个角色，分别是：Service Provider（服务提供方）和Service Consumer（服务消费方）。 2. Eureka基础架构原理 Eureka Server：提供服务注册和发现，多个Eureka Server之间会同步数据，做到状态一致（最终一致性） Service Provider：服务提供方，将自身服务注册到Eureka。 Service Consumer：服务消费方，通过Eureka Server发现服务，并消费。 2.1 Eureka自我保护机制微服务在Eureka上注册后，会每30秒发送心跳包，Eureka通过心跳来判断服务时候健康，默认90s没有得到客户端的心跳，则注销该实例。 导致Eureka Server收不到心跳包的可能：一是微服务自身故障，二是微服务与Eureka之间的网络故障。通常微服务的自身的故障只会导致个别服务出现故障，而网络故障通常会导致大面积服务出现故障。 Eureka设置了一个阀值，当判断挂掉的服务的数量超过阀值（心跳失败比例在15分钟之内低于85%）时，Eureka Server认为很大程度上出现了网络故障，将不再删除心跳过期的服务，这种服务保护算法叫做Eureka Server的服务保护模式。 当网络故障恢复后，Eureka Server会退出”自我保护模式”。 Eureka还有客户端缓存功能(也就是微服务的缓存功能)。即便Eureka Server集群中所有节点都宕机失效，微服务的Provider和Consumer都能正常通信。只要Consumer不关闭，缓存始终有效，直到一个应用下的所有Provider访问都无效的时候，才会访问Eureka Server重新获取服务列表。 12345eureka: server: enable-self-preservation: false # 是否开启服务的自我保护: true开启，false关闭 eviction: interval-timer-in-ms: 60000 # 清理无效节点的频率，默认 60000 毫秒（60 秒） 3. Eureka注册中心搭建 在Spring Cloud父工程中创建module 导入依赖 12345&lt;!-- 引入SpringCloud Eureka server的依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 启动类上加注解@EnableEurekaServer 12345678// 声明当前项目为Eureka Server@EnableEurekaServer@SpringBootApplicationpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 配置文件 123456789101112server: port: 18000eureka: client: # 由于该应用为注册中心，所以设置为false，代表不向注册中心注册自己 registerWithEureka: false # 不主动发现别人 fetchRegistry: false # 声明注册中心的地址 serviceUrl: defaultZone: http://localhost:18000/eureka/ 启动注册中心 访问注册中心的监控页面http://localhost:18000 4. Eureka客户端搭建 创建项目 导入依赖 12345&lt;!-- 引入SpringCloud Eureka client的依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 启动类上加注解@EnableDiscoveryClient 123456789// @EnableEurekaClient: 声明一个Eureka客户端，只能注册到Eureka Server// @EnableDiscoveryClient: 声明一个可以被发现的客户端，可以是其他注册中心@EnableDiscoveryClient@SpringBootApplicationpublic class UserApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(UserApplication.class, args); &#125;&#125; 配置文件 12345678910111213server: port: 18001# 指定当前服务的名称，这个名称会注册到注册中心spring: application: name: @artifactId@# 指定服务注册中心的地址eureka: client: serviceUrl: defaultZone: http://localhost:18000/eureka 通过以上四步 就完成了一个 Eureka客户端的搭建，直接启动项目， 访问Eureka的注册中心http://localhost:18000查看当前服务。 5. CAP定理 分布式系统有三个指标： Consistency 一致性； Availability 可用性； Partition tolerance 分区容错性； 这三个指标不可能同时做到。这个结论就叫做 CAP 定理。 指标 描述 数据一致性 (Consistency) 也叫做数据原子性系统在执行某项操作后仍然处于一致的状态。在分布式系统中，更新操作执行成功后所有的用户都应该读到最新的值，这样的系统被认为是具有强一致性的。等同于所有节点访问同一份最新的数据副本。 优点： 数据一致，没有数据错误可能。 缺点： 相对效率降低。 服务可用性 (Availablity) 每一个操作总是能够在一定的时间内返回结果，这里需要注意的是”一定时间内”和”返回结果”。一定时间内指的是，在可以容忍的范围内返回结果，结果可以是成功或者是失败。 分区容错性 (Partition-torlerance) 在网络分区的情况下，被分隔的节点仍能正常对外提供服务(分布式集群，数据被分布存储在不同的服务器上，无论什么情况，服务器都能正常被访问) CAP由Eric Brewer在2000年PODC会议上提出。该猜想在提出两年后被证明成立，成为我们熟知的CAP定理。 一般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是成立。CAP 定理告诉我们，剩下的 C 和 A 无法同时做到。 因为可能通信失败（即出现分区容错），所以，对于分布式系统，我们只能能考虑当发生分区错误时，如何选择一致性和可用性。 需要强调的是：C 和 A 的抉择是发生在有分区问题的时候，正常情况下系统就应该有完美的数据一致性和可用性。 而根据一致性和可用性的选择不同，开源的分布式系统往往又被分为 CP 系统和 AP 系统。 当一套系统在发生分区故障后，客户端的任何请求都被卡死或者超时，但是，系统的每个节点总是会返回一致的数据，则这套系统就是 CP 系统，经典的比如 Zookeeper。 如果一套系统发生分区故障后，客户端依然可以访问系统，但是获取的数据有的是新的数据，有的还是老数据，那么这套系统就是 AP 系统，经典的比如 Eureka。 很多时候一致性和可用性并不是二选一的问题，大部分的时候，系统设计会尽可能的实现两点，在二者之间做出妥协，当强调一致性的时候，并不表示可用性是完全不可用的状态，比如，Zookeeper 只是在 master 出现问题的时候，才可能出现几十秒的不可用状态，而别的时候，都会以各种方式保证系统的可用性。而强调可用性的时候，也往往会采用一些技术手段，去保证数据最终是一致的。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://chaooo.github.io/tags/SpringCloud/"},{"name":"Eureka","slug":"Eureka","permalink":"http://chaooo.github.io/tags/Eureka/"}]},{"title":"「Spring」SpringBoot使用RocketMQ实战样例","date":"2021-06-11T14:11:23.000Z","path":"2021/06/11/spring-boot-rocketmq.html","text":"通过rocketmq-spring-boot-starter可以快速的搭建RocketMQ生产者和消费者服务。 pom.xml引入组件rocketmq-spring-boot-starter依赖 123456&lt;!-- https://mvnrepository.com/artifact/org.apache.rocketmq/rocketmq-spring-boot-starter --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 修改application.yml，添加RocketMQ相关配置 12345# 多个name-server(集群)使用英文;分割rocketmq: name-server: 192.168.2.100:9876 producer: group: test-group 发送消息与消费消息 使用RocketMQTemplate实现消息的发送;使用实现RocketMQListener接口，并添加@RocketMQMessageListener注解，声明消费主题，消费者分组等，且默认消费模式是集群消费。 1. 普通消息发送消息测试接口：http://localhost:8080/send/common 1234567891011@RestController@RequiredArgsConstructorpublic class RocketMqController &#123; private final RocketMQTemplate rocketMQTemplate; @GetMapping(&quot;send/common&quot;) public Object sendCommon() &#123; return rocketMQTemplate.syncSend(&quot;common_topic&quot;, &quot;普通消息&quot;); &#125;&#125; 普通消息监听消费 12345678910111213141516/** * `@RocketMQMessageListener`默认的消费模式是集群消费 * 在集群消费模式中，在同一个topic下，相同的ConsumerGroup只会有一个Consumer收到消息。 * RocketMQListener&lt;T&gt; 泛型必须和接收的消息类型相同，这里是String */@Slf4j@Component@RocketMQMessageListener( topic = &quot;common_topic&quot;, consumerGroup = &quot;test_group&quot;)public class CommonListener implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; log.info(&quot;&#123;&#125;收到消息：&#123;&#125;&quot;, this.getClass().getSimpleName(), message); &#125;&#125; 2. 带Tag的消息发送消息测试接口：http://localhost:8080/send/tag 1234@GetMapping(&quot;send/tag&quot;)public Object sendWithTag() &#123; return rocketMQTemplate.syncSend(&quot;tag_topic&quot;+ &quot;:&quot; + &quot;tag&quot;, &quot;tag消息,tag:tag&quot;);&#125; 监听消费 1234567891011121314151617/** * 如果我们的消费者指定了消费的Tag后，发送的消息如果不带tag，将会消费不到； * 如果我们的生产者指定了Tag，但是消费者的selectorExpression没有设置，即用默认的“*”,那么这个消费者也会消费到。 */@Slf4j@Component@RocketMQMessageListener( topic = &quot;tag_topic&quot;, selectorType = SelectorType.TAG, selectorExpression = &quot;tag&quot;,//指定了tag后，发送的消息如果不带tag，将会消费不到 consumerGroup = &quot;tag_group&quot;)public class TagMsgListener implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; log.info(&quot;&#123;&#125;收到消息：&#123;&#125;&quot;, this.getClass().getSimpleName(), message); &#125;&#125; 3. 消费模式为广播消费发送消息测试接口：http://localhost:8080/send/broadcast 1234@GetMapping(&quot;send/broadcast&quot;)public Object sendWithManyTag() &#123; return rocketMQTemplate.syncSend(&quot;broadcast_topic&quot;, &quot;广播消息&quot;);&#125; 监听消费 123456789101112131415/** * 通过messageModel = MessageModel.BROADCASTING 指定消费模式为广播消费。(默认集群模式) */@Slf4j@Component@RocketMQMessageListener( topic = &quot;broadcast_topic&quot;, messageModel = MessageModel.BROADCASTING,//指定为广播消费 consumerGroup = &quot;broadcast_group&quot;)public class BroadcastListener implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; log.info(&quot;&#123;&#125;收到消息：&#123;&#125;&quot;, this.getClass().getSimpleName(), message); &#125;&#125; 4. 顺序发送的消息，随机消费发送消息测试接口：http://localhost:8080/send/random 123456789@GetMapping(&quot;send/random&quot;)public Object sendRandom() &#123; List&lt;SendResult&gt; results = new ArrayList&lt;&gt;(); for (int i = 0; i &lt;= 3; i++) &#123; SendResult sendResult = rocketMQTemplate.syncSend(&quot;random_topic&quot;, &quot;无序消息&quot; + i); results.add(sendResult); &#125; return results;&#125; 监听消费 123456789101112131415/** * 顺序发送的消息，消费顺序不一定是按照我们发送的顺序来消费的。 */@Slf4j@Component@RocketMQMessageListener( topic = &quot;random_topic&quot;, consumerGroup = &quot;random_group&quot;)public class RandomListener implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; //发送消息是顺序发送的0,1,2,3,消费的顺序不一定是顺序的 log.info(&quot;&#123;&#125;收到消息：&#123;&#125;&quot;, this.getClass().getSimpleName(), message); &#125;&#125; 5. 顺序消费发送消息测试接口：http://localhost:8080/send/order 123456789@GetMapping(&quot;send/order&quot;)public Object sendOrder() &#123; List&lt;SendResult&gt; results = new ArrayList&lt;&gt;(); for (int i = 0; i &lt;= 3; i++) &#123; SendResult sendResult = rocketMQTemplate.syncSendOrderly(&quot;order_topic&quot;, &quot;有序消息&quot; + i, &quot;hashkey&quot;); results.add(sendResult); &#125; return results;&#125; 监听消费 12345678910111213141516/** * 通过设置属性consumeMode = ConsumeMode.ORDERLY，指定消费模式为顺序消费，消费的顺序也和发送顺序一致 */@Slf4j@Component@RocketMQMessageListener( topic = &quot;order_topic&quot;, consumeMode = ConsumeMode.ORDERLY,//指定为顺序消费 consumerGroup = &quot;order_group&quot;)public class OrderListener implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; //发送消息是顺序发送的0,1,2,3,消费的顺序也是顺序的 log.info(&quot;&#123;&#125;收到消息：&#123;&#125;&quot;, this.getClass().getSimpleName(), message); &#125;&#125; 6. 异步消息producer向broker发送消息时指定消息发送成功及发送异常的回调方法，调用API后立即返回，producer发送消息线程不阻塞 ，消息发送成功或失败的回调任务在一个新的线程中执行。 发送消息测试接口：http://localhost:8080/send/async 123456789101112131415@GetMapping(&quot;send/async&quot;)public Object sendAsync() &#123; rocketMQTemplate.asyncSend(&quot;async_topic&quot;, &quot;异步消息&quot;, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; log.info(&quot;发送成功:&#123;&#125;&quot;, JSON.toJSONString(sendResult)); //可以处理相应的业务 &#125; @Override public void onException(Throwable throwable) &#123; //可以处理相应的业务 &#125; &#125;); return null;&#125; 监听消费 1234567891011@Slf4j@Component@RocketMQMessageListener( topic = &quot;async_topic&quot;, consumerGroup = &quot;async_group&quot;)public class AsyncListener implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; log.info(&quot;&#123;&#125;收到消息：&#123;&#125;&quot;, this.getClass().getSimpleName(), message); &#125;&#125; 7. 单向发送消息单向发送消息这种方式主要用在不特别关心发送结果的场景，例如日志发送。 发送消息测试接口：http://localhost:8080/send/oneway 1234@GetMapping(&quot;send/oneway&quot;)public void sendOneway() &#123; rocketMQTemplate.sendOneWay(&quot;oneway_topic&quot;, &quot;单向消息&quot;);&#125; 监听消费 1234567891011@Slf4j@Component@RocketMQMessageListener( topic = &quot;oneway_topic&quot;, consumerGroup = &quot;oneway_group&quot;)public class OnewayListener implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; log.info(&quot;&#123;&#125;收到消息：&#123;&#125;&quot;, this.getClass().getSimpleName(), message); &#125;&#125; 8. 延时消息发送消息测试接口：http://localhost:8080/send/delay 123456789@GetMapping(&quot;send/delay&quot;)public Object sendDelay() &#123; // 延时消息的使用限制messageDelayLevel:&quot;1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h&quot; // 开源版RocketMQ并不支持任意时间的延时，需要设置几个固定的延时等级，从1s到2h分别对应着等级1到18 消息消费失败会进入延时消息队列 String txt = &quot;延时消息:&quot; + LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;)); Message&lt;String&gt; message = MessageBuilder.withPayload(txt).build(); // 延时等级取4，延时30s return rocketMQTemplate.syncSend(&quot;delay_topic&quot;, message, 2000, 4);&#125; 监听消费 1234567891011@Slf4j@Component@RocketMQMessageListener( topic = &quot;delay_topic&quot;, consumerGroup = &quot;delay_group&quot;)public class DelayListener implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; log.info(&quot;&#123;&#125;于&#123;&#125;收到消息：&#123;&#125;&quot;, this.getClass().getSimpleName(), LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;)), message); &#125;&#125; 9. 事务消息（半消息） 事务消息共有三种状态，提交状态、回滚状态、中间状态： TransactionStatus.CommitTransaction: 提交事务，它允许消费者消费此消息。 TransactionStatus.RollbackTransaction: 回滚事务，它代表该消息将被删除，不允许被消费。 TransactionStatus.Unknown: 中间状态，它代表需要检查消息队列来确定状态。 事务消息仅仅只是保证本地事务和MQ消息发送形成整体的 原子性，而投递到MQ服务器后，并无法保证消费者一定能消费成功。 发送消息测试接口：http://localhost:8080/send/tx 12345678910@GetMapping(&quot;send/tx&quot;)public Object sendTransaction() &#123; int i = new Random().nextInt(1000); Map&lt;String, String&gt; txtMap = new HashMap&lt;&gt;(2); txtMap.put(&quot;key&quot;, &quot;key&quot; + i); txtMap.put(&quot;name&quot;, &quot;事务消息&quot;); txtMap.put(&quot;desc&quot;, &quot;事务消息&quot; + i); Message&lt;Map&lt;String, String&gt;&gt; message = MessageBuilder.withPayload(txtMap).setHeader(&quot;key&quot;, txtMap.get(&quot;key&quot;)).build(); return rocketMQTemplate.sendMessageInTransaction(&quot;tx_topic&quot;, message , i);&#125; 生产者端需要实现RocketMQLocalTransactionListener接口，重写执行本地事务的方法和检查本地事务方法；@RocketMQTransactionListener注解表明这个一个生产端的消息监听器，需要配置监听的事务消息生产者组。 12345678910111213141516171819202122232425262728293031323334353637383940414243@Slf4j@Service@RocketMQTransactionListenerpublic class TxProducerListener implements RocketMQLocalTransactionListener &#123; /** * 每次推送消息会执行executeLocalTransaction方法，首先会发送半消息，到这里的时候是执行具体本地业务， * 执行成功后手动返回RocketMQLocalTransactionState.COMMIT状态， * 这里是保证本地事务执行成功，如果本地事务执行失败则可以返回ROLLBACK进行消息回滚。 * 此时消息只是被保存到broker，并没有发送到topic中，broker会根据本地返回的状态来决定消息的处理方式。 */ @Override public RocketMQLocalTransactionState executeLocalTransaction(Message message, Object arg) &#123; log.info(&quot;开始执行本地事务&quot;); RocketMQLocalTransactionState state; try&#123; Integer i = (Integer) arg; if (i % 2 == 0) &#123; // 偶数抛出异常 int a = i / 0; &#125; // COMMIT:即生产者通知Rocket该消息可以消费 state = RocketMQLocalTransactionState.COMMIT; log.info(&quot;本地事务已提交。&#123;&#125;&quot;,message.getHeaders().get(&quot;key&quot;).toString()); &#125;catch (Exception e)&#123; log.info(&quot;执行本地事务失败。&#123;&#125;&quot;,e); // ROLLBACK:即生产者通知Rocket将该消息删除 state = RocketMQLocalTransactionState.ROLLBACK; &#125; return state; &#125; @Override public RocketMQLocalTransactionState checkLocalTransaction(Message msg) &#123; log.info(&quot;开始执行回查&quot;); // 判断具体业务逻辑，来决定是否回滚还是提交 boolean flag = false; if ( flag ) &#123; log.info(&quot;回滚半消息&quot;); return RocketMQLocalTransactionState.ROLLBACK; &#125; log.info(&quot;提交半消息&quot;); return RocketMQLocalTransactionState.COMMIT; &#125;&#125; 监听消费 12345678910111213141516/** * 事务消息只是保证了本地事务和消息发送的原子性， * 如果 消费端消费失败 后的处理方式，建议是记录异常信息然后 人工处理 ， * 并不建议回滚上游服务的数据(因为两者是 解耦 的，而且 复杂度 太高) */@Slf4j@Component@RocketMQMessageListener( topic = &quot;tx_topic&quot;, consumerGroup = &quot;tx_group&quot;)public class TxConsumerListener implements RocketMQListener&lt;Map&lt;String, String&gt;&gt; &#123; @Override public void onMessage(Map&lt;String, String&gt; message) &#123; log.info(&quot;&#123;&#125;收到消息：&#123;&#125;&quot;, this.getClass().getSimpleName(), message); &#125;&#125; 10. 部分测试日志打印12345678910111213141516171819202122232021-06-11 17:25:02.861 INFO 13904 --- [MessageThread_3] com.demo.CommonListener : CommonListener收到消息：普通消息2021-06-11 17:25:10.296 INFO 13904 --- [MessageThread_2] com.demo.TagMsgListener : TagMsgListener收到消息：tag消息,tag:tag2021-06-11 17:26:05.070 INFO 13904 --- [MessageThread_1] com.demo.BroadcastListener : BroadcastListener收到消息：广播消息2021-06-11 17:27:19.969 INFO 13904 --- [MessageThread_2] com.demo.RandomListener : RandomListener收到消息：无序消息12021-06-11 17:27:19.969 INFO 13904 --- [MessageThread_1] com.demo.RandomListener : RandomListener收到消息：无序消息02021-06-11 17:27:19.970 INFO 13904 --- [MessageThread_4] com.demo.RandomListener : RandomListener收到消息：无序消息32021-06-11 17:27:19.970 INFO 13904 --- [MessageThread_3] com.demo.RandomListener : RandomListener收到消息：无序消息22021-06-11 17:28:15.530 INFO 13904 --- [MessageThread_2] com.demo.OrderListener : OrderListener收到消息：有序消息02021-06-11 17:28:15.531 INFO 13904 --- [MessageThread_3] com.demo.OrderListener : OrderListener收到消息：有序消息12021-06-11 17:28:15.533 INFO 13904 --- [MessageThread_4] com.demo.OrderListener : OrderListener收到消息：有序消息22021-06-11 17:28:15.540 INFO 13904 --- [MessageThread_5] com.demo.OrderListener : OrderListener收到消息：有序消息32021-06-11 17:29:24.630 INFO 13904 --- [MessageThread_1] com.demo.AsyncListener : AsyncListener收到消息：异步消息2021-06-11 17:29:24.644 INFO 13904 --- [ublicExecutor_1] o.example.controller.RocketMqController : 发送成功:&#123;&quot;messageQueue&quot;:&#123;&quot;brokerName&quot;:&quot;localhost.localdomain&quot;,&quot;queueId&quot;:0,&quot;topic&quot;:&quot;async_topic&quot;&#125;,&quot;msgId&quot;:&quot;7F000001365018B4AAC237405B920066&quot;,&quot;offsetMsgId&quot;:&quot;C0A8026400002A9F000000000004FF02&quot;,&quot;queueOffset&quot;:0,&quot;regionId&quot;:&quot;DefaultRegion&quot;,&quot;sendStatus&quot;:&quot;SEND_OK&quot;,&quot;traceOn&quot;:true&#125;2021-06-11 17:30:12.790 INFO 13904 --- [MessageThread_1] com.demo.OnewayListener : OnewayListener收到消息：单向消息2021-06-11 17:31:06.185 INFO 13904 --- [MessageThread_1] com.demo.DelayListener : DelayListener于2021-06-11 17:31:06收到消息：延时消息:2021-06-11 17:30:362021-06-11 17:31:22.216 INFO 13904 --- [nio-8080-exec-5] com.demo.TxProducerListener : 开始执行本地事务2021-06-11 17:31:22.217 INFO 13904 --- [nio-8080-exec-5] com.demo.TxProducerListener : 本地事务已提交。key52021-06-11 17:31:22.233 INFO 13904 --- [MessageThread_3] com.demo.TxConsumerListener : TxConsumerListener收到消息：&#123;name=事务消息, key=key5, desc=事务消息5&#125;2021-06-11 17:31:27.872 INFO 13904 --- [nio-8080-exec-4] com.demo.TxProducerListener : 开始执行本地事务2021-06-11 17:31:27.880 INFO 13904 --- [nio-8080-exec-4] com.demo.TxProducerListener : 执行本地事务失败。&#123;&#125;java.lang.ArithmeticException: / by zero at com.demo.TxProducerListener.executeLocalTransaction(TxProducerListener.java:42) ~[classes/:na]... 源码地址：https://gitee.com/chaoo/mq-test.git","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://chaooo.github.io/tags/RocketMQ/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"}]},{"title":"「环境配置」RocketMQ安装并整合SpringBoot","date":"2021-06-10T10:11:23.000Z","path":"2021/06/10/env-mq-rocketmq.html","text":"RocketMQ 是阿里巴巴团队使用Java语言开发的一个分布式、队列模型的消息中间件，后开源给Apache基金会成为了Apache的顶级开源项目，具有高性能、高可靠、高实时、分布式特点。 RocketMQ 主要由Producer、Broker、Consumer、NameServer组成；其中Producer负责生产消息；Consumer负责消费消息；Broker是MQ服务,负责接收、分发消息；NameServer是路由中心，负责MQ服务之间的协调。 1. Centos安装RocketMQ 官网下载RocketMQ安装包 1234# 进入自定义软件安装目录cd /mnt/newdatadrive/apps# wget下载RocketMQ安装包wget -c &quot;https://mirrors.bfsu.edu.cn/apache/rocketmq/4.8.0/rocketmq-all-4.8.0-bin-release.zip&quot; 解压安装（环境基于JDK1.8或以上） 123456# 解压unzip rocketmq-all-4.8.0-bin-release.zip# 重命名为rocketmqmv rocketmq-all-4.8.0-bin-release rocketmq# 进入安装目录cd rocketmq 修改配置 1234567# RocketMQ的默认内存占用非常高，是4×4g的，将4g调整为512m# 编辑runserver.shvim bin/runserver.shJAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms512m -Xmx512m -Xmn256m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot;# 编辑runbroker.shvim bin/runbroker.shJAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms512m -Xmx512m -Xmn256m&quot; 配置RocketMQ的环境变量 1234567# 编辑/etc/profilevim /etc/profile# 添加：export ROCKETMQ_HOME=/mnt/newdatadrive/apps/rocketmqexport PATH=$JAVA_HOME/bin:$ROCKETMQ_HOME/bin:$PATH# 使rocketmq的配置生效source /etc/profile 启动RockerMQ顺序 123# 先启动 NameServer，然后启动 Brokernohup sh bin/mqnamesrv &amp;nohup sh bin/mqbroker -n localhost:9876 &amp; 关闭RockerMQ顺序 123# 先关闭Broker，再关闭NameServersh bin/mqshutdown brokersh bin/mqshutdown namesrv 启动日志 1234567891011# 查看 Name Server 启动日志tail -f ~/logs/rocketmqlogs/namesrv.log# 查看 Broker Server 启动日志tail -f ~/logs/rocketmqlogs/broker.log# 若出现如下报错file doesn&#x27;t exist on this path: /root/store/commitlogfile doesn&#x27;t exist on this path: /root/store/consumequeue# 对应创建即可：cd ~/storemkdir commitlog consumequeue 防火墙 若外网IP调试，关闭防火墙 或 开放防火墙端口9876,10911 123456# NameServer默认端口：9876firewall-cmd --add-port=9876/tcp --permanent# Broker对外服务的监听端口firewall-cmd --add-port=10911/tcp --permanent# 更新防火墙规则firewall-cmd --reload 2. SpringBoot 整合 RocketMQ pom.xml引入组件rocketmq-spring-boot-starter依赖 123456&lt;!-- https://mvnrepository.com/artifact/org.apache.rocketmq/rocketmq-spring-boot-starter --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 修改application.yml，添加RocketMQ相关配置 12345# 多个name-server(集群)使用英文;分割rocketmq: name-server: 192.168.2.100:9876 producer: group: test-group 消息生产者 123456789101112131415@Componentpublic class MessageProducer &#123; @Resource private RocketMQTemplate rocketMQTemplate; /** * 生产者发送消息 * @param topic 主题 * @param message 消息体 */ public void sendMessage(String topic, String message)&#123; this.rocketMQTemplate.convertAndSend(topic, message); &#125;&#125; 消费者 12345678910111213141516/** * 消费者监听消息 */@Slf4j@Component@RocketMQMessageListener( topic = &quot;test-topic&quot;, // 指定topic consumerGroup = &quot;test-group&quot;, // 指定消费组 selectorExpression = &quot;*&quot;)public class MessageConsumer implements RocketMQListener&lt;String&gt; &#123; @Override public void onMessage(String message) &#123; log.info(&quot;收到的消息是: &#123;&#125;&quot;, message); &#125;&#125; 测试类 1234567891011121314@Slf4j@SpringBootTestclass DemoApplicationTests &#123; @Autowired private MessageProducer messageProducer; @Test void testMQ() &#123; String message = &quot;Hello RocketMQ!&quot;; messageProducer.sendMessage(&quot;test-topic&quot;,message); log.info(&quot;生产一条消息：&quot; + message); &#125;&#125; 运行结果12345672021-06-10 14:56:25.180 INFO 17720 --- [ main] a.r.s.s.DefaultRocketMQListenerContainer : running container: DefaultRocketMQListenerContainer&#123;consumerGroup=&#x27;test-group&#x27;, nameServer=&#x27;192.168.2.100:9876&#x27;, topic=&#x27;test-topic&#x27;, consumeMode=CONCURRENTLY, selectorType=TAG, selectorExpression=&#x27;*&#x27;, messageModel=CLUSTERING&#125;2021-06-10 14:56:25.188 INFO 17720 --- [ main] o.a.r.s.a.ListenerContainerConfiguration : Register the listener to container, listenerBeanName:messageConsumer, containerBeanName:org.apache.rocketmq.spring.support.DefaultRocketMQListenerContainer_12021-06-10 14:56:25.230 INFO 17720 --- [ main] c.e.fastdfsdemo.DemoApplicationTests : Started DemoApplicationTests in 10.371 seconds (JVM running for 13.888)2021-06-10 14:56:26.410 INFO 17720 --- [ main] c.e.fastdfsdemo.DemoApplicationTests : 生产一条消息：Hello RocketMQ!2021-06-10 14:56:26.426 INFO 17720 --- [MessageThread_1] c.e.f.rocketmq.MessageConsumer : 收到的消息是: Hello RocketMQ!2021-06-10 14:56:26.496 INFO 17720 --- [lientSelector_1] RocketmqRemoting : closeChannel: close the connection to remote address[192.168.2.100:10911] result: true2021-06-10 14:56:26.496 INFO 17720 --- [lientSelector_1] RocketmqRemoting : closeChannel: close the connection to remote address[192.168.2.100:9876] result: true","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"},{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://chaooo.github.io/tags/RocketMQ/"}]},{"title":"「环境配置」记一次生产事故引发的FastDFS图片迁移","date":"2021-05-28T14:11:23.000Z","path":"2021/05/28/env-centos8-fastdfs-rsync.html","text":"事件经过：由于前端同学不小心把上传图片服务器地址写死了测试域名（指向测试服务器），然后项目上到正式环境，一段时间后，发现用户发布商品时的商品详情富文本中的图片全部指向测试图片服务器域名，然后图片又太多了，手动逐条修复数据不太现实。 解决思路： 从数据库中查询商品详情富文本，分析出所有测试域名图片的Url地址； 通过Url下载图片到本地服务器并保持图片存放路径与图片文件名和原本一致； 通过rsync远程同步命令同步到正式服务器； 修改数据库中商品图片Url地址指向正式服务器域名(因为路径和文件名与测试服务器一致，只需替换域名即可)。 引入pom.xml依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;/groupId&gt; &lt;artifactId&gt;jsoup&lt;/artifactId&gt; &lt;version&gt;1.11.2&lt;/version&gt;&lt;/dependency&gt; 提供出临时接口以备调用同步图片 12345678910/** * 提供出临时接口以备调用修复 */@PostMapping(&quot;/product/img/repair&quot;)public void repairProductImg() &#123; // 获取带有测试域名图片的富文本列表 List&lt;String&gt; infoList = productDao.getProductInfo(); // 下载图片到本地服务器 infoList.forEach(RepairImgUtil::saveProductImg);&#125; 1234&lt;!-- 这里测试图片服务器域名为：img-test.abc.com --&gt;&lt;select id=&quot;getProductInfo&quot; resultType=&quot;java.lang.String&quot;&gt; select detail from productInfo where detail like &#x27;%img-test.abc.com%&#x27;&lt;/select&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Slf4jpublic class RepairImgUtil &#123; /** * 保存图片到本地服务器 * @param textBody 富文本 */ public static void saveProductImg(String textBody) &#123; // 解析富文本 Element doc = Jsoup.parseBodyFragment(textBody).body(); Elements images = doc.select(&quot;img[src]&quot;); List&lt;String&gt; srcList = new ArrayList&lt;&gt;(); for (Element element : images) &#123; String imgUrl = element.attr(&quot;src&quot;); // 筛选测试服务器的图片路径 if (imgUrl.indexOf(&quot;img-test.abc.com&quot;) &gt; 0) &#123; srcList.add(imgUrl); &#125; &#125; // 本地存放路径 String basePath = &quot;/apps/fdfs/storage/data/&quot;; srcList.forEach(img -&gt; &#123; try &#123; // 下载图片，并根据路径规律保持原有路径 RepairImgUtil.downloadImage(img, img.substring(45), basePath+img.substring(39, 45)); &#125; catch (Exception e) &#123; log.error(&quot;try-catch:&quot;,e); &#125; &#125;); &#125; /** * 下载图片 * * @param urlString 图片链接 * @param filename 图片名称 * @param savePath 保存路径 */ private static void downloadImage(String urlString, String filename, String savePath) throws Exception &#123; // 构造URL打开连接，并设置输入流与缓冲 URL url = new URL(urlString); URLConnection con = url.openConnection(); con.setConnectTimeout(5*1000); InputStream is = con.getInputStream(); byte[] bs = new byte[1024]; // 读取到的数据长度 int len; // 输出的文件流 File sf=new File(savePath); if(!sf.exists())&#123; sf.mkdirs(); &#125; OutputStream os = new FileOutputStream(sf.getPath()+&quot;/&quot;+filename); // 开始读取 while ((len = is.read(bs)) != -1) &#123; os.write(bs, 0, len); &#125; os.close(); is.close(); // 打印图片链接 log.info(urlString); &#125;&#125; 调用接口后，图片会先备份到本地服务器，通过rsync远程同步命令同步到正式服务器 12345678910111213141516171819202122232425# 登录远程服务器，进入图片服务器目录cd /apps/fdfs# 远程登陆下载图片的服务器并同步数据rsync -avz myuser@119.29.36.15:/apps/fdfs/storage ./# 根据提示，确认连接输入 yes，输入本地服务器（下载图片的服务器）用户密码后开始同步The authenticity of host &#x27;119.29.36.15 (119.29.36.15)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:5m4KgPF0QgBO1xE7Tz1RT7U/tfCue+QBE/t4zEDEDJQ.Are you sure you want to continue connecting (yes/no/[fingerprint])? yesWarning: Permanently added &#x27;119.29.36.15&#x27; (ECDSA) to the list of known hosts.myuser@119.29.36.15&#x27;s password: receiving incremental file liststorage/storage/data/storage/data/03/storage/data/03/08/storage/data/03/08/Cmgy61-BU3GANDoCAAGOnj-x-ws715.jpgstorage/data/03/08/Cmgy61-BU8SAeTvqAAGOnj-x-ws868.jpgstorage/data/03/08/Cmgy61-BVn-AdXoKAA2Msh3VVsk076.jpg...storage/data/03/3C/Cmgy62CwSTOARHdWABLV4kLKua4925.pngsent 15,786 bytes received 422,224,711 bytes 10,425,691.28 bytes/sectotal size is 448,804,964 speedup is 1.06 把测试域名换成正式域名访问图片成功！最后修改数据库商品详情：1update productInfo set detail = REPLACE(detail, &#x27;img-test.abc.com%&#x27;&#x27;, &#x27;img.abc.com%&#x27;&#x27;) where detail like like &#x27;%img-test.abc.com%&#x27;","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"},{"name":"FastDFS","slug":"FastDFS","permalink":"http://chaooo.github.io/tags/FastDFS/"}]},{"title":"「Spring」SpringBoot Thymeleaf页面静态化实现","date":"2021-05-11T06:12:42.000Z","path":"2021/05/11/spring-boot-html.html","text":"页面静态化是指把动态生成的HTML页面变为静态文件保存，当请求到来，直接访问静态文件，而不需要经过项目服务器的渲染。 1. 配置Nginx代理静态页面1234567891011location / &#123; root D:/temp/static; # 自定义静态文件存放根目录 set $www_temp_path $request_filename; # 设置请求的文件名到临时变量 if ($uri = &#x27;/&#x27;) &#123; # 若为根目录则加上/index.html set $www_temp_path $request_filename/index.html; &#125; if (!-f $www_temp_path) &#123; # 若请求的文件不存在，就反向代理服务器的渲染 proxy_pass http://127.0.0.1:8080; &#125; # 其他配置...&#125; 然后重启Nginx。 2. Thymeleaf实现手动把模板渲染结果写入到指定位置Thymeleaf手动渲染原理：当与SpringBoot结合时，放入Model的数据就会被放到上下文(Context)中，并且此时模板解析器(TemplateResolver)已经创建完成(默认模板存放位置:templates，默认模板文件类型:html)；然后通过模板引擎(TemplateEngine)结合上下文(Context)与模板解析器(TemplateResolver)，利用内置的语法规则解析，从而输出解析后的文件到指定目录。 12345678/** * 模板引擎处理函数 * * @param templateName 模板名 * @param context 上下文 * @param writer 输出目的地的流 */templateEngine.process(templateName, context, writer); 具体实现 相关依赖： 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件： 12345server: port: 8080# 自定义静态文件存放根目录myserver: destPath: D:/temp/static 接口和实现类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@RestControllerpublic class GenerateApi &#123; @Resource private GenerateHtml generateHtml; /** * 生成静态首页 */ @GetMapping(&quot;/generate/home&quot;) public String generateStaticHome(ServerWebExchange exchange) &#123; return generateHtml.generateStaticHome(exchange); &#125;&#125;public interface GenerateHtml &#123; String generateStaticHome(ServerWebExchange exchange);&#125;@Slf4j@Servicepublic class GenerateHtmlImpl implements GenerateHtml &#123; /** 打包时间 */ @Value(&quot;$&#123;spring.application.build-time&#125;&quot;) private String buildTime; /** 静态文件存放根目录 */ @Value(&quot;$&#123;myserver.destPath&#125;&quot;) private String destPath; /** 模板引擎 */ @Resource private TemplateEngine templateEngine; /** * 生成静态首页 */ @Override public String generateStaticHome(ServerWebExchange exchange) &#123; // 上下文 SpringWebFluxContext context = new SpringWebFluxContext(exchange); // 设置页面数据 Map&lt;String, Object&gt; modelMap = new HashMap&lt;&gt;(); modelMap.put(&quot;name&quot;, &quot;testGenerateStaticHome&quot;); modelMap.put(&quot;version&quot;, buildTime); context.setVariables(modelMap); // 输出流 File dest = new File(destPath, &quot;index.html&quot;); if (dest.exists()) &#123; boolean delete = dest.delete(); &#125; try (PrintWriter writer = new PrintWriter(dest, &quot;UTF-8&quot;)) &#123; // 模板引擎生成html templateEngine.process(&quot;index&quot;, context, writer); return &quot;[静态页服务]：生成静态首页成功! ^_^&quot;; &#125; catch (Exception e) &#123; log.error(&quot;[静态页服务]：生成静态首页异常!&quot;, e); return &quot;[静态页服务]：生成静态首页异常!&quot;+e.toString(); &#125; &#125;&#125; html模板原型： 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Thymeleaf 静态页面&lt;/title&gt; &lt;link th:href=&quot;@&#123;/css/index.css(v=$&#123;version&#125;)&#125;&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;h1 th:text=&quot;&#x27;页面名字：&#x27; + $&#123;name&#125;&quot;&gt;&lt;/h1&gt; &lt;h1 th:text=&quot;&#x27;页面版本：&#x27; + $&#123;version&#125;&quot;&gt;&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 启动测试：浏览器访问：http://127.0.0.1:8080/generate/home或编写测试类测试。 3. 将项目resources目录下静态资源复制到指定位置相关依赖： 12345&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.8.0&lt;/version&gt;&lt;/dependency&gt; 接口和实现类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public interface GenerateHtml &#123; /** * 拷贝静态资源文件 */ String generateStaticFiles();&#125;@Slf4j@Servicepublic class GenerateHtmlImpl implements GenerateHtml &#123; /** 静态文件存放根目录 */ @Value(&quot;$&#123;myserver.destPath&#125;&quot;) private String destPath; /** * 拷贝静态资源文件 */ @Override public String generateStaticFiles() &#123; try &#123; copyResourceToFile(); return &quot;[静态页服务]：拷贝静态资源文件成功! ^_^&quot;; &#125; catch (Exception e) &#123; log.error(&quot;[静态页服务]：拷贝静态资源文件异常!&quot;, e); return &quot;[静态页服务]：拷贝静态资源文件异常!&quot;+e.toString(); &#125; &#125; /** * 资源清单获取并拷贝 */ private void copyResourceToFile() throws IOException &#123; // 资源清单获取 ResourcePatternResolver resolver = new PathMatchingResourcePatternResolver(); org.springframework.core.io.Resource[] resources = resolver.getResources(&quot;static/**&quot;); for (org.springframework.core.io.Resource resource : resources) &#123; String fileName = resource.getFilename(); assert fileName != null; if (fileName.indexOf(&quot;.&quot;) &gt; 0) &#123; InputStream inputStream = null; try &#123; inputStream = resource.getInputStream(); &#125; catch (Exception e) &#123; log.warn(String.format(&quot;[%s]获取输入流发生异常!&quot;, resource.getURL())); throw new RuntimeException(String.format(&quot;[%s]获取输入流发生异常!&quot;, resource.getURL())); &#125; // 分析相对目录 String tempPath = &quot;&quot;; String[] urls = resource.getURL().toString().split(&quot;/static/&quot;); if (urls.length &gt;= 2 )&#123; tempPath = urls[urls.length-1]; &#125; else &#123; throw new RuntimeException(&quot;relativeRootPath有误：无法分析相对目录&quot;); &#125; tempPath = tempPath.substring(0, tempPath.length() - fileName.length()); if (StringUtils.isEmpty(tempPath)) &#123; tempPath = File.separator; &#125; String filePath = destPath + File.separator + tempPath; if (createDir(filePath)) &#123; String destName = filePath + fileName; // 输出流 File dest = new File(destName); if (dest.exists()) &#123; boolean delete = dest.delete(); &#125; FileUtils.copyInputStreamToFile(inputStream, dest); &#125; else &#123; throw new RuntimeException(String.format(&quot;创建本地目录[%s]失败！&quot;, resource.getURL())); &#125; &#125; &#125; &#125; /** * 创建目录 */ private static boolean createDir(String dirName) &#123; File dir = new File(dirName); if (dir.exists()) &#123; return true; &#125; if (!dirName.endsWith(File.separator)) &#123; dirName = dirName + File.separator; &#125; if (dir.mkdirs()) &#123; log.warn(&quot;创建目录&quot; + dirName + &quot;成功！&quot;); return true; &#125; else &#123; log.warn(&quot;创建目录&quot; + dirName + &quot;失败！&quot;); return false; &#125; &#125;&#125; 可在项目启动后执行覆盖拷贝操作： 1234567891011121314@Slf4j@Component@Order(value = 10)public class MyCommandLineRunner implements CommandLineRunner &#123; @Resource private GenerateHtml generateHtml; @Override public void run(String... args) throws Exception &#123; log.info(&quot;执行MyCommandLineRunner:拷贝静态文件！&quot;); // 拷贝静态资源文件 generateHtml.generateStaticFiles(); &#125;&#125; 重启测试：发现指定目录生成了静态文件，并且请求速度得到了极大提升。 生成静态文件： 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Thymeleaf 静态页面&lt;/title&gt; &lt;link href=&quot;/css/index.css?v=20210511-075911&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;页面名字：testGenerateStaticHome&lt;/h1&gt; &lt;h1&gt;页面版本：20210511-075911&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"},{"name":"Thymeleaf","slug":"Thymeleaf","permalink":"http://chaooo.github.io/tags/Thymeleaf/"}]},{"title":"「Redis」Redis故障处理-持久化时内存不足","date":"2021-05-06T08:01:23.000Z","path":"2021/05/06/redis-error.html","text":"问题描述1234567# Java错误日志:redis.clients.jedis.exceptions.JedisDataException: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.# Redis错误日志:Can&#x27;t save in background: fork: Resource temporaily unavailable# 或Can’t save in background: fork: Cannot allocate memory Redis在个默认情况下，如果在RDB snapshots持久化过程中出现问题，Redis不允许用户进行任何更新操作；即：stop-writes-on-bgsave-error yes。 临时解决方案是通过命令：config set stop-writes-on-bgsave-error no 设置这个选项为false，让程序忽略了这个异常，使得程序能够继续往下运行，但写硬盘仍然是失败的！ 问题分析Redis数据回写机制Redis在进行持久化的时候，有的时候可以在日志中看到fork进程失败的提示，一般是系统可用的内存空间不够导致，这需要我们对fork原理明白，才能更好的进行参数调整。 一般来说Redis在进行RDB的时候，会fork出一个子进程，子进程和父进程会共享一个地址空间，在fork子进程的时候，会检查当前机器可用的内存是否满足fork出一个子进程的要求，一般由操作系统overcommit_memory(系统内存分配策略)决定。 Redis的数据回写机制分同步和异步两种， 同步回写即SAVE命令，主进程直接向磁盘回写数据。在数据大的情况下会导致系统假死很长时间，所以一般不是推荐的。 异步回写即BGSAVE命令，主进程fork后，复制自身并通过这个新的进程回写磁盘，回写结束后新进程自行关闭。由于这样做不需要主进程阻塞，系统不会假死，一般默认会采用这个方法。 Redis默认采用异步回写，所以如果我们要将数据刷到硬盘上，这时Redis分配内存不能太大，否则很容易发生内存不够用无法fork的问题；设置一个合理的写磁盘策略，否则写频繁的应用，也会导致频繁的fork操作，对于占用了大内存的Redis来说，fork消耗资源的代价是很大的； 系统内存分配策略Linux对大部分申请内存的请求都回复yes，以便能跑更多更大的程序。 因为申请内存后，并不会马上使用内存，将这些不会使用的空闲内存分配给其它程序使用，以提高内存利用率，这种技术叫做Overcommit。 一般情况下，当所有程序都不会用到自己申请的所有内存时，系统不会出问题，但是如果程序随着运行，需要的内存越来越大，在自己申请的大小范围内，不断占用更多内存，直到超出物理内存，当Linux发现内存不足时，会发生OOM killer(OOM=out-of-memory)。 OOM killer会选择杀死一些进程，以便释放内存。当发生OOM killer时，会记录在系统日志中/var/log/messages。 用户态进程，非内核线程，占用内存越多和运行时间越短的进程越有可能被杀掉。 在Linux下有个vm内核参数：CommitLimit用于限制系统应用使用的内存资源；执行grep -i commit /proc/meminfo，看到CommitLimit和Committed_As参数。 CommitLimit是一个内存分配上限，CommitLimit = 物理内存 * overcommit_ratio(/proc/sys/vm/overcmmit_ratio，默认50，即50%) + swap大小 Committed_As是已经分配的内存大小(应用程序要申请的内存 + 系统已经分配的内存)。 vm.overcommit_memory文件指定了内核针对内存分配的策略，其值可以是0、1、2。 0：启发策略(默认)；表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。系统在为应用进程分配虚拟地址空间时，会判断当前申请的虚拟地址空间大小是否超过剩余内存大小，如果超过，则虚拟地址空间分配失败。因此，也就是如果进程本身占用的虚拟地址空间比较大或者剩余内存比较小时，fork、malloc等调用可能会失败。 0即是启发式的overcommitting handle，会尽量减少swap交换分区的使用，root可以分配比一般用户略多的内存。 1：允许overcommit；表示内核允许分配所有的物理内存，而不管当前的内存状态如何，允许超过CommitLimit，这种情况下，避免了fork可能产生的失败，但由于malloc是先分配虚拟地址空间，而后通过异常陷入内核分配真正的物理内存，在内存不足的情况下，这相当于完全屏蔽了应用进程对系统内存状态的感知，即malloc总是能成功，一旦内存不足，会引起系统OOM杀进程，应用程序对于这种后果是无法预测的。 直至内存用完为止。在数据库服务器上不建议设置为1，从而尽量避免使用swap交换分区。 2：禁止overcommit；表示不允许超过CommitLimit值。由于很多情况下，进程的虚拟地址空间占用远大于其实际占用的物理内存，这样一旦内存使用量上去以后，对于一些动态产生的进程(需要复制父进程地址空间)则很容易创建失败，如果业务过程没有过多的这种动态申请内存或者创建子进程，则影响不大，否则会产生比较大的影响 。这种情况下系统所能分配的内存不会超过上面提到的CommitLimit大小，如果这么多资源已经用光，那么后面任何尝试申请内存的行为都会返回错误，这通常意味着此时没法运行任何新程序。 解决方案修改系统内存分配策略我们可以通过设置overcommit_memory=1的优化，减少操作系统内存，提高Redis的fork成功率，因为fork后的进程和父进程共享一个数据空间，持久化要新增的内存空间都会小于父进程已经使用的空间，具体有三种方式修改内核参数，但要有root权限： 编辑/etc/sysctl.conf ，改vm.overcommit_memory=1，然后sysctl -p使配置文件生效； 命令：sysctl vm.overcommit_memory=1 ； 命令：echo 1 &gt; /proc/sys/vm/overcommit_memory； 关闭THP（Transparent Huge Pages）当Redis持久化fork子进程后，占用内存大小和父进程等同，由于Linux在写时有copy-on-write机制，父子进程共享相同的物理内存页，当父进程处理写请求的时候会把要修改的页创建副本，而子进程在fork过程中共享整个父进程的内存快照。如果我们要减少创建的副本的大小，就涉及操作系统的另外一个概念Huge Pages(大页)。 在Redhat Linux中，内存都是以页的形式划分的，默认情况下每页是4K，这就意味着如果物理内存很大，则映射表的条目将会非常多，会影响CPU的检索效率。因为内存大小是固定的，为了减少映射表的条目，可采取的办法只有增加页的尺寸。Linux Kernel在2.6.38内核中增加了THP(Transparent Huge Pages)的特性，支持大内存页（2MB）分配，默认开启。当开启后可以加快fork子进程的速度，但fork操作之后，每个内存页从原来的4KB变成了2MB，会大幅增加重写期间父进程内存消耗，同时每次写命令引起的复制内存页单位放大了512倍，会拖慢写操作的执行时间，因此在使用Redis的时候Redis建议关闭THP，方法为：echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled。为了让机器重启该参数仍然生效，建议在/etc/rc.local中追加echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled，避免失效。当大页被关闭后，可以看到同等操作下，RDB备份时候的copy-on-write变化内存空间会减少。 综上分析，我们可以操作系统物理内存和Redis内存之间的一些关系，尤其Redis在持久化的时候fork进程会随操作系统的参数不同，需要的内存也有所不同，为了加快fork子进程的速度以及主备之间的文件传输同步，一般我们建议一个Redis节点的最大内存在10G-15G左右，操作系统的内存适当冗余，尽量控制同一台机器的多个Redis节点在同一个时间点进行RDB备份（可以通过缓存中心定时备份），导致内存同一时刻增加避免内存空间不足导致的fork失败，最安全保险的情况是内存为Redis的2倍，但是在vm.overcommit_memory&#x3D;1和大页关闭的情况下，可以根据实际使用，降低操作系统的整个内存大小 。 参考文章： https://www.jianshu.com/p/785ee3bea266 https://www.cnblogs.com/wjoyxt/p/3777042.html","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"}]},{"title":"「Spring Reactive Stack」响应式 HTTP 请求客户端 WebClient","date":"2021-03-20T04:01:42.000Z","path":"2021/03/20/spring-reactive-http.html","text":"WebClient是Spring WebFlux模块提供的一个非阻塞的基于响应式编程的进行HTTP请求的客户端工具。 引入WebFlux依赖则可使用WebClient： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;&lt;/dependency&gt; 1. 创建WebClient实例WebClient接口提供了三个不同的静态方法（create()，create(String baseUrl)，builder()）和一个内部类（WebClient.Bulider）来创建WebClient实例： 1234567891011121314151617/** * 自动注入WebClient的内部类WebClient.Builder */@Autowiredprivate WebClient.Builder clientBuilder;void test() &#123; // WebClient.create()创建 WebClient webClient1 = WebClient.create(); // WebClient.create(String baseUrl) WebClient webClient2 = WebClient.create(&quot;http://www.test.com&quot;); // WebClient.builder()创建 WebClient webClient3 = WebClient.builder().build(); // 内部类WebClient.Builder创建 WebClient webClient4 = clientBuilder.build(); WebClient webClient5 = clientBuilder.baseUrl(&quot;http://www.test.com&quot;).build();&#125; 2. GET 请求2.1 发起GET请求1234567891011121314151617181920@Slf4j@RestControllerpublic class TestController &#123; @Autowired private WebClient.Builder clientBuilder; @GetMapping(&quot;/test&quot;) public Mono&lt;String&gt; test()&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() // GET 请求 .uri(&quot;/test/string&quot;) // 请求路径 .retrieve() // 获取响应体 .bodyToMono(String.class); // 响应数据类型转换(这里是String，也可以是自定义对象或集合) // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125;&#125; 2.2 GET请求参数传递1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@Slf4j@RestControllerpublic class TestController &#123; @Autowired private WebClient.Builder clientBuilder; /** * 1. 请求路径里带参数(?id=5&amp;name=abc) */ @GetMapping(&quot;/test/get&quot;) public Mono&lt;String&gt; test()&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() // GET 请求 .uri(&quot;/test/param?id=5&amp;name=abc&quot;) // 请求路径(带参数) .retrieve() // 获取响应体 .bodyToMono(String.class); // 响应数据类型转换(这里是String，也可以是自定义对象或集合) // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125; /** * 2. 占位符的形式传递参数 */ @GetMapping(&quot;/test2&quot;) public Mono&lt;String&gt; test2()&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() // GET 请求 .uri(&quot;/test/&#123;1&#125;/&#123;2&#125;&quot;, name, id) // 请求路径(占位符参数,若id=5,name=abc，则为&quot;/test/abc/5&quot;) .retrieve() // 获取响应体 .bodyToMono(String.class); // 响应数据类型转换(这里是String，也可以是自定义对象或集合) // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125; /** * 3. 另一种占位符的形式传递参数(类似@PathVariable) */ @GetMapping(&quot;/test/&#123;name&#125;/&#123;id&#125;&quot;) public Mono&lt;String&gt; test3(@PathVariable(&quot;id&quot;) Integer id, @PathVariable(&quot;name&quot;) String name)&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() // GET 请求 .uri(&quot;/test/&#123;name&#125;/&#123;id&#125;&quot;, name, id) // 请求路径(另一种占位符参数) .retrieve() // 获取响应体 .bodyToMono(String.class); // 响应数据类型转换(这里是String，也可以是自定义对象或集合) // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125; /** * 4. 使用 map 装载参数 */ @GetMapping(&quot;/test5&quot;) public Mono&lt;String&gt; test5()&#123; Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;name&quot;, &quot;abc&quot;); map.put(&quot;id&quot;, 5); WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() // GET 请求 .uri(&quot;/test/&#123;name&#125;/&#123;id&#125;&quot;, map) // 请求路径(使用map装载参数) .retrieve() // 获取响应体 .bodyToMono(String.class); // 响应数据类型转换(这里是String，也可以是自定义对象或集合) // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125; &#125; 3. POST请求3.1 发起POST请求1234567891011121314151617181920@Slf4j@RestControllerpublic class TestController &#123; @Autowired private WebClient.Builder clientBuilder; @PostMapping(&quot;/test/post&quot;) public Mono&lt;String&gt; test()&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .post() // POST 请求 .uri(&quot;/test/post&quot;) // 请求路径 .retrieve() // 获取响应体 .bodyToMono(String.class); // 响应数据类型转换(这里是String，也可以是自定义对象或集合) // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125;&#125; 3.2 POST请求参数传递123456789101112131415161718@PostMapping(&quot;/test6&quot;)public Mono&lt;String&gt; test6()&#123; //提交参数设置 MultiValueMap&lt;String, String&gt; mulMap = new LinkedMultiValueMap&lt;&gt;(); mulMap.add(&quot;name&quot;, &quot;abc&quot;); mulMap.add(&quot;id&quot;, &quot;5&quot;); WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .post() // POST 请求 .uri(&quot;/test/post&quot;) // 请求路径 .contentType(MediaType.APPLICATION_FORM_URLENCODED) // Content-Type: application/x-www-form-urlencoded .body(BodyInserters.fromFormData(mulMap)) // 请求参数 .retrieve() // 获取响应体 .bodyToMono(String.class); // 响应数据类型转换(这里是String，也可以是自定义对象或集合) // 打印响应数据 stringMono.subscribe(log::info); return stringMono;&#125; 4. 请求异常处理使用WebClient发送请求时， 如果接口返回的不是200状态（而是4xx、5xx这样的异常状态），则会抛出WebClientResponseException异常。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105@Slf4j@RestControllerpublic class TestController &#123; @Autowired private WebClient.Builder clientBuilder; /** * 1. 【doOnError】方法适配所有异常 */ @GetMapping(&quot;/test/error1&quot;) public Mono&lt;String&gt; test7()&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() .uri(&quot;/test/error&quot;) .retrieve() .bodyToMono(String.class) .doOnError(WebClientResponseException.class, err -&gt; &#123; log.error(&quot;发生错误：&quot; + err.getRawStatusCode() + &quot; &quot; + err.getResponseBodyAsString()); &#125;); // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125; /** * 2. 【onStatus】方法根据状态码来适配指定异常 */ @GetMapping(&quot;/test/error2&quot;) public Mono&lt;String&gt; test8()&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() .uri(&quot;/test/error&quot;) .retrieve() .onStatus(HttpStatus::is4xxClientError, resp -&gt; &#123; log.error(&quot;发生错误：&quot; + resp.statusCode().value() + &quot; &quot; + resp.statusCode().getReasonPhrase()); return Mono.error(new RuntimeException(&quot;请求失败&quot;)); &#125;) .onStatus(HttpStatus::is5xxServerError, resp -&gt; &#123; log.error(&quot;发生错误：&quot; + resp.statusCode().value() + &quot; &quot; + resp.statusCode().getReasonPhrase()); return Mono.error(new RuntimeException(&quot;服务器异常&quot;)); &#125;) .bodyToMono(String.class); // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125; /** * 3. 【onErrorReturn】方法来设置在发生异常时返回默认值， * 当请求发生异常是会使用该默认值作为响应结果 */ @GetMapping(&quot;/test/error3&quot;) public Mono&lt;String&gt; test9()&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() .uri(&quot;/test/error&quot;) .retrieve() .bodyToMono(String.class) .onErrorReturn(&quot;请求失败&quot;); // 失败时返回默认值“请求失败” // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125; /** * 4. 设置【超时属性】 * 使用timeout()方法设置一个超时时长。如果HTTP请求超时，便会发生TimeoutException异常。 */ @GetMapping(&quot;/test/setting&quot;) public Mono&lt;String&gt; test10()&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() // GET 请求 .uri(&quot;/test/setting&quot;) // 请求路径 .retrieve() // 获取响应体 .bodyToMono(String.class) // 响应数据类型转换(这里是String，也可以是自定义对象或集合) .timeout(Duration.ofSeconds(3)); // 3秒超时 // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125; /** * 5. 设置【异常自动重试】 * 使用retry()方法可以设置当请求异常时的最大重试次数，如果不带参数则表示无限重试，直至成功。 */ @GetMapping(&quot;/test/setting2&quot;) public Mono&lt;String&gt; test11()&#123; WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .get() // GET 请求 .uri(&quot;/test/setting&quot;) // 请求路径 .retrieve() // 获取响应体 .bodyToMono(String.class) // 响应数据类型转换(这里是String，也可以是自定义对象或集合) .timeout(Duration.ofSeconds(3)) // 3秒超时 .retry(2); // 重试2次 // 打印响应数据 stringMono.subscribe(log::info); return stringMono; &#125;&#125; 5. Exchange获取完整的请求响应结果前面我们都是使用retrieve()方法是直接获取响应体的内容。 使用exchangeToMono()和exchangeToFlux()方法获取完整的代表响应结果的对象，通过该对象我们可以获取响应码、contentType、contentLength、响应消息体等。 123456789101112131415161718192021222324@PostMapping(&quot;/test/res&quot;)public Mono&lt;String&gt; test12() &#123; //提交参数设置 MultiValueMap&lt;String, String&gt; mulMap = new LinkedMultiValueMap&lt;&gt;(); mulMap.add(&quot;name&quot;, &quot;abc&quot;); mulMap.add(&quot;id&quot;, &quot;5&quot;); WebClient webClient = webClientBuilder.baseUrl(&quot;https://www.test.com&quot;).build(); Mono&lt;String&gt; stringMono = webClient .post() // POST 请求 .uri(&quot;/test/post&quot;) // 请求路径 .contentType(MediaType.APPLICATION_FORM_URLENCODED) // Content-Type: application/x-www-form-urlencoded .body(BodyInserters.fromFormData(mulMap)) // 请求参数 .exchangeToMono(response -&gt; &#123; // 响应结果response if (response.statusCode().equals(HttpStatus.OK)) &#123; // return response.bodyToMono(String.class); &#125; else &#123; return response.createException().flatMap(Mono::error); &#125; &#125;); // 打印响应数据 stringMono.subscribe(log::info); return stringMono;&#125; 6. WebClient在Spring Cloud中的使用引入依赖： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-loadbalancer&lt;/artifactId&gt;&lt;/dependency&gt; 编写配置，创建WebClient.Bulider类型的Bean，加上@LoadBalaced为WebClient增加负载均衡的支持。 123456789101112import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.web.reactive.function.client.WebClient;@Configurationpublic class LoadbalanceConfiguration &#123; @Bean @LoadBalanced public WebClient.Builder builder() &#123; return WebClient.builder(); &#125;&#125; 编写Controller，客户端实现访问服务端资源，并对外提供访问接口： 12345678910111213@RestController@RequestMapping(&quot;/test&quot;)public class ReactiveClient &#123; @Autowired private WebClient.Builder clientBuilder; @GetMapping(&quot;/get/string&quot;) public Mono&lt;String&gt; getServerString() &#123; // 通过 WebClient 访问 CLOUD-SERVER 服务的资源 （这里的CLOUD-SERVER是服务注册中心注册的微服务名） return clientBuilder.baseUrl(&quot;http://CLOUD-SERVER&quot;).build().get().uri(&quot;/test/string&quot;).retrieve().bodyToMono(String.class); &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringReactive","slug":"SpringReactive","permalink":"http://chaooo.github.io/tags/SpringReactive/"},{"name":"WebClient","slug":"WebClient","permalink":"http://chaooo.github.io/tags/WebClient/"}]},{"title":"「Spring Reactive Stack」服务端事件推送Server-Sent Events","date":"2021-03-12T09:01:42.000Z","path":"2021/03/12/spring-reactive-sse.html","text":"SSE：Server-Sent Events服务器推送事件，是一种仅发送文本消息的技术。SSE基于HTTP协议中的持久连接。SSE是HTML5标准协议中的一部分。 客户端接收服务端异步更新的消息可以分为两类：客户端拉取和服务端推送。 客户端拉取：通过短轮询或者长轮询定期请求服务器进行更新。 服务端推送：SSE和WebSocket，SSE是单向，WebSocket是双向；SSE基于HTTP协议，WebSocket基于WebSocket协议(HTTP以外的协议); SSE网络协议 基于纯文本的简单协议。服务器端的响应内容类型必须是text/event-stream。响应文本的内容是一个事件流，事件流是一个简单的文本流，仅支持UTF-8格式的编码。 事件流由不同的事件组成。不同事件间通过仅包含回车符和换行符的空行（\\r\\n）来分隔。 每个事件可以由多行构成，每行由类型和数据两部分组成。类型与数据通过冒号（:）进行分隔，冒号前的为类型，冒号后的为其对应的值。每个事件可以包含如下类型的行： 类型为空白，表示该行是注释，会在处理时被忽略。 类型为data，表示该行是事件所包含的数据。以data开头的行可以出现多次。所有这些行都是该事件的数据。 类型为event，表示该行用来声明事件的类型，即事件名称。浏览器在收到数据时，会产生对应名称的事件。 类型为id，表示该行用来声明事件的标识符。 类型为retry，表示该行用来声明浏览器在连接断开之后进行重连的等待时间。 12345678910111213data: china // 该事件仅包含数据data: Beijing // 该事件包含数据与事件标识id: 100event: myevent // 该事件指定了名称data:shanghaiid: 101: this is a comment // 该事件具有注释、名称，且包含两行数据event:citydata: guangzhoudata: shenzhen 事件标识id作用: 如果服务端发送的事件中包含事件标识id，那么浏览器会将最近一次接收到的事件标识id记录到HTTP头的Last-Event-ID属性中。如果浏览器与服务端的连接中断，当浏览器再次连接时，会将Last-Event-ID记录的事件标识id发送给服务端。服务器端通过浏览器端发送的事件标识id来确定将继续连接哪个事件。 订阅一个服务端推送事件(GET请求)，需要设置 包含如下请求头的Request： 123Accept: text/event-stream # 指明MediaType是事件流Cache-Control: no-cache # 不要对事件进行缓存Connection: keep-alive # 长连接 服务端需要提供 包含以下响应头的Response： 12Content-Type: text/event-stream;charset=UTF-8 # 告诉客户端响应是一个事件流Transfer-Encoding: chunked # 告诉客户端内容大小未知，为流传输 在WebFlux中实现发送事件首先，在pom.xml文件中，引入webflux； 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;&lt;/dependency&gt; 创建一个controller类并用@RestController注解标记；创建一个接受Http GET请求的方法，该方法返回一个Flux对象，并配置produces=text/event-stream； 1234@GetMapping(value = &quot;/test/sse&quot;, produces = MediaType.TEXT_EVENT_STREAM_VALUE)private Flux&lt;String&gt; flux() &#123; return Flux.interval(Duration.ofMillis(1000)).map(i -&gt; &quot; -&gt; &quot; +i);&#125; 浏览器访问/test/sse就会看到每一秒推送一个数据： 12345data: -&gt; 0data: -&gt; 1data: -&gt; 2data: -&gt; 3... 一次性事件：短暂的去发送事件是比较简单的，只需要使用Flux.just()将消息列表里的消息一条条发送出去即可。 周期性事件：长期的发送事件在发送本身上是没有区别，主要是需要一个周期性线程定期处理发送事务，这里直接使用Flux.inteval()来轮询。 非周期性事件：可以通过Spring的事件监听接口来实现，关键点在于要把监听消息的处理器和Flux的构造结合起来。 SSE的注意事项 SSE只适合发送文本消息；尽管可以使用Base64编码和gzip压缩来发送二进制消息，但效率可能很低。 早期的一些浏览器，如Internet Explorer不支持。 Internet Explorer/Edge和许多移动浏览器不支持SSE；尽管可以使用polyfills，但它们可能效率低下 在系统设计时，同一个页面最好只维持1个SSE连接，通过事件来区分。因为浏览器对同时并发的连接数有限制，一般最大是6个。 前端接收Server-Sent事件通知JavaScript里用EventSource对象来接收服务器发送事件通知： 12345678if (typeof(EventSource)!==&quot;undefined&quot;) &#123; var source=new EventSource(&quot;http://localhost:8080/test/sse&quot;); source.onmessage=function(event) &#123; alert(event.data); &#125;;&#125; else &#123; alert(&quot;抱歉，你的浏览器不支持 server-sent 事件...&quot;);&#125; EventSource是服务器推送的一个网络事件接口。一个EventSource实例会对HTTP服务开启一个持久化的连接，以text/event-stream格式发送事件, 会一直保持开启直到被要求关闭。 EventSource属性 EventSource.onerror：EventHandler，当发生错误时被调用，并且在此对象上派发error事件。 EventSource.onmessage：EventHandler，当收到一个message事件，当接收到消息时被调用。 EventSource.onopen：EventHandler，当收到一个open事件，当连接刚打开时被调用。 EventSource.readyState(只读)：unsigned short值，代表连接状态。可能值是CONNECTING(0), OPEN(1), 或者CLOSED(2)。 EventSource.url(只读)：一个DOMString，代表事件源的URL。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringReactive","slug":"SpringReactive","permalink":"http://chaooo.github.io/tags/SpringReactive/"},{"name":"SSE","slug":"SSE","permalink":"http://chaooo.github.io/tags/SSE/"}]},{"title":"「Spring Reactive Stack」响应式方式访问Redis","date":"2021-03-04T11:01:42.000Z","path":"2021/03/04/spring-reactive-redis.html","text":"Spring Data Redis中同时支持了Jedis客户端和Lettuce客户端。但是仅Lettuce是支持Reactive方式的操作；这里选择默认的Lettuce客户端。 创建Maven项目，并在pom.xml导入依赖: 12345&lt;!-- reactive redis依赖包（包含Lettuce客户端） --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis-reactive&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件application.yml 123456789spring: redis: host: 127.0.0.1 port: 6379 password: 123456 #Redis数据库索引（默认为0） database: 0 #连接超时时间（毫秒） timeout: 5000 注入配置类： 1234567@Configurationpublic class ReactiveRedisConfig &#123; @Bean public ReactiveRedisTemplate reactiveRedisTemplate(ReactiveRedisConnectionFactory factory) &#123; return new ReactiveRedisTemplate&lt;&gt;(factory, RedisSerializationContext.string()); &#125;&#125; 简单的RedisService封装 12345678910111213141516171819202122@Service@AllArgsConstructorpublic class RedisService &#123; private final ReactiveRedisTemplate&lt;String, String&gt; redisTemplate; public Mono&lt;String&gt; get(String key) &#123; return key==null ? null : redisTemplate.opsForValue().get(key); &#125; public Mono&lt;Boolean&gt; set(String key, String value) &#123; return redisTemplate.opsForValue().set(key, value); &#125; public Mono&lt;Boolean&gt; set(String key, String value, Long time) &#123; return redisTemplate.opsForValue().set(key, value, Duration.ofSeconds(time)); &#125; public Mono&lt;Boolean&gt; exists(String key) &#123; return redisTemplate.hasKey(key); &#125; public Mono&lt;Long&gt; remove(String key) &#123; return redisTemplate.delete(key); &#125;&#125; 测试 123456789101112@SpringBootTestclass ReactiveRedisTest &#123; @Resource private RedisService redisService; @Test void test1() &#123; // 保存5分钟 redisService.set(&quot;test1&quot;, &quot;test1_value&quot;, 5 * 60L).subscribe(System.out::println); redisService.get(&quot;test1&quot;).subscribe(System.out::println); &#125;&#125; 测试运行结果： 12truetest1_value 本文使用Spring Boot版本：2.4.3","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringReactive","slug":"SpringReactive","permalink":"http://chaooo.github.io/tags/SpringReactive/"}]},{"title":"「Spring Reactive Stack」使用R2DBC访问MySQL","date":"2021-03-03T13:01:42.000Z","path":"2021/03/03/spring-reactive-r2dbc.html","text":"MySQL等一系列的关系型数据库也在R2DBC等包的帮助下支持响应式。R2DBC基于Reactive Streams反应流规范，它是一个开放的规范，为驱动程序供应商和使用方提供接口(r2dbc-spi)，与JDBC的阻塞特性不同，它提供了完全反应式的非阻塞API与关系型数据库交互。 创建Maven项目，并导入依赖pom.xml: 12345678910111213141516171819202122232425&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- data-r2dbc同时也会将r2dbc-pool导入 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-r2dbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- r2dbc mysql 库--&gt;&lt;dependency&gt; &lt;groupId&gt;dev.miku&lt;/groupId&gt; &lt;artifactId&gt;r2dbc-mysql&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 配置文件application.yml 123456789101112spring: r2dbc: url: r2dbcs:mysql://192.168.2.100:3306/test?characterEncoding=UTF8&amp;serverTimezone=Asia/Shanghai username: developer password: 123456 #日志配置logging: level: root: info # 调试环境查看执行的sql dev.miku.r2dbc.mysql.client.ReactorNettyClient: debug SQL与Model类 123456789create table `base_user` ( `userId` int not null auto_increment, `userName` varchar(100) default null comment &#x27;用户名&#x27;, `userMobile` varchar(20) default null comment &#x27;手机号&#x27;, primary key (`userId`), unique key `userMobile` (`userMobile`)) engine=innodb comment=&#x27;测试用户信息&#x27;;insert into `base_user` (`userName`, `userMobile`) values (&#x27;黑子&#x27;, &#x27;15914061216&#x27;);insert into `base_user` (`userName`, `userMobile`) values (&#x27;大黄&#x27;, &#x27;15914061217&#x27;); 123456@Datapublic class BaseUser &#123; private Integer id; private String name; private String mobile;&#125; 实际开发中，由于历史原因数据库字段大多与Model类字段无法对应，这里也不对应，在sql中用别名对应。 编写DAO层，这里继承的事响应式的crud类ReactiveCrudRepository 1234567891011121314151617181920212223242526272829303132333435363738@Repositorypublic interface BaseUserDao extends ReactiveCrudRepository&lt;BaseUser, Integer&gt; &#123; /** * 根据用户id查询用户 * @param id userId * @return BaseUser */ @Override @Query(&quot;select userId as id, userMobile as mobile, userName as name from base_user where userId= :id&quot;) Mono&lt;BaseUser&gt; findById(Integer id); /** * 更新用户名 * @param id userId * @param name userName */ @Modifying @Query(&quot;update base_user set userName= :name where userId= :id&quot;) Mono&lt;Integer&gt; updateNameById(Integer id, String name); /** * 新增用户 * @param name userName * @param mobile userMobile */ @Modifying @Query(&quot;insert into base_user(userName, userMobile) values (:name, :mobile)&quot;) Mono&lt;Void&gt; insertUser(String name, String mobile); /** * 根据用户id删除用户 * @param id userId */ @Override @Modifying @Query(&quot;delete from base_user where userId= :id&quot;) Mono&lt;Void&gt; deleteById(Integer id);&#125; 编写Service层，这里返回的值为Reactor的对象Flux或Mono 1234567891011121314151617181920212223@Service@RequiredArgsConstructorpublic class BaseUserService &#123; private final BaseUserDao baseUserDao; public Mono&lt;BaseUser&gt; findById (Integer id) &#123; return baseUserDao.findById(id); &#125; public Mono&lt;Integer&gt; updateById(BaseUser user)&#123; return baseUserDao.updateNameById(user.getId(), user.getName()); &#125; public Mono&lt;Void&gt; insertUser(BaseUser user)&#123; return baseUserDao.insertUser(user.getName(), user.getMobile()); &#125; public Mono&lt;Void&gt; deleteById(Integer id) &#123; return baseUserDao.deleteById(id); &#125;&#125; 编写Controller层 123456789101112131415161718192021222324252627@RestController@RequiredArgsConstructor@RequestMapping(&quot;/user&quot;)public class TestApi &#123; private final BaseUserService baseUserService; @GetMapping(&quot;/get&quot;) public Mono&lt;BaseUser&gt; findById(Integer id) &#123; return baseUserService.findById(id); &#125; @PostMapping(&quot;/update&quot;) public Mono&lt;Integer&gt; updateById(BaseUser user) &#123; return baseUserService.updateById(user); &#125; @PostMapping(&quot;/insert&quot;) public Mono&lt;Void&gt; insertUser(BaseUser user) &#123; return baseUserService.insertUser(user); &#125; @PostMapping(&quot;/delete&quot;) public Mono&lt;Void&gt; deleteById(Integer id) &#123; return baseUserService.deleteById(id); &#125;&#125; 测试: 查询：curl -X GET &quot;http://localhost:8080/user/get?id=1&quot; 更新：curl -X POST &quot;http://localhost:8080/user/update?id=1&amp;name=小黑子&quot; 新增：curl -X POST &quot;http://localhost:8080/user/insert?name=小蓝子&amp;mobile=15815011618&quot; 删除：curl -X POST &quot;http://localhost:8080/user/delete?id=1&quot; 本文使用Spring Boot版本：2.4.3","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"MySQL","slug":"MySQL","permalink":"http://chaooo.github.io/tags/MySQL/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringReactive","slug":"SpringReactive","permalink":"http://chaooo.github.io/tags/SpringReactive/"}]},{"title":"「Spring Reactive Stack」Reactor异常处理","date":"2021-03-02T12:12:42.000Z","path":"2021/03/02/spring-reactive-exception.html","text":"不管是在响应式编程还是普通的程序设计中，异常处理都是一个非常重要的方面。 对于Flux或者Mono来说，所有的异常都是一个终止的操作，即使你使用了异常处理，原生成序列也不会继续。但是如果你对异常进行了处理，那么它会将oneError信号转换成为新的序列的开始，并将替换掉之前上游产生的序列。 先看一个Flux产生异常的例子 123456@Testvoid test1() &#123; Flux.just(10, 5, 0) .map(i -&gt; &quot;100 / &quot; + i + &quot; = &quot; + (10 / i)) .subscribe(System.out::println);&#125; 会得到一个异常ErrorCallbackNotImplemented： 1234100 / 10 = 1100 / 5 = 2reactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.ArithmeticException: / by zeroCaused by: java.lang.ArithmeticException: / by zero 1. onError方法Reactor中subscribe的onError方法(subscribe第二个参数)，就是try catch的一个具体应用： 1234567@Testvoid test2() &#123; Flux.just(10, 5, 0) .map(i -&gt; &quot;100 / &quot; + i + &quot; = &quot; + (10 / i)) .subscribe(System.out::println, error -&gt; System.err.println(&quot;Error: &quot; + error));&#125; 运行结果： 123100 / 10 = 1100 / 5 = 2Error: java.lang.ArithmeticException: / by zero 2. onErrorReturn方法onErrorReturn可以在遇到异常的时候fallback到一个静态的默认值： 1234567@Testvoid test3() &#123; Flux.just(10, 5, 0) .map(i -&gt; &quot;100 / &quot; + i + &quot; = &quot; + (10 / i)) .onErrorReturn(&quot;Divided by zero :(&quot;) .subscribe(System.out::println);&#125; 运行结果： 123100 / 10 = 1100 / 5 = 2Divided by zero :( onErrorReturn还支持一个Predicate参数，用来判断要falback的异常是否满足条件。 1public final Flux&lt;T&gt; onErrorReturn(Predicate&lt;? super Throwable&gt; predicate, T fallbackValue) 3. onErrorResume方法onErrorResume可以在捕获异常之后调用其他的方法。 1234567@Testvoid test4() &#123; Flux.just(10, 5, 0) .map(i -&gt; &quot;100 / &quot; + i + &quot; = &quot; + (10 / i)) .onErrorResume(e -&gt; System.out::println) .subscribe(System.out::println);&#125; 运行结果： 123100 / 10 = 1100 / 5 = 2reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber@23469199 4. retry方法retry的作用就是当遇到异常的时候，重启一个新的序列 12345678@Testvoid test8() &#123; Flux.just(10, 5, 0) .map(i -&gt; &quot;100 / &quot; + i + &quot; = &quot; + (10 / i)) .retry(1) .elapsed() .subscribe(System.out::println);&#125; 运行结果： 123456[4,100 / 10 = 1][0,100 / 5 = 2][9,100 / 10 = 1][0,100 / 5 = 2]reactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.ArithmeticException: / by zeroCaused by: java.lang.ArithmeticException: / by zero elapsed是用来展示产生的value时间之间的duration。从结果我们可以看到，retry之前是不会产生异常信息的。 5. doOnError方法doOnError只记录异常信息，不破坏原来的React结构。 1234567@Testvoid test5() &#123; Flux.just(10, 5, 0) .map(i -&gt; &quot;100 / &quot; + i + &quot; = &quot; + (10 / i)) .doOnError(error -&gt; System.out.println(&quot;we got the error: &quot;+ error)) .subscribe(System.out::println);&#125; 运行结果： 12345100 / 10 = 1100 / 5 = 2we got the error: java.lang.ArithmeticException: / by zeroreactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.ArithmeticException: / by zeroCaused by: java.lang.ArithmeticException: / by zero doOn系列方法是publisher的同步钩子方法，在subscriber触发一系列事件的时候触发 6. doFinally方法doFinally可以像传统的同步代码那样使用finally去做一些事情，比如关闭http连接，清理资源等。 1234567@Testvoid test6() &#123; Flux.just(10, 5, 0) .map(i -&gt; &quot;100 / &quot; + i + &quot; = &quot; + (10 / i)) .doFinally(error -&gt; System.out.println(&quot;Finally，I will make sure to do something:&quot;+error)) .subscribe(System.out::println);&#125; 运行结果： 123456100 / 10 = 1100 / 5 = 2reactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.ArithmeticException: / by zeroCaused by: java.lang.ArithmeticException: / by zero at ...Finally，我会确保做一些事情:onError 第二种收尾操作的方法是using，我们先看一个using的定义： 12public static &lt;T, D&gt; Flux&lt;T&gt; using(Callable&lt;? extends D&gt; resourceSupplier, Function&lt;? super D, ? extends Publisher&lt;? extends T&gt;&gt; sourceSupplier, Consumer&lt;? super D&gt; resourceCleanup) 可以看到using支持三个参数，resourceSupplier是一个生成器，用来在subscribe的时候生成要发送的resource对象。sourceSupplier是一个生成Publisher的工厂，接收resourceSupplier传过来的resource，然后生成Publisher对象。resourceCleanup用来对resource进行收尾操作。 12345678910111213141516171819@Testvoid test7() &#123; AtomicBoolean isDisposed = new AtomicBoolean(); Disposable disposableInstance = new Disposable() &#123; @Override public void dispose() &#123; isDisposed.set(true); &#125; @Override public String toString() &#123; return &quot;DISPOSABLE&quot;; &#125; &#125;; Flux.using( () -&gt; disposableInstance, disposable -&gt; Flux.just(disposable.toString()), Disposable::dispose) .subscribe(System.out::println);&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringReactive","slug":"SpringReactive","permalink":"http://chaooo.github.io/tags/SpringReactive/"},{"name":"Reactor","slug":"Reactor","permalink":"http://chaooo.github.io/tags/Reactor/"}]},{"title":"「Spring Reactive Stack」Spring WebFlux响应式Web框架入门","date":"2021-02-25T07:50:42.000Z","path":"2021/02/25/spring-reactive-webflux.html","text":"Spring WebFlux是Spring Framework 5.0中引入的以Reactor为基础的响应式编程Web框架。 WebFlux 的异步处理是基于Reactor实现的，是将输入流适配成Mono或Flux进行统一处理。 1. 响应式流(Reactive Streams) Reactor 是一个响应式流，它有对应的发布者(Publisher)，用两个类来表示： Flux(返回0-n个元素) Mono(返回0或1个元素) Reactor 的订阅者(Subscriber)则是由Spring框架去完成。 响应式流(Reactive Streams) 其实就是一个规范，其特点： 无阻塞； 一个数据流； 可以异步执行； 能够处理背压； 背压(Backpressure) 可以简单理解为 消费决定生产，生产者可以根据消费压力进行动态调节生产速率的机制。 2. 发布者(Publisher)由于响应流的特点，我们不能再返回一个简单的POJO对象来表示结果了。必须返回一个类似Java中的Future的概念，在有结果可用时通知消费者进行消费响应。 Reactive Stream规范中这种被定义为Publisher ，Publisher是一个可以提供0-N个序列元素的提供者，并根据其订阅者Subscriber的需求推送元素。一个Publisher可以支持多个订阅者，并可以根据订阅者的逻辑进行推送序列元素。 可以通过下图Excel来理解，1-9行可以看作发布者Publisher提供的元素序列，10-13行的结果计算看作订阅者Subscriber。 响应式的一个重要特点：当没有订阅时发布者(Publisher)什么也不做。 而Flux和Mono都是Publisher在Reactor3实现。Publisher提供了subscribe方法，允许消费者在有结果可用时进行消费。如果没有消费者Publisher不会做任何事情，他根据消费情况进行响应。Publisher可能返回零或者多个，甚至可能是无限的，为了更加清晰表示期待的结果就引入了两个实现模型Mono和Flux。 在WebFlux中，你的方法只需返回Mono或Flux即可。你的代码基本也只和Mono或Flux打交道。而WebFlux则会实现Subscriber ，onNext时将业务开发人员编写的Mono或Flux转换为HTTP Response返回给客户端。 3. Mono和Flux的抽象模型Mono和Flux都是Publisher(发布者)的实现模型。 3.1 FluxFlux是一个发出(emit)0-N个元素组成的异步序列的Publisher&lt;T&gt;,可以被onComplete信号或者onError信号所终止。在响应流规范中存在三种给下游消费者调用的方法 onNext, onComplete, 和onError。下面这张图表示了Flux的抽象模型： 3.2 MonoMono是一个发出(emit)0-1个元素的Publisher&lt;T&gt;,可以被onComplete信号或者onError信号所终止。下面这张图表示了Mono的抽象模型(整体和Flux差不多,只不过这里只会发出0-1个元素)： 4. Mono APIMono和Flux都是实现org.reactivestreams.Publisher接口的抽象类。 Mono代表0-1个元素的发布者(Publisher)。 Mono里面有很多API： **just()**：可以指定序列中包含的全部元素。创建出来的 Mono序列在发布这些元素之后会自动结束。 **empty()**：创建一个不包含任何元素，只发布结束消息的序列。 **justOrEmpty(Optional&lt;? extends T&gt; data)**：从一个Optional对象或可能为null的对象中创建Mono。只有Optional对象中包含值或对象不为null时，Mono序列才产生对应的元素。 **error(Throwable error)**：创建一个只包含错误消息的序列。 **never()**：创建一个不包含任何消息通知的序列。 **delay(Duration duration)和delayMillis(long duration)**：创建一个Mono序列，在指定的延迟时间之后，产生数字 0 作为唯一值。 **fromCallable()、fromCompletionStage()、fromFuture()、fromRunnable()和fromSupplier()**：分别从 Callable、CompletionStage、CompletableFuture、Runnable和Supplier中创建Mono。 **ignoreElements(Publisher source)**：创建一个Mono序列，忽略作为源的Publisher中的所有元素，只产生结束消息。 **create()**：通过create()方法来使用MonoSink来创建Mono。 API使用案例如下所示。 123456789101112131415161718192021222324252627282930313233343536373839@Slf4j@SpringBootTestpublic class MonoTest &#123; @Test public void mono() &#123; // 通过just直接赋值 Mono.just(&quot;my name is charles&quot;).subscribe(log::info); // empty 创建空mono Mono.empty().subscribe(); // ustOrEmpty 只有 Optional 对象中包含值或对象不为 null 时，Mono 序列才产生对应的元素。 Mono.justOrEmpty(null).subscribe(System.out::println); Mono.justOrEmpty(&quot;测试justOrEmpty&quot;).subscribe(System.out::println); Mono.justOrEmpty(Optional.of(&quot;测试justOrEmpty&quot;)).subscribe(System.out::println); // error 创建一个只包含错误消息的序列。 Mono.error(new RuntimeException(&quot;error&quot;)).subscribe(System.out::println, System.err::println); // never 创建一个不包含任何消息通知的序列。 Mono.never().subscribe(System.out::println); // 延迟生成0 Mono.delay(Duration.ofMillis(2)).map(String::valueOf).subscribe(log::info); // 通过fromRunnable创建，并实现异常处理 Mono.fromRunnable(() -&gt; &#123; System.out.println(&quot;thread run&quot;); throw new RuntimeException(&quot;thread run error&quot;); &#125;).subscribe(System.out::println, System.err::println); // 通过Callable Mono.fromCallable(() -&gt; &quot;callback function&quot;).subscribe(log::info); // future Mono.fromFuture(CompletableFuture.completedFuture(&quot;from future&quot;)).subscribe(log::info); // 通过runnable Mono&lt;Void&gt; runnableMono = Mono.fromRunnable(() -&gt; log.warn(Thread.currentThread().getName())); runnableMono.subscribe(); // 通过使用 Supplier Mono.fromSupplier(() -&gt; new Date().toString()).subscribe(log::info); // flux中 Mono.from(Flux.just(&quot;from&quot;, &quot;flux&quot;)).subscribe(log::info); // 只返回flux第一个 //通过 create()方法来使用 MonoSink 来创建 Mono。 Mono.create(sink -&gt; sink.success(&quot;测试create&quot;)).subscribe(System.out::println); &#125;&#125; 运行结果： 5. Flux APIMono和Flux都是实现org.reactivestreams.Publisher接口的抽象类。 Flux表示连续序列，和Mono的创建方法有些不同，Mono是Flux的简化版，Flux可以用来表示流。 Flux API： **just()**：可以指定序列中包含的全部元素。 **range()**：可以用来创建连续数值。 **empty()**：创建一个不包含任何元素。 **error(Throwable error)**：创建一个只包含错误消息的序列。 **fromIterable()**：通过迭代器创建如list，set **fromStream()**：通过流创建 **fromArray(T[])**：通过列表创建 如 String[], Integer[] **merge()**：通过将两个flux合并得到新的flux **interval()**：每隔一段时间生成一个数字，从1开始递增 API使用案例如下所示。 1234567891011121314151617181920212223242526272829@Slf4j@SpringBootTestpublic class FluxTest &#123; @Test public void flux () throws InterruptedException &#123; // 通过just赋值 Flux&lt;Integer&gt; intFlux = Flux.just(1, 2, 3, 4, 5); // 以6开始，取4个值：6,7,8,9 Flux&lt;Integer&gt; rangeFlux = Flux.range(6, 4); // 通过merge合并 Flux&lt;Integer&gt; intMerge = Flux.merge(intFlux, rangeFlux); intMerge.subscribe(System.out::print); System.out.println();//换行 // 通过fromArray构建 Flux.fromArray(new Integer[]&#123;1,3,5,7,9&#125;).subscribe(System.out::print); System.out.println();//换行 // 通过流和迭代器创建 Flux&lt;String&gt; strFluxFromStream = Flux.fromStream(Stream.of(&quot; just&quot;, &quot; test&quot;, &quot; reactor&quot;, &quot; Flux&quot;, &quot; and&quot;, &quot; Mono&quot;)); Flux&lt;String&gt; strFluxFromList = Flux.fromIterable(Arrays.asList(&quot; just&quot;, &quot; test&quot;, &quot; reactor&quot;, &quot; Flux&quot;, &quot; and&quot;, &quot; Mono&quot;)); // 通过merge合并 Flux&lt;String&gt; strMerge = Flux.merge(strFluxFromStream, strFluxFromList); strMerge.subscribe(System.out::print); System.out.println(); // 通过interval创建流数据 Flux.interval(Duration.ofMillis(100)).map(String::valueOf) .subscribe(System.out::print); Thread.sleep(2000); &#125;&#125; 运行结果： 123412345678913579 just test reactor Flux and Mono just test reactor Flux and Mono012345678910111213141516171819 6. subscribe方法subscribe()方法表示对数据流的订阅动作，subscribe()方法有多个重载的方法，最多可以传入四个参数； 123456789101112131415161718192021222324252627282930@Testpublic void subscribe () throws InterruptedException &#123; // 测试Mono Mono.just(1).subscribe(System.out::println); // 测试Flux Flux.just(&#x27;a&#x27;, &#x27;b&#x27;).subscribe(System.out::println); // 测试2个参数的subscribe方法 Flux.just(&#x27;i&#x27;, &#x27;j&#x27;).map(chr -&gt; &#123; if (&#x27;j&#x27;== chr) throw new RuntimeException(&quot;test 2 parameters&quot;); else return String.valueOf(chr); &#125;) .subscribe(System.out::println, // 参数1,接受内容 err -&gt; log.error(err.getMessage())); // 参数2,对err处理的lambda函数 // 测试3个参数的subscribe方法 Flux.just(&quot;你&quot;, &quot;我&quot;, &quot;他&quot;, &quot;它&quot;, &quot;ta&quot;) .subscribe(System.out::print, // 参数1,接受内容 System.err::println, // 参数2,对err处理的lambda函数 () -&gt; System.out.println(&quot;complete for 3&quot;));// 参数3,完成subscribe之后执行的lambda函数 // 测试4个参数的subscribe方法 Flux.interval(Duration.ofMillis(100)) .map(i -&gt; &#123; if (i == 3) throw new RuntimeException(&quot;fake a mistake&quot;); else return String.valueOf(i); &#125;) .subscribe(info -&gt; log.info(&quot;info: &#123;&#125;&quot;, info), // 参数1,接受内容 err -&gt; log.error(&quot;error: &#123;&#125;&quot;, err.getMessage()),// 参数2,对err处理的lambda函数 () -&gt; log.info(&quot;Done&quot;), // 参数3,完成subscribe之后执行的lambda函数 sub -&gt; sub.request(10)); // 参数4,Subscription操作,设定从源头获取元素的个数 Thread.sleep(2000);&#125; 运行结果： 7. 使用StepVerifier测试响应式异步代码通过expectNext执行类似断言的功能，如果断言不符合实际情况，就会报错。 12345678910111213141516@Testpublic void StepVerifier () &#123; // 使用StepVerifier测试Flux，正常 Flux flux = Flux.just(1, 2, 3, 4, 5, 6); StepVerifier.create(flux) // 测试下一个期望的数据元素 .expectNext(1, 2, 3, 4, 5, 6) // 测试下一个元素是否为完成信号 .expectComplete() .verify(); // 使用StepVerifier测试Mono，报错 Mono&lt;String&gt; mono = Mono.just(&quot;charles&quot;).log(); StepVerifier.create(mono) .expectNext(&quot;char&quot;) .verifyComplete();&#125; 运行结果： 12java.lang.AssertionError: expectation &quot;expectNext(char)&quot; failed (expected value: char; actual value: charles)... 参考连接：https://mp.weixin.qq.com/s/O1VGS7d1TLQhgrCaQ-UQCw","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringReactive","slug":"SpringReactive","permalink":"http://chaooo.github.io/tags/SpringReactive/"},{"name":"WebFlux","slug":"WebFlux","permalink":"http://chaooo.github.io/tags/WebFlux/"}]},{"title":"Java接口幂等性设计及实例","date":"2021-02-17T06:12:42.000Z","path":"2021/02/17/spring-boot-idempotent.html","text":"幂等性：多次调用方法或者接口不会改变业务状态，可以保证重复调用的结果和单次调用的结果一致。select和delete操作具有天然幂等性：select多次结果总是一致，delete第一次执行后继续再执行也不会对数据有影响；一般没有幂等性而出现异常的操作：insert操作，update操作，混合类型操作(同时包含增删改等)。 1. 使用幂等的场景 前端重复提交：前端瞬时点击多次造成表单重复提交； 接口超时重试：接口可能会因为某些原因而调用失败，出于容错性考虑会加上失败重试的机制。如果接口调用一半，再次调用就会因为脏数据的存在而出现异常。 消息重复消费：在使用消息中间件来处理消息队列，且手动ack确认消息被正常消费时。如果消费者突然断开连接，那么已经执行了一半的消息会重新放回队列。被其他消费者重新消费时就会导致结果异常，如数据库重复数据，数据库数据冲突，资源重复等。 请求重发：网络抖动引发的nginx重发请求，造成重复调用； 2. 幂等性设计 update操作 根据唯一业务id去更新数据。 使用乐观锁(增加版本号或修改时间字段)。 insert操作 若该操作具有唯一业务号，则可通过数据库层面的唯一&#x2F;联合唯一索引来限制重复数据；或通过分布式锁来保证接口幂等性。 若该操作没有唯一业务号，可以使用Token机制，保证幂等性。 混合操作（一个接口包含多种操作） 使用Token机制，或使用Token + 分布式锁的方案来解决幂等性问题。 3. 解决方案3.1 Token机制实现通过Token 机制实现接口的幂等性，这是一种比较通用性的实现方法。 具体流程步骤： 客户端会先发送一个请求去获取Token，服务端会生成一个全局唯一的ID作为Token保存在Redis中，同时把这个ID返回给客户端； 客户端第二次调用业务请求的时候必须携带这个Token； 服务端会校验这个 Token，如果校验成功，则执行业务，并删除Redis中的 Token； 如果校验失败，说明Redis中已经没有对应的 Token，则表示重复操作，直接返回指定的结果给客户端。 3.2 基于MySQL实现通过MySQL唯一索引的特性实现接口的幂等性。 具体流程步骤： 建立一张去重表，其中某个字段需要建立唯一索引； 客户端去请求服务端，服务端会将这次请求的一些信息插入这张去重表中； 因为表中某个字段带有唯一索引，如果插入成功，证明表中没有这次请求的信息，则执行后续的业务逻辑； 如果插入失败，则代表已经执行过当前请求，直接返回。 3.3 基于Redis实现通过Redis的SETNX命令实现接口的幂等性。 SETNX key value：当且仅当key不存在时将key的值设为value；若给定的key已经存在，则SETNX不做任何动作。设置成功时返回1，否则返回0。 具体流程步骤： 客户端先请求服务端，会拿到一个能代表这次请求业务的唯一字段； 将该字段以SETNX的方式存入Redis中，并根据业务设置相应的超时时间； 如果设置成功，证明这是第一次请求，则执行后续的业务逻辑； 如果设置失败，则代表已经执行过当前请求，直接返回。 4. 实例：自定义注解实现API幂等处理（基于Redis实现）4.1 引入redis支持 pom.xml引入Redis的依赖 123456789101112131415161718192021&lt;!-- redis依赖包 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;!-- 排除lettuce包，使用jedis代替--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;io.lettuce&lt;/groupId&gt; &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- aop切面 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件application.yml 12345678910111213141516171819spring: redis: host: 127.0.0.1 port: 6379 password: 123456 #Redis数据库索引（默认为0） database: 0 #连接池最大连接数（使用负值表示没有限制） jedis: pool: max-active: 50 #连接池最大阻塞等待时间（使用负值表示没有限制） max-wait: -1 #连接池中的最大空闲连接 max-idle: 20 #连接池中的最小空闲连接 min-idle: 2 #连接超时时间（毫秒） timeout: 5000 测试Redis连接 1234567891011121314@SpringBootTestclass RedisTest &#123; @Resource private RedisTemplate&lt;String,String &gt; redisTemplate; @Test void simpleTest() &#123; ValueOperations&lt;String,String&gt; valueOperations = redisTemplate.opsForValue(); String key = &quot;RedisTemplateTest-simpleTest-001&quot;; valueOperations.set(key,key+key); System.out.println(valueOperations.get(key)); &#125;&#125; 4.2 编码实现 添加幂等异常 12345678910111213141516package com.example.demo;/** * 处理幂等相关异常 * * @author : Charles * @date : 2021/3/1 */public class IdempotentException extends RuntimeException &#123; public IdempotentException(String message) &#123; super(message); &#125; @Override public String getMessage() &#123; return super.getMessage(); &#125;&#125; 自定义幂等注解 1234567891011121314151617181920212223242526272829package com.example.demo;import java.lang.annotation.*;/** * 自定义幂等注解 * * @author : Charles * @date : 2021/3/1 */@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Idempotent &#123; /** * 前缀属性，作为redis缓存Key的一部分。 */ String prefix() default &quot;idempotent_&quot;; /** * 需要的参数名数组 */ String[] keys(); /** * 幂等过期时间（秒），即：在此时间段内，对API进行幂等处理。 */ int expire() default 3; /** * 提示错误码，也可自定义为字符串直接提醒 */ int errorCode() default 1001;&#125; 幂等切面 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package com.example.demo;import lombok.extern.slf4j.Slf4j;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.aspectj.lang.reflect.MethodSignature;import org.springframework.boot.autoconfigure.condition.ConditionalOnClass;import org.springframework.core.LocalVariableTableParameterNameDiscoverer;import org.springframework.data.redis.core.RedisCallback;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.expression.EvaluationContext;import org.springframework.expression.Expression;import org.springframework.expression.ExpressionParser;import org.springframework.expression.spel.standard.SpelExpressionParser;import org.springframework.expression.spel.support.StandardEvaluationContext;import org.springframework.stereotype.Component;import redis.clients.jedis.commands.JedisCommands;import redis.clients.jedis.params.SetParams;import javax.annotation.Resource;import java.lang.reflect.Method;/** * 幂等切面 * * @author : Charles * @date : 2021/3/1 */@Slf4j@Aspect@Component@ConditionalOnClass(RedisTemplate.class)public class IdempotentAspect &#123; private static final String LOCK_SUCCESS = &quot;OK&quot;; @Resource private RedisTemplate&lt;String,String&gt; redisTemplate; /** * 切入点,根据自定义Idempotent实际路径进行调整 */ @Pointcut(&quot;@annotation(com.example.demo.Idempotent)&quot;) public void executeIdempotent() &#123; &#125; @Around(&quot;executeIdempotent()&quot;) public Object around(ProceedingJoinPoint joinPoint) throws Throwable &#123; // 获取参数对象列表 Object[] args = joinPoint.getArgs(); //获取方法 Method method = ((MethodSignature) joinPoint.getSignature()).getMethod(); // 得到方法名 String methodName = method.getName(); // 获取参数数组 String[] parameters = new LocalVariableTableParameterNameDiscoverer().getParameterNames(method); //获取幂等注解 Idempotent idempotent = method.getAnnotation(Idempotent.class); // 初始化springEL表达式解析器实例 ExpressionParser parser = new SpelExpressionParser(); // 初始化解析内容上下文 EvaluationContext context = new StandardEvaluationContext(); // 把参数名和参数值放入解析内容上下文里 for (int i = 0; i &lt; parameters.length; i++) &#123; if (args[i] != null) &#123; // 添加解析对象目标 context.setVariable(parameters[i], args[i]); &#125; &#125; // 解析定义key对应的值，拼接成key StringBuilder idempotentKey = new StringBuilder(idempotent.prefix() + &quot;:&quot; + methodName); for (String s : idempotent.keys()) &#123; // 解析对象 Expression expression = parser.parseExpression(s); idempotentKey.append(&quot;:&quot;).append(expression.getValue(context)); &#125; // 通过 setnx 确保只有一个接口能够正常访问 String result = redisTemplate.execute( (RedisCallback&lt;String&gt;) connection -&gt; ( (JedisCommands) connection.getNativeConnection() ).set( idempotentKey.toString(), idempotentKey.toString(), new SetParams().nx().ex(idempotent.expire()) ) ); if (LOCK_SUCCESS.equals(result)) &#123; return joinPoint.proceed(); &#125; else &#123; log.error(&quot;API幂等处理, key=&quot; + idempotentKey); throw new IdempotentException(&quot;API幂等处理, key=&quot; + idempotentKey); &#125; &#125;&#125; 4.3 幂等注解的使用 接口添加@Idempotent注解 12345@Idempotent(prefix=&quot;idempotent&quot;, keys=&#123;&quot;#id&quot;, &quot;#str&quot;&#125;, expire=5)@PostMapping(&quot;/test&quot;)public String testApi(Integer id, String str) &#123; return &quot;测试幂等API:&quot; + id + str;&#125; 连续调用API测试 1curl -X POST &quot;http://localhost:8080/test?id=1002&amp;str=TestIdempotentParameterString&quot; 测试结果 第一次调用/test，会正常返回测试幂等API:1002TestIdempotentParameterString；并且在5秒内，Redis会存在key为idempotent:testApi:1002:TestIdempotentParameterString的唯一值。 12127.0.0.1:6379&gt; keys &quot;idempotent:testApi:1002:TestIdempotentParameterString&quot;1) &quot;idempotent:testApi:1002:TestIdempotentParameterString&quot; 再次调用/test，会返回异常。 123456789&#123; &quot;timestamp&quot;: &quot;2021-03-01T06:42:14.282+00:00&quot;, &quot;path&quot;: &quot;/test&quot;, &quot;status&quot;: 500, &quot;error&quot;: &quot;Internal Server Error&quot;, &quot;message&quot;: &quot;API幂等处理, key=idempotent:testApi:1002:TestIdempotentParameterString&quot;, &quot;requestId&quot;: &quot;b42b9639-8&quot;, &quot;trace&quot;: &quot;com.example.demo.IdempotentException: API幂等处理, key=idempotent:testApi:1002:TestIdempotentParameterString\\r\\n\\tat com.example.demo.IdempotentAspect.around ... 中间省略 ... (FastThreadLocalRunnable.java:30)\\r\\n\\t\\tat java.lang.Thread.run(Thread.java:748)\\r\\n&quot;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"},{"name":"幂等","slug":"idempotent","permalink":"http://chaooo.github.io/tags/idempotent/"}]},{"title":"「环境配置」Nginx常用配置详解","date":"2020-12-29T10:07:05.000Z","path":"2020/12/29/env-nginx.html","text":"Nginx 的配置系统由一个主配置文件和其他一些辅助的配置文件构成。这些配置文件均是纯文本文件，全部位于Nginx安装目录下的conf目录下。 Nginx配置文件中每个指令必须有;结束。以#开头的行会被当做注释。 配置文件通常命名为nginx.conf并且默认放置在/usr/local/nginx/conf，/etc/nginx，或/usr/local/etc/nginx。 1. Nginx配置文件结构 Nginx配置文件常用到的几个部分：main（全局设置），server（主机设置），upstream（上游服务器设置，主要为反向代理、负载均衡相关配置），location（URL匹配特定位置后的设置） 每部分包含若干个指令。 main模块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。 events模块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。 http模块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。 upstream模块：使nginx跨越单机的限制，完成网络数据的接收、处理和转发，该指令用于设置一系列的后端服务器，设置反向代理及后端服务器的负载均衡； server模块：主要用于指定虚拟主机域名、IP和端口； location模块：配置请求的路由，以及各种页面的处理情况（比如，根目录“&#x2F;”,“&#x2F;images”,等等）。 他们之间的关系式： server继承main，location继承server； upstream既不会继承指令也不会被继承。它有自己的特殊指令，不需要在其他地方的应用。 123456789101112131415161718192021222324... # main(全局)模块events &#123; # events模块 ...&#125;http &#123; # http模块 ... upstream &#123; # upstream模块 ... &#125; server &#123; # server模块 ... location [PATTERN] &#123; # location模块 ... &#125; location [PATTERN] &#123; ... &#125; &#125; server &#123; ... &#125; ...&#125; 2. Nginx配置文件详解2.1 全局模块12345678910111213141516# ------------------------------ 全局模块 start ------------------------------# user www www; # 配置用户或者组，默认为nobody nobody。# worker_processes 1; # 允许生成的进程数，默认为1，建议设置为等于CPU总核心数。# pid /nginx/pid/nginx.pid; # 指定nginx进程pid文件存放地址。error_log /usr/local/nginx/logs/error.log info; # 全局错误日志路径，级别，[ debug|info|notice|warn|error|crit|alert|emerg ]。 # 这个设置可以放入全局块，http块，server块worker_rlimit_nofile 65535; # 指定进程可以打开的最大描述符。 # 工作模式与连接数上限：这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。 # 现在在linux 2.6内核下开启文件打开数为65535。因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。# ------------------------------ 全局模块 end ------------------------------events &#123; ... &#125; # events块http &#123; ... &#125; # http块 2.2 events模块1234567891011121314151617181920212223242526272829303132333435363738# ------------------------------ events模块 start ------------------------------events &#123; use epoll; # 参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; # epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 # nginx针对不同的操作系统，有不同的事件模型： # 1.标准事件模型：Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll # 2.高效事件模型：Kqueue，Epoll，/dev/poll，Eventport # Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 # Epoll：使用于Linux内核2.6版本及以后的系统。 # /dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 # Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 worker_connections 65535; # 单个进程最大连接数（最大连接数=连接数*进程数）。 # 根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 accept_mutex on; # 网路连接序列化，防止惊群现象发生，默认:on multi_accept on; # 一个进程是否同时接受多个网络连接，默认:off keepalive_timeout 60; # keepalive超时时间。 client_header_buffer_size 4k; # 客户端请求头部的缓冲区大小。 # 分页大小可以用shell命令getconf PAGESIZE取得，如：[root@centos ~]# getconf PAGESIZE # 返回4096。 # 但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 open_file_cache max=65535 inactive=60s; # 为打开文件指定缓存，默认是关闭的。max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache_valid 80s; # 检查open_file_cache中缓存项目的有效信息的时间间隔。 # 默认值:60；使用字段:http, server, location open_file_cache_min_uses 1; # 指定在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态。 # 如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 # 默认值:1；使用字段:http, server, location open_file_cache_errors on; # 指定是否在搜索一个文件时记录cache错误[ on | off ]，默认值:off&#125;# ------------------------------ events模块 end ------------------------------http &#123; ... &#125; # http块 2.2 http模块1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556... # 全局块events &#123; ... &#125; # events块# ------------------------------ http模块 start ------------------------------http &#123; # 设定http服务器，利用它的反向代理功能提供负载均衡支持 include mime.types; # 文件扩展名与文件类型映射表 default_type application/octet-stream; # 默认文件类型，默认为text/plain #charset utf-8; # 默认编码 server_names_hash_bucket_size 128; # 服务器名字的hash表大小 # 保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. client_header_buffer_size 32k; # 客户端请求头部的缓冲区大小。 # 这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 large_client_header_buffers 4 64k; # 客户请求头缓冲大小。 # nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 client_max_body_size 8m; # 设定通过nginx上传文件的大小 sendfile on; # 允许sendfile方式（高效文件传输模式）传输文件，默认为off，可以在http块，server块，location块。 # 指定nginx是否调用sendfile函数（zero copy方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 autoindex on; # 开启目录列表访问，合适下载服务器，默认关闭。 tcp_nopush on; # 此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nodelay on; # 该参数最核心的功能，就是把小包组成成大包，提高带宽利用率也就是著名的nagle算法 keepalive_timeout 120; # 长连接超时时间，单位是秒 # FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; # gzip模块设置 gzip on; # 开启gzip压缩输出 gzip_min_length 1k; # 最小压缩文件大小 gzip_buffers 4 16k; # 压缩缓冲区 gzip_http_version 1.0; # 压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; # 压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; # 压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; # limit_zone crawler $binary_remote_addr 10m; # 开启限制IP连接数的时候需要使用 upstream &#123; ... &#125; # upstream模块 server &#123; ... &#125; # server模块&#125;# ------------------------------ http模块 end ------------------------------ 2.3 upstream模块1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162... # 全局块events &#123; ... &#125; # events块http &#123; ... # http块# ------------------------------ upstream模块 start ------------------------------ upstream www.test.com &#123; # upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; &#125; # nginx的upstream目前支持4种方式的分配: ①轮询（默认），②weight，③ip_hash，④第三方 # 轮询（默认）：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 # weight：指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 # 例如： upstream bakend &#123; # server 192.168.0.14 weight=10; # server 192.168.0.15 weight=10; # &#125; # ip_hash：每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 # 例如： upstream bakend &#123; # ip_hash; # server 192.168.0.14:88; # server 192.168.0.15:80; # &#125; # fair（第三方）：按后端服务器的响应时间来分配请求，响应时间短的优先分配。 # 例如： upstream backend &#123; # server server1; # server server2; # fair; # &#125; # url_hash（第三方）：按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 # 例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 # upstream backend &#123; # server squid1:3128; # server squid2:3128; # hash $request_uri; # hash_method crc32; # &#125; # tips: # upstream bakend&#123; # 定义负载均衡设备的Ip及设备状态 # ip_hash; # server 127.0.0.1:9090 down; # server 127.0.0.1:8080 weight=2; # server 127.0.0.1:6060 max_fails=10 fail_timeout=60s; # server 127.0.0.1:7070 backup; # &#125; # 在需要使用负载均衡的server中增加 proxy_pass http://bakend/; # 每个设备的状态设置为: # 1.down表示单前的server暂时不参与负载 # 2.weight为weight越大，负载的权重就越大。 # 3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 # fail_timeout:max_fails次失败后，暂停的时间。 # 4.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 # nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 # client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug # client_body_temp_path设置记录文件的目录 可以设置最多3层目录 # location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 server &#123; ... &#125; # server模块&#125;# ------------------------------ upstream模块 end ------------------------------ 2.3 server模块12345678910111213141516171819202122232425... # 全局块events &#123; ... &#125; # events块http &#123; ... # http块 upstream &#123; ... &#125; # upstream模块 # ------------------------------ server模块 start ------------------------------ server &#123; # 虚拟主机的配置 listen 80; # 监听端口 server_name www.test.com test.com; # 监听地址，域名可以有多个，用空格隔开 index index.html index.htm; # 指定网站初始页 root /data/www/test; # 指定网站根目录 charset utf-8; rewrite ^(.*)$ https://www.test.com$1 permanent; # URL重写 access_log /usr/local/nginx/logs/host.access.log; # 定义本虚拟主机的访问日志 &#125; server &#123; location ...&#123; ... &#125; # location模块 &#125; # ------------------------------ server模块 end ------------------------------ # include /etc/nginx/conf.d/*.conf; # 导入其他server配置 URL重写（rewrite）配置及信息详解： 语法：rewrite &lt;regex&gt; &lt;replacement&gt; [flag]; regex：perl兼容正则表达式语句进行规则匹配 replacement：将正则匹配的内容替换成replacement flag rewrite支持的flag标记 last:本条规则匹配完成后，继续向下匹配新的location URI规则 break:本条规则匹配完成即终止，不再匹配后面的任何规则 redirect:返回302临时重定向，浏览器地址会显示跳转后的URL地址 permanent:返回301永久重定向，浏览器地址栏会显示跳转后的URL地址 使用位置：server, location, if 2.4 location模块语法：location [ = | ~ | ~* | ^~ | @] /uri/ &#123; configuration &#125;。匹配模式分为两种：普通字符串（literal string）和正则表达式（regular expression），其中 ~ 和 ~* 用于正则表达式， 其他前缀和无任何前缀都用于普通字符串。 前缀含义： =：&#x3D;开头表示精确前缀匹配，只有完全匹配才能生效。 ~：~开头表示区分大小写的正则匹配。 ~*：~*开头表示不区分大小写的正则匹配。 ^~：^~开头表示普通字符串匹配上以后不再进行正则匹配。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354... # 全局块events &#123; ... &#125; # events块http &#123; ... # http块 upstream &#123; ... &#125; # upstream模块 server &#123; # 虚拟主机的配置 listen 80; # 监听端口 server_name www.test.com test.com; # 监听地址 # ------------------------------ location模块 start ------------------------------ ### alias: 别名配置 location /test/ &#123; alias /usr/local/; # 在匹配到location配置的URL路径后，指向alias配置的路径 &#125; location ~* /img/(.+\\.(gif|jpg|jpeg|png|bmp|swf)) &#123; alias /usr/local/images/$1; # 请求中只要能匹配到正则，比如/img/test.png或者/resource/img/test.png，都会转换为请求/usr/local/images/test.png。 &#125; ### root: 根路径配置 location /test/ &#123; root /usr/local/; # 用于访问文件系统，在匹配到location配置的URL路径后，指向root配置的路径，并把请求路径附加到其后 &#125; ### proxy_pass: 反向代理配置 location /test/ &#123; proxy_pass http://127.0.0.1:8080/; # 用于代理请求，适用于前后端负载分离或多台机器、服务器负载分离的场景 &#125; ### JS和CSS缓存时间设置 location ~ .*.(js|css)?$ &#123; expires 1h; # JS和CSS缓存时间设置 &#125; ### 反向代理的其他配置 location / &#123; # 对 &quot;/&quot; 启用反向代理 proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; # 后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 以下是一些反向代理的配置，可选。 proxy_set_header Host $host; client_max_body_size 10m; # 允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; # 缓冲区代理缓冲用户端请求的最大字节数， proxy_intercept_errors on; # 表示使nginx阻止HTTP应答代码为400或者更高的应答。 proxy_connect_timeout 90; # 后端服务器连接的超时时间_发起握手等候响应超时时间 proxy_send_timeout 90; # 后端服务器数据回传时间(代理发送超时) proxy_read_timeout 90; # 连接成功后，后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; # 设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; # proxy_buffers缓冲区，网页平均在32k以下的设置，设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k proxy_busy_buffers_size 64k; # 高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; # 设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 # 设定缓存文件夹大小大于这个值将从upstream服务器传 &#125; # ------------------------------ location模块 end ------------------------------ &#125; 3. Nginx日志配置Nginx日志主要分为两种：access_log(访问日志)和error_log(错误日志)。 访问日志可以记录用户的IP地址、浏览器的信息，请求的处理时间等信息。 access_log指令的作用域：http，server，location，limit_except。 错误日志记录了访问出错的信息，可以帮助我们定位错误的原因。 error_log指令的作用域：main， http, mail, stream, server, location 3.1 Nginx访问日志access_log访问日志主要记录客户端的请求。客户端向Nginx服务器发起的每一次请求都记录在这里。客户端IP，浏览器信息，referer，请求处理时间，请求URL等都可以在访问日志中得到。可以通过log_format指令定义具体要记录哪些信息。 语法： 1234# 关闭访问日志access_log off; # 设置访问日志# access_log path [format [buffer=size] [gzip[=level]] [flush=time] [if=condition]]; path：指定日志的存放位置。 format：指定日志的格式。默认使用预定义的combined。 buffer：用来指定日志写入时的缓存大小。默认是64k。 gzip：日志写入前先进行压缩。压缩率可以指定，从1到9数值越大压缩比越高，同时压缩的速度也越慢。默认是1。 flush：设置缓存的有效时间。如果超过flush指定的时间，缓存中的内容将被清空。 if条件判断：如果指定的条件计算为0或空字符串，那么该请求不会写入日志。 3.1.1 使用log_format自定义日志格式Nginx预定义了名为combined日志格式，如果没有明确指定日志格式默认使用该格式： 1log_format combined &#x27;$remote_addr - $remote_user [$time_local] &#x27;&#x27;&quot;$request&quot; $status $body_bytes_sent &#x27;&#x27;&quot;$http_referer&quot; &quot;$http_user_agent&quot;&#x27;; 如果不想使用Nginx预定义的格式，可以通过log_format指令来自定义: 1log_format name [escape=default|json] string ...; name 格式名称。在access_log指令中引用。 escape 设置变量中的字符编码方式是json还是default，默认是default。 string 要定义的日志格式内容。该参数可以有多个。参数中可以使用Nginx变量。 实例： 12345log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;access_log /usr/local/nginx/logs/access.log main; 日志格式设定常用Nginx变量： $remote_addr &#x2F; $http_x_forwarded_for：用以记录客户端的ip地址； $remote_user：用来记录客户端用户名称； $time_local： 用来记录访问时间与时区； $request： 用来记录请求的url与http协议； $status： 用来记录请求状态；成功是200， $body_bytes_sent：记录发送给客户端文件主体内容大小； $http_referer：用来记录从那个页面链接访问过来的； $http_user_agent：记录客户浏览器的相关信息； 3.2 Nginx错误日志error_log错误日志在Nginx中是通过error_log指令实现的。该指令记录服务器和请求处理过程中的错误信息。 语法：error_log file [level];，默认是：error_log logs/error.log error;；级别有：[ debug|info|notice|warn|error|crit|alert|emerg ]。 基本用法： 1error_log /usr/local/nginx/logs/error.log info; 3.3 通过open_log_file_cache指令来设置日志文件描述符缓存Nginx中通过access_log和error_log指令配置访问日志和错误日志，通过log_format我们可以自定义日志格式。 如果日志文件路径中使用了变量，我们可以通过open_log_file_cache指令来设置缓存，提升性能。它可以配置在http、server、location作用域中。 语法: open_log_file_cache max=N [inactive=time] [min_uses=N] [valid=time]; max 设置缓存中最多容纳的文件描述符数量，如果被占满，采用LRU算法将描述符关闭。 inactive 设置缓存存活时间，默认是10s。 min_uses 在inactive时间段内，日志文件最少使用几次，该日志文件描述符记入缓存，默认是1次。 valid：设置多久对日志文件名进行检查，看是否发生变化，默认是60s。 off：不使用缓存。默认为off。 基本用法: 1open_log_file_cache max=1000 inactive=20s valid=1m min_uses=2; 4. Nginx配置文件实例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100user cmuser;worker_processes 2;events &#123; worker_connections 8096; multi_accept on; use epoll;&#125;worker_rlimit_nofile 40000;http &#123; include mime.types; default_type application/octet-stream; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log /data/runtime/nginx/logs/access.log main; error_log /data/runtime/nginx/logs/error.log error; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 15; # ------------ www.test.com的访问配置 -------------------- upstream www.test.com &#123; ip_hash; server 10.104.60.165:8100; server 10.104.60.166:8100; &#125; server &#123; listen 80; server_name www.test.com; charset utf-8; rewrite ^(.*)$ https://www.test.com$1 permanent; &#125; server &#123; # 配置SSL证书 listen 443; server_name www.test.com; ssl on; ssl_certificate sslkey/_.test.com_bundle.crt; ssl_certificate_key sslkey/_.test.com.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers AESGCM:ALL:!DH:!EXPORT:!RC4:+HIGH:!MEDIUM:!LOW:!aNULL:!eNULL; ssl_prefer_server_ciphers on; # 上传文件的路径配置 location /uploadFile/ &#123; alias /mnt/newdatadrive/nfs_client/upload/; &#125; location / &#123; proxy_pass http://www.test.com; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 10m; client_body_buffer_size 256k; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 8k; proxy_buffers 4 64k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; &#125; &#125; # ------------------ 静态文件夹 ----------------- server &#123; listen 80; server_name static.test.com; charset utf-8; location / &#123; root /mnt/newdatadrive/static_files; &#125; &#125; server &#123; listen 443; server_name static.test.com; ssl on; ssl_certificate sslkey/_.test.com_bundle.crt; ssl_certificate_key sslkey/_.test.com.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers AESGCM:ALL:!DH:!EXPORT:!RC4:+HIGH:!MEDIUM:!LOW:!aNULL:!eNULL; ssl_prefer_server_ciphers on; location / &#123; root /mnt/newdatadrive/static_files; &#125; &#125; # ------------------ 引入外部其他配置 ----------------- include /etc/nginx/conf.d/*.conf;&#125; 附. Nginx启动命令运行可执行文件就可以启动nginx，可以使用-c参数指定配置文件，比如: 1nginx -c /usr/local/nginx/conf/nginx.conf 如果nginx已经启动，可以使用-s参数的可执行命令来控制： 123456nginx -s [reload | stop | quit | reopen]# stop — 直接关闭 nginx# quit — 会在处理完当前正在的请求后退出，也叫优雅关闭# reload — 重新加载配置文件，相当于重启# reopen — 重新打开日志文件 比如，重载配置文件： 1nginx -s reload","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Nginx","slug":"Nginx","permalink":"http://chaooo.github.io/tags/Nginx/"}]},{"title":"「环境配置」Centos 8 私人Git服务器搭建(Gogs)","date":"2020-11-25T10:11:23.000Z","path":"2020/11/25/env-centos8-git-gogs.html","text":"1. 创建gogs用户为Gogs创建一个MySQL用户gogs 1234567#先创建一个MySQL用户use mysql;create user &#x27;gogs&#x27;@&#x27;localhost&#x27; identified by &#x27;J5p&quot;;~OVazNl%y)?&#x27;; #再进行授权grant all privileges on *.* to &#x27;gogs&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;J5p&quot;;~OVazNl%y)?&#x27; with grant option;flush privileges; 为Gogs创建一个系统用户git 123456# 创建一个用户[root@localhost ~]# useradd -mU git -s /bin/bash[root@localhost ~]# passwd git# 切换到git用户[root@localhost ~]# su git[git@localhost ~]$ cd /home/git 2. 下载安装下载Gogs二进制安装包 123wget https://dl.gogs.io/0.12.3/gogs_0.12.3_linux_amd64.tar.gz# 解压安装包tar -zxvf gogs_0.12.3_linux_amd64.tar.gz 使用Gogs脚本创建gogs数据库 12345678910111213141516# 切换目录到gogs脚本文件夹cd /home/git/gogs/scripts/# 使用mysql.sql创建gogs数据库，这里会要求输入密码mysql -u root -p &lt; mysql.sql# 假如执行这条命令会报错【ERROR 1115 (42000) at line 2: Unknown character set: &#x27;utf8mb4&#x27;】的话继续执行下面这个可选操作,在重新执行上面的命令。# 修改mysql.sqlvim mysql.sql/*************** 原文 ***************/DROP DATABASE IF EXISTS gogs;CREATE DATABASE IF NOT EXISTS gogs CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;/*************** 修改为 *************/DROP DATABASE IF EXISTS gogs;CREATE DATABASE IF NOT EXISTS gogs CHARACTER SET utf8 COLLATE utf8_general_ci;/*************** 结束 ***************/ 开放端口： 12firewall-cmd --zone=public --add-port=3000/tcp --permanentfirewall-cmd --reload 启动Gogs服务 1/home/git/gogs/gogs web 访问Gogs网站 http://你的服务器IP:3000 填写正确的配置信息，点击 “立即安装” 3. 配置开机自启动配置Gogs服务自启动 1234567# 关闭gogs服务ctrl + c # 切换到root用户su rootcp /home/git/gogs/scripts/systemd/gogs.service /usr/lib/systemd/system/systemctl enable gogs.servicesystemctl start gogs.service 若CentOS 8开机启动gogs失败，先禁用SELinux: 1vim /etc/selinux/config 将SELinux属性设置为Disabled，如下所示： 1234567891011# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of these three values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected.# mls - Multi Level Security protection.SELINUXTYPE=targeted 重启系统： 1reboot","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"},{"name":"Git","slug":"Git","permalink":"http://chaooo.github.io/tags/Git/"},{"name":"Gogs","slug":"Gogs","permalink":"http://chaooo.github.io/tags/Gogs/"}]},{"title":"「环境配置」使用Nexus搭建Maven私服（CentOS 8）","date":"2020-11-25T10:11:23.000Z","path":"2020/11/25/env-centos8-nexus.html","text":"Maven私服 就是在内网架设一个Maven仓库服务器，在代理远程仓库的同时维护本地仓库。当我们需要下载一些构件（artifact）时，如果本地仓库没有，再去私服下载，私服没有，再去中央仓库下载。 Nexus是一个专门的 Maven仓库管理软件。它提供了强大的仓库管理功能，构件搜索功能；它占用较少的内存，基于REST，基于简单文件系统而非数据库。 1. 安装Nexus服务1.1 前置条件：jdk1.8环境 1.2 下载Nexus1wget https://sonatype-download.global.ssl.fastly.net/repository/repositoryManager/3/nexus-3.18.1-01-unix.tar.gz 下载失败的话，可以试试这个： 12百度云: https://pan.baidu.com/s/16IfFUtL3W0YGciS-XPlPfQ提取码: naxi 1.3 安装Nexus解压到安装目录（/data/apps/nexus/），会得到两个文件夹：nexus-3.18.1-01（nexus 服务目录）、sonatype-work（私有库目录）。 123456[root@localhost ~]# mkdir -p /data/apps/nexus/[root@localhost ~]# mv nexus-3.18.1-01-unix.tar.gz /data/apps/nexus/nexus-3.18.1-01-unix.tar.gz[root@localhost ~]# cd /data/apps/nexus/[root@localhost nexus]# tar -zvxf nexus-3.18.1-01-unix.tar.gz[root@localhost nexus]# lsnexus-3.18.1-01 nexus-3.18.1-01-unix.tar.gz sonatype-work 进入 nexus-3.18.1-01 文件夹，其中 etc&#x2F;nexus-default.properties 文件配置端口（默认为 8081）和 work 目录信息，可以按需修改。 1.4 开放端口并启动服务1234567[root@localhost nexus-3.18.1-01]# firewall-cmd --zone=public --add-port=8081/tcp --permanent &amp;&amp; firewall-cmd --reload[root@localhost nexus-3.18.1- /data/apps/nexus/nexus-3.18.1-01/bin[root@localhost bin]# ./nexus startWARNING: ************************************************************WARNING: Detected execution as &quot;root&quot; user. This is NOT recommended!WARNING: ************************************************************Starting nexus 首次启动，初始账号为：admin，查看初始密码： 12[root@localhost bin]# cat /data/apps/nexus/sonatype-work/nexus3/admin.passworde6c47f75-dc91-4f87-a2a9-0188df6e4b7c 2. 配置Nexus2.1 登录NexusNexus服务启动以后，使用浏览器访问http://IP:8081/，并用初始账号密码登录，登陆后会让我们先修改初始密码。 仓库浏览在左侧菜单栏Browse，这里有多种仓库： maven-central：maven 中央库，默认从 https://repo1.maven.org/maven2/ 拉取 jar maven-releases：私库发行版 jar，初次安装请将 Deployment policy 设置为 Allow redeploy maven-snapshots：私库快照（调试版本）jar maven-public：仓库分组，把上面三个仓库组合在一起对外提供服务，在本地 maven 基础配置 settings.xml 或项目 pom.xml 中使用 仓库类型说明： group：这是一个仓库聚合的概念，用户仓库地址选择 Group 的地址，即可访问 Group 中配置的，用于方便开发人员自己设定的仓库。maven-public 就是一个 Group 类型的仓库，内部设置了多个仓库，访问顺序取决于配置顺序，3.x 默认为 Releases、Snapshots、Central，当然你也可以自己设置。 hosted：私有仓库，内部项目的发布仓库，专门用来存储我们自己生成的 jar 文件 snapshots：本地项目的快照仓库 releases： 本地项目发布的正式版本 proxy：代理类型，从远程中央仓库中寻找数据的仓库（可以点击对应的仓库的 Configuration 页签下 Remote Storage 属性的值即被代理的远程仓库的路径），如可配置阿里云 maven 仓库 central：中央仓库 2.2 设置 配置Releases版本可重复上传: Deployment pollcy --&gt; Allow redeploy。 增加一个代理仓库，使用的是阿里云公共仓库。首先点击Create repository按钮开始创建一个仓库，类型选择 maven2（proxy）。 配置阿里云地址http://maven.aliyun.com/nexus/content/groups/public/，并创建。 阿里云代理仓库创建完毕后，我们编辑maven-public，将其添加到放入group中，并调整优先级，然后保存。 点击maven-public条目的copy按钮即可拷贝私服地址 3. Maven配置使用私服3.1 使用配置（下载依赖）两种方式：①通过Maven的setting.xml文件配置（全局模式），②通过项目的pom.xml文件配置（项目独享模式）。 注意：若pom.xml和setting.xml同时配置了，以pom.xml为准。 全局模式：通过Maven的setting.xml文件配置 123456789101112&lt;mirrors&gt; &lt;mirror&gt; &lt;!--该镜像的唯一标识符。id用来区分不同的mirror元素。 --&gt; &lt;id&gt;maven-public&lt;/id&gt; &lt;!--镜像名称 --&gt; &lt;name&gt;maven-public&lt;/name&gt; &lt;!--*指的是访问任何仓库都使用我们的私服--&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;!--该镜像的URL。构建系统会优先考虑使用该URL，而非使用默认的服务器URL。 --&gt; &lt;url&gt;http://192.168.2.100:8081/repository/maven-public/&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 项目独享模式：通过项目的pom.xml文件配置 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;maven-nexus&lt;/id&gt; &lt;name&gt;maven-nexus&lt;/name&gt; &lt;url&gt;http://192.168.2.100:8081/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 3.2 发布配置（发布依赖） 修改setting.xml文件，指定releases和snapshots server的用户名和密码： 123456789101112&lt;servers&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;456&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 在发布依赖项目的pom.xml文件中加入distributionManagement节点（这里repository id需要和上一步里的server id名称保持一致）： 123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;name&gt;Releases&lt;/name&gt; &lt;url&gt;http://192.168.2.100:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;name&gt;Snapshot&lt;/name&gt; &lt;url&gt;http://192.168.2.100:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 执行mvn deploy命令发布。 登录Nexus，查看对应的仓库就能看到发布的依赖包了。 发布到的仓库说明：①若项目版本号末尾带有-SNAPSHOT，则会发布到snapshots快照版本仓库。②若项目版本号末尾带有-RELEASES或什么都不带，则会发布到releases正式版本仓库。 4. 设置Nexus开机自启1vim /usr/lib/systemd/system/nexus.service nexus.service： 123456789101112[Unitt]Description=nexus serviceAfter=network.target[Service]Type=forkingExecStart=/data/apps/nexus/nexus-3.18.1-01/bin/nexus startExecReload=/data/apps/nexus/nexus-3.18.1-01/bin/nexus restartExecStop=/data/apps/nexus/nexus-3.18.1-01/bin/nexus stop[Install]WantedBy=multi-user.target 123456# 开启开机启动systemctl enable nexus.service# 启动服务systemctl start nexus.service# 停止服务systemctl stop nexus.service","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"},{"name":"Nexus","slug":"Nexus","permalink":"http://chaooo.github.io/tags/Nexus/"},{"name":"Maven","slug":"Maven","permalink":"http://chaooo.github.io/tags/Maven/"}]},{"title":"「环境配置」Tomcat9安装及多实例多应用配置(CentOS 8)","date":"2020-11-20T10:11:23.000Z","path":"2020/11/20/env-centos8-tomcat9.html","text":"1. Tomcat9安装使用wget获取： 1wget https://mirrors.aliyun.com/apache/tomcat/tomcat-9/v9.0.41/bin/apache-tomcat-9.0.41.tar.gz 下载之后如果没有进入别的文件夹，压缩包一般是在&#x2F;root下面。进入&#x2F;root下面进行解压： 1tar xzf apache-tomcat-9.0.41.tar.gz 然后复制到指定的文件夹下(在&#x2F;data&#x2F;apps下面创建了一个tomcat的文件夹)： 12mkdir -p /data/appsmv apache-tomcat-9.0.41 /data/apps/tomcat 然后配置jvm内存参数（也可以不用配置）： 1vim /data/apps/tomcat/bin/setenv.sh 在setenv.sh中写入如下语句： 1JAVA_OPTS=&#x27;-Djava.security.egd=file:/dev/./urandom -server -Xms512m -Xmx1024m -Dfile.encoding=UTF-8&#x27; 开放端口 1234#开放8080端口firewall-cmd --add-port=8080/tcp --permanent &amp;&amp; firewall-cmd --reload#重新加载防火墙规则firewall-cmd --reload 启动&#x2F;停用 1234#启动cd /data/apps/tomcat/bin &amp;&amp; sh startup.sh#停用cd /data/apps/tomcat/bin &amp;&amp; sh shutdown.sh 通过浏览器访问 【ip】:8080 2. Tomcat多实例配置Tomcat多实例部署的好处在于升级方便，配置及安装文件间互不影响。 123456 安装路径 实例位置【CATALINA_HOME】 【CATALINA_BASE】 |————bin &lt;-------------| |————lib &lt;-------------| |————（conf,webapps,logs,temp,work） 创建一个实例存放路径 1mkdir -p /data/runtime/tomcat-instance 新建两个tomcat实例，把安装路径下的conf,webapps,logs,temp,work文件拷贝到实例中： 123456cd /data/runtime/tomcat-instancemkdir tomcat1 tomcat2# cd /data/apps/tomcat/cp conf/ webapps/ temp/ logs/ work/ -rt /data/runtime/tomcat-instance/tomcat1cp conf/ webapps/ temp/ logs/ work/ -rt /data/runtime/tomcat-instance/tomcat2 新建 Tomcat实例启动&#x2F;停止脚本 123cd /data/runtime/tomcat-instancevim tomcat1.shvim tomcat2.sh tomcat1.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#!/bin/bash. /etc/rc.d/init.d/functionssource /etc/profileexport CATALINA_BASE=/data/runtime/tomcat-instance/tomcat1export CATALINA_HOME=/data/apps/tomcatexport CATALINA_PID=$CATALINA_BASE/CATALINA_PIDexport JAVA_OPTS=&quot;-server -Xms256m -Xmx512m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=256m&quot;sname=tomcat1mark=$CATALINA_BASE/temppid=`ps -ef | grep &quot;$mark&quot; | grep -v &quot;grep $mark&quot; | awk -F &quot; &quot; &#x27;&#123;print $2&#125;&#x27;`function start()&#123; if [ -f &quot;$CATALINA_PID&quot; ]; then rm -rf $CATALINA_PID fi echo &quot;$sname start ......&quot; $CATALINA_HOME/bin/catalina.sh start 2&gt;/dev/null pid=`ps -ef | grep &quot;$mark&quot; | grep -v &quot;grep $mark&quot; | awk -F &quot; &quot; &#x27;&#123;print $2&#125;&#x27;` echo &quot;PID: $pid&quot;&#125;function stop()&#123; echo &quot;$sname stop ......&quot; if [ $pid ]; then kill -9 $pid rm -f $CATALINA_PID fi&#125;function status()&#123; if [ $pid ]; then echo &quot;$sname run ......&quot; else echo &quot;$sname stop ......&quot; fi echo `ps -ef | grep &quot;$CATALINA_BASE&quot; | grep -v &quot;grep $CATALINA_BASE&quot;`&#125;case $1 instart) start ;;stop) stop ;;restart) stop start ;;status) status ;;*) echo &quot;Option in (start|stop|restart|status)&quot; ;;esac tomcat2.sh参考tomcat1.sh 赋予权限 1chmod 777 tomcat1.sh tomcat2.sh 配置实例server.xml端口 Server Port：该端口用于监听关闭tomcat的shutdown命令，默认为8005 Connector Port：该端口用于监听HTTP的请求，默认为8080 AJP Port：该端口用于监听AJP（ Apache JServ Protocol ）协议上的请求，通常用于整合Apache Server等其他HTTP服务器，默认为8009 Redirect Port：重定向端口，出现在Connector配置中，如果该Connector仅支持非SSL的普通http请求，那么该端口会把 https 的请求转发到这个Redirect Port指定的端口，默认为8443； 这里tomcat1实例保持默认，把tomcat2实例的Server Port改为了8006，Connector Port改为了 8081: 1234cd /data/runtime/tomcat-instance/tomcat2vim conf/server.xml# 8081端口firewall-cmd --add-port=8081/tcp --permanent &amp;&amp; firewall-cmd --reload 分别在 tomcat1、tomcat2 的 webapps&#x2F;ROOT 目录下放入了页面文件。 启动两个实例 123cd /data/runtime/tomcat-instance./tomcat1.sh start./tomcat2.sh start 通过浏览器：【ip】:8080/，【ip】:8081/，如：192.168.2.100:8080 3. nginx使用https代理tomcat修改nginx配置文件，在server中添加如下内容： 1234567891011121314location /tomcat1/ &#123; proxy_pass http://127.0.0.1:8080/; proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Scheme $scheme;&#125;location /tomcat2/ &#123; proxy_pass http://127.0.0.1:8081/; proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Scheme $scheme;&#125; 通过浏览器访问：【ip】/tomcat1/，【ip】/tomcat2/，如：192.168.2.100&#x2F;tomcat1&#x2F; tips: Tomcat默认上传到服务器中文件的权限是- -rw-r-----640权限，其他人不可读。到Tomcat中bin目录下面，修改catalina.sh中UMASK=&quot;0027&quot;为UMASK=&quot;0022&quot;。12345# Set UMASK unless it has been overriddenif [ -z &quot;$UMASK&quot; ]; then UMASK=&quot;0022&quot;fiumask $UMASK","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"},{"name":"Tomcat9","slug":"Tomcat9","permalink":"http://chaooo.github.io/tags/Tomcat9/"}]},{"title":"「环境配置」CentOS 8 安装和配置 NFS 服务器","date":"2020-08-10T10:11:23.000Z","path":"2020/08/10/env-centos8-nfs.html","text":"网络文件系统（NFS）是一个分布式文件系统协议，它允许你通过网络共享远程文件夹。 NFS 协议默认是不加密的，不提供用户身份鉴别。服务端通过限定客户端的 IP 地址和端口来限制访问。 NFS 特点： 基于TCP&#x2F;IP协议，服务于linux之间资源共享 将远程主机上共享资源挂载到本地目录，使得像使用本地文件一样方便。 1. 建立 NFS 服务器1.1 安装 NFS 服务端（CentOS8中默认安装了nfs-utils软件包） 用rpm检查是否有nfs-utils的包已安装： 12[root@localhost ~]# rpm -qa | grep nfs-utilsnfs-utils-2.3.3-31.el8.x86_64 如果没有安装 执行如下命令安装： 1[root@localhost ~]# dnf install nfs-utils 启用并启动 NFS 服务： 12[root@localhost ~]# systemctl enable --now nfs-serverCreated symlink /etc/systemd/system/multi-user.target.wants/nfs-server.service → /usr/lib/systemd/system/nfs-server.service. 默认情况下，在 CentOS8 上，NFS3 和 NFS4 都可以用，NFS2 被禁用。想要验证，运行下面的cat命令： 12[root@localhost ~]# cat /proc/fs/nfsd/versions-2 +3 +4 +4.1 +4.2 NFS 服务器配置选项在/etc/nfsmount.conf和/etc/nfs.conf文件中。默认的设置足够满足我们的要求。 1.2 创建文件系统 创建共享目录，并开放目录权限 123456[root@localhost ~]# mkdir /nfs_database[root@localhost ~]# chmod 777 /nfs_database[root@localhost ~]# cd /nfs_database/ &amp;&amp; echo &quot;this is nfs database test !!&quot; &gt; nfs_test.txtcd /nfs_database/ &amp;&amp; echo &quot;this is nfs database test chmod 777 /nfs_database&quot; &gt; nfs_test.txt[root@localhost nfs_database]# lsnfs_test.txt 编辑NFS服务程序配置文件(允许IP地址192.168.2.*的所有主机访问NFS共享资源文件夹) 12[root@localhost ~]# vim /etc/exports/nfs_database 192.168.2.* (rw,no_root_squash,async) 配置完成后，使nfs配置生效，使用exportfs实用程序有选择地导出目录，而无需重新启动NFS服务 1234#使用exportfs实用程序有选择地导出目录，而无需重新启动NFS服务[root@localhost ~]# exportfs -rv#查看当前配置为nfs共享的目录及其状态[root@localhost ~]# exportfs -v NFS配置文件参数 作用 ro 只读（read only） rw 读写（read write） root_squash 当NFS客户端以root管理员访问时，映射为NFS服务器匿名用户 no_root_squash 当NFS客户端以root管理员访问时，映射为NFS服务器的root管理员 all_squash 表示客户机所有用户访问时，映射为NFS服务器匿名用户 sync 同时将数据写入到内存与硬盘中，保证不丢失数据 async 优先将数据保存到内存，然后再写入硬盘，效率更高，但可能丢失数据 启动和启用rpcbind服务程序 123456# 安装rpcbind[root@localhost ~]# yum install -y rpcbind# 重启并启用rpc服务程序[root@localhost ~]# systemctl restart rpcbind &amp;&amp; systemctl enable rpcbind# 重启并启用NFS服务程序[root@localhost ~]# systemctl restart nfs-server &amp;&amp; systemctl enable nfs-server 1.3 防火墙设置将该服务添加到防火墙中进行放行。 部署nfs服务不仅需要nfs服务软件包，还需要rpc-bind服务和mountd服务。因为nfs服务需要向客户端广播地址和端口信息，nfs客户端需要使用mount对远程nfs服务器目录进行挂载。 12345678[root@localhost ~]# firewall-cmd --permanent --zone=public --add-service=nfssuccess[root@localhost ~]# firewall-cmd --permanent --zone=public --add-service=rpc-bindsuccess[root@localhost ~]# firewall-cmd --permanent --zone=public --add-service=mountdsuccess[root@localhost ~]# firewall-cmd --reloadsuccess 2. 客户端配置 查询远程nfs服务器是否能够连通 123[root@localhost ~]# showmount -e 192.168.2.100Export list for 192.168.2.100:/nfs_database (everyone) showmount参数 作用 -e 显示NFS服务器共享列表 -a 显示本地挂载的文件资源情况 -v 显示版本号 创建本地nfs专用共享目录 12[root@localhost ~]# mkdir /nfs_database[root@localhost ~]# chmod 777 /nfs_database 将远程nfs服务器共享目录挂载到本地创建的nfs共享目录 1[root@localhost ~]# mount -t nfs 192.168.2.100:/nfs_database /nfs_database mount参数： -t：使用TCP协议 nfs：nfs服务 192.168.2.100:/nfs_database：远程nfs服务器资源共享目录 /nfs-database：本地资源共享目录 本机查看共享文件 1234[root@localhost ~]# cd /nfs_database[root@localhost nfs_database]# ll总用量 4-rw-r--r-- 1 root root 50 12月 17 14:52 nfs_test.txt","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"},{"name":"NFS","slug":"NFS","permalink":"http://chaooo.github.io/tags/NFS/"}]},{"title":"「Spring」SpringBoot 整合 FastDFS","date":"2020-08-02T12:25:23.000Z","path":"2020/08/02/spring-boot-fastdfs.html","text":"1. 在maven项目pom.xml中添加依赖12345&lt;dependency&gt; &lt;groupId&gt;com.github.tobato&lt;/groupId&gt; &lt;artifactId&gt;fastdfs-client&lt;/artifactId&gt; &lt;version&gt;1.27.2&lt;/version&gt;&lt;/dependency&gt; 2. 在application.yml中添加配置123456789#FDFS配置fdfs: so-timeout: 5000 #上传的超时时间 connect-timeout: 2000 #连接超时时间 thumb-image: #缩略图生成参数 width: 150 height: 150 tracker-list: #TrackerList参数,支持多个 - 192.168.2.100:22122 3. 编写FastDFS工具类123456789101112131415161718192021222324252627@Componentpublic class FastDfsUtil &#123; private static ThumbImageConfig thumbImageConfig; private static FastFileStorageClient fastFileStorageClient; private static FdfsWebServer fdfsWebServer; public FastDfsUtil(ThumbImageConfig thumbImageConfig, FastFileStorageClient fastFileStorageClient, FdfsWebServer fdfsWebServer) &#123; FastDfsUtil.thumbImageConfig = thumbImageConfig; FastDfsUtil.fastFileStorageClient = fastFileStorageClient; FastDfsUtil.fdfsWebServer = fdfsWebServer; &#125; /** * @param multipartFile 文件对象 * @return 返回文件地址 * @author qbanxiaoli * @description 上传文件 */ @SneakyThrows public static String uploadFile(MultipartFile multipartFile) &#123; StorePath storePath = fastFileStorageClient.uploadFile(multipartFile.getInputStream(), multipartFile.getSize(), FilenameUtils.getExtension(multipartFile.getOriginalFilename()), null); return storePath.getFullPath(); &#125; // 其他代码...&#125; 4. 编写测试Controller12345678910111213141516171819202122232425262728293031@Slf4j@RestControllerpublic class FileController &#123; /** * 上传图片并保存 * @param file * @param request * @return * @throws IOException */ @PostMapping(&quot;/upload&quot;) public Map&lt;String, Object&gt; uploadFile(MultipartFile[] file, HttpServletRequest request) throws IOException &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); if (file != null &amp;&amp; file.length &gt; 0) &#123; String saveFile = null; for (int i = 0; i &lt; file.length; i++) &#123; MultipartFile partFile = file[i]; // 保存文件 saveFile = FastDfsUtil.uploadFile(partFile);; saveFile = &quot;http://192.168.2.100/&quot; + saveFile; &#125; map.put(&quot;data&quot;, saveFile); map.put(&quot;msg&quot;, &quot;上传成功&quot;); log.info(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;图片上传成功:&quot; + saveFile); &#125; else &#123; map.put(&quot;msg&quot;, &quot;上传失败&quot;); log.info(&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;图片上传失败&quot;); &#125; return map; &#125;&#125; 5. 编写前端页面代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;body&gt; &lt;div&gt; 图片上传： &lt;input type=&quot;file&quot; name=&quot;file&quot; value=&quot;&quot; id=&quot;input-file&quot; accept=&quot;image/png,image/jpeg,image/gif,image/jpg&quot;&gt; &lt;/div&gt; &lt;div&gt; 图片回显： &lt;img id=&quot;upload-file&quot; src=&quot;&quot; alt=&quot;&quot; width=&quot;300&quot;&gt; &lt;/div&gt;&lt;script charset=&quot;utf-8&quot; type=&quot;text/javascript&quot; src=&quot;/lib/jquery-3.5.1.min.js&quot;&gt;&lt;/script&gt;&lt;script charset=&quot;utf-8&quot; type=&quot;text/javascript&quot;&gt;$(&#x27;#input-file&#x27;).on(&quot;change&quot;, function() &#123; var file = this.files[0]; var formData = new FormData(); formData.append(&#x27;file&#x27;, file); // 开始上传 fileUpload(&quot;/upload&quot;, formData); // 测试读取图片(本地显示) var fileReader = new FileReader(); fileReader.readAsDataURL(file); fileReader.onloadend = function(fEvent)&#123; var src = fEvent.target.result; console.log(src); $(&quot;#upload-file&quot;).attr(&quot;src&quot;, src); &#125;&#125;);function fileUpload(url, formData)&#123; $.ajax(&#123; url: url, type: &#x27;POST&#x27;, cache: false, data: formData, processData: false, contentType: false, dataType: &quot;json&quot;, success: function (res) &#123; console.log(res); &#125;, error: function (xhr, type, errorThrown) &#123; console.log(&quot;照片上传失败&quot;) &#125; &#125;);&#125;&lt;/script&gt; 6. 浏览器访问页面测试 demo源码地址：https://gitee.com/chaoo/fastdfs-demo.git","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"FastDFS","slug":"FastDFS","permalink":"http://chaooo.github.io/tags/FastDFS/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"}]},{"title":"「环境配置」Centos8 安装 FastDFS 6.06","date":"2020-08-02T10:11:23.000Z","path":"2020/08/02/env-centos8-fastdfs.html","text":"FastDFS是一款开源的分布式文件系统，功能主要包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了文件大容量存储和高性能访问的问题。FastDFS特别适合以文件为载体的在线服务，如图片、视频、文档等等。 FastDFS为互联网应用量身定做，解决大容量文件存储问题，追求高性能和高扩展性。FastDFS可以看做是基于文件的key value存储系统，key为文件ID，value为文件内容，因此称作分布式文件存储服务更为合适。 FastDFS特点如下： 分组存储，简单灵活； 对等结构，不存在单点； 文件ID由FastDFS生成，作为文件访问凭证。FastDFS不需要传统的name server或meta server； 大、中、小文件均可以很好支持，可以存储海量小文件； 一台storage支持多块磁盘，支持单盘数据恢复； 提供了nginx扩展模块，可以和nginx无缝衔接； 支持多线程方式上传和下载文件，支持断点续传； 存储服务器上可以保存文件附加属性。 【官方GitHub地址】 1. 准备安装文件 安装文件：fastdfs,libfastcommon,fastdfs-nginx-module 文件存放位置：&#x2F;usr&#x2F;local&#x2F;src 12345cd /usr/local/srcwget -c &quot;https://github.com/happyfish100/fastdfs/archive/V6.06.tar.gz&quot; -O fastdfs-6.06.tar.gzwget -c &quot;https://github.com/happyfish100/libfastcommon/archive/V1.0.43.tar.gz&quot; -O libfastcommon-1.0.43.tar.gzwget -c &quot;https://github.com/happyfish100/fastdfs-nginx-module/archive/V1.22.tar.gz&quot; -O fastdfs-nginx-module-1.22.tar.gzwget -c http://nginx.org/download/nginx-1.18.0.tar.gz 2. 编译安装准备编译环境 1234yum install git gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl-devel wget vim -ytar -zxvf libfastcommon-1.0.43.tar.gztar -zxvf fastdfs-6.06.tar.gztar -zxvf fastdfs-nginx-module-1.22.tar.gz 2.1 编译安装libfatscommon12cd /usr/local/src/libfastcommon-1.0.43./make.sh &amp;&amp; ./make.sh install 检查(出现libfastcommon.so即成功) 12ls /usr/lib64|grep libfastcommonls /usr/lib|grep libfastcommon 2.2 编译安装fastdfs12cd /usr/local/src/fastdfs-6.06./make.sh &amp;&amp; ./make.sh install 检查 1ls /usr/bin|grep fdfs 3. 配置Tracker(FastFDS跟踪器) 启用并修改配置文件 123456789101112cd /etc/fdfs/cp tracker.conf.sample tracker.confvim /etc/fdfs/tracker.conf# 配置文件生效disabled = false# 提供服务端口port = 22122# Tracker 数据和日志目录地址（根目录必须存在，子目录会自动创建）base_path = /fastdfs/tracker# HTTP 服务端口 默认8080，建议修改防止冲突http.server_port = 8080 创建Tracker基础数据目录，即base_path对应的目录 1mkdir -p /fastdfs/tracker 启动Tracker服务 123456789101112# 启动systemctl start fdfs_trackerd# 重启systemctl restart fdfs_trackerd# 查看服务状态systemctl status fdfs_trackerd# 检查服务是否启动ps -ef | grep fdfs# 22122端口正在被监听，则算Tracker服务安装成功netstat -tulnp | grep fdfs# 关闭systemctl stop fdfs_trackerd 设置开机启动 1systemctl enable fdfs_trackerd.service 4. 配置Storage(FastFDS存储器) 启用并修改配置文件 12345678910111213cd /etc/fdfs/cp storage.conf.sample storage.confvim /etc/fdfs/storage.conf# Storage 数据和日志目录地址（根目录必须存在，子目录会自动创建）base_path = /fastdfs/storage/base# 逐一配置 store_path_count 个路径，索引号基于 0# 如果不配置，那就和base_path一样store_path0 = /fastdfs/storage# tracker_server 的列表，多个时，每个写一行（会主动连接 tracker_server）tracker_server = ip:22122# HTTP 服务端口 默认8888，建议修改防止冲突http.server_port = 8888 创建Tracker基础数据目录，即base_path对应的目录 1234# 对应base_pathmkdir -p /fastdfs/storage/base# 对应store_path0，有多个要创建多个mkdir -p /fastdfs/storage 启动Storage服务 123456789101112# 启动systemctl start fdfs_storaged# 重启systemctl restart fdfs_storaged# 查看服务状态systemctl status fdfs_storaged# 检查服务是否启动ps -ef | grep fdfs# 22122端口正在被监听，则算Tracker服务安装成功netstat -tulnp | grep fdfs# 关闭systemctl stop fdfs_storaged 查看Storage和Tracker是否在通信 1/usr/bin/fdfs_monitor /etc/fdfs/storage.conf 设置开机自启 1systemctl enable fdfs_storaged.service 5. 客户端配置 启用修改Client配置文件 12345678cd /etc/fdfs/cp client.conf.sample client.confvim /etc/fdfs/client.conf# Client 的数据和日志目录base_path= /fastdfs/client# Tracker 端口tracker_server=ip:22122 创建Client基础数据目录1mkdir -p /fastdfs/client 6. 配置Nginx模块安装配置fastfds-nginx-module模块 进入 &#x2F;usr&#x2F;local&#x2F;src&#x2F;fastdfs-nginx-module-1.22&#x2F;src&#x2F; 1cd /usr/local/src/fastdfs-nginx-module-1.22/src/ 编辑配置文件（FastDFS 服务脚本设置的 bin 目录是 &#x2F;usr&#x2F;local&#x2F;bin， 但实际命令安装在 &#x2F;usr&#x2F;bin&#x2F; 下） 123vim config# 将config文件中的/usr/local替换成/usr:%s+/usr/local+/usr 进入 nginx 解压目录，添加fastdfs-nginx-module 12cd /mnt/newdatadrive/apps/nginx-1.18.0./configure --add-module=/mnt/newdatadrive/apps/fastdfs/fastdfs-nginx-module-1.22/src/ --with-http_stub_status_module 若是SSL(https)，没有SSL证书忽略此步 1./configure --add-module=/mnt/newdatadrive/apps/fastdfs/fastdfs-nginx-module-1.22/src/ --with-http_ssl_module 编译安装 1make &amp;&amp; make install 复制并修改fastdfs-ngin-module中的配置文件 1234567cp /usr/local/src/fastdfs-nginx-module-1.22/src/mod_fastdfs.conf /etc/fdfs/vi /etc/fdfs/mod_fastdfs.confconnect_timeout=10tracker_server=ip:22122url_have_group_name = truestore_path0=/fastdfs/storage 进入fastdfd源码conf目录，将http.conf，mime.types两个文件拷贝到&#x2F;etc&#x2F;fdfs&#x2F;目录下 12cd /usr/local/src/fastdfs-6.06/conf/cp http.conf mime.types /etc/fdfs/ 创建一个软连接，在&#x2F;fastdfs&#x2F;storage文件存储目录下创建软连接，将其链接到实际存放数据 的目录（可以省略） 1ln -s /fastdfs/storage/data/ /fastdfs/storage/data/M00 编辑Nginx配置 12345678910vi /usr/local/nginx/conf/nginx.confserver &#123; listen 80;# 建议修改，防止冲突 server_name ip; location ~/group([0-9])/M00 &#123; root /fastdfs/storage/data; ngx_fastdfs_module; &#125;&#125; 启动Ngnix 1/usr/local/nginx/sbin/nginx 7. 测试123456# 下载测试图片到本地cd /usr/local/srcwget -c &quot;https://www.caimei365.com/img/base/placeholder.png&quot; -O test.png/usr/bin/fdfs_upload_file /etc/fdfs/client.conf /usr/local/src/test.png# 得到返回文件名group1/M00/00/00/wKgCZF_YlreAcZCZAAHolZLymZE514.png 与ip拼接后浏览器访问：http://192.168.2.100/group1/M00/00/00/wKgCZF_YlreAcZCZAAHolZLymZE514.png","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"},{"name":"FastDFS","slug":"FastDFS","permalink":"http://chaooo.github.io/tags/FastDFS/"}]},{"title":"「环境配置」CentOS 8 常用软件安装(MySQL Nginx SVN Redis)","date":"2020-07-18T10:11:23.000Z","path":"2020/07/18/env-centos8-apps.html","text":"1. CentOS 8 安装（MySQL8.0&#x2F;MySQL5.7)1.1 安装 MySQL 8.01.1.1 使用最新的包管理器安装MySQL1dnf install @mysql 1.1.2 配置表大小写不敏感在首次启动之前要配置表大小写不敏感，这是和 MySQL 7 不一样的地方。 mysql的配置文件是 &#x2F;etc&#x2F;my.cnf，它 include 了 &#x2F;etc&#x2F;my.cnf.d 目录下的配置，所以在 &#x2F;etc&#x2F;my.cnf.d&#x2F;mysql-server.cnf 配置文件里[mysqld]下面的配置lower_case_table_names=1: 12[mysqld]lower_case_table_names=1 1.1.3 设置开机启动安装完成后，运行以下命令来启动MySQL服务并使它在开机时自动启动： 1systemctl enable --now mysqld 检查MySQL服务器是否正在运行： 1systemctl status mysqld 1.1.4 添加密码及安全设置运行mysql_secure_installation脚本，该脚本执行一些与安全性相关的操作并设置MySQL根密码 1mysql_secure_installation 步骤如下： 要求你配置VALIDATE PASSWORD component（验证密码组件）： 输入y ，回车进入该配置 选择密码验证策略等级， 我这里选择0 （low），回车 输入新密码两次 确认是否继续使用提供的密码？输入y ，回车 移除匿名用户？ 输入y ，回车 不允许root远程登陆？ 我这里需要远程登陆，所以输入n ，回车 移除test数据库？ 输入y ，回车 重新载入权限表？ 输入y ，回车 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@localhost ~]# mysql_secure_installationSecuring the MySQL server deployment.Connecting to MySQL using a blank password.VALIDATE PASSWORD COMPONENT can be used to test passwordsand improve security. It checks the strength of passwordand allows the users to set only those passwords which aresecure enough. Would you like to setup VALIDATE PASSWORD component?Press y|Y for Yes, any other key for No: yThere are three levels of password validation policy:LOW Length &gt;= 8MEDIUM Length &gt;= 8, numeric, mixed case, and special charactersSTRONG Length &gt;= 8, numeric, mixed case, special characters and dictionary filePlease enter 0 = LOW, 1 = MEDIUM and 2 = STRONG: 0Please set the password for root here.New password: Re-enter new password: Estimated strength of the password: 100 Do you wish to continue with the password provided?(Press y|Y for Yes, any other key for No) : yBy default, a MySQL installation has an anonymous user,allowing anyone to log into MySQL without having to havea user account created for them. This is intended only fortesting, and to make the installation go a bit smoother.You should remove them before moving into a productionenvironment.Remove anonymous users? (Press y|Y for Yes, any other key for No) : ySuccess.Normally, root should only be allowed to connect from&#x27;localhost&#x27;. This ensures that someone cannot guess atthe root password from the network.Disallow root login remotely? (Press y|Y for Yes, any other key for No) : n ... skipping.By default, MySQL comes with a database named &#x27;test&#x27; thatanyone can access. This is also intended only for testing,and should be removed before moving into a productionenvironment.Remove test database and access to it? (Press y|Y for Yes, any other key for No) : n ... skipping.Reloading the privilege tables will ensure that all changesmade so far will take effect immediately.Reload privilege tables now? (Press y|Y for Yes, any other key for No) : ySuccess.All done! 1.1.5 配置远程登陆用户 本机登录MySQL: 1mysql -uroot -p&lt;上面步骤中设置的密码&gt; 创建用户和授权 在mysql8.0创建用户和授权和之前不太一样了，其实严格上来讲，也不能说是不一样,只能说是更严格,mysql8.0需要先创建用户和设置密码,然后才能授权；将root用户的host字段设为’%’，意为接受root所有IP地址的登录请求： 123456789#先创建一个用户use mysql;create user &#x27;developer&#x27;@&#x27;%&#x27; identified by &#x27;05bZ/OxTB:X+yd%1&#x27;; #再进行授权grant all privileges on *.* to &#x27;developer&#x27;@&#x27;%&#x27; with grant option;flush privileges;# 若用SQLyog连接MySQL8.0出现错误2058，执行如下命令ALTER USER &#x27;developer&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;05bZ/OxTB:X+yd%1&#x27;; 开放防火墙端口3306 MySQL默认监听3306端口，设置完成后输入exit退出mysql，回到终端shell界面，接着开启系统防火墙的3306端口： 12firewall-cmd --add-port=3306/tcp --permanentfirewall-cmd --reload 1.1.6 关闭MySQL主机查询dnsMySQL会反向解析远程连接地址的dns记录，如果MySQL主机无法连接外网，则dns可能无法解析成功，导致第一次连接MySQL速度很慢，所以在配置中可以关闭该功能。参考文档打开/etc/my.cnf文件，添加以下配置： 12[mysqld]skip-name-resolve 1.1.7 重启服务1systemctl restart mysqld 本机测试安装后，MySQL8.0默认已经是utf8mb4字符集，所以字符集不再修改。 1.2 CentOS 8 安装配置 MySQL 5.71.2.1 添加MySQL存储库禁用MySQL默认的存储库： 12dnf remove @mysqldnf module reset mysql &amp;&amp; dnf module disable mysql CentOS 8没有MySQL5.7存储库，创建一个新的存储库文件。 1vi /etc/yum.repos.d/mysql-community.repo 将以下数据粘贴到文件中。 1234567891011121314151617[mysql57-community]name=MySQL 5.7 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/7/$basearch/enabled=1gpgcheck=0[mysql-connectors-community]name=MySQL Connectors Communitybaseurl=http://repo.mysql.com/yum/mysql-connectors-community/el/7/$basearch/enabled=1gpgcheck=0[mysql-tools-community]name=MySQL Tools Communitybaseurl=http://repo.mysql.com/yum/mysql-tools-community/el/7/$basearch/enabled=1gpgcheck=0 1.2.2 在CentOS 8 上安装MySQL 5.7禁用MySQL 8 存储库： 1dnf config-manager --disable mysql80-community 然后启用MySQL 5.7存储库： 1dnf config-manager --enable mysql57-community 然后在CentOS 8 上安装MySQL 5.7： 1dnf install mysql-community-server 按y开始安装。 安装完成后检查软件包的转速详细信息，以确认它是5.7。 1rpm -qi mysql-community-server 1.2.3 在CentOS 8 &#x2F; RHEL 8上配置MySQL 5.7安装后，启动mysqld服务。 1systemctl enable --now mysqld.service 2.2 –复制为root用户生成的随机密码 1grep &#x27;A temporary password&#x27; /var/log/mysqld.log 若没有生成临时密码，执行下面命令 1234rm -rf /var/lib/mysqlsystemctl restart mysqldgrep &#x27;temporary password&#x27; /var/log/mysqld.log2020-12-11T09:41:41.459519Z 1 [Note] A temporary password is generated for root@localhost: #4q0R3/#qmC0 运行mysql_secure_installation脚本，该脚本执行一些与安全性相关的操作并设置MySQL根密码，可以参考【1.1.3 添加密码及安全设置】及之后的操作。 2. CentOS 8 安装配置 Nginx2.1 下载安装官网下载：http://nginx.org/en/download.html或者直接在linux执行命令：wget http://nginx.org/download/nginx-1.18.0.tar.gz 123456789101112# 安装依赖yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel# 解压缩tar -zxvf nginx-1.18.0.tar.gzcd nginx-1.18.0/# 执行配置【没有SSL证书】./configure# 执行配置【有SSL证书】./configure --prefix=/usr/local/nginx --with-http_ssl_module# 编译安装(默认安装在/usr/local/nginx)makemake install 防火墙配置，nginx默认监听80端口 12firewall-cmd --zone=public --add-port=80/tcp --permanentfirewall-cmd --reload SSL (sslkey存放路径：/usr/local/nginx/conf) 12345678910server &#123; listen 443 ssl; server_name 【域名】; ssl_certificate sslkey/_.caimei365.com_bundle.crt; ssl_certificate_key sslkey/_.caimei365.com.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers AESGCM:ALL:!DH:!EXPORT:!RC4:+HIGH:!MEDIUM:!LOW:!aNULL:!eNULL; ssl_prefer_server_ciphers on;&#125; 2.2 Nginx验证与配置 测试配置文件：/usr/local/nginx/sbin/nginx -t nginx主配置文件：/usr/local/nginx/conf/nginx.conf nginx日志文件：/usr/local/nginx/logs/access.log 启动Nginx：/usr/local/nginx/sbin/nginx 加入环境变量 1vim /etc/profile 最尾输入以下内容： 1export PATH=$PATH:$JAVA_HOME/bin:/usr/local/nginx/sbin 使用source /etc/profile命令使配置文件生效。 2.3 Centos 8 配置Nginx开机自启动 进入到&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;目录，编辑nginx.service 12345678910111213141516[root@localhost ~]# cd /lib/systemd/system/[root@localhost system]# vim nginx.service[Unit]Description=nginx serviceAfter=network.target[Service]Type=forkingExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s stopPrivateTmp=yes[Install]WantedBy=multi-user.target [Unit]:服务的说明： - Description:描述服务 - After:描述服务类别 [Service]服务运行参数的设置： - Type&#x3D;forking是后台运行的形式 - ExecStart为服务的具体运行命令 - ExecReload为重启命令 - ExecStop为停止命令 - PrivateTmp&#x3D;True表示给服务分配独立的临时空间 - 注意：[Service]的启动、重启、停止命令全部要求使用绝对路径 [Install]运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3 加入开机自启动 1systemctl enable nginx.service 常用命令1234567891011121314# 启动nginx服务systemctl start nginx.service# 停止服务systemctl stop nginx.service# 重新启动服务systemctl restart nginx.service# 查看所有已启动的服务systemctl list-units --type=service# 查看服务当前状态systemctl status nginx.service# 设置开机自启动systemctl enable nginx.service# 停止开机自启动systemctl disable nginx.service 3. CentOS 8 安装配置 SVN 安装 123456# 安装SVN[root@localhost ~]# yum -y install subversion# 创建目录[root@localhost ~]# mkdir -p /var/svn/svnrepos# 创建版本库(codes为自定义仓库目录)[root@localhost ~]# svnadmin create /var/svn/svnrepos/codes 配置 authz：负责账号权限的管理，控制账号是否读写权限 passwd：负责账号和密码的用户名单管理 svnserve.conf：svn服务器配置文件123[root@localhost ~]# cd /var/svn/svnrepos/codes/conf[root@localhost conf]# lsauthz passwd svnserve.conf 添加账户，并赋予读写权限，在authz末尾加入： 12345[root@localhost conf]# vim authz[/]admin=rwtest1=rwtest2=rw 设置账户密码： 1234567[root@localhost conf]# vim passwd[users]# harry = harryssecret# sally = sallyssecretadmin = 123456test1 = 123456test2 = 123456 设置svn服务器配置文件 12345678910[root@localhost conf]# vim svnserve.conf[general]anon-access = readauth-access = writepassword-db = passwdauthz-db = authzrealm = My First Repository 设置开机自启修改&#x2F;etc&#x2F;sysconfig&#x2F;svnserve 将OPTIONS修改为自己的库版本保存目录（保留引号和-r）12vim /etc/sysconfig/svnserveOPTIONS=&quot;-r /var/svn/svnrepos&quot; 开机自启 1systemctl enable svnserve.service 启动服务1svnserve -d -r /var/svn/svnrepos 默认端口是3690需要修改监听端口或者监听IP可以通过修改–listen-port和 –listen-host来进行修改 可以通过svn协议进行访问：svn://[ip]:[port]/codes 4. CentOS 8 安装和配置 Redis Redis版本5.0.x包含在默认的CentOS 8存储库中，查询可用的redis安装包： 1234[root@localhost ~]# dnf list redis上次元数据过期检查：3:04:47 前，执行于 2020年12月21日 星期一 23时03分10秒。可安装的软件包redis.x86_64 5.0.3-2.module_el8.2.0+318+3d7e67ea AppStream 执行安装： 1[root@localhost ~]# dnf install redis 安装完成后，启用并启动Redis服务： 12[root@localhost ~]# systemctl enable --now redisCreated symlink /etc/systemd/system/multi-user.target.wants/redis.service → /usr/lib/systemd/system/redis.service. 检查Redis服务器是否正在运行 1[root@localhost ~]# systemctl status redis 配置Redis远程访问 修改Redis配置文件： 1234567[root@localhost ~] vim /etc/redis.conf# 查找 /bind 找到：bind 127.0.0.1并注释，使其它ip地址也可访问# bind 127.0.0.1# # 查找 /requirepass 去掉注释#，并把foobared 替换为密码，例如：qwe123# requirepass foobaredrequirepass qwe123 防火墙配置，Redis默认监听6379端口 1firewall-cmd --zone=public --add-port=6379/tcp --permanent &amp;&amp; firewall-cmd --reload 重新启动Redis服务以使更改生效： 1[root@localhost ~] systemctl restart redis 其他服务器远程访问测试 12redis-cli -h 192.168.2.100 -p 6379 -a qwe123192.168.2.100:6379&gt;","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"},{"name":"Nginx","slug":"Nginx","permalink":"http://chaooo.github.io/tags/Nginx/"},{"name":"MySQL","slug":"MySQL","permalink":"http://chaooo.github.io/tags/MySQL/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"},{"name":"SVN","slug":"SVN","permalink":"http://chaooo.github.io/tags/SVN/"}]},{"title":"「环境配置」Centos8安装部署Node+MongDB+YApi(接口管理)","date":"2020-07-15T10:11:23.000Z","path":"2020/07/15/env-centos8-yapi.html","text":"YApi:权限管理、Mock服务、可视化接口管理、数据导入（支持postman），其依赖NodeJS+MongDB。 1. 安装NodeJS 下载NodeJS稳定版14.15.1 123[root@localhost ~]# mkdir -p /local/node-server[root@localhost ~]# cd /local/node-server[root@localhost node-server]# wget https://nodejs.org/dist/v14.15.1/node-v14.15.1-linux-x64.tar.xz 解压下载好的node包到安装目录下 1[root@localhost node-server]# tar xvf node-v14.15.1-linux-x64.tar.xz node -v查看版本号 1234[root@localhost node-server]# node -vbash: node: 未找到命令...[root@localhost node-server]# /local/node-server/node-v14.15.1-linux-x64/bin/node -vv14.15.1 创建软链接，就可以全局使用node和npm命令 123456[root@localhost node-server]# ln -s /local/node-server/node-v14.15.1-linux-x64/bin/node /usr/local/bin/node[root@localhost node-server]# ln -s /local/node-server/node-v14.15.1-linux-x64/bin/npm /usr/local/bin/npm[root@localhost node-server]# node -vv14.15.1[root@localhost node-server]# npm -v6.14.8 2. 安装MongDB 创建yum源文件 12345678[root@localhost ~]# vim /etc/yum.repos.d/mongodb-org-4.2.repo# 输入如下内容[mongodb-org-4.2] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.2/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-4.2.asc 安装mongodb 1[root@localhost ~]# yum -y install mongodb-org 查看安装目录 12[root@localhost ~]# whereis mongodmongod: /usr/bin/mongod /etc/mongod.conf /usr/share/man/man1/mongod.1 编辑配置文件/etc/mongod.conf(根据自己需要进行修改bindip地址，可监听127.0.0.1或内网地址。如果需要绑定多个ip ) 12[root@localhost ~]# vim /etc/mongod.confbindIp: 127.0.0.1,192.168.2.101 启动Mongodb 1234567891011# 启动mongodbsystemctl start mongod.service# 停止mongodbsystemctl stop mongod.service# 查询 mongodb 状态：systemctl status mongod.service# 设置为开机启动systemctl enable mongod.service 如果在不同服务器下访问或者修改端口需要配置防火墙或者阿里云服务器安全组件 默认为27017 如修改可在&#x2F;etc&#x2F;mongod.conf下修改端口，到此安装完成。 启动 mongo shell 1234567891011[root@localhost ~]# mongoMongoDB shell version v4.2.11connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodb# 其他信息省略...&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GB&gt; exitbye 3. 安装YApi 开始安装 123mkdir yapi &amp;&amp; cd yapiwget http://registry.npm.taobao.org/yapi-vendor/download/yapi-vendor-1.9.2.tgztar -zxvf yapi-vendor-1.9.2.tgz 拷贝依赖package至vendors 1234567[root@localhost yapi]# lldrwxr-xr-x. 9 root root 4096 11月 20 16:18 package-rw-r--r--. 1 root root 9403779 5月 29 14:14 yapi-vendor-1.9.2.tgz[root@localhost yapi]# mv package vendors[root@localhost yapi]# lldrwxr-xr-x. 9 root root 4096 11月 20 16:18 vendors-rw-r--r--. 1 root root 9403779 5月 29 14:14 yapi-vendor-1.9.2.tgz 安装依赖 12[root@localhost yapi]# cd vendors/[root@localhost vendors]# npm install --production --registry https://registry.npm.taobao.org 拷贝配置并修改(MongoDB地址、端口、用户) 123[root@localhost vendors]# cd ../[root@localhost yapi]# cp vendors/config_example.json ./config.json[root@localhost yapi]# vim config.json 安装服务 123[root@localhost yapi]# cd vendors/[root@localhost vendors]# npm run install-server初始化管理员账号成功,账号名：&quot;admin@admin.com&quot;，密码：&quot;ymfe.org&quot; 启动 1234[root@localhost vendors]# node server/app.jslog: -------------------------------------swaggerSyncUtils constructor-----------------------------------------------log: 服务已启动，请打开下面链接访问: http://127.0.0.1:3000/ 安装pm管理服务启动 123456789[root@localhost vendors]# npm install pm2 -g# 配置服务启动项[root@localhost vendors]# pm2 start &quot;/local/yapi/vendors/server/app.js&quot; --name yapi# 查看服务信息[root@localhost vendors]# pm2 info yapi# 停止服务[root@localhost vendors]# pm2 stop yapi# 重启服务[root@localhost vendors]# pm2 restart yapi 若找不到pm2命令，可先把node安装路径&#x2F;bin添加到环境变量。 设置开机启动 运行 pm2 startup，即在&#x2F;etc&#x2F;init.d&#x2F;目录下生成pm2-root的启动脚本，且自动将pm2-root设为服务。 1pm2 startup 运行 pm2 save，会将当前pm2所运行的应用保存在&#x2F;root&#x2F;.pm2&#x2F;dump.pm2下，当开机重启时，运行pm2-root服务脚本，并且到&#x2F;root&#x2F;.pm2&#x2F;dump.pm2下读取应用并启动。 1pm2 save 还可以配合IDEA插件Api Generator使用 Preferences → Plugins → Marketplace → 搜索“Api Generator” → 安装该插件 → 重启IDE Api Generator使用教程","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"},{"name":"YApi","slug":"YApi","permalink":"http://chaooo.github.io/tags/YApi/"}]},{"title":"「Shell」Shell脚本部署Java应用","date":"2020-06-10T07:12:46.000Z","path":"2020/06/10/env-shell-java.html","text":"Shell脚本部署Java应用 1. 根据PID获取进程信息1234567891011121314151617181920212223#!/bin/bash## Function: 根据用户输入的PID，过滤出该PID所有的信息#read -p &quot;请输入要查询的PID: &quot; Pn=`ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $11&#125;&#x27;|wc -l`if [ $n -eq 0 ];then echo &quot;该PID不存在！&quot; exitfiecho &quot;--------------------------------&quot;echo &quot;进程PID: $P&quot;echo &quot;进程命令：`ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $11&#125;&#x27;`&quot;echo &quot;进程所属用户: `ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $1&#125;&#x27;`&quot;echo &quot;CPU占用率：`ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $3&#125;&#x27;`%&quot;echo &quot;内存占用率：`ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $4&#125;&#x27;`%&quot;echo &quot;进程开始运行的时刻：`ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $9&#125;&#x27;`&quot;echo &quot;进程运行的时间：`ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $10&#125;&#x27;`&quot;echo &quot;进程状态：`ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $8&#125;&#x27;`&quot;echo &quot;进程虚拟内存：`ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $5&#125;&#x27;`&quot;echo &quot;进程共享内存：`ps -aux| awk &#x27;$2~/^&#x27;$P&#x27;$/&#123;print $6&#125;&#x27;`&quot;echo &quot;--------------------------------&quot; 若执行出现权限不够，为shell文件增加执行权限:chmod +x test.sh 2. 使用Shell脚本部署Jar包将跳板机上(或本地服务器)的jar包文件拷贝到发布服务器，然后通过发布服务器上的脚本实现旧jar包的备份，新jar包的启动。 实现部署的操作：拷贝jar包到服务器 -&gt; 备份旧服务jar包 -&gt; 启动新服务jar包 使用命令：./begin.sh demo-0.0.1-SNAPSHOT.jar 跳板机(本地服务器)的脚本begin.sh 12345678910#!/bin/bashfileName=$1if [ -z &quot;$fileName&quot; ]; then echo &quot;文件名不能为空&quot; exit 0fiecho &quot;开始拷贝jar文件【$fileName】到192.168.2.100&quot;scp $fileName cmuser@192.168.2.100:/usr/local/test/$fileName.prevecho &quot;文件传输结束，准备启动192.168.2.100的部署脚本&quot;ssh cmuser@192.168.2.100 /usr/local/test/demo-deploy.sh $fileName 通过上面的跳板机上的代码，我们知道跳板机最终会调用发布服务器上/usr/local/test/demo-deploy.sh这个shell脚本命令。 发布服务器脚本demo-deploy.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/usr/bin/env bashecho &quot;服务器开始部署服务&quot;projectname=&quot;demo&quot;# 打开文件所属的目录，不然远程执行会找不到当前目录cd /usr/local/test# 新的jar包会当成参数传过来newJar=$1echo &quot;新的jar为：$newJar&quot; # 如果新的jar包为空则退出if [ -z &quot;$newJar&quot; ]; then echo &quot;新的jar不能为空&quot; exit 0fi# 获取旧的jar包名称，当然可能是空的，也可能跟当前名称一致oldJar=$(ps -ef | grep $&#123;projectname&#125;|grep -v &#x27;demo-deploy.sh&#x27;|grep -v grep|awk &#x27;&#123;print $10&#125;&#x27;|cut -d &#x27;/&#x27; -f 2)echo &quot;当前运行的旧的jar包为：$oldJar&quot; #如果新的jar包为空则退出if [ -z &quot;$oldJar&quot; ]; then echo &quot;没有启动的demo服务&quot;else # 如果旧的进程还在就将旧的进程杀掉 oldId=`ps -ef|grep $&#123;projectname&#125;|grep -v &quot;$0&quot;|grep -v &quot;grep&quot;|awk &#x27;&#123;print $2&#125;&#x27;` echo &quot;$oldId&quot; echo &quot;kill old process start ...&quot; for id in $oldId do kill -9 $id echo &quot;killed $id&quot; done echo &quot;kill old process end&quot; # 获取当前时间戳 suffix=&quot;.bak-&quot;`date &#x27;+%s%3N&#x27;`; echo $suffix; # 将旧的jar包进行备份 mv $oldJar $&#123;oldJar&#125;$&#123;suffix&#125;fi# 开始启动新的进程mv $&#123;1&#125;.prev $&#123;1&#125;nohup java -jar $&#123;1&#125; &gt; run.txt 2&gt;&amp;1 &amp;echo &quot;服务启动查看进程:&quot;echo `ps -ef | grep $&#123;projectname&#125;|grep -v &#x27;demo-deploy.sh&#x27;|grep -v grep` 演示12345678910111213141516# 跳板机[cmuser@localhost test]$ ./begin.sh demo-0.0.1-SNAPSHOT.jar开始拷贝jar文件【demo-0.0.1-SNAPSHOT.jar】到192.168.2.100cmuser@192.168.2.100&#x27;s password: demo-0.0.1-SNAPSHOT.jar 100% 20MB 11.2MB/s 00:01 文件传输结束，准备启动192.168.2.100的部署脚本cmuser@192.168.2.100&#x27;s password: 服务器开始部署服务新的jar为：demo-0.0.1-SNAPSHOT.jar当前运行的旧的jar包为：demo-0.0.1-SNAPSHOT.jar2581kill old process start ...killed 2581kill old process end.bak-20201117服务启动查看进程: 1234567891011121314# 发布服务器(部署前pid:2581)[cmuser@192.168.2.100 test]$ ps -ef | grep democmuser 2581 1 16 17:03 ? 00:00:06 java -jar demo-0.0.1-SNAPSHOT.jar[cmuser@192.168.2.100 test]$ lltotal 40260-rw-rw-r--. 1 cmuser cmuser 20606107 Nov 18 15:45 demo-0.0.1-SNAPSHOT.jar-rw-rw-r--. 1 cmuser cmuser 20606111 Nov 18 15:06 demo-0.0.1-SNAPSHOT.jar.bak-1605685530423-rwxrwxr-x. 1 jenkins jenkins 1356 Nov 17 17:56 demo-deploy.sh-rwxrwxrwx. 1 jenkins jenkins 1207 Nov 18 15:45 run.txt# 发布服务器(部署后:2653)[cmuser@192.168.2.100 test]$ ps -ef | grep democmuser 2653 1 59 17:04 ? 00:00:07 java -jar demo-0.0.1-SNAPSHOT.jar","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://chaooo.github.io/tags/Shell/"}]},{"title":"「Shell」Shell 文本处理","date":"2020-06-09T07:12:46.000Z","path":"2020/06/09/env-shell-text.html","text":"1. 文件查找命令1.1 find命令语法格式：find 搜索路径 [选项] 搜索内容 选项 解析 -name 按照文件名搜索 -iname 按照文件名搜索，不区分文件名大小 -inum 按照 inode 号搜索 -type 根据文件类型(f:文件,d:目录,c:字符设备文件,b:块设备文件,l:链接文件,p:管道文件)搜索 -size 根据文件大小(单位:ckMGTP)搜索，-小于，+大于，例如查找&#x2F;etc目录下大于1M的文件：find &#x2F;etc -size +1M -mtime 根据修改时间(单位:smhdw)搜索 -ctime 根据创建时间(单位:smhdw)搜索 -atime 根据被访问时的时间间隔(单位:smhdw)搜索 -mmin n分钟以(-n:内，+n:外)内修改的文件 -mindepth 从n级子目录开始搜索,最多搜索到n-1级子目录 -depth 检索深度为 n 的文件，即位于指定目录以下 n 层的文件 -empty 检索空文件或空目录 -perm 根据文件权限搜索 -ls 打印搜索到的文件的详细信息 -delete 删除检索到的文件 -exec 对搜索的文件常用操作(“-exec”和”-ok”相似，对文件执行特定的操作，”-ok”得到确认命令后，才会执行；-print打印输出) 1.2 locate命令 locate命令，不同于find命令是在整块磁盘中搜索，locate命令是在数据库文件中查找 find是默认全局匹配，locate则是默认部分匹配 文件更新后，用updatedb命令把文件更新到数据库(默认是第二天系统才会自动更新到数据库)，否则locate查找不到 1.3 whereis命令 whereis命令，只能用于程序名的搜索 命令参数：二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 1.4 which命令 which命令，仅查找二进制程序文件 2. Linux文本处理三剑客文本处理三剑客工具grep，sed和awk都是基于行处理的，它们会一行行读入数据，处理完一行之后再处理下一行。 2.1 文件处理三剑客之grepgrep命令，用于查找文件里符合条件的字符串。 语法格式： 语法1：grep [option] [ pattern] [file1, file2..] 语法2：command | grep [option] [pattern] 选项(option) 解析 -v 不显示匹配行信息 -i 搜索时忽略大小写 -n 显示行号 -r 递归搜索 -E 支持扩展正则表达式 -F 不按正则表达式匹配，按照字符串字面意思匹配 -c 只显示匹配行总数 -w 匹配整词 -x 匹配整行 -l 只显示文件名，不显示内容 -s 不显示错误信息 grep和egrep grep默认不支持扩展正则表达式，只支持基础正则表达式 使用grep-E可以支持扩展正则表达式使用 egrep可以支持扩展正则表达式，与grep-E等价 2.2 文件处理三剑客之sedsed（Stream Editor），流编辑器。对标准输出或文件逐行进行处理 语法格式： 语法1：stdout |sed [option] &quot;pattern command&quot; 语法2：sed [option] &quot;pattern command&quot; file 2.2.1 sed选项 选项(option) 解析 -n 只打印模式匹配行 -e 直接在命令行进行sed编辑，默认选项 -f 编辑动作保存在文件中，指定文件执行 -r 支持扩展正则表达式 -i 直接修改文件内容 2.2.2 sed匹配模式 匹配模式(pattern) 解析 10command 匹配到第10行 10,20command 匹配从第10行开始，到第20行结束10， 10,+5command 匹配从第10行开始，到第16行结束 &#x2F;pattern1&#x2F;command 匹配到pattern1的行 &#x2F;pattern1&#x2F;,&#x2F;pattern2&#x2F;command 匹配到pattern1的行开始，到匹配到patern2的行结束 10,&#x2F;pattern1&#x2F;command 匹配从第10行开始，到匹配到pettern1的行结束 &#x2F;pattern1&#x2F;,10command 匹配到pattern1的行开始，到第10行匹配结束 1234567891011121314151617181920# 打印test.txt文件的第17行sed -n &quot;17p&quot; test.txt# 打印文件的10到20行sed -n &quot;10,20p&quot; test.txt# 打印test.txt文件中从第10行开始，往后面加5行sed -n &quot;10,+5p&quot; test.txt# 打印test.txt文件中以root开头的行sed -n &quot;/^root/p&quot; test.txt# 打印test.txt文件中第一个匹配到以ftp开头的行，到mail开头的行结束sed -n &quot;/^ftp/,/^mail/p&quot; test.txt# 打印test.txt文件中从第4行开始匹配，直到以hdfs开头的行结束sed -n &quot;4,/^hdfs/p&quot; test.txt# 打印test.txt文件中匹配root的行，直到第10行结束sed -n &quot;/root/,10p&quot;test.txt 2.2.3 sed中的编辑命令 编辑命令 类别 含义 p 查询 打印 &#x3D; 查询 只显示行号 a 增加 行后追加 i 增加 行前追加 r 增加 外部文件读入，行后追加 w 增加 匹配行写入外部文件 d 删除 删除 s&#x2F;old&#x2F;new 修改 将行内第一个old替换为new s&#x2F;old&#x2F;new&#x2F;g 修改 将行内全部的old替换为new s&#x2F;old&#x2F;new&#x2F;2g 修改 将行内从第2个old开始到剩下所有的old替换为new s&#x2F;old&#x2F;new&#x2F;ig 修改 将行内全部的old替换为new，忽略大小写 反向引用 123456789101112131415[cmuser@localhost test]$ cat abc.txtFirst LineSecond haha[cmuser@localhost test]$ sed -i &#x27;s/Sec..d/&amp;s/g&#x27; abc.txt[cmuser@localhost test]$ cat abc.txtFirst LineSeconds haha[cmuser@localhost test]$ sed -i &#x27;s/\\(Sec..ds\\)/\\10/g&#x27; abc.txt[cmuser@localhost test]$ cat abc.txtFirst LineSeconds0 haha[cmuser@localhost test]$ sed -i &#x27;s/\\(Sec\\)...../\\1FFFFFFFFFF/g&#x27; abc.txt[cmuser@localhost test]$ cat abc.txtFirst LineSecFFFFFFFFFF haha &amp;和\\1引用模式匹配到的整个串 两者区别在于只能表示匹配到的完整字符串，只能引用整个字符串；而\\1可以使用()匹配到的字符 sed中引用变量时注意事项： 匹配模式中存在变量，则建议使用双引号 sed中需要引入自定义变量时，如果外面使用单引号，则自定义变量也必须使用单引号 123456[cmuser@localhost test]$ old_str=First[cmuser@localhost test]$ new_str=One[cmuser@localhost test]$ sed -i &quot;s/$old_str&quot;/&quot;$new_str/g&quot; abc.txt[cmuser@localhost test]$ cat abc.txtOne LineSecFFFFFFFFFF haha 2.2.4 sed实例 sed查找文件内容（处理一个MySQL配置文件my.cnf的文本，示例如下；编写脚本实现以下功能：输出文件有几个段，并且针对每个段可以统计配置参数总个数）1234567891011121314151617181920[jenkins@caimeidev1 test]$ vim test1.sh#!/bin/bashFILE_NAME=/etc/my.cnffunction get_all_segments &#123; echo &quot;`sed -n &#x27;/\\[.*\\]/p&#x27; $FILE_NAME |sed -e &#x27;s/\\[//g&#x27; -e &#x27;s/\\]//g&#x27;`&quot;&#125;function count_items_in_segment &#123; echo &quot;`sed -n &#x27;/\\[&#x27;$1&#x27;\\]/,/\\[.*\\]/p&#x27; $FILE_NAME | grep -v ^# |grep -v ^$ |grep -v &quot;\\[.*\\]&quot; |wc -l `&quot;&#125;num=0for seg in `get_all_segments`do num=`expr $num + 1` items_count=`count_items_in_segment $seg` echo &quot;$num:$seg $items_count&quot;done 输出结果： 12345[jenkins@caimeidev1 test]$ ./test1.sh1:mysqld 92:mysqld_safe 23:mysql 14:client 1 sed删除和修改文件内容 1234567891011121314151617# 删除文件中的所有注释行和空行sed -i &#x27;/[:blank:]*#/d;/^$/d&#x27; nginx.conf# 在配置文件中所有不以#开头的行前面添加*符号，注意：以#开头的行不添加sed -i &#x27;s/^[^#]/\\*&amp;/g&#x27; nginx.conf# 修改/etc/passwd中第1行中第1个root为ROOT sed -i &#x27;1s/root/ROOT/&#x27; passwd# 修改/etc/passwd中第5行到第10行中所有的/sbin/nologin为/bin/bashsed -i &#x27;5,10s/\\/sbin\\/nologin/\\/bin\\/bash/g&#x27; passwd# 修改/etc/passwd中匹配到/sbin/nologin的行，将匹配到行中的login改为大写的LOGINsed -i &#x27;/\\/sbin\\/nologin/s/login/LOGIN/g&#x27; passwd# 修改/etc/passwd中从匹配到以root开头的行，到匹配到行中包含mail的所有行。修改内为将这些所有匹配到的行中的bin改为HADOOPsed -i &#x27;/^root/,/mail/s/bin/HADOOP/g&#x27; passwd# 修改/etc/passwd中从匹配到以root开头的行，到第15行中的所有行，修改内容为将这些行中的nologin修改为SPARKsed -i &#x27;/^root/,15s/nologin/SPARK/g&#x27; passwd# 修改/etc/passwd中从第15行开始，到匹配到以yarn开头的所有行，修改内容为将这些行中的bin换位BINsed -i &#x27;15,/^yarn/s/bin/BIN/g&#x27; passwd sed追加文件内容（a:在匹配行后面追加，i:在匹配行前面追加，r:将文件内容追加到匹配行后面，w:将匹配行写入指定文件） 123456789101112131415161718192021222324252627# a:在匹配行后面追加# (1)、passwd文件第10行后面追加&quot;Add Line Behind&quot; sed -i &#x27;10a Add Line Begind&#x27; passwd# (2)、passwd文件第10行到第20行，每一行后面都追加&quot;Test Line Behind&quot;sed -i &#x27;10,20a Test Line Behind&#x27; passwd# (3)、passwd文件匹配到/bin/bash的行后面追加&quot;Insert Line For /bin/bash Behind&quot;sed -i &#x27;/\\/bin\\/bash/a Insert Line For /bin/bash Behind&#x27; passwd# i:在匹配行前面追加# (1)、passwd文件匹配到以yarn开头的行，在匹配行前面追加&quot;Add Line Before&quot;sed -i &#x27;/^yarn/i Add Line Before&#x27; passwd# (2)、passwd文件每一行前面都追加&quot;Insert Line Before Every Line&quot;sed -i &#x27;i Insert Line Before Every Line&#x27; passwd# r:将文件内容追加到匹配行后面# (1)、将/etc/fstab文件的内容追加到passwd文件的第20行后面sed -i &#x27;20r /etc/fstab&#x27; passwd# (2)、将/etc/inittab文件内容追加到passwd文件匹配/bin/bash行的后面sed -i &#x27;/\\/bin\\/bash/r /etc/inittab&#x27; passwd# (3)、将/etc/vconsole.conf文件内容追加到passwd文件中特定行后面，匹配以ftp开头的行，到第18行的所有行sed -i &#x27;//,18r /etc/vconsole.conf&#x27; passwd# w:将匹配行写入指定文件 # (1)、将passwd文件匹配到/bin/bash的行追加到/tmp/sed.txt文件中sed -i &#x27;/\\/bin\\/bash/w /tmp/sed.txt&#x27; passwd# (2)、将passwd文件从第10行开始，到匹配到hdfs开头的所有行内容追加到/tmp/sed-1.txtsed -i &#x27;10,/^hdfs/w /tmp/sed-1.txt&#x27; passwd 2.3 文件处理三剑客之awkawk是一个文本处理工具，通常用于处理数据并生成结果报告 12awk &#x27;BEGIN&#123;&#125; pattern&#123;commands&#125;END&#123;&#125;&#x27; file_namestandard output | awk &#x27;BEGIN&#123;&#125;pattern&#123;commands&#125;END&#123;&#125;&#x27; 语法格式 解析 BEGIN{} 正式处理数据之前执行 pattern 匹配模式 {commands} 处理命令，可能多行 END{} 处理完所有匹配数据后执行 2.3.1 awk内置变量 内置变量 解析 $0 打印行所有信息 $1~$n 打印行的第1到n个字段的信息 NF Number Field 处理行的字段个数 NR Number Row 处理行的行号，从1开始计数 FNR File Number Row 多文件处理时，每个文件单独记录行号，都是从0康凯斯 FS Field Separator 字段分割符，不指定时默认以空格或tab键分割 RS Row Separator 行分隔符，不指定时以回车分割\\n OFS Output Filed Separator 输出字段分隔符。 ORS Output Row Separator 输出行分隔符 FILENAME 处理文件的文件名 ARGC 命令行参数个数 ARGV 命令行参数数组 123456789[cmuser@localhost test]$ awk &#x27;&#123;print $0&#125;&#x27; abc.txtOne LineSecFFFFFFFFFF haha[cmuser@localhost test]$ awk &#x27;&#123;print $1&#125;&#x27; abc.txtOneSecFFFFFFFFFF[cmuser@localhost test]$ awk &#x27;&#123;print NR&#125;&#x27; abc.txt12 2.3.2 awk格式化输出之printf 格式符 解析 %s 打印字符串 %d 打印10进制数 %f 打印浮点数 %x 打印16进制数 %o 打印8进制数 %e 打印数字的科学计数法格式 %c 打印单个字符的ASCII码 修饰符 解析 - 左对齐 + 右对齐 # 显示8进制在前面加o，显示16进制在前面加0x 123456789101112# 1、以字符串格式打印/etc/passwd中的第7个字段，以&quot;:&quot;作为分隔符awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125; &#123;printf &quot;%s\\n&quot;,$7&#125;&#x27; /etc/passwd# 2、以10进制格式打印/etc/passwd中的第3个字段，以&quot;:&quot;作为分隔符awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125; &#123;printf &quot;%d\\n&quot;,$3&#125;&#x27; /etc/passwd# 3、以浮点数格式打印/etc/passwd中的第3个字段，以&quot;:&quot;作为分隔符awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125; &#123;printf &quot;%0.3f\\n&quot;,$3&#125;&#x27; /etc/passwd# 4、以16进制数格式打印/etc/passwd中的第3个字段，以&quot;:&quot;作为分隔符awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125; &#123;printf &quot;%#x\\n&quot;,$3&#125;&#x27; /etc/passwd# 5、以8进制数格式打印/etc/passwd中的第3个字段，以&quot;:&quot;作为分隔符awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125; &#123;printf &quot;%#o\\n&quot;,$3&#125;&#x27; /etc/passwd# 6、以科学计数法格式打印/etc/passwd中的第3个字段，以&quot;:&quot;作为分隔符awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125; &#123;printf &quot;%e\\n&quot;,$3&#125;&#x27; /etc/passwd 2.3.3 awk模式匹配的两种用法 awk模式匹配： RegExp：按正则表达式匹配 关系运算匹配：按关系运算匹配 RegExp(正则表达式匹配) 1234# 匹配/etc/passwd文件行中含有root字符串的所有行awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;/root/&#123;print $0&#125;&#x27; /etc/passwd# 匹配/etc/passwd文件行中以yarn开头的所有行awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;/^yarn/&#123;print $0&#125;&#x27; /etc/passwd 运算符匹配(&lt;:小于，&gt;:大于，&lt;=:小于等于，&gt;=:大于等于，==:等于，!=:不等于，~:匹配正则表达式，!~:不匹配正则表达式) 12345678910# 以:为分隔符，匹配/etc/passwd文件中第3个字段小于50的所有行信息awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;$3&lt;50&#123;print $0&#125;&#x27; /etc/passwd# 以:为分隔符，匹配/etc/passwd文件中第3个字段大于50的所有行信息awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;$3&gt;50&#123;print $0&#125;&#x27; /etc/passwd# 以:为分隔符，匹配/etc/passwd文件中第7个字段为/bin/bash的所有行信息awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;$7==&quot;/bin/bash&quot;&#123;print $0&#125;&#x27; /etc/passwd# 以:为分隔符，匹配/etc/passwd文件中第7个字段不为/bin/bash的所有行信息awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;$7!=&quot;/bin/bash&quot;&#123;print $0&#125;&#x27; /etc/passwd# 以:为分隔符，匹配/etc/passwd中第3个字段包含3个以上数字的所有行信息awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;$3~/[0-9]&#123;3,&#125;/&#123;print $0&#125;&#x27; /etc/passwd 布尔运算符匹配(||:或，&amp;&amp;:与，!:非) 1234# 以:为分隔符，匹配/etc/passwd文件中包含hdfs或yarn的所有行信息awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;$1==&quot;hdfs&quot; || $1==&quot;yarn&quot; &#123;print $0&#125;&#x27; /etc/passwd# 以:为分隔符，匹配/etc/passwd文件中第3个字段小于50并且第4个字段大于50的所有行信息awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;$3&lt;50 &amp;&amp; $4&gt;50 &#123;print $0&#125;&#x27; /etc/passwd 2.3.4 awk表达式用法 运算符 解析 + 加 / 除 % 模 ^或** 乘方 ++x 在返回x变量之前，×变量加1 X++ 在返回x变量之后，×变量加1 12# 使用awk计算/etc/services中的空白行数量awk &#x27;/^$/&#123;sum++&#125;END&#123;print sum&#125;&#x27; /etc/services 2.3.5 awk动作中的条件及循环语句12345678910# 以:为分隔符，只打印/etc/passwd中第3个字段的数值在50-100范围内的行信息awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125;&#123; if($3&lt;50)&#123; printf &quot;%-20s%-20s%-5d\\n&quot;,&quot;小于50的UID&quot;,$1,$3 &#125; else if($3&gt;100) &#123; printf &quot;%-20s%-20s%-5d\\n&quot;,&quot;大 于100的UID&quot;,$1,$3 &#125; else&#123; printf &quot;%-20s%-20s%-5d\\n&quot;,&quot;大于50 小于100的UID&quot;,$1,$3 &#125;&#125;&#x27; /etc/passwd 2.3.6 awk中的字符串函数 函数名 解析 length(str) 计算长度 index(str1,str2) 返回在str1中查询到的str2的位置 tolower(str) 小写转换 toupper(str) 大写转换 split(str,arr,fs) 分隔字符串，并保存到数组中 match(str,RE) 返回正则表达式匹配到的子串的位置 substr(str,m,n) 截取子串，从m个字符开始，截取n位。n若不指定，则默认截取到字符串尾 sub(RE,RepStr,str) 替换查找到的第一个子串 gsub(RE,RepStr,str) 替换查找到的所有子串 12345678910111213141516171819202122# 搜索字符串&quot;I have a dream&quot;中出现&quot;ea&quot;子串的位置awk &#x27;BEGIN&#123;str=&quot;I hava a dream&quot;;location=index(str,&quot;ea&quot;);print location&#125;awk &#x27;BEGIN&#123;str=&quot;I hava a dream&quot;;location=match(str,&quot;ea&quot;);print location&#125;&#x27;# 将字符串&quot;Hadoop is a bigdata Framawork&quot;全部转换为小写awk &#x27;BEGIN&#123;str=&quot;Hadoop is a bigdata Framework&quot;;print tolower(str)&#125;&#x27;# 将字符串&quot;Hadoop is a bigdata Framawork&quot;全部转换为大写awk &#x27;BEGIN&#123;str=&quot;Hadoop is a bigdata Framework&quot;;print toupper(str)&#125;&#x27;# 将字符串&quot;Hadoop Kafka Spark Storm HDFS YARN Zookeeper&quot;，按照空格为分隔符，分隔每部分保存到数组array中awk &#x27;BEGIN&#123;str=&quot;Hadoop Kafka Spark Storm HDFS YARN Zookeeper&quot;;split(str,arr,&quot; &quot;);for(a in arr) print arr[a]&#125;&#x27;# 搜索字符串&quot;Tranction 2345 Start:Select * from master&quot;第一个数字出现的位置awk &#x27;BEGIN&#123;str=&quot;Tranction 2345 Start:Select * from master&quot;;location=match(str,/[0-9]/);print location&#125;&#x27;# 截取字符串&quot;transaction start&quot;的子串，截取条件从第4个字符开始，截取5位awk &#x27;BEGIN&#123;str=&quot;transaction start&quot;;print substr(str,4,5)&#125;&#x27;# 替换字符串&quot;Tranction 243 Start,Event ID:9002&quot;中第一个匹配到的数字串为$符号awk &#x27;BEGIN&#123;str=&quot;Tranction 243 Start,Event ID:9002&quot;;count=sub(/[0-9]+/,&quot;$&quot;,str);print count,str&#125;&#x27;awk &#x27;BEGIN&#123;str=&quot;Tranction 243 Start,Event ID:9002&quot;;count=gsub(/[0-9]+/,&quot;$&quot;,str);print count,str&#125;&#x27; 2.3.7 awk的常用选项 选项 解析 -v 参数传递 -f 指定脚本文件 -F 指定分隔符 -V 查看awk的版本号 2.3.8 awk中数组的用法在awk中，使用数组时，不仅可以使用1.2…n作为数组下标，也可以使用字符串作为数组下标。 12345678910111213141516171819202122232425262728293031323334# 打印元素：echo $&#123;array[2]&#125;# 打印元素个数：echo $&#123;#array[@]&#125;# 打印元素长度：echo $&#123;#array[3]&#125;# 给元素赋值：array[3]=&quot;Li&quot;# 删除元素：unset array[2];unset array# 分片访问：echo $&#123;array[@]:1:3&#125;# 元素内容替换：$&#123;array[@]/e/E&#125;只替换第一个e；$tarray[@]//e/E&#125;替换所有的e# 数组的遍历：for a in array do echo $a done # 使用字符串作为数组下标array[&quot;var1&quot;]=&quot;Jin&quot;array[&quot;var2&quot;]=&quot;Hao&quot;array[&quot;var3&quot;]=&quot;Fang&quot;for(a in array) print array[a] 12# 统计主机上所有的TCP连接状态数，按照每个TCP状态分类netstat -an | grep tcp | awk &#x27;&#123;array[$6]++&#125; END&#123;for(a in array) print a,array[a]&#125;&#x27;","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://chaooo.github.io/tags/Shell/"}]},{"title":"「Shell」Shell 函数的用法","date":"2020-06-08T07:12:46.000Z","path":"2020/06/08/env-shell-function.html","text":"1. Shell 函数定义1234function name() &#123; # 函数要执行的代码 [return value]&#125; 函数的简化写法可以省略function或()之一，不过还是推荐标准写法，才能做到做到“见名知意”。 2. Shell函数调用调用 Shell 函数时可以给它传递参数，也可以不传递。如果不传递参数，直接给出函数名字即可： 1name 如果传递参数，那么多个参数之间以空格分隔： 1name param1 param2 param3 不管是哪种形式，函数名字后面都不需要带括号。 和其它编程语言不同的是，Shell 函数在定义时不能指明参数，但是在调用时却可以传递参数，并且给它传递什么参数它就接收什么参数。 Shell 也不限制定义和调用的顺序，你可以将定义放在调用的前面，也可以反过来，将定义放在调用的后面。 定义一个函数，计算所有参数的和： 123456789101112[cmuser@localhost ~]$ #!/bin/bash[cmuser@localhost ~]$ function getsum()&#123;&gt; local sum=0&gt; for n in $@&gt; do&gt; ((sum+=n))&gt; done&gt; return $sum&gt; &#125;[cmuser@localhost ~]$ getsum 10 20 55 15[cmuser@localhost ~]$ echo $?100 3. Shell函数参数函数参数是 Shell 位置参数的一种，在函数内部可以使用$n来接收，例如，$1表示第一个参数，$2表示第二个参数，依次类推。 除了$n，还有另外三个比较重要的变量： $#可以获取传递的参数的个数； $@或者$*可以一次性获取所有的参数。 使用 $n 来接收函数参数: 1234567891011121314[cmuser@localhost ~]$ #!/bin/bash# 定义函数[cmuser@localhost ~]$ function show()&#123;&gt; echo &quot;Tutorial: $1&quot;&gt; echo &quot;URL: $2&quot;&gt; echo &quot;Author: &quot;$3&gt; echo &quot;Total $# parameters&quot;&gt; &#125;# 调用函数[cmuser@localhost ~]$ show test www.test.com TomTutorial: testURL: www.test.comAuthor: TomTotal 3 parameters 使用 $@ 来遍历函数参数，计算所有参数的和： 1234567891011121314[cmuser@localhost ~]$ #!/bin/bash# 定义函数[cmuser@localhost ~]$ function getsum()&#123;&gt; local sum=0&gt; for n in $@&gt; do&gt; ((sum+=n))&gt; done&gt; echo $sum&gt; return 0&gt; &#125;# 调用函数[cmuser@localhost ~]$ echo $(getsum 10 20 55 15)100 4. Shell函数返回值Shell中的return返回值表示的是函数的退出状态：返回值为0表示函数执行成功了，返回值为非 0 表示函数执行失败（出错）了。if、while、for 等语句都是根据函数的退出状态来判断条件是否成立。 Shell 函数的返回值只能是一个介于0~255之间的整数，其中只有0表示成功，其它值都表示失败。 如果函数体中没有return语句，那么使用默认的退出状态，也就是最后一条命令的退出状态。更加严谨的写法为：return $?。$?是一个特殊变量，用来获取上一个命令的退出状态，或者上一个函数的返回值。 如何得到函数的处理结果？ 借助全局变量，将得到的结果赋值给全局变量； 在函数内部使用 echo、printf 命令将结果输出，在函数外部使用$()或者`捕获结果。 具体来定义一个函数 getsum，计算从 m 加到 n 的和，并使用以上两种解决方案。 【实例1】将函数处理结果赋值给一个全局变量。 1234567891011[cmuser@localhost ~]$ #!/bin/bash[cmuser@localhost ~]$ sum=0 # 全局变量[cmuser@localhost ~]$ function getsum()&#123;&gt; for((i=$1; i&lt;=$2; i++)); do&gt; ((sum+=i))&gt; done&gt; return $? # 返回上一条命令的退出状态&gt; &#125;[cmuser@localhost ~]$ getsum 1 100[cmuser@localhost ~]$ echo &quot;The sum is $sum&quot; # 输出全局变量The sum is 5050 【实例2】在函数内部使用 echo 输出结果。12345678910[cmuser@localhost ~]$ function getsum()&#123;&gt; local sum=0 # 局部变量&gt; for((i=$1; i&lt;=$2; i++)); do&gt; ((sum+=i))&gt; done&gt; echo $sum&gt; return $?&gt; &#125;[cmuser@localhost ~]$ echo &quot;The sum is &quot;$(getsum 1 100)The sum is 5050 代码中总共执行了两次 echo 命令，但是却只输出一次，这是因为$()捕获了第一个 echo 的输出结果，它并没有真正输出到终端上。除了$()，你也可以使用`来捕获 echo 的输出结果。 5. Shell函数库的使用shell函数库实质为一个脚本，脚本内包含了多个函数（函数具有普遍适用性），经常使用的重复代码封装成库函数文件。 库函数一般不直接执行，而是由其他脚本调用，库函数文件名的后缀是任意的，但一般使用.lib，库文件通常没有可执行权限。 第一行一般使用#!&#x2F;bin&#x2F;echo，输出警告信息，避免用户执行。 123456789101112131415[cmuser@localhost test]$ vim base.lib #!/usr/bin/echoadd()&#123; echo &quot;$(expr $1 + $2)&quot;&#125;reduce()&#123; echo &quot;$(expr $1 - $2)&quot;&#125;multiple()&#123; echo &quot;$(expr $1 \\* $2)&quot;&#125;divide()&#123; echo &quot;$(expr $1 / $2)&quot;&#125; 1234567891011[cmuser@localhost test]$ vim calculate.sh#!/usr/bin/bash# 加载函数库文件source ./base# 调用函数，传入参数add $1 $2reduce $1 $2multiple $1 $2divide $1 $2 12345[cmuser@localhost test]$ sh calculate.sh 40 545352008","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://chaooo.github.io/tags/Shell/"}]},{"title":"「Shell」Shell 变量的用法与数学运算","date":"2020-06-07T07:12:46.000Z","path":"2020/06/07/env-shell-variable.html","text":"1. 变量替换 语法 说明 $&#123;变量#匹配规则&#125; 从变量开头进行规则匹配，将符合最短的数据删除 $&#123;变量##匹配规则&#125; 从变量开头进行规则匹配，将符合最长的数据删除【贪婪模式】 $&#123;变量%匹配规则&#125; 从变量尾部进行规则匹配，将符合最短的数据删除 $&#123;变量%%匹配规则&#125; 从变量尾部进行规则匹配，将符合最长的数据删除【贪婪模式】 $&#123;变量/旧字符串/新字符串&#125; 变量内容符合旧字符串，则第一个旧字符串会被新字符串取代 $&#123;变量//旧字符串/新字符串&#125; 变量内容符合旧字符串，则全部的旧字符串会被新字符串取代 123456789101112131415161718192021222324[chao@localhost ~]$ var1=&quot;I love you, Do you love me&quot;[chao@localhost ~]$ echo $var1I love you, Do you love me# 头部匹配删除[chao@localhost ~]$ var2=$&#123;var1#*ov&#125;[chao@localhost ~]$ echo $var2e you, Do you love me[chao@localhost ~]$ var3=$&#123;var1##*ov&#125; # 贪婪模式[chao@localhost ~]$ echo $var3e me# 尾部匹配删除[chao@localhost ~]$ var4=$&#123;var1%ov*&#125;[chao@localhost ~]$ echo $var4I love you, Do you l[chao@localhost ~]$ var5=$&#123;var1%%ov*&#125; # 贪婪模式[chao@localhost ~]$ echo $var5I l# 替换[chao@localhost ~]$ var6=$&#123;var1/love/hate&#125;[chao@localhost ~]$ echo $var6I hate you, Do you love me[chao@localhost ~]$ var7=$&#123;var1//love/hate&#125;[chao@localhost ~]$ echo $var7I hate you, Do you hate me 2. 字符串相关操作 计算字符串的长度 语法 说明 &#96;$","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://chaooo.github.io/tags/Shell/"}]},{"title":"「Shell」Shell 编程入门","date":"2020-06-06T07:12:46.000Z","path":"2020/06/06/env-shell-base.html","text":"1. Shell概述 Shell简介 shell是操作系统提供给我们用户来访问系统资源的一个接口。 shell同时还是一个Linux下的命令行解释器，类似Windows下的cmd。 shell 同时还是解释型的脚本语言：运行时翻译，执行一条语句翻译一条，每次执行程序都需要进行解释。 Shell的发展 shell有多个版本：Bourne Shell，C Shell，Korn Shell，Bash Shell。现在广泛使用的是 Bash Shell，也就是Linux中默认内嵌的Shell。 Shell脚本 Shell脚本(shell script)，是一种为Shell编写的脚本程序。 2. Shell变量基础2.1 Shell变量的分类 用户自定义变量：由用户自己定义，修改和使用的变量 Shell环境变量：用于设置shell的运行环境，只要少数的环境变量可以修改其值。环境变量也是可以自定义的。 位置参数变量：通过命令行给脚本传递参数，变量名已经固定，不能自定义。 内部参数变量：是bash中已经定义好的变量，变量名不能自定义，变量作用也是固定的。 2.2 变量赋值与设置123456789# 赋值有两种格式var1=valuevar2=`command` # 注意：command是可以执行的命令# 清除变量 与 设置只读unset var1 # 清除var1变量readonly var2 # 设置var2变量为只读# 变量的引用有两种方式（使用时加美元符号）$var1 # $变量名$&#123;var1&#125; # $&#123;变量名&#125; 2.3 位置参数变量位置参数变量是一种特殊的shell变量，用于从命令行向脚本中传递参数。$0表示脚本的名称，$1表示第一个参数，$2表示第二个参数，依次下去代表第几个参数，但是从第十个参数位开始表示方法有所改变，需要加大括号，例如：${10}，${11} 1[chao@localhost ~]$ ls anaconda-ks.cfg install.log install.log.syslog $0的值就是ls命令本身，$1的值就是anaconda-ks.cfg这个文件，$2是install.log文件，$3是install.log.syslog文件 位置参数变量 作 用 $n n 为数字，$0 代表命令本身，$1〜$9 代表第 1〜9 个参数，10 以上的参数需要用大括号包含， 如${10} $* 这个变量代表命令行中所有的参数，把所有的参数看成一个整体 $@ 这个变量也代表命令行中所有的参数，不过 $@ 把每个参数区别对待 $# 这个变量代表命令行中所有参数的个数 位置参数变量要用于向命令或程序脚本中传递信息，比如： 123456789[chao@localhost ~]$ vi count.sh#!/bin/bashnum1=$1num2=$2sum=$(($num1 + $num2))echo $sum[chao@localhost ~]$ ./count.sh 11 2233 2.4 Shell环境变量 Shell 变量的作用域可以分为三种： 有的变量只能在函数内部使用，这叫做局部变量（local variable）； 有的变量可以在当前 Shell 进程中使用，这叫做全局变量（global variable），在 Shell 中定义的变量，默认就是全局变量； 而有的变量还可以在子进程中使用，这叫做环境变量（environment variable）。全局变量只在当前 Shell 进程中有效，对其它 Shell 进程和子进程都无效。如果使用export命令将全局变量导出，那么它就在所有的子进程中也有效了，这称为“环境变量”。 2.5 内置参数变量内部参数分为两类：命令行参数，与进程相关的内部参数 命令行参数 1234$@ # 表示传递给脚本或函数的所有参数。被双引号引用时，与$*有所不同。$* # 表示传递给脚本或函数的所有参数$0 # 表示命令行输入的脚本名称$# # 表示命令行上的参数个数 注意： $@和$*不加引号时,二者都是返回传入的参数,但加了引号后$*把参数作为一个字符串整体(单字符串)返回, $@把每个参数作为一个一个字符串返回 与进程相关的内部参数 1234$? # 表示上一个命令执行的返回结果$$ # 表示当前程序运行的 PID $! # 表示获取上一个在后台工作进程的PID$_ # 表示获取在此之前执行命令或脚本的最后一个参数 3. 退出状态码exit是一个Shell内置命令，用来退出当前Shell进程，并返回一个退出状态；使用$?可以接收这个退出状态。exit命令可以接受一个整数值作为参数，代表退出状态。如果不指定，默认状态值是 0。exit退出状态只能是一个介于 0~255 之间的整数，其中只有 0 表示成功，其它值都表示失败，可以根据退出状态来判断具体出现了什么错误。 12345678910[chao@localhost ~]$ vi test.sh#!/bin/bashecho &quot;befor exit&quot;exit 8echo &quot;after exit&quot;[chao@localhost ~]$ bash ./test.shbefor exit[chao@localhost ~]$ echo $?8","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://chaooo.github.io/tags/Shell/"}]},{"title":"「环境配置」Linux常用命令整理","date":"2020-06-05T12:08:46.000Z","path":"2020/06/05/env-linux.html","text":"1. 常用基础命令 文件管理 12345ls # 显示指定工作目录下的内容及属性信息cp # 复制文件或目录mkdir # 创建目录mv # 移动或改名文件pwd # 显示当前路径 文档编辑 12345cat # 在终端设备上显示文件内容echo # 输出字符串或提取Shell变量的值rm # 移除文件或目录tail # 查看文件尾部内容rmdir # 删除空目录 系统管理 12345startx # 初始化X-windowsrpm # RPM软件包管理器find # 查找和搜索文件vmstat # 显示虚拟内存状态uname # 显示系统信息 磁盘管理 12345df # 显示磁盘空间使用情况fdisk # 磁盘分区hdparm # 显示与设定硬盘参数lsblk # 查看系统的磁盘vgextend # 扩展卷组 文件传输 12345tftp # 上传及下载文件curl # 文件传输工具fsck # 检查并修复Linux文件系统ftpwho # 显示ftp会话信息lprm # 删除打印队列中的打印任务 网络通讯 12345ssh # 安全连接客户端netstat # 显示网络状态ping # 测试主机间网络连通性ifconfig # 显示或设置网络设备dhclient # 动态获取或释放IP地址 设备管理 12345mount # 文件系统挂载MAKEDEV # 建立设备setleds # 设定键盘上方三个 LED 的状态lspci # 显示当前设备所有PCI总线信息sensors # 检测服务器内部温度及电压 备份压缩 12345zipinfo # 查看压缩文件信息gzip # 压缩和解压文件zip # 压缩文件unarj # 解压.arj文件unzip # 解压缩zip文件 其他命令 1234567891011121314151617181920212223242526chsh # 查看和修改当前登录的Shellexport # 查看和设置Shell环境变量read # 读取从键盘或文件输入的数据expr # 四则远算和字符串运算tmux # 一个窗口操作多个会话alias # 给命令定义别名history # 显示与操纵历史命令xargs # 一个给其他命令传递参数的过滤器time # 测量命令的执行时间或者系统资源的使用情况sleep # 让程序暂停或休眠一段时间file # 查看文件信息或类型ln # 创建文件链接du # 查看文件夹和文件的磁盘占用情况bzip2 # 压缩和解压文件（.bz2文件）dd # 复制（拷贝）文件，并对原文件进行转换grep # 文本搜索工具（可使用正则表达式）cut # 剪切文件中的数据wc # 计算单个文件中的字数、单词数和字节数split # 切割（拆分）文件paste # 合并（拼接）文件Vim # 替换文本内容sed # 替换、删除、更新文件中的内容hash # 显示与清除命令运行时查询的哈希表wait # 等待指令bc # 浮点运算rmmod # 删除模块 2. 命令实例 系统信息 12345678910111213141516171819arch # 显示机器的处理器架构(1) uname -m # 显示机器的处理器架构(2) uname -r # 显示正在使用的内核版本 dmidecode -q # 显示硬件系统部件 - (SMBIOS / DMI) hdparm -i /dev/hda # 罗列一个磁盘的架构特性 hdparm -tT /dev/sda # 在磁盘上执行测试性读取操作 cat /proc/cpuinfo # 显示CPU info的信息 cat /proc/interrupts # 显示中断 cat /proc/meminfo # 校验内存使用 cat /proc/swaps # 显示哪些swap被使用 cat /proc/version # 显示内核的版本 cat /proc/net/dev # 显示网络适配器及统计 cat /proc/mounts # 显示已加载的文件系统 lspci -tv # 罗列 PCI 设备 lsusb -tv # 显示 USB 设备 date # 显示系统日期 cal 2007 # 显示2007年的日历表 date 041217002007.00 # 设置日期和时间 - 月日时分年.秒 clock -w # 将时间修改保存到 BIOS 关机 (系统的关机、重启以及登出) 12345678shutdown -h now # 关闭系统(1) init 0 # 关闭系统(2) telinit 0 # 关闭系统(3) shutdown -h hours:minutes &amp; # 按预定时间关闭系统 shutdown -c # 取消按预定时间关闭系统 shutdown -r now # 重启(1) reboot # 重启(2) logout # 注销 文件和目录 1234567891011121314151617181920212223242526272829cd /home # 进入 &#x27;/ home&#x27; 目录&#x27; cd .. # 返回上一级目录 cd ../.. # 返回上两级目录 cd # 进入个人的主目录 cd ~user1 # 进入个人的主目录 cd - # 返回上次所在的目录 pwd # 显示工作路径 ls # 查看目录中的文件 ls -F # 查看目录中的文件 ls -l # 显示文件和目录的详细资料 ls -a # 显示隐藏文件 ls *[0-9]* # 显示包含数字的文件名和目录名 tree # 显示文件和目录由根目录开始的树形结构(1) lstree # 显示文件和目录由根目录开始的树形结构(2) mkdir dir1 # 创建一个叫做 &#x27;dir1&#x27; 的目录&#x27; mkdir dir1 dir2 # 同时创建两个目录 mkdir -p /tmp/dir1/dir2 # 创建一个目录树 rm -f file1 # 删除一个叫做 &#x27;file1&#x27; 的文件&#x27; rmdir dir1 # 删除一个叫做 &#x27;dir1&#x27; 的目录&#x27; rm -rf dir1 # 删除一个叫做 &#x27;dir1&#x27; 的目录并同时删除其内容 rm -rf dir1 dir2 # 同时删除两个目录及它们的内容 mv dir1 new_dir # 重命名/移动 一个目录 cp file1 file2 # 复制一个文件 cp dir/* . # 复制一个目录下的所有文件到当前工作目录 cp -a /tmp/dir1 . # 复制一个目录到当前工作目录 cp -a dir1 dir2 # 复制一个目录 ln -s file1 lnk1 # 创建一个指向文件或目录的软链接 ln file1 lnk1 # 创建一个指向文件或目录的物理链接 touch -t 0712250000 file1 # 修改一个文件或目录的时间戳 - (YYMMDDhhmm) 文件搜索 12345678910find / -name file1 # 从 &#x27;/&#x27; 开始进入根文件系统搜索文件和目录 find / -user user1 # 搜索属于用户 &#x27;user1&#x27; 的文件和目录 find /home/user1 -name \\*.bin # 在目录 &#x27;/ home/user1&#x27; 中搜索带有&#x27;.bin&#x27; 结尾的文件 find /usr/bin -type f -atime +100 # 搜索在过去100天内未被使用过的执行文件 find /usr/bin -type f -mtime -10 # 搜索在10天内被创建或者修改过的文件 find / -name \\*.rpm -exec chmod 755 &#x27;&#123;&#125;&#x27; \\; # 搜索以 &#x27;.rpm&#x27; 结尾的文件并定义其权限 find / -xdev -name \\*.rpm # 搜索以 &#x27;.rpm&#x27; 结尾的文件，忽略光驱、捷盘等可移动设备 locate \\*.ps # 寻找以 &#x27;.ps&#x27; 结尾的文件 - 先运行 &#x27;updatedb&#x27; 命令 whereis halt # 显示一个二进制文件、源码或man的位置 which halt # 显示一个二进制文件或可执行文件的完整路径 挂载一个文件系统 123456789101112mount /dev/hda2 /mnt/hda2 # 挂载一个叫做hda2的盘 - 确定目录 &#x27;/ mnt/hda2&#x27; 已经存在 umount /dev/hda2 # 卸载一个叫做hda2的盘 - 先从挂载点 &#x27;/ mnt/hda2&#x27; 退出 fuser -km /mnt/hda2 # 当设备繁忙时强制卸载 umount -n /mnt/hda2 # 运行卸载操作而不写入 /etc/mtab 文件- 当文件为只读或当磁盘写满时非常有用 mount /dev/fd0 /mnt/floppy # 挂载一个软盘 mount /dev/cdrom /mnt/cdrom # 挂载一个cdrom或dvdrom mount /dev/hdc /mnt/cdrecorder # 挂载一个cdrw或dvdrom mount /dev/hdb /mnt/cdrecorder # 挂载一个cdrw或dvdrom mount -o loop file.iso /mnt/cdrom # 挂载一个文件或ISO镜像文件 mount -t vfat /dev/hda5 /mnt/hda5 # 挂载一个Windows FAT32文件系统 mount /dev/sda1 /mnt/usbdisk # 挂载一个usb 捷盘或闪存设备 mount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share # 挂载一个windows网络共享 磁盘空间 123456df -h # 显示已经挂载的分区列表 ls -lSr |more # 以尺寸大小排列文件和目录 du -sh dir1 # 估算目录 &#x27;dir1&#x27; 已经使用的磁盘空间&#x27; du -sk * | sort -rn # 以容量大小为依据依次显示文件和目录的大小 rpm -q -a --qf &#x27;%10&#123;SIZE&#125;t%&#123;NAME&#125;n&#x27; | sort -k1,1n # 以大小为依据依次显示已安装的rpm包所使用的空间 (fedora, redhat类系统) dpkg-query -W -f=&#x27;$&#123;Installed-Size;10&#125;t$&#123;Package&#125;n&#x27; | sort -k1,1n # 以大小为依据显示已安装的deb包所使用的空间 (ubuntu, debian类系统) 用户和群组 12345678910111213groupadd group_name # 创建一个新用户组 groupdel group_name # 删除一个用户组 groupmod -n new_group_name old_group_name # 重命名一个用户组 useradd -c &quot;Name Surname &quot; -g admin -d /home/user1 -s /bin/bash user1 # 创建一个属于 &quot;admin&quot; 用户组的用户 useradd user1 # 创建一个新用户 userdel -r user1 # 删除一个用户 ( &#x27;-r&#x27; 排除主目录) usermod -c &quot;User FTP&quot; -g system -d /ftp/user1 -s /bin/nologin user1 # 修改用户属性 passwd # 修改口令 passwd user1 # 修改一个用户的口令 (只允许root执行) chage -E 2005-12-31 user1 # 设置用户口令的失效期限 pwck # 检查 &#x27;/etc/passwd&#x27; 的文件格式和语法修正以及存在的用户 grpck # 检查 &#x27;/etc/passwd&#x27; 的文件格式和语法修正以及存在的群组 newgrp group_name #登陆进一个新的群组以改变新创建文件的预设群组 文件的权限 - 使用 “+” 设置权限，使用 “-“ 用于取消 123456789101112131415ls -lh # 显示权限 ls /tmp | pr -T5 -W$COLUMNS # 将终端划分成5栏显示 chmod ugo+rwx directory1 # 设置目录的所有人(u)、群组(g)以及其他人(o)以读（r ）、写(w)和执行(x)的权限 chmod go-rwx directory1 # 删除群组(g)与其他人(o)对目录的读写执行权限 chown user1 file1 # 改变一个文件的所有人属性 chown -R user1 directory1 # 改变一个目录的所有人属性并同时改变改目录下所有文件的属性 chgrp group1 file1 # 改变文件的群组 chown user1:group1 file1 # 改变一个文件的所有人和群组属性 find / -perm -u+s # 罗列一个系统中所有使用了SUID控制的文件 chmod u+s /bin/file1 # 设置一个二进制文件的 SUID 位 - 运行该文件的用户也被赋予和所有者同样的权限 chmod u-s /bin/file1 # 禁用一个二进制文件的 SUID位 chmod g+s /home/public # 设置一个目录的SGID 位 - 类似SUID ，不过这是针对目录的 chmod g-s /home/public # 禁用一个目录的 SGID 位 chmod o+t /home/public # 设置一个文件的 STIKY 位 - 只允许合法所有人删除文件 chmod o-t /home/public # 禁用一个目录的 STIKY 位 文件的特殊属性 - 使用 “+” 设置权限，使用 “-“ 用于取消 12345678chattr +a file1 # 只允许以追加方式读写文件 chattr +c file1 # 允许这个文件能被内核自动压缩/解压 chattr +d file1 # 在进行文件系统备份时，dump程序将忽略这个文件 chattr +i file1 # 设置成不可变的文件，不能被删除、修改、重命名或者链接 chattr +s file1 # 允许一个文件被安全地删除 chattr +S file1 # 一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘 chattr +u file1 # 若文件被删除，系统会允许你在以后恢复这个被删除的文件 lsattr # 显示特殊的属性 打包和压缩文件 123456789101112131415161718192021bunzip2 file1.bz2 # 解压一个叫做 &#x27;file1.bz2&#x27;的文件 bzip2 file1 # 压缩一个叫做 &#x27;file1&#x27; 的文件 gunzip file1.gz # 解压一个叫做 &#x27;file1.gz&#x27;的文件 gzip file1 # 压缩一个叫做 &#x27;file1&#x27;的文件 gzip -9 file1 # 最大程度压缩 rar a file1.rar test_file # 创建一个叫做 &#x27;file1.rar&#x27; 的包 rar a file1.rar file1 file2 dir1 # 同时压缩 &#x27;file1&#x27;, &#x27;file2&#x27; 以及目录 &#x27;dir1&#x27; rar x file1.rar # 解压rar包 unrar x file1.rar # 解压rar包 tar -cvf archive.tar file1 # 创建一个非压缩的 tarball tar -cvf archive.tar file1 file2 dir1 # 创建一个包含了 &#x27;file1&#x27;, &#x27;file2&#x27; 以及 &#x27;dir1&#x27;的档案文件 tar -tf archive.tar # 显示一个包中的内容 tar -xvf archive.tar # 释放一个包 tar -xvf archive.tar -C /tmp # 将压缩包释放到 /tmp目录下 tar -cvfj archive.tar.bz2 dir1 # 创建一个bzip2格式的压缩包 tar -xvfj archive.tar.bz2 # 解压一个bzip2格式的压缩包 tar -cvfz archive.tar.gz dir1 # 创建一个gzip格式的压缩包 tar -xvfz archive.tar.gz # 解压一个gzip格式的压缩包 zip file1.zip file1 # 创建一个zip格式的压缩包 zip -r file1.zip file1 file2 dir1 # 将几个文件和目录同时压缩成一个zip格式的压缩包 unzip file1.zip # 解压一个zip格式压缩包 RPM 包 - （Fedora, Redhat及类似系统） 1234567891011121314151617181920212223242526rpm -ivh package.rpm # 安装一个rpm包 rpm -ivh --nodeeps package.rpm # 安装一个rpm包而忽略依赖关系警告 rpm -U package.rpm # 更新一个rpm包但不改变其配置文件 rpm -F package.rpm # 更新一个确定已经安装的rpm包 rpm -e package_name.rpm # 删除一个rpm包 rpm -qa # 显示系统中所有已经安装的rpm包 rpm -qa | grep httpd # 显示所有名称中包含 &quot;httpd&quot; 字样的rpm包 rpm -qi package_name # 获取一个已安装包的特殊信息 rpm -qg &quot;System Environment/Daemons&quot; # 显示一个组件的rpm包 rpm -ql package_name # 显示一个已经安装的rpm包提供的文件列表 rpm -qc package_name # 显示一个已经安装的rpm包提供的配置文件列表 rpm -q package_name --whatrequires # 显示与一个rpm包存在依赖关系的列表 rpm -q package_name --whatprovides # 显示一个rpm包所占的体积 rpm -q package_name --scripts # 显示在安装/删除期间所执行的脚本l rpm -q package_name --changelog # 显示一个rpm包的修改历史 rpm -qf /etc/httpd/conf/httpd.conf # 确认所给的文件由哪个rpm包所提供 rpm -qp package.rpm -l # 显示由一个尚未安装的rpm包提供的文件列表 rpm --import /media/cdrom/RPM-GPG-KEY # 导入公钥数字证书 rpm --checksig package.rpm # 确认一个rpm包的完整性 rpm -qa gpg-pubkey # 确认已安装的所有rpm包的完整性 rpm -V package_name # 检查文件尺寸、 许可、类型、所有者、群组、MD5检查以及最后修改时间 rpm -Va # 检查系统中所有已安装的rpm包- 小心使用 rpm -Vp package.rpm # 确认一个rpm包还未安装 rpm2cpio package.rpm | cpio --extract --make-directories *bin* # 从一个rpm包运行可执行文件 rpm -ivh /usr/src/redhat/RPMS/`arch`/package.rpm # 从一个rpm源码安装一个构建好的包 rpmbuild --rebuild package_name.src.rpm # 从一个rpm源码构建一个 rpm 包 YUM 软件包升级器 - （Fedora, RedHat及类似系统） 12345678910yum install package_name # 下载并安装一个rpm包 yum localinstall package_name.rpm # 将安装一个rpm包，使用你自己的软件仓库为你解决所有依赖关系 yum update package_name.rpm # 更新当前系统中所有安装的rpm包 yum update package_name # 更新一个rpm包 yum remove package_name # 删除一个rpm包 yum list # 列出当前系统中安装的所有包 yum search package_name # 在rpm仓库中搜寻软件包 yum clean packages # 清理rpm缓存删除下载的包 yum clean headers # 删除所有头文件 yum clean all # 删除所有缓存的包和头文件 DEB 包 (Debian, Ubuntu 以及类似系统) 12345678dpkg -i package.deb # 安装/更新一个 deb 包 dpkg -r package_name # 从系统删除一个 deb 包 dpkg -l # 显示系统中所有已经安装的 deb 包 dpkg -l | grep httpd # 显示所有名称中包含 &quot;httpd&quot; 字样的deb包 dpkg -s package_name # 获得已经安装在系统中一个特殊包的信息 dpkg -L package_name # 显示系统中已经安装的一个deb包所提供的文件列表 dpkg --contents package.deb # 显示尚未安装的一个包所提供的文件列表 dpkg -S /bin/ping # 确认所给的文件由哪个deb包提供 APT 软件工具 (Debian, Ubuntu 以及类似系统) 12345678apt-get install package_name # 安装/更新一个 deb 包 apt-cdrom install package_name # 从光盘安装/更新一个 deb 包 apt-get update # 升级列表中的软件包 apt-get upgrade # 升级所有已安装的软件 apt-get remove package_name # 从系统删除一个deb包 apt-get check # 确认依赖的软件仓库正确 apt-get clean # 从下载的软件包中清理缓存 apt-cache search searched-package # 返回包含所要搜索字符串的软件包名称 查看文件内容 1234567cat file1 # 从第一个字节开始正向查看文件的内容 tac file1 # 从最后一行开始反向查看一个文件的内容 more file1 # 查看一个长文件的内容 less file1 # 类似于 &#x27;more&#x27; 命令，但是它允许在文件中和正向操作一样的反向操作 head -2 file1 # 查看一个文件的前两行 tail -2 file1 # 查看一个文件的最后两行 tail -f /var/log/messages # 实时查看被添加到一个文件中的内容 文本处理 123456789101112131415161718192021222324252627282930cat file1 | command( sed, grep, awk, grep, etc...) &gt; result.txt # 合并一个文件的详细说明文本，并将简介写入一个新文件中 cat file1 | command( sed, grep, awk, grep, etc...) &gt;&gt; result.txt # 合并一个文件的详细说明文本，并将简介写入一个已有的文件中 grep Aug /var/log/messages # 在文件 &#x27;/var/log/messages&#x27;中查找关键词&quot;Aug&quot; grep ^Aug /var/log/messages # 在文件 &#x27;/var/log/messages&#x27;中查找以&quot;Aug&quot;开始的词汇 grep [0-9] /var/log/messages # 选择 &#x27;/var/log/messages&#x27; 文件中所有包含数字的行 grep Aug -R /var/log/* # 在目录 &#x27;/var/log&#x27; 及随后的目录中搜索字符串&quot;Aug&quot; sed &#x27;s/stringa1/stringa2/g&#x27; example.txt # 将example.txt文件中的 &quot;string1&quot; 替换成 &quot;string2&quot; sed &#x27;/^$/d&#x27; example.txt # 从example.txt文件中删除所有空白行 sed &#x27;/ *#/d; /^$/d&#x27; example.txt # 从example.txt文件中删除所有注释和空白行 echo &#x27;esempio&#x27; | tr &#x27;[:lower:]&#x27; &#x27;[:upper:]&#x27; # 合并上下单元格内容 sed -e &#x27;1d&#x27; result.txt # 从文件example.txt 中排除第一行 sed -n &#x27;/stringa1/p&#x27; # 查看只包含词汇 &quot;string1&quot;的行 sed -e &#x27;s/ *$//&#x27; example.txt # 删除每一行最后的空白字符 sed -e &#x27;s/stringa1//g&#x27; example.txt # 从文档中只删除词汇 &quot;string1&quot; 并保留剩余全部 sed -n &#x27;1,5p;5q&#x27; example.txt # 查看从第一行到第5行内容 sed -n &#x27;5p;5q&#x27; example.txt # 查看第5行 sed -e &#x27;s/00*/0/g&#x27; example.txt # 用单个零替换多个零 cat -n file1 # 标示文件的行数 cat example.txt | awk &#x27;NR%2==1&#x27; # 删除example.txt文件中的所有偶数行 echo a b c | awk &#x27;&#123;print $1&#125;&#x27; # 查看一行第一栏 echo a b c | awk &#x27;&#123;print $1,$3&#125;&#x27; # 查看一行的第一和第三栏 paste file1 file2 # 合并两个文件或两栏的内容 paste -d &#x27;+&#x27; file1 file2 # 合并两个文件或两栏的内容，中间用&quot;+&quot;区分 sort file1 file2 # 排序两个文件的内容 sort file1 file2 | uniq # 取出两个文件的并集(重复的行只保留一份) sort file1 file2 | uniq -u # 删除交集，留下其他的行 sort file1 file2 | uniq -d # 取出两个文件的交集(只留下同时存在于两个文件中的文件) comm -1 file1 file2 # 比较两个文件的内容只删除 &#x27;file1&#x27; 所包含的内容 comm -2 file1 file2 # 比较两个文件的内容只删除 &#x27;file2&#x27; 所包含的内容 comm -3 file1 file2 # 比较两个文件的内容只删除两个文件共有的部分 字符设置和文件格式转换 1234dos2unix filedos.txt fileunix.txt # 将一个文本文件的格式从MSDOS转换成UNIX unix2dos fileunix.txt filedos.txt # 将一个文本文件的格式从UNIX转换成MSDOS recode ..HTML &lt; page.txt &gt; page.html # 将一个文本文件转换成html recode -l | more # 显示所有允许的转换格式 文件系统分析 123456789badblocks -v /dev/hda1 # 检查磁盘hda1上的坏磁块 fsck /dev/hda1 # 修复/检查hda1磁盘上linux文件系统的完整性 fsck.ext2 /dev/hda1 # 修复/检查hda1磁盘上ext2文件系统的完整性 e2fsck /dev/hda1 # 修复/检查hda1磁盘上ext2文件系统的完整性 e2fsck -j /dev/hda1 # 修复/检查hda1磁盘上ext3文件系统的完整性 fsck.ext3 /dev/hda1 # 修复/检查hda1磁盘上ext3文件系统的完整性 fsck.vfat /dev/hda1 # 修复/检查hda1磁盘上fat文件系统的完整性 # fsck.msdos /dev/hda1 # 修复/检查hda1磁盘上dos文件系统的完整性 dosfsck /dev/hda1 # 修复/检查hda1磁盘上dos文件系统的完整性 初始化一个文件系统 123456mkfs /dev/hda1 # 在hda1分区创建一个文件系统 mke2fs /dev/hda1 # 在hda1分区创建一个linux ext2的文件系统 mke2fs -j /dev/hda1 # 在hda1分区创建一个linux ext3(日志型)的文件系统 mkfs -t vfat 32 -F /dev/hda1 # 创建一个 FAT32 文件系统 fdformat -n /dev/fd0 # 格式化一个软盘 mkswap /dev/hda3 # 创建一个swap文件系统 SWAP文件系统 123mkswap /dev/hda3 # 创建一个swap文件系统 swapon /dev/hda3 # 启用一个新的swap文件系统 swapon /dev/hda2 /dev/hdb3 # 启用两个swap分区 备份 1234567891011121314151617dump -0aj -f /tmp/home0.bak /home # 制作一个 &#x27;/home&#x27; 目录的完整备份 dump -1aj -f /tmp/home0.bak /home # 制作一个 &#x27;/home&#x27; 目录的交互式备份 restore -if /tmp/home0.bak # 还原一个交互式备份 rsync -rogpav --delete /home /tmp # 同步两边的目录 rsync -rogpav -e ssh --delete /home ip_address:/tmp # 通过SSH通道rsync rsync -az -e ssh --delete ip_addr:/home/public /home/local # 通过ssh和压缩将一个远程目录同步到本地目录 rsync -az -e ssh --delete /home/local ip_addr:/home/public # 通过ssh和压缩将本地目录同步到远程目录 dd bs=1M if=/dev/hda | gzip | ssh user@ip_addr &#x27;dd of=hda.gz&#x27; # 通过ssh在远程主机上执行一次备份本地磁盘的操作 dd if=/dev/sda of=/tmp/file1 # 备份磁盘内容到一个文件 tar -Puf backup.tar /home/user # 执行一次对&#x27;/home/user&#x27;目录的交互式备份操作 ( cd /tmp/local/ &amp;&amp; tar c . ) | ssh -C user@ip_addr &#x27;cd /home/share/ &amp;&amp; tar x -p&#x27; # 通过ssh在远程目录中复制一个目录内容 ( tar c /home ) | ssh -C user@ip_addr &#x27;cd /home/backup-home &amp;&amp; tar x -p&#x27; # 通过ssh在远程目录中复制一个本地目录 tar cf - . | (cd /tmp/backup ; tar xf - ) # 本地将一个目录复制到另一个地方，保留原有权限及链接 find /home/user1 -name &#x27;*.txt&#x27; | xargs cp -av --target-directory=/home/backup/ --parents # 从一个目录查找并复制所有以 &#x27;.txt&#x27; 结尾的文件到另一个目录 find /var/log -name &#x27;*.log&#x27; | tar cv --files-from=- | bzip2 &gt; log.tar.bz2 # 查找所有以&#x27;.log&#x27;结尾的文件并做成一个bzip包 dd if=/dev/hda of=/dev/fd0 bs=512 count=1 # 做一个将 MBR (Master Boot Record)内容复制到软盘的动作 dd if=/dev/fd0 of=/dev/hda bs=512 count=1 # 从已经保存到软盘的备份中恢复MBR内容 网络 - （以太网和WIFI无线） 123456ifconfig eth0 # 显示一个以太网卡的配置 ifup eth0 # 启用一个 &#x27;eth0&#x27; 网络设备 ifdown eth0 # 禁用一个 &#x27;eth0&#x27; 网络设备 ifconfig eth0 192.168.1.1 netmask 255.255.255.0 # 控制IP地址 ifconfig eth0 promisc # 设置 &#x27;eth0&#x27; 成混杂模式以嗅探数据包 (sniffing) dhclient eth0 # 以dhcp模式启用 &#x27;eth0&#x27;","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"}]},{"title":"「Docker」Docker容器与镜像的使用","date":"2020-05-07T03:56:53.000Z","path":"2020/05/07/docker-image.html","text":"1. Docker客户端docker客户端非常简单，我们可以直接输入docker命令来查看到Docker客户端的所有命令选项。 1[root@localhost jenkins]# docker 可以通过命令docker &lt;command&gt; --help更深入的了解指定的Docker命令使用方法。例如我们要查看docker stats指令的具体使用方法： 12345678[root@localhost jenkins]# docker stats --helpUsage: docker stats [OPTIONS] [CONTAINER...]Display a live stream of container(s) resource usage statisticsOptions: -a, --all Show all containers (default shows just running) --format string Pretty-print images using a Go template --no-stream Disable streaming stats and only pull the first result --no-trunc Do not truncate output 2. Docker镜像(Image)使用当运行容器时，使用的镜像如果在本地中不存在，docker就会自动从docker镜像仓库中下载，默认是从Docker Hub公共镜像源下载。 查看现有的镜像(Image)123[root@localhost jenkins]# docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest bf756fb1ae65 10 months ago 13.3kB 各个选项说明: REPOSTITORY：表示镜像的仓库源 TAG：镜像的标签(同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本) IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 获取一个新的镜像1[root@localhost jenkins]# docker pull &lt;REPOSITORY:TAG&gt; 使用 REPOSTITORY:TAG 来定义不同的镜像，如ubuntu仓库源15.10的版本：ubuntu:15.10。 2.1 查找镜像我们可以从Docker Hub网站来搜索镜像。也可以使用docker search命令来搜索镜像。比如我们需要一个httpd的镜像来作为我们的web服务。 1234[root@localhost jenkins]# docker search httpdNAME DESCRIPTION STARS OFFICIAL AUTOMATEDhttpd The Apache HTTP Server Project 3248 [OK] centos/httpd 33 [OK] NAME：镜像仓库源的名称 DESCRIPTION：镜像的描述 OFFICIAL：是否docker官方发布 2.2 创建镜像 当我们从docker镜像仓库中下载的镜像不能满足我们的需求时，我们可以通过以下两种方式对镜像进行更改。 使用Dockerfile指令来创建一个新的镜像 从已经创建的容器中更新镜像，并且提交这个镜像 需要创建一个Dockerfile文件，文件名必须是Dockerfile 123456[root@localhost jenkins]# vim DockerfileFROM centosRUN yum -y install httpdRUN yum -y install net-toolsRUN yum -y install elinksCMD [&quot;/bin/bash&quot;] 构建镜像，使用docker build进行镜像的构建，最后需要指定Dockerfile文件所在路径； 看到最后输出两条Successfully则构建成功。 它会根据文件中写的内容，使用centos镜像实例化一个容器，进入容器中执行三个yum命令123[root@localhost jenkins]# docker build -t jenkins/centos-http-net /home/jenkinsSuccessfully built 09266c896243Successfully tagged jenkins/centos-http-net:latest 查看已经构建好的镜像 1234[root@localhost jenkins]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest bf756fb1ae65 10 months ago 13.3kBjenkins/centos-http-net latest 09266c896243 10 seconds ago 581MB 镜像构建过程在构建命令执行时输出的一大堆信息中，是执行Dockerfile中的每一行，最关键的几行信息如下 1234567891011121314151617181920212223Step 1/5 : FROM centos # 调用centos 5e35e350aded # centos镜像id Step 2/5 : RUN yum install httpd -y Running in a16ddf07c140 # 运行一个临时容器来执行install httpdRemoving intermediate container a16ddf07c140 # 完成后删除临时的容器id b51207823459 # 生成一个镜像 Step 3/5 : RUN yum install net-tools -y Running in 459c8823018a # 运行一个临时容器执行install net-toolsRemoving intermediate container 459c8823018a # 完成后删除临时容器id 5b6c30a532d4 # 再生成一个镜像 Step 4/5 : RUN yum install elinks -y Running in a2cb490f9b2f # 运行一个临时容器执行install elinksRemoving intermediate container a2cb490f9b2f # 完成后删除临时容器id 24ba4735814b # 生成一个镜像 Step 5/5 : CMD [&quot;/bin/bash&quot;] Running in 792333c88ba8 # 运行临时容器，执行/bin/bashRemoving intermediate container 792333c88ba8 # 完成后删除临时容器id 09266c896243 # 生成镜像Successfully built 09266c896243 # 最终成功后的镜像id就是最后生成的镜像id 每一步生成一个镜像，都属于一个docker commit的执行结果在这个过程中一共生成了三个镜像层，都会被存储在graph中，包括层与层之间的关系，查看docker images中生成的镜像id是否为最后生成的镜像id，FROM和CMD都不算做镜像层 通过docker history也可以看到简单的构建过程，看到的是三个yum就是形成的三个镜像层 123456789[root@localhost jenkins]# docker history chai/centos-http-net:latest IMAGE CREATED CREATED BY SIZE COMMENT09266c896243 17 minutes ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B 24ba4735814b 17 minutes ago /bin/sh -c yum install elinks -y 121MB 5b6c30a532d4 18 minutes ago /bin/sh -c yum install net-tools -y 112MB b51207823459 18 minutes ago /bin/sh -c yum install httpd -y 145MB 5e35e350aded 4 months ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) ADD file:45a381049c52b5664… 203MB 3. Docker容器(Container)镜像(Image)的概念更多偏向于一个环境包，这个环境包可以移动到任意的Docker平台中去运行；而容器(Container)就是你运行环境包的实例，换句话说container是images的一种具体表现形式。 容器相关命令：12345678910111213docker container ls，列出现在正在运行的容器docker container ls -a，列出所有的容器，包括没有在运行的docker ps -a，列出所有的容器，包括没有在运行的docker run -it centos，交互式运行容器的方法，这样的话运行之后contianer不会退出，但是这个对话关闭这个，contianer还是就会退出了docker container rm container_id，可以删除一个容器（注意，这里的id可以是完整的容器id，也可以是缩写，只要是可以和别的id区分开的就可以了）docker rm container_id，默认就是去删除container的，所以是和docker container rm id是一样的docker images，和docker image ls是一样的，显示本机的所有镜像docker image rm image_id，删除image，可以是image_id或者image_namedocker rmi id，是和docker image rm id一样的，是简写docker container ls -aq，只会显示出容器的id，在批量删除容器的时候有用docker rm $(docker container ls -aq)，删除所有的容器docker rm $(docker ps -aq)，也是删除所有的容器docker rm $(docker container ls -f &quot;status=exited&quot; -q)，删除所有不在运行的容器 4. Dockerfile文件语法常用构建镜像指令 123456789101112131415161718192021222324FROM # 指定base镜像MAINTAINER # 指定镜像作者，后面根任意字符串COPY # 把文件从host复制到镜像内 COPY src dest COPY [&quot;src&quot;,&quot;dest&quot;] src:只能是文件ADD # 用法和COPY一样，唯一不同时src可以是压缩包，表示解压缩到dest位置，src也可以是目录ENV # 设置环境变量可以被接下来的镜像层引用，并且会加入到镜像中 ENV MY_VERSION 1.3 RUN yum -y install http-$MY_VERSION # 当进入该镜像的容器中echo $MY_VERSION会输出1.3EXPOSE # 指定容器中的进程监听的端口（接口），会在docker ps -a中的ports中显示 EXPOSE 80VOLUME # 容器卷，后面会讲到，把host的路径mount到容器中 VOLUME /root/htdocs /usr/local/apahce2/htdocsWORKDIR # 为后续的镜像层设置工作路径 # 如果不设置，Dockerfile文件中的每一条命令都会返回到初始状态 # 设置一次后，会一直在该路经执行之后的分层，需要WORKDIR /回到根目录CMD # 启动容器后默认运行的命令，使用构建完成的镜像实例化为容器时，进入后默认执行的命令 # 这个命令会被docker run启动命令替代 # 如：docker -it --rm centos echo &quot;hello&quot; # echo &quot;hello&quot;会替代CMD运行的命令 CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off&quot;] # 该镜像实例化后的容器，进入后运行nginx启动服务ENTRYPOINT # 容器启动时运行的命令，不会被docker run的启动命令替代 4.1 RUN&#x2F;CMD&#x2F;ENTRYPOINT区别 RUN：执行命令并创建新的镜像层，主要用于安装软件包 ENTRYPOINT和CMD都算作是启动指令，也就是必须启动容器才会去执行的指令，一般用来启动运行程序使用；当ENTRYPOINT和CMD同时存在时，ENTRYPOINT生效。 ENTRYPOINT和CMD使用格式： shell格式： 会始终调用一个shell程序去执行命令，如ENTRYPOINT echo &quot;hello $name&quot; exec格式：CMD [&quot;命令&quot;, &quot;选项&quot;, &quot;参数&quot;]、ENTRYPOINT [&quot;命令&quot;, &quot;选项&quot;, &quot;参数&quot;] exec格式下无法去调用ENV定义的变量，如果非要让exec格式去读取变量的话，它的命令的位置就要使用一个shell环境。因为变量的读取就是使用shell去读取的。如：ENTRYPOINT [“&#x2F;bin&#x2F;sh”, “-c”, “echo hello,$变量名”] 当使用exec格式时，ENTRYPOINT的第一个参数被识别为命令，CMD的参数按顺序变为ENTRYPOINT命令的参数","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Docker","slug":"Docker","permalink":"http://chaooo.github.io/tags/Docker/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"}]},{"title":"「Docker」Docker简介与安装（Linux环境centos）","date":"2020-05-06T03:56:53.000Z","path":"2020/05/06/docker-install.html","text":"Docker 是一个开源的应用容器引擎，基于Go语言并遵从Apache2.0协议开源。Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的Linux机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口（类似于app）,更重要的是容器性能开销极低。 1. Docker解决的问题 解决运行环境和配置问题，方便发布，方便做持续集成。 由于不同的机器有不同的操作系统，以及不同的库和组件，在将一个应用部署到多台机器上需要进行大量的环境配置操作。 Docker主要解决环境配置问题，它是一种虚拟化技术，对进程进行隔离，被隔离的进程独立于宿主操作系统和其它隔离的进程。 使用Docker可以不修改应用程序代码，不需要开发人员学习特定环境下的技术，就能够将现有的应用程序部署在其它机器上。 使用Docker的好处： 部署方便且安全，隔离性好，快速回滚，成本低 Docker的应用场景： Web 应用的自动化打包和发布。 自动化测试和持续集成、发布。 在服务型环境中部署和调整数据库或其他的后台应用。 2. Docker架构Docker使用客户端-服务器(C&#x2F;S)架构模式，使用远程API来管理和创建Docker容器。Docker容器通过Docker镜像(Images)来创建，容器与镜像的关系类似于面向对象编程中的对象与类。 Docker镜像(Images) 用于创建Docker容器(Container)的模板。 Docker容器(Container) 独立运行的一个或一组应用。 Docker客户端(Client) 通过命令行或者其他工具使用Docker API与Docker的守护进程通信。 Docker主机(Host) 一个物理或者虚拟的机器用于执行Docker守护进程和容器。 Docker仓库(Registry) 用来保存镜像(Images)，可以理解为代码控制中的代码仓库，Docker Hub提供了庞大的镜像集合供使用。 Docker Machine 一个简化Docker安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装Docker。 3. CentOS下安装Docker要求CentOS 6.5或更高的版本，要求系统为64位、系统内核版本为 2.6.32-431 或者更高版本。这里演示环境CentOS 8.2。 添加Docker存储库 首先，必须添加一个外部存储库以获得Docker CE。这里使用官方的Docker CE CentOS存储库。 下载docker-ce的repo 1234[root@localhost jenkins]# curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 1919 100 1919 0 0 3408 0 --:--:-- --:--:-- --:--:-- 3402 安装依赖 1[root@localhost jenkins]# yum install https://download.docker.com/linux/fedora/32/x86_64/stable/Packages/containerd.io-1.3.7-3.1.fc32.x86_64.rpm 安装docker-ce 1[root@localhost jenkins]# yum install docker-ce 启动docker 1[root@localhost jenkins]# systemctl start docker 检查docker服务是否正常运行： 1[root@localhost jenkins]# systemctl status docker 测试运行 hello-world 由于本地没有hello-world这个镜像，所以会下载一个hello-world的镜像，并在容器内运行。123456789101112131415161718192021222324252627[root@localhost jenkins]# docker run hello-worldUnable to find image &#x27;hello-world:latest&#x27; locallylatest: Pulling from library/hello-world0e03bdcc26d7: Pull complete Digest: sha256:8c5aeeb6a5f3ba4883347d3747a7249f491766ca1caa47e5da5dfcf6b9b717c0Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Docker","slug":"Docker","permalink":"http://chaooo.github.io/tags/Docker/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"}]},{"title":"「Jenkins」Jenkins自动部署Jar到远程服务器","date":"2020-04-10T11:30:53.000Z","path":"2020/04/10/jenkins-shell.html","text":"1. 配置远程服务器SSH免密登录 本地客户端生成公私钥（一路回车默认即可），会在用户目录.ssh文件夹下创建公私钥 123[localuser@localhost .ssh]$ ssh-keygen[localuser@localhost .ssh]$ lsid_rsa id_rsa.pub 上传公钥到服务器，这里远程服务器地址为：192.168.2.200，用户为：testuser 1ssh-copy-id -i ~/.ssh/id_rsa.pub testuser@192.168.2.200 上面这条命令会在远程服务器的~/.ssh目录生成authorized_keys，里面是id_rsa.pub(公钥)内容。 若目标服务器已经存在了authorized_keys，则可拷贝公钥内容追加到authorized_keys内容的末尾。 测试免密登录，本地客户端通过ssh连接远程服务器，就可以免密登录了。 12345[localuser@localhost ~]$ ssh testuser@192.168.2.200Last login: Tue Nov 17 20:57:25 2020 from 192.168.2.202[testuser@caimeidev1 ~]$ exitlogoutConnection to 192.168.2.200 closed. 2. 登录Jenkins客户端并配置先安装插件：Git Paramater，这里只演示部署，因为已在本地打好包推送到了git服务器。 新建Item， 输入任务名称：MavenTest(自己定义)，选择自由项目，点击确定。 勾选This project is parameterized(参数化构建) 选择Choice Parameter,添加打包环境参数(名称：buildEnv，选项：beta和prod)； 选择Git Parameter,定义参数名称:gitBranch，参数类型选择分支； 源码管理，选择Git，填写仓库地址(Repository URL)和选择凭据(Credentials)。 构建环境，勾选Add timestamps to the Console Output，加上时间戳。 Post Steps，选择执行shell脚本Execute shell，输入： 123cd /home/jenkins/shell# 对应两个构建参数$buildEnv，$gitBranch./demo-deploy-$buildEnv.sh demo-0.0.1-SNAPSHOT.jar $&#123;gitBranch&#125; 3. 编写部署脚本可参考shell脚本部署Java应用 本地客户端 123[localuser@localhost ~]$ cd /home/jenkins/shell# 推送不同服务器用不同shell脚本，beta:demo-deploy-beta.sh，product:demo-deploy-prod.sh，对应$buildEnv参数[localuser@localhost ~]$ vim demo-deploy-beta.sh 123456789101112131415161718#!/usr/bin/bashfileName=$1gitBranch=$2if [ -z &quot;$fileName&quot; ]; then echo &quot;文件名不能为空&quot; exit 0fiecho &quot;准备发布【$gitBranch】分支，到【Beta：192.168.2.200】&quot;echo &quot;开始拷贝jar文件【$fileName】到远程服务器&quot;scp /home/jenkins/root/workspace/MavenTest/target/$fileName testuser@192.168.2.200:/usr/local/test/$fileName.prev# 捕获上一条命令的输出$? (if 0 正常 else 错误)if [ &quot;$?&quot; == &quot;0&quot; ]; then echo &quot;文件传输结束，准备启动远程服务器的部署脚本&quot; ssh testuser@192.168.2.200 /usr/local/test/demo-deploy.sh $fileNameelse echo &quot;拷贝文件错误&quot; exit 0fi 12# 添加执行权限[localuser@localhost ~]$ chmod +x demo-deploy-beta.sh 远程服务器12[localuser@localhost ~]$ cd /usr/local/test[localuser@localhost ~]$ vim demo-deploy.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/env bashecho &quot;服务器开始部署服务&quot;projectname=&quot;demo-0.0.1-SNAPSHOT&quot;# 打开文件所属的目录，不然远程执行会找不到当前目录cd /usr/local/test# 新的jar包会当成参数传过来newJar=$1echo &quot;新的jar为：$newJar&quot; # 如果新的jar包为空则退出if [ -z &quot;$newJar&quot; ]; then echo &quot;新的jar不能为空&quot; exit 0fi# 获取旧的jar包名称，当然可能是空的，也可能跟当前名称一致oldJar=$(ps -ef | grep $&#123;projectname&#125;|grep -v &#x27;demo-deploy.sh&#x27;|grep -v grep|awk &#x27;&#123;print $10&#125;&#x27;|cut -d &#x27;/&#x27; -f 2)echo &quot;当前运行的旧的jar包为：$oldJar&quot; #如果新的jar包为空则退出if [ -z &quot;$oldJar&quot; ]; then echo &quot;没有启动的demo服务&quot;else # 如果旧的进程还在就将旧的进程杀掉 oldId=`ps -ef|grep $&#123;projectname&#125;|grep -v &quot;$0&quot;|grep -v &quot;grep&quot;|awk &#x27;&#123;print $2&#125;&#x27;` echo &quot;$oldId&quot; echo &quot;kill old process start ...&quot; for id in $oldId do kill -9 $id echo &quot;killed $id&quot; done echo &quot;kill old process end&quot; # 获取当前时间 suffix=&quot;.bak-&quot;`date &#x27;+%s%3N&#x27;`; echo $suffix; # 将旧的jar包进行备份 mv $oldJar $&#123;oldJar&#125;$&#123;suffix&#125;fi# 开始启动新的进程mv $&#123;1&#125;.prev $&#123;1&#125;nohup java -jar $&#123;1&#125; &gt; run.txt 2&gt;&amp;1 &amp;echo &quot;服务启动查看进程:&quot;echo `ps -ef | grep $&#123;projectname&#125;|grep -v &#x27;demo-deploy.sh&#x27;|grep -v grep` 12# 添加执行权限[localuser@localhost ~]$ chmod +x demo-deploy.sh 4. 部署测试Build with Parameters -&gt; 选择要部署的git分支$&#123;gitBranch&#125;和环境参数$&#123;buildEnv&#125; 构建完成后点击Build History(构建历史)里的构建版本号，点击控制台输出查看日志","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"},{"name":"Jenkins","slug":"Jenkins","permalink":"http://chaooo.github.io/tags/Jenkins/"}]},{"title":"「Jenkins」使用Maven构建Java应用程序","date":"2020-04-08T04:30:53.000Z","path":"2020/04/08/jenkins-maven.html","text":"1. 添加git凭据若安装Jenkins时安装的推荐插件，git已经默认安装了，我们添加git凭据： 系统管理(Manage Jenkins) -&gt; Manage Credentials -&gt; 全局 -&gt; 添加凭据。 类型选择SSH Username with private key，往下滑到Private Key，并勾选Enter directly，在Enter directly的key区域点击Add，粘贴本机git私钥，点击确定。 类型选择Username with password，填写git用户名和密码，点击确定。 本机Git私钥获取： 1[root@localhost ~]# cat .ssh/id_rsa git公钥私钥生成详细细节参考物理机安装CentOS 8.0的安装配置Git。 2. Jenkins配置Maven与Publish Over SSH插件 在插件管理搜索Maven，找到Maven Integration并勾选，然后点击直接安装。 接下来同样方法安装Publish Over SSH插件(用来通过ssh命令发送Maven的构建)。 配置全局变量，添加要部署的远程用户： 系统管理(Manage Jenkins) -&gt; 点击Configure System -&gt; 往下滑到SSH Server； 在SSH Servers这里点击新增，填写用户信息(Name:此配置名，HostNmae:要连接的SSH主机名或IP地址，UserName:远程用户名)； 点击高级配置密码，勾选Use password authentication, or use a different key，在Passphrase / Password输入远程用户密码； 点击Test Configuration测试能否连通，最后点击保存。 配置mavne的jenkins本地仓库： 系统管理(Manage Jenkins) -&gt; 点击Configure System -&gt; 找到Maven项目配置； Local Maven Repository -&gt; 选择Local to the workspace -&gt; 保存。 全局工具配置： 系统管理(Manage Jenkins) -&gt; 点击Global Tool Configuration； Maven配置 -&gt; 默认setting/全局setting选择Settings file in filesystem -&gt; 填写本机安装Maven的setting路径，如/usr/local/apache-maven-3.6.3/conf/settings.xml JDK -&gt; JDK安装 -&gt; 新增JDK -&gt; JAVA_HOME填写java安装目录，如/usr/lib/jvm/java-openjdk Git -&gt; Git installations -&gt; Path to Git executable填写/usr/bin/git(可用which -a git查看) Maven -&gt; Maven安装 -&gt; 新增Maven -&gt; MAVEN_HOME填写Maven安装目录，如/usr/local/apache-maven-3.6.3 3. 使用Maven构建Java应用程序 新建Item， 输入任务名称，选择Maven项目，点击“确定” 源码管理，选择Git，填写仓库地址(Repository URL)和选择凭据(Credentials) 构建环境，勾选Add timestamps to the Console Output，加上时间戳 Build，Goals and options根据自己情况自行修改：clean package -pl demo -am -Dmaven.test.skip=true -P beta -pl 选项后可跟随{groupId}:{artifactId}或者所选模块的相对路径(多个模块以逗号分隔)，这里只想打包demo模块 -am 表示同时处理选定模块所依赖的模块 -P 打包的环境 Post Steps，发布步骤，这里可以选择执行shell脚本Execute shell（发布到和Jenkins是同一台服务器），Send files or execute commands over SSH（不同服务器）。 4. Maven构建配置【Post Steps】 若发布到和Jenkins是同一台服务器，下拉框列表选择Execute shell，然后填写shell脚本； 若不在同一服务器，下拉框选择Send files or execute commands over SSH，配置SSH Server Source files填写Maven本地打包后Jar包路径，如：target/demo-0.0.1-SNAPSHOT.jar Remove prefix去除前缀，如：target/ Remote directory拷贝到Linux服务器的路径，如：/home/jenkins/test Exec commandJar包拷贝后，执行脚本运行Jar包。 shell脚本详情(填写时去掉注释)：12345678910111213#后台执行BUILD_ID=DONTKILLME# 进入到项目cd /home/jenkins/test# 找到原进程,killProject_name=testpid=$(ps -ef | grep java| grep $Project_name|awk -F &#x27;[ ]+&#x27; &#x27;&#123;print $2&#125;&#x27;)kill -9 $pid# 启动jarnohup java -jar $Project_name.jar &gt; run.txt &amp;","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"},{"name":"Jenkins","slug":"Jenkins","permalink":"http://chaooo.github.io/tags/Jenkins/"}]},{"title":"「Jenkins」声明式流水线Jenkins Pipeline","date":"2020-04-06T03:56:53.000Z","path":"2020/04/06/jenkins-pipeline.html","text":"1. Jenkins Pipeline 基本概念流水线是用户定义的一个CD流水线模型 。流水线的代码定义了整个的构建过程, 他通常包括构建, 测试和交付应用程序的阶段 。Jenkins Pipeline（或简称为”Pipeline”）是一套插件，将持续交付的实现和实施集成到Jenkins中。持续交付Pipeline自动化的表达了这样一种流程：将基于版本控制管理的软件持续的交付。Jenkins Pipeline 的定义通常被写入到一个Jenkinsfile文本文件中，该文件可以被放入项目的源代码控制库中。 2.Jenkinsfile 基础语法123456789101112131415161718192021pipeline &#123; // 特定语法，pipeline 块定义了整个流水线中完成的所有的工作 agent any // agent为整个流水线分配一个执行器 (在节点上)和工作区 stages &#123; // 所有流程（状态）的外层块，仅有一个 stage(&#x27;Build&#x27;) &#123; // 每个stage为一流程，定义名称 steps &#123; // 步骤块，内部包含具体操作 sh &#x27;make&#x27; // sh操作，其引号间的文字会当成shell直接执行 &#125; &#125; stage(&#x27;Test&#x27;)&#123; steps &#123; sh &#x27;make check&#x27; junit &#x27;reports/**/*.xml&#x27; //junit使用匹配的定义测试xml进行单元测试 &#125; &#125; stage(&#x27;Deploy&#x27;) &#123; steps &#123; sh &#x27;make publish&#x27; &#125; &#125; &#125;&#125; 3.创建 Hello World 流水线 登录Jenkins，新建任务(New Item)，选择流水线，输入工程名称Hello Pipeline，确定。 填写描述，勾选参数化构建过程(This project is parameterized)。 添加参数，选择字符参数(String parameter)，并设置这个字符串参数(名称,默认值,描述)，这样我们在Jenkinsfile中就可以取到这个值了。 向下滑动到流水线，定义选择Pipeline script，脚本输入如下内容，然后保存。 123456789101112131415161718192021222324pipeline &#123; agent any environment &#123; //环境变量 GREETING=&quot;Hello&quot; &#125; stages&#123; stage(&#x27;打招呼&#x27;) &#123; steps&#123; sh &#x27;echo &quot;$GREETING $TITLE&quot;&#x27; &#125; &#125; &#125; post &#123; //构建完成后置操作 aborted &#123; //如果构建中断，则执行 echo &#x27;构建被中止!&#x27; &#125; success &#123; //构建成功执行 echo &#x27;构建成功!&#x27; &#125; failure &#123; //构建失败执行 echo &#x27;构建失败!&#x27; &#125; &#125;&#125; 点击Build with Parameters(参数化构建)，然后开始构建。 构建完成输出界面： 把鼠标放在打招呼下边的绿色框上，点出现的logs，可以看到输出了预期的值。 找到左下角的Build History(构建历史)的构建版本号，如当前是#1，点进去，选择Console Output查看详细的执行日志。 12345678910111213141516171819202122232425262728# 成功Console OutputStarted by user charlesRunning in Durability level: MAX_SURVIVABILITY[Pipeline] Start of Pipeline[Pipeline] nodeRunning on Jenkins in /home/jenkins/root/workspace/Hello Pipeline[Pipeline] &#123;[Pipeline] withEnv[Pipeline] &#123;[Pipeline] stage[Pipeline] &#123; (打招呼)[Pipeline] sh+ echo &#x27;Hello Jenkins Pipeline&#x27;Hello Jenkins Pipeline[Pipeline] &#125;[Pipeline] // stage[Pipeline] stage[Pipeline] &#123; (Declarative: Post Actions)[Pipeline] echo构建成功![Pipeline] &#125;[Pipeline] // stage[Pipeline] &#125;[Pipeline] // withEnv[Pipeline] &#125;[Pipeline] // node[Pipeline] End of PipelineFinished: SUCCESS","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"},{"name":"Jenkins","slug":"Jenkins","permalink":"http://chaooo.github.io/tags/Jenkins/"}]},{"title":"「Jenkins」安装Jenkins（Linux环境centos）","date":"2020-04-04T09:56:53.000Z","path":"2020/04/04/jenkins-install.html","text":"1. 下载Jenkins这里选择清华大学的Jenkins镜像源站下载稳定2.249.3版本(war包)https://mirrors.tuna.tsinghua.edu.cn/jenkins/war-stable/2.249.3/ 创建Jenkins用户来操作Jenkins： 123456sudo useradd -mU jenkins -s /bin/bash # 创建jenkins用户并添加同名组、创建用户目录,默认shell为bashsudo passwd jenkins # 重置密码New password: Retype new password: su jenkins # 切换用户cd ~ # 进入/home/jenkins目录 将jenkins.war上传到/home/jenkins下 2. 启动Jenkins后台运行Jenkins 1nohup java -DJENKINS_HOME=/home/jenkins/root -jar /home/jenkins/jenkins.war --httpPort=8888 &amp; 我这里的主机IP：192.168.2.100，端口号指定了8888，防火墙需要开放8888端口。 Centos6/7 配置iptables规则： 123456# 编辑配置文件vim /etc/sysconfig/iptables# 在文件中间添加iptables规则-A INPUT -m state --state NEW -m tcp -p tcp --dport 8888 -j ACCEPT# 重启防火墙service iptables restart Centos8 配置firewalld： 1234firewall-cmd --query-port=8888/tcp # 查询端口是否开放firewall-cmd --add-port=8888/tcp --permanent #永久添加8080端口例外(全局)systemctl stop firewalld.service #停止防火墙systemctl start firewalld.service #启动防火墙 访问 Jenkins生成更新目录，我这里访问：192.168.2.100:8888。等待初始化完成出现解锁Jenkins时，先不急填密码，先把插件源换掉，这样提升安装速度和降低失败率。 3. 修改默认插件源若一直停留在Please wait while Jenkins is getting ready to work页面，那么可能是你的网络连不到Jenkins官方仓库上。可以先进入/home/jenkins/root目录，打开hudson.model.UpdateCenter.xml将url中的https://updates.jenkins.io/update-center.json更改为清华大学的镜像地址:https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 1sed -i &quot;s/https:\\/\\/updates.jenkins.io\\/update-center.json/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins\\/updates\\/update-center.json/g&quot; /home/jenkins/root/hudson.model.UpdateCenter.xml 重启Jenkins 1234ps -ef|grep jenkins #定位Jenkins的进程号pidkill -9 &lt;pid&gt; #杀jenkins进程# 重启Jenkinsnohup java -DJENKINS_HOME=/home/jenkins/root -jar /home/jenkins/jenkins.war --httpPort=8888 &amp; 修改默认Jenkins插件源与连接检测位置 12sed -i &#x27;s/http:\\/\\/updates.jenkins-ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g&#x27; /home/jenkins/root/updates/default.jsonsed -i &#x27;s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g&#x27; /home/jenkins/root/updates/default.json 重启Jenkins，使插件源生效 1234ps -ef|grep jenkins #定位Jenkins的进程号pidkill -9 &lt;pid&gt; #杀jenkins进程# 重启Jenkinsnohup java -DJENKINS_HOME=/home/jenkins/root -jar /home/jenkins/jenkins.war --httpPort=8888 &amp; 4. 解锁Jenkins并创建管理员用户 解锁Jenkins1cat /home/jenkins/root/secrets/initialAdminPassword 复制输出的密码，访问192.168.2.100:8888，粘贴到管理员密码框中，继续。安装推荐的插件就可以。 创建管理员用户等待插件安装完成 -&gt; 创建新管理员账户 -&gt; 一路保存并完成。 重启Jenkins因为更新了管理员用户，需要重启下Jenkins服务。 登录Jenkins访问192.168.2.100:8888，输入刚才创建的账号与密码登录。 5. 设置开机启动Jenkins 到/home/jenkins/shell目录下创建启动脚本jenkins.sh 12cd /home/jenkins/shellvim jenkins.sh 脚本jenkins.sh: 1234567891011121314151617181920212223#!/usr/bin/bash# 导入环境变量export JENKINS_HOME=/home/jenkins/cd $JENKINS_HOMEpid=`ps -ef | grep jenkins.war | grep -v &#x27;grep&#x27;| awk &#x27;&#123;print $2&#125;&#x27;`if [ &quot;$1&quot; = &quot;start&quot; ];thenif [ -n &quot;$pid&quot; ];then echo &#x27;jenkins is running...&#x27;else # java启动服务 配置java安装根路径,和启动war包存的根路径 nohup java -DJENKINS_HOME=$JENKINS_HOME/root -jar $JENKINS_HOME/jenkins.war --httpPort=8888 &gt;/dev/null 2&gt;&amp;1 &amp; echo &quot;服务启动查看进程:&quot; echo `ps -ef | grep jenkins.war | grep -v &#x27;jenkins.sh&#x27;|grep -v grep`fielif [ &quot;$1&quot; = &quot;stop&quot; ];then exec ps -ef | grep jenkins | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;| xargs kill -9 echo &#x27;jenkins is stop...&#x27;else echo &#x27;Please input like this:&quot;./jenkins.sh start&quot; or &quot;./jenkins stop&quot;&#x27;fi 切换root用户，并添加可执行权限 123su root# 添加可执行权限chmod +x /home/jenkins/shell/jenkins.sh 到 &#x2F;lib&#x2F;systemd&#x2F;system 服务注册目录下创建 jenkins.service 123456789101112131415[Unit]Description=JenkinsAfter=network.target [Service]Type=forkingUser=jenkinsGroup=jenkinsExecStart=/home/jenkins/shell/jenkins.sh startExecReload=/home/jenkins/shell/jenkins.sh reloadExecStop=/home/jenkins/shell/jenkins.sh stopPrivateTmp=true [Install]WantedBy=multi-user.target 设置开机启动 1234# 刷新配置systemctl daemon-reload# 设置开机启动systemctl enable jenkins.service 查看设置开机启动的服务列表 1systemctl list-units --type=service 到此，安装Jenkins已全部完成。","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"Linux","slug":"Linux","permalink":"http://chaooo.github.io/tags/Linux/"},{"name":"Jenkins","slug":"Jenkins","permalink":"http://chaooo.github.io/tags/Jenkins/"}]},{"title":"「环境配置」物理机安装CentOS 8","date":"2020-03-15T15:08:46.000Z","path":"2020/03/15/env-centos8.html","text":"CentOS 8 所需的最低硬件配置: 2GB RAM 64位x86 &#x2F; 2GHz或以上的 CPU 20GB 硬盘空间 1. 下载 CentOS 8 ISO 文件在 CentOS 官方网站 https://www.centos.org/download/ 下载 CentOS 8 ISO 文件。 2. 创建 CentOS 8 启动介质（USB 或 DVD）下载 CentOS 8 ISO 文件之后，将 ISO 文件烧录到 USB 移动硬盘或 DVD 光盘中，作为启动介质。常用的烧录工具如：UltraISO。然后重启系统，在 BIOS 中设置为从上面烧录好的启动介质启动。 3. 选择“安装 CentOS Linux 8.0”选项当系统从 CentOS 8 ISO 启动介质启动之后，就可以看到启动选择界面。选择“Install CentOS Linux 8”（安装 CentOS Linux 8）选项并按回车。 4. 安装错误处理实体机安装，写入镜像到U盘会出现找不到U盘的情况，报错是：/dev/root does not exist。界面尾部出现 dracut:/# 时，键入 ls /dev 查看启动U盘所在盘符。 1dracut:/# ls /dev 前缀sd表示磁盘，如sda、sdb、sdc；sda后面的4表示磁盘a的分区，因为物理机磁盘删除了分区，是未分区状态，所以sda4就是U盘了。确定好sda是启动U盘后，键入 reboot 重启。进入到开机系统选择界面，根据提示 按键盘 e 或 tab。界面会出现：initrd=initrd.img inst.stage2=hd:LABEL=CentOSx86_64 rd.live.check quiet，修改为U盘的分区（sda4）：initrd=initrd.img inst.stage2=hd:/dev/sda4 quiet。然后保存后按回车键安装。 5. 开始安装CentOS 8根据图像安装向导的引导进行配置。设置root用户的密码，创建一个本地用户。在安装完成后，安装向导会提示重启系统。 注意：重启完成后，记得要把安装介质断开，并将 BIOS 的启动介质设置为硬盘。 同意 CentOS 8 的许可证，使用刚创建的本地用户登录。开始使用 CentOS LinuxStart Using CentOS Linux。 6. 安装配置JDK使用 yum 直接安装，环境变量会自动配置好。 检查 yum 中有没有 java1.8 包 1yum list java-1.8* 开始安装java1.8 1yum install java-1.8.0-openjdk* -y 用验证java -version 1234[root@localhost ~]$ java -versionopenjdk version &quot;1.8.0_272&quot;OpenJDK Runtime Environment (build 1.8.0_272-b10)OpenJDK 64-Bit Server VM (build 25.272-b10, mixed mode) 7. 安装配置Git 使用Yum安装Git 1yum install git 用验证git --version 12[root@localhost ~]$ git --versiongit version 2.18.4 配置Git 12[root@localhost ~]$ git config --global user.name &quot;yourname&quot;[root@localhost ~]$ git config --global user.email &quot;youremail@email.com&quot; 验证配置 123[root@localhost ~]$ git config --listuser.name=yournameuser.email=youremail@email.com 生成git授权证书键入命令ssh-keygen -t rsa -C &quot;youremail@email.com&quot;一路回车键就好: 1[root@localhost ~]$ ssh-keygen -t rsa -C &quot;youremail@email.com&quot; 拷贝公钥(私钥:id_rsa，公钥:id_rsa.pub)到git服务器，如gitee添加SSH公钥。 12345[root@localhost ~]$ cd ~/.ssh/[root@localhost .ssh]# lsid_rsa id_rsa.pub[root@localhost .ssh]# cat id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQD8blWzO+L+H8h8GkNOEEfLBQGCszU= youremail@email.com 添加公钥完成后进行测试SSH链接(gitee为例)键入命令ssh -T git@gitee.com，根据提示键入yes确认，当终端提示hi/welcome/successfully等表示链接成功。 8. 安装配置Maven 下载并解压Maven 1234567[root@localhost ~]# wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.zip[root@localhost ~]# unzip apache-maven-3.6.3-bin.zip# 把maven安装到/usr/local/下[root@localhost apache-maven-3.6.3]# mv apache-maven-3.6.3 /usr/local/apache-maven-3.6.3[root@localhost ~]# cd /usr/local/apache-maven-3.6.3[root@localhost apache-maven-3.6.3]# pwd/usr/local/apache-maven-3.6.3 修改系统配置/etc/profile，添加Maven环境变量 1234[root@localhost apache-maven-3.6.3]# vim /etc/profile# 在文件末尾添加：export MAVEN_HOME=/usr/local/apache-maven-3.6.3export PATH=$MAVEN_HOME/bin:$PATH 重新加载系统配置：. /etc/profile 1[root@localhost apache-maven-3.6.3]# . /etc/profile 验证Maven123456[root@localhost apache-maven-3.6.3]# mvn -versionApache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)Maven home: /usr/local/apache-maven-3.6.3Java version: 1.8.0_272, vendor: Red Hat, Inc., runtime: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.272.b10-1.el8_2.x86_64/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;4.18.0-193.el8.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot; 9. Centos8开放防火墙端口 查看防火墙某个端口是否开放 1firewall-cmd --query-port=3306/tcp 开放防火墙端口3306 1firewall-cmd --zone=public --add-port=3306/tcp --permanent 查看防火墙状态 1systemctl status firewalld 关闭防火墙 1systemctl stop firewalld 打开防火墙 1systemctl start firewalld 开放一段端口 1firewall-cmd --zone=public --add-port=40000-45000/tcp --permanent 查看开放的端口列表 1firewall-cmd --zone=public --list-ports 重启防火墙 1firewall-cmd --reload # 重启防火墙(修改配置后要重启防火墙)","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"CentOS","slug":"CentOS","permalink":"http://chaooo.github.io/tags/CentOS/"}]},{"title":"「环境配置」Redis与MySQL多实例配置","date":"2020-02-11T15:08:46.000Z","path":"2020/02/11/env-redis-mysql.html","text":"最近由于工作的需要，需要在同一台服务器上搭建两个Redis与MySQL的实例。多实例：就是在一台机器上面开启多个不同的端口(如Redis用6379&#x2F;6380，MySQL用3306&#x2F;3307等)，运行多个服务进程；公用一套安装程序，使用不同的配置文件，数据文件。 1. Redis多实例配置1.1 查看主机Redis信息 用ps命令查看Redis进程 123[root@localhost ~] ps -ef |grep redisroot 1706 1 0 2019 ? 04:12:09 /usr/local/bin/redis-server *:6379 root 18174 2560 0 15:35 pts/0 00:00:00 grep redis 查找配置文件位置 12[root@localhost ~] locate redis.conf/etc/redis.conf 1.2 拷贝配置文件并修改 拷贝redis.conf并命名为redis6380.conf，并修改参数 12345678910111213141516[root@localhost ~] cp /etc/redis.conf /etc/redis6380.conf[root@localhost ~] vim /etc/redis6380.conf# 查找 /pidfile 找到pid位置# pidfile /var/run/redis.pid #修改pid，每个实例需要运行在不同的pidpidfile /var/run/redis6380.pid# # 查找 /port 6379 找到端口位置# port 6379 #修改端口port 6380# # 查找 /dir 找到数据目录位置# dir /mnt/newdatadrive/data/redis #修改数据存放目录dir /mnt/newdatadrive/data/redis6380# # 已开启Redis持久化appendonly yes 准备上面配置的文件 12[root@localhost ~] mkdir –p /mnt/newdatadrive/data/redis6380[root@localhost ~] cp /var/run/redis.pid /var/run/redis6380.pid 1.3 启动测试 启动6380端口Redis服务，并查看Redis进程 12345[root@localhost ~] /usr/local/bin/redis-server /etc/redis6380.conf[root@localhost ~] ps -ef |grep redisroot 1706 1 0 2019 ? 04:12:00 /usr/local/bin/redis-server *:6379 root 15967 1 0 12:16 ? 00:00:00 /usr/local/bin/redis-server *:6380 root 15994 8014 0 12:16 pts/2 00:00:00 grep redis 测试登录Redis客户端 12[root@localhost ~] redis-cli -p 6380127.0.0.1:6380&gt; QUIT #退出 停止6380端口的Redis服务 1redis-cli -p 6380 shutdown 1.4 Redis数据迁移 登录原Redis客户端(6379) 123456[root@localhost ~] redis-cli -p 6379127.0.0.1:6379&gt; SAVE #数据备份127.0.0.1:6379&gt; CONFIG GET dir #查看Redis数据目录1) &quot;dir&quot;2) &quot;/mnt/newdatadrive/data/redis&quot;127.0.0.1:6379&gt; QUIT #退出 拷贝数据文件appendonly.aof和dump.rdb到6380 12345678# 查看6379的数据文件[root@localhost ~] cd /mnt/newdatadrive/data/redis &amp;&amp; lltotal 55176-rw-r--r-- 1 root root 55411226 Feb 11 09:25 appendonly.aof-rw-r--r-- 1 root root 1017181 Feb 11 12:28 dump.rdb# 拷贝到6380[root@localhost ~] \\cp /mnt/newdatadrive/data/redis/appendonly.aof /mnt/newdatadrive/data/redis6380/appendonly.aof[root@localhost ~] \\cp /mnt/newdatadrive/data/redis/dump.rdb /mnt/newdatadrive/data/redis6380/dump.rdb 启动6380端口Redis服务，导入AOF数据文件 12[root@localhost ~] /usr/local/bin/redis-server /etc/redis6380.conf[root@localhost ~] redis-cli -p 6380 --pipe &lt; /mnt/newdatadrive/data/redis6380/appendonly.aof 登录Redis查看数据 12[root@localhost ~] redis-cli -p 6380127.0.0.1:6380&gt; #输入具体命令查看数据 1.5 配置远程可访问 修改配置文件redis6380.conf 1234567[root@localhost ~] vim /etc/redis6380.conf# 查找 /bind 找到：bind 127.0.0.1并注释，其它ip地址也可访问# bind 127.0.0.1# # 查找 /requirepass 去掉注释#，并把foobared 替换为密码，例如：password123456# requirepass foobaredrequirepass password123456 开启防火墙的端口号规则（安全组），将6380端口号开通 1[root@localhost ~] /sbin/iptables -I INPUT -p tcp --dport 6380 -j ACCEPT 修改完成后，要在服务里重启Redis服务才能使设置生效 1/usr/local/bin/redis-server /etc/redis6380.conf 测试远程访问 12C:\\Users\\zc&gt; redis-cli -h 192.168.111.226 -p 6380 -a password123456192.168.111.226:6380&gt; 停止6380的Redis服务也需要密码 1[root@localhost ~] redis-cli -p 6380 -a password123456 shutdown 2. MySQL多实例配置2.1 查看主机MySQL信息 查看现有MySQL数据库实例占用端口 123[root@localhost ~] netstat -anp | grep mysqldtcp6 0 0 :::3306 :::* LISTEN 1089/mysqld unix 2 [ ACC ] STREAM LISTENING 20497 1089/mysqld /var/lib/mysql/mysql.sock 须先关闭单实例，跟多实例会有冲突 备份数据：[root@localhost ~] mysqldump -P 3306 -u root -p --all-databases &gt; /home/backup/data3306.bak 停止单实例服务：[root@localhost ~] service mysqld stop 查找配置文件位置 123[root@localhost ~] locate my.cnf/etc/my.cnf/etc/my.cnf.d 2.2 添加一个3307端口的实例 拷贝my.cnf并命名为my3307.cnf，并修改参数，主要修改port,sockt,datadir 123456789101112131415161718192021222324252627282930[root@localhost ~] cp /etc/my.cnf /etc/my3307.cnf[root@localhost ~] vi /etc/my3307.cnf[mysqld]# server端字符集character-set-server=utf8collation-server=utf8_general_ciuser=root# 修改端口port=3307# 修改数据存放目录datadir=/var/lib/mysql3307# 客户端连接socketsocket=/var/lib/mysql/mysql3307.sock# 修改日志文件log-error=/var/log/mysqld3307.log# 修改pid，每个实例需要运行在不同的pidpid-file=/var/run/mysqld/mysqld3307.pid# 解决问题：TIMESTAMP with implicit DEFAULT value is deprecatedexplicit_defaults_for_timestamp=true# skip_grant_tables[mysql]socket=/var/lib/mysql/mysql3307.sockdefault-character-set=utf8[mysql.server]default-character-set=utf8[mysql_safe]default-character-set=utf8[client]socket=/var/lib/mysql/mysql3307.sockdefault-character-set=utf8 初始化数据库 123# 写入host避免反解析报错[root@localhost ~] echo &quot;127.0.0.1 `hostname`&quot; &gt;&gt; /etc/hosts &amp;&amp; cat /etc/hosts[root@localhost ~] mysqld --defaults-file=/etc/my3307.cnf --initialize-insecure 启动3307端口MySQL服务，并查看MySQL进程 1[root@localhost ~] mysqld --defaults-file=/etc/my3307.cnf --user=root &amp; 登录MySQL 1234# 多实例为root增加密码[root@localhost ~] mysqladmin -u root -S /var/lib/mysql/mysql3307.sock password &#x27;123qwe&#x27;# 登录[root@localhost ~] mysql -S /var/lib/mysql/mysql3307.sock -p 停止本实例MySQL服务 1[root@localhost ~] mysqladmin -u root -S /var/lib/mysql/mysql3307.sock shutdown 2.3 再添加一个3308端口的实例 拷贝my.cnf并命名为my3308.cnf，并修改参数，主要修改port,sockt,datadir 123456789101112131415161718192021222324252627282930[root@localhost ~] cp /etc/my.cnf /etc/my3308.cnf[root@localhost ~] vi /etc/my3308.cnf[mysqld]# server端字符集character-set-server=utf8collation-server=utf8_general_ciuser=root# 修改端口port=3308# 修改数据存放目录datadir=/var/lib/mysql3308# 客户端连接socketsocket=/var/lib/mysql/mysql3308.sock# 修改日志文件log-error=/var/log/mysqld3308.log# 修改pid，每个实例需要运行在不同的pidpid-file=/var/run/mysqld/mysqld3308.pid# 解决问题：TIMESTAMP with implicit DEFAULT value is deprecatedexplicit_defaults_for_timestamp=true# skip_grant_tables[mysql]socket=/var/lib/mysql/mysql3308.sockdefault-character-set=utf8[mysql.server]default-character-set=utf8[mysql_safe]default-character-set=utf8[client]socket=/var/lib/mysql/mysql3308.sockdefault-character-set=utf8 初始化数据库 1[root@localhost ~] mysqld --defaults-file=/etc/my3308.cnf --initialize-insecure 启动3308端口MySQL服务 1[root@localhost ~] mysqld --defaults-file=/etc/my3308.cnf --user=root &amp; 登录MySQL 1234# 多实例为root增加密码[root@localhost ~] mysqladmin -u root -S /var/lib/mysql/mysql3308.sock password &#x27;123qwe&#x27;# 登录[root@localhost ~] mysql -S /var/lib/mysql/mysql3308.sock -p 停止本实例MySQL服务 1[root@localhost ~] mysqladmin -u root -S /var/lib/mysql/mysql3308.sock shutdown 2.4 实例3307开启远程访问 开启3307端口防火墙 1[root@localhost ~] /sbin/iptables -I INPUT -p tcp --dport 3307 -j ACCEPT 测试远程访问 12C:\\Users\\zc&gt;mysql -h 192.168.111.227 -P 3307 -u root -pEnter password: ******","tags":[{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"},{"name":"MySQL","slug":"MySQL","permalink":"http://chaooo.github.io/tags/MySQL/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"}]},{"title":"「安全认证」基于Shiro前后端分离的认证与授权(三.前端篇)","date":"2020-02-06T18:10:46.000Z","path":"2020/02/07/authenticate-shiro-vue.html","text":"前两篇我们整合了SpringBoot+Shiro+JWT+Redis实现了登录认证，接口权限控制，接下来将要实现前端 Vue 的动态路由控制。 1. 前端权限控制思路（Vue）前端的权限控制，不同的权限对应着不同的路由，同时菜单也需根据不同的权限，异步生成。先回顾下整体流程： 登录: 提交账号和密码到服务端签发token，拿到token之后存入浏览器，再携带token(一般放在请求头中)再去获取用户的详细信息(包括用户权限等信息)。 权限验证：通过用户权限信息 构建 对应权限的路由，通过router.addRoutes动态挂载这些路由。 接下来将基于 Vue 开源后台模板vue-admin-template来演示具体流程，这里只演示重要代码，完整项目移步文章末尾获取源码。 2. 登录 先准备基础的静态路由(src/router/index.js) 12345678910111213141516171819202122232425export const constantRoutes = [ // 登陆页面 &#123; path: &#x27;/login&#x27;, component: () =&gt; import(&#x27;@/views/login/index&#x27;), hidden: true &#125;, // 首页 &#123; path: &#x27;/&#x27;, component: Layout, redirect: &#x27;/dashboard&#x27;, children: [&#123; path: &#x27;dashboard&#x27;, name: &#x27;Dashboard&#x27;, component: () =&gt; import(&#x27;@/views/dashboard/index&#x27;), meta: &#123; title: &#x27;首页&#x27;, icon: &#x27;dashboard&#x27; &#125; &#125;] &#125;, &#123; path: &#x27;/404&#x27;, component: () =&gt; import(&#x27;@/views/404&#x27;), hidden: true &#125;] 登录页面(src&#x2F;views&#x2F;login&#x2F;index.vue)click事件触发登录操作 12345this.$store.dispatch(&#x27;user/login&#x27;, this.loginForm).then(() =&gt; &#123; this.$router.push(&#123; path: &#x27;/&#x27; &#125;); //登录成功之后重定向到首页&#125;).catch(err =&gt; &#123; this.$message.error(err); //登录失败提示错误&#125;); 登录逻辑(src&#x2F;store&#x2F;modules&#x2F;user.js)action: 12345678910111213141516171819202122232425262728293031323334353637383940const actions = &#123; // user login login(&#123; commit &#125;, userInfo) &#123; const &#123; account, password &#125; = userInfo return new Promise((resolve, reject) =&gt; &#123; // 这里的login调用api接口请求数据 login(&#123; account: account.trim(), password: password &#125;).then(response =&gt; &#123; const &#123; data &#125; = response commit(&#x27;SET_TOKEN&#x27;, data) setToken(data) resolve() &#125;).catch(error =&gt; &#123; reject(error) &#125;) &#125;) &#125;, // get user info getInfo(&#123; commit, state &#125;) &#123; return new Promise((resolve, reject) =&gt; &#123; // 这里的getInfo调用api接口请求数据 getInfo(state.token).then(response =&gt; &#123; const &#123; data &#125; = response if (!data) &#123; reject(&#x27;Verification failed, please Login again.&#x27;) &#125; const &#123; nickname, avatar, roles, permissions &#125; = data // 全局储存用户信息 commit(&#x27;SET_NAME&#x27;, nickname) commit(&#x27;SET_AVATAR&#x27;, avatar) // 角色信息 commit(&#x27;SET_ROLES&#x27;, roles) // 指令权限信息 commit(&#x27;SET_PERMISSIONS&#x27;, permissions) resolve(data) &#125;).catch(error =&gt; &#123; reject(error) &#125;) &#125;) &#125;,&#125; /login与/getInfo接口与返回的数据 1234567891011121314151617181920POST (localhost:8282/login)请求参数: &#123;&quot;username&quot;:&quot;admin&quot;,&quot;password&quot;:&quot;123456&quot;&#125;响应: &#123; &quot;code&quot;:0, &quot;msg&quot;:&quot;登录成功&quot;, &quot;data&quot;:&quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJhZG1pbiIsInVpZCI6MSwiZXhwIjoxNTgwOTk4MTIzfQ.6jgqt_opjnosASlJ2oSIYZn1Sb2BQO-eUo_6OVTHv50&quot;&#125;// ------------getInfo--------------GET (localhost:8282/user/info)Headers: &#123;&quot;X-Token&quot;:&quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJhZG1pbiIsInVpZCI6MSwiZXhwIjoxNTgwOTk4MTIzfQ.6jgqt_opjnosASlJ2oSIYZn1Sb2BQO-eUo_6OVTHv50&quot;&#125;响应: &#123; &quot;code&quot;: 0, &quot;msg&quot;: &quot;获取成功&quot;, &quot;data&quot;: &#123; &quot;account&quot;: &quot;admin&quot;, &quot;nickname&quot;: &quot;超级管理员&quot;, &quot;roles&quot;: [&quot;admin&quot;], &quot;permissions&quot;: [&quot;user:list&quot;,&quot;user:add&quot;,&quot;user:delete&quot;,&quot;user:update&quot;] &#125;&#125; 获取用户信息(src&#x2F;permission.js) 用户登录成功之后，我们会在全局钩子router.beforeEach中拦截路由，判断是否已获得token，在获得token之后我们就要去获取用户的基本信息 并且根据用户角色动态挂载路由。 123456789101112131415161718192021222324252627282930313233343536373839const whiteList = [&#x27;/login&#x27;] // 白名单router.beforeEach(async(to, from, next) =&gt; &#123; // 判断是否已获得token const hasToken = getToken() if (hasToken) &#123; if (to.path === &#x27;/login&#x27;) &#123; next(&#123; path: &#x27;/&#x27; &#125;) &#125; else &#123; const hasRole = store.getters.role if (hasRole) &#123; next() &#125; else &#123; try &#123; // 获取用户角色 [&#x27;admin&#x27;] 或,[&#x27;developer&#x27;,&#x27;editor&#x27;] const &#123; roles &#125; = await store.dispatch(&#x27;user/getInfo&#x27;) // 动态根据 角色 算出其对应有权限的路由 const accessRoutes = await store.dispatch(&#x27;permission/generateRoutes&#x27;, roles) // 动态挂载路由 router.addRoutes(accessRoutes) // addRouter是让挂载的路由生效，但是挂载后&#x27;router.options.routes&#x27;并未刷新(应该是个bug) // 所以还需要手动将路由加入&#x27;router.options.routes&#x27; router.options.routes = constantRoutes.concat(accessRoutes) next() &#125; catch (error) &#123; await store.dispatch(&#x27;user/resetToken&#x27;) Message.error(error || &#x27;Has Error&#x27;) next(`/login?redirect=$&#123;to.path&#125;`) &#125; &#125; &#125; &#125; else &#123; /* has no token*/ if (whiteList.indexOf(to.path) !== -1) &#123; next() &#125; else &#123; next(`/login?redirect=$&#123;to.path&#125;`) &#125; &#125;&#125;) 3. 动态挂载路由主要思路，前端会有一份包含所有路由的路由表。创建Vue实例时会先挂载登录等公共路由；当用户登录之后，通过getInfo(token)获取用户的角色(roles)，动态根据用户的roles算出其对应有权限的路由，再通过router.addRoutes动态挂载路由；使用vuex管理路由表，根据vuex中可访问的路由渲染菜单。但这些控制都只是页面级的，后端接口也需要做权限验证。 改造一下路由表，添加异步路由列表，将角色添加到元数据meta中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// 动态路由export const asyncRoutes = [ &#123; path: &#x27;/user&#x27;, component: Layout, redirect: &#x27;/user/list&#x27;, name: &#x27;User&#x27;, meta: &#123; title: &#x27;用户管理&#x27;, icon: &#x27;example&#x27; &#125;, children: [ &#123; path: &#x27;list&#x27;, name: &#x27;UserList&#x27;, component: () =&gt; import(&#x27;@/views/user/list&#x27;), meta: &#123; title: &#x27;用户列表&#x27;, icon: &#x27;nested&#x27; &#125; &#125;, &#123; path: &#x27;edit&#x27;, name: &#x27;UserEdit&#x27;, component: () =&gt; import(&#x27;@/views/user/form&#x27;), meta: &#123; title: &#x27;添加用户&#x27;, icon: &#x27;form&#x27; &#125; &#125; ] &#125;, &#123; path: &#x27;/admin&#x27;, component: Layout, children: [ &#123; path: &#x27;index&#x27;, name: &#x27;Form1&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), meta: &#123; title: &#x27;管理员角色测试&#x27;, icon: &#x27;form&#x27;, roles: [&#x27;admin&#x27;] &#125; &#125; ] &#125;, &#123; path: &#x27;/editor&#x27;, component: Layout, children: [ &#123; path: &#x27;index&#x27;, name: &#x27;Form2&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), meta: &#123; title: &#x27;编辑角色测试&#x27;, icon: &#x27;form&#x27;, roles: [&#x27;editor&#x27;] &#125; &#125; ] &#125;, &#123; path: &#x27;/form&#x27;, component: Layout, children: [ &#123; path: &#x27;index&#x27;, name: &#x27;Form3&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), meta: &#123; title: &#x27;用户角色测试&#x27;, icon: &#x27;form&#x27;, roles: [&#x27;user&#x27;] &#125; &#125; ] &#125;, &#123; path: &#x27;/nested&#x27;, component: Layout, redirect: &#x27;/nested/menu3&#x27;, name: &#x27;Nested&#x27;, meta: &#123; title: &#x27;子菜单权限测试&#x27;, icon: &#x27;form&#x27; &#125;, children: [ &#123; path: &#x27;menu1&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), name: &#x27;Menu1&#x27;, meta: &#123; title: &#x27;管理员可见&#x27;, roles: [&#x27;admin&#x27;] &#125; &#125;, &#123; path: &#x27;menu2&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), name: &#x27;Menu1&#x27;, meta: &#123; title: &#x27;编辑者可见&#x27;, roles: [&#x27;editor&#x27;] &#125; &#125;, &#123; path: &#x27;menu3&#x27;, component: () =&gt; import(&#x27;@/views/test/index&#x27;), name: &#x27;Menu1&#x27;, meta: &#123; title: &#x27;普通用户可见&#x27;, roles: [&#x27;user&#x27;] &#125; &#125; ] &#125;, 根据前面获取用户信息的代码可发现，通过store.dispatch(&#39;permission/generateRoutes&#39;,roles)来获得有权限的路由，新建src/store/modules/permission.js如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import &#123; asyncRoutes, constantRoutes &#125; from &#x27;@/router&#x27;/** 判断用户是否拥有此路由的权限 */function hasPermission(roles, route) &#123; if (route.meta &amp;&amp; route.meta.roles) &#123; return roles.some(role =&gt; route.meta.roles.includes(role)) &#125; else &#123; return true &#125;&#125;/** 递归组装路由表，返回符合用户角色权限的路由列表 */export function filterAsyncRoutes(routes, roles) &#123; const res = [] routes.forEach(route =&gt; &#123; const tmp = &#123; ...route &#125; if (hasPermission(roles, tmp)) &#123; if (tmp.children) &#123; // 递归调用 tmp.children = filterAsyncRoutes(tmp.children, roles) &#125; res.push(tmp) &#125; &#125;) return res&#125;const state = &#123; routes: [], // 所有路由,包括静态路由和动态路由 addRoutes: [] // 动态路由&#125;const mutations = &#123; SET_ROUTES: (state, routes) =&gt; &#123; state.addRoutes = routes // 合并路由 state.routes = constantRoutes.concat(routes) &#125;&#125;const actions = &#123; // 生成动态路由 generateRoutes(&#123; commit &#125;, roles) &#123; return new Promise(resolve =&gt; &#123; let accessedRoutes if (roles.includes(&#x27;admin&#x27;)) &#123; // &#x27;超级管理员&#x27;拥有所有的路由，这样判断节省加载时间 accessedRoutes = asyncRoutes || [] &#125; else &#123; // 筛选出该角色有权限的路由 accessedRoutes = filterAsyncRoutes(asyncRoutes, roles) &#125; commit(&#x27;SET_ROUTES&#x27;, accessedRoutes) resolve(accessedRoutes) &#125;) &#125;&#125;export default &#123; namespaced: true, state, mutations, actions&#125; 4. axios 拦截器通过request拦截器在每个请求头里面塞入token，好让后端对请求进行权限验证；代码位置:src/utils/request.js。 12345678910111213141516171819202122import axios from &#x27;axios&#x27;import store from &#x27;@/store&#x27;import &#123; getToken &#125; from &#x27;@/utils/auth&#x27;// create an axios instanceconst service = axios.create(&#123; baseURL: process.env.VUE_APP_BASE_API, // url = base url + request url withCredentials: false, // send cookies when cross-domain requests timeout: 5000 // request timeout&#125;)service.interceptors.request.use( config =&gt; &#123; if (store.getters.token) &#123; // 登陆后将token放入headers[&#x27;X-Token&#x27;]中 config.headers[&#x27;X-Token&#x27;] = getToken() &#125; return config &#125;, error =&gt; &#123; return Promise.reject(error) &#125;)export default service 5. 指令权限可以使用全局权限判断函数，实现按钮级别的权限判断。 12345678910111213141516&lt;template&gt; &lt;el-button v-if=&quot;checkPermission(&#x27;user:add&#x27;)&quot;&gt;添加&lt;/el-button&gt; &lt;el-button v-if=&quot;checkPermission(&#x27;user:delete&#x27;)&quot;&gt;删除&lt;/el-button&gt; &lt;el-button v-if=&quot;checkPermission(&#x27;user:update&#x27;)&quot;&gt;修改&lt;/el-button&gt; &lt;el-button v-if=&quot;checkPermission(&#x27;user:list&#x27;)&quot;&gt;查看&lt;/el-button&gt;&lt;/template&gt;&lt;script&gt; import checkPermission from &#x27;@/utils/permission&#x27; // 权限判断函数 export default &#123; methods: &#123; checkPermission(value) &#123; return checkPermission(value) &#125;, &#125;, &#125;&lt;/script&gt; src/utils/permission.js: 12345import store from &#x27;@/store&#x27;export default function checkPermission(value) &#123; const permissions = store.getters.permissions return permissions.indexOf(value) &gt; -1&#125; 6. 效果演示： 注：搭建到这里的代码在github源码tag的V3.0中。源码地址: https://github.com/chaooo/springboot-vue-shiro.git","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"}]},{"title":"「安全认证」基于Shiro前后端分离的认证与授权(二.授权篇)","date":"2020-01-21T10:10:55.000Z","path":"2020/01/21/authenticate-shiro-authorize.html","text":"前面我们整合了SpringBoot+Shiro+JWT实现了登录认证，但还没有实现权限控制，这是接下来的工作。 1. JWT的Token续签1.1 续签思路 业务逻辑： 登录成功后，用户在未过期时间内继续操作，续签token。 登录成功后，空闲超过过期时间，返回token已失效，重新登录。 实现逻辑： 登录成功后将token存储到redis里面(这时候k、v值一样都为token)，并设置过期时间为token过期时间 当用户请求时token值还未过期，则重新设置redis里token的过期时间。 当用户请求时token值已过期，但redis中还在，则JWT重新生成token并覆盖v值(这时候k、v值不一样了)，然后设置redis过期时间。 当用户请求时token值已过期，并且redis中也不存在，则用户空闲超时，返回token已失效，重新登录。 1.2 编码实现 pom.xml引入Redis 12345678910&lt;!-- Redis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.8.0&lt;/version&gt;&lt;/dependency&gt; 编写Redis工具类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Componentpublic class RedisUtil &#123; @Resource private RedisTemplate&lt;String, Object&gt; redisTemplate; /** * 指定缓存失效时间 * @param key 键 * @param time 时间(秒) */ public boolean expire(String key, long time) &#123; try &#123; if (time &gt; 0) &#123; redisTemplate.expire(key, time, TimeUnit.SECONDS); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 根据key 获取过期时间 * @param key 键 不能为null * @return 时间(秒) 返回0代表为永久有效 */ public long getExpire(String key) &#123; return redisTemplate.getExpire(key, TimeUnit.SECONDS); &#125; /** * 普通缓存放入并设置时间 * @param key 键 * @param value 值 * @param time 时间(秒) time要大于0 如果time小于等于0 将设置无限期 * @return true成功 false 失败 */ public boolean set(String key, Object value, long time) &#123; try &#123; if (time &gt; 0) &#123; redisTemplate.opsForValue().set(key, value, time, TimeUnit.SECONDS); &#125; else &#123; set(key, value); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125;&#125; JwtUtil中增加返回过期秒数的方法 123456789101112public class JwtUtil &#123; /** 设置过期时间: 30分钟 */ private static final long EXPIRE_TIME = 30 * 60 * 1000; //... 其他代码省略 /** * 返回设置的过期秒数 * @return long 秒数 */ public static long getExpireTime()&#123; return EXPIRE_TIME/1000; &#125;&#125; 改写登录逻辑，生成token后存入Redis 1234567891011121314151617181920212223242526272829303132@Servicepublic class SysServiceImpl implements SysService &#123; private String getToken(User user)&#123; // 生成token String token = JwtUtil.createToken(user); // 为了过期续签，将token存入redis，并设置超时时间 redisUtil.set(token, token, JwtUtil.getExpireTime()); return token; &#125; /** * 用户登录(用户名，密码) * @param account 用户名 * @param password 密码 * @return token */ @Override public ResponseVo&lt;String&gt; login(String account, String password) &#123; //处理比对密码 User user = sysDao.selectByAccount(account); if(user!=null) &#123; String salt = user.getSalt(); String md5Password = Md5Util.md5(password+salt); String dbPassword = user.getPassword(); if(md5Password.equals(dbPassword)) &#123; //生成token给用户，并存入redis String token = getToken(user); return new ResponseVo&lt;&gt;(0,&quot;登录成功&quot;, token); &#125; &#125; return new ResponseVo&lt;&gt;( -1, &quot;登录失败&quot;); &#125;&#125; 改写MyRealm，加入token续签逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Slf4j@Component(&quot;MyRealm&quot;)public class MyRealm extends AuthorizingRealm &#123; /** * JWT Token续签： * 业务逻辑：登录成功后，用户在未过期时间内继续操作，续签token。 * 登录成功后，空闲超过过期时间，返回token已失效，重新登录。 * 实现逻辑： * 1.登录成功后将token存储到redis里面(这时候k、v值一样都为token)，并设置过期时间为token过期时间 * 2.当用户请求时token值还未过期，则重新设置redis里token的过期时间。 * 3.当用户请求时token值已过期，但redis中还在，则JWT重新生成token并覆盖v值(这时候k、v值不一样了)，然后设置redis过期时间。 * 4.当用户请求时token值已过期，并且redis中也不存在，则用户空闲超时，返回token已失效，重新登录。 */ public boolean tokenRefresh(String token, User user) &#123; String cacheToken = String.valueOf(redisUtil.get(token)); // 过期后会得到&quot;null&quot;值，所以需判断字符串&quot;null&quot; if (cacheToken != null &amp;&amp; cacheToken.length() != 0 &amp;&amp; !&quot;null&quot;.equals(cacheToken)) &#123; // 校验token有效性 if (!JwtUtil.isVerify(cacheToken)) &#123; // 生成token String newToken = JwtUtil.createToken(user); // 将token存入redis,并设置超时时间 redisUtil.set(token, newToken, JwtUtil.getExpireTime()); &#125; else &#123; // 重新设置超时时间 redisUtil.expire(token, JwtUtil.getExpireTime()); &#125; log.info(&quot;打印存入redis的过期时间：&quot;+redisUtil.getExpire(token)); return true; &#125; return false; &#125; /** * 重写认证逻辑 */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken auth) throws AuthenticationException &#123; log.info(&quot;————————身份认证——————————&quot;); String token = (String) auth.getCredentials(); if (null == token) &#123; throw new AuthenticationException(&quot;token为空!&quot;); &#125; // 解密获得username，用于和数据库进行对比 String account = JwtUtil.parseTokenAud(token); User user = sysService.selectByAccount(account); if (null == user) &#123; throw new AuthenticationException(&quot;用户不存在!&quot;); &#125; // 校验token是否过期 if (!tokenRefresh(token, user)) &#123; throw new AuthenticationException(&quot;Token已过期!&quot;); &#125; return new SimpleAuthenticationInfo(user, token,&quot;MyRealm&quot;); &#125;&#125; 到此，JWT的Token续签的功能已经全部实现了。 2. 权限管理2.1 首先增加三张数据表123456789101112131415161718192021222324252627282930313233343536/** 角色表 */DROP TABLE IF EXISTS `sys_role`;CREATE TABLE `sys_role` ( `id` INT(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;主键id&#x27;, `role_name` VARCHAR(100) DEFAULT NULL COMMENT &#x27;角色名称&#x27;, `description` VARCHAR(100) DEFAULT NULL COMMENT &#x27;描述&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=INNODB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT=&#x27;角色表&#x27;;INSERT INTO `sys_role`(`id`,`role_name`,`description`) VALUES (1,&#x27;admin&#x27;,&#x27;管理角色&#x27;),(2,&#x27;user&#x27;,&#x27;用户角色&#x27;);/** 权限表 */DROP TABLE IF EXISTS `sys_permission`;CREATE TABLE `sys_permission` ( `id` VARCHAR(32) NOT NULL COMMENT &#x27;主键id&#x27;, `name` VARCHAR(100) DEFAULT NULL COMMENT &#x27;菜单标题&#x27;, `url` VARCHAR(255) DEFAULT NULL COMMENT &#x27;路径&#x27;, `menu_type` INT(11) DEFAULT NULL COMMENT &#x27;菜单类型(0:一级菜单; 1:子菜单:2:按钮权限)&#x27;, `perms` VARCHAR(255) DEFAULT NULL COMMENT &#x27;菜单权限编码&#x27;, `sort_no` INT(10) DEFAULT NULL COMMENT &#x27;菜单排序&#x27;, `del_flag` INT(1) DEFAULT &#x27;0&#x27; COMMENT &#x27;删除状态 0正常 1已删除&#x27;, PRIMARY KEY (`id`) USING BTREE, KEY `index_prem_sort_no` (`sort_no`) USING BTREE, KEY `index_prem_del_flag` (`del_flag`) USING BTREE) ENGINE=INNODB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT=&#x27;菜单权限表&#x27;;INSERT INTO `sys_permission`(`id`,`name`,`url`,`menu_type`,`perms`,`sort_no`,`del_flag`) VALUES (&#x27;1&#x27;,&#x27;新增用户&#x27;,&#x27;/user/add&#x27;,2,&#x27;user:add&#x27;,1,0),(&#x27;2&#x27;,&#x27;删除用户&#x27;,&#x27;/user/delete&#x27;,2,&#x27;user:delete&#x27;,2,0),(&#x27;3&#x27;,&#x27;修改用户&#x27;,&#x27;/user/update&#x27;,2,&#x27;user:update&#x27;,3,0),(&#x27;4&#x27;,&#x27;查询用户&#x27;,&#x27;/user/list&#x27;,2,&#x27;user:list&#x27;,4,0);/** 角色与权限关联表 */DROP TABLE IF EXISTS `sys_role_permission`;CREATE TABLE `sys_role_permission` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `role_id` INT(11) DEFAULT NULL COMMENT &#x27;角色id&#x27;, `permission_id` INT(11) DEFAULT NULL COMMENT &#x27;权限id&#x27;, PRIMARY KEY (`id`) USING BTREE, KEY `index_group_role_per_id` (`role_id`,`permission_id`) USING BTREE, KEY `index_group_role_id` (`role_id`) USING BTREE, KEY `index_group_per_id` (`permission_id`) USING BTREE) ENGINE=INNODB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT=&#x27;角色权限表&#x27;;INSERT INTO `sys_role_permission`(`id`,`role_id`,`permission_id`) VALUES (1,1,1),(2,1,2),(3,1,3),(4,1,4),(5,2,4); 2.2 编码实现 补全MyRealm中授权验证逻辑 1234567891011121314151617181920212223242526272829@Slf4j@Component(&quot;MyRealm&quot;)public class MyRealm extends AuthorizingRealm &#123; //...其他代码省略/** * 获取用户权限信息，包括角色以及权限。 * 只有当触发检测用户权限时才会调用此方法，例如checkRole,checkPermissionJwtToken */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; log.info(&quot;————权限认证 [ roles、permissions]————&quot;); User user = null; if (principals != null) &#123; user = (User) principals.getPrimaryPrincipal(); &#125; SimpleAuthorizationInfo simpleAuthorizationInfo = new SimpleAuthorizationInfo(); if (user != null) &#123; // 用户拥有的角色，比如“admin/user” String role = sysService.getRoleByRoleid(user.getRoleid()); simpleAuthorizationInfo.addRole(role); log.info(&quot;角色为：&quot;+role); // 用户拥有的权限集合，比如“role:add,user:add” Set&lt;String&gt; permissions = sysService.getPermissionsByRoleid(user.getRoleid()); simpleAuthorizationInfo.addStringPermissions(permissions); log.info(&quot;权限有：&quot;+permissions.toString()); &#125; return simpleAuthorizationInfo; &#125;&#125; Service中添加获取角色与权限的方法，DAO与Mapper请移步源码。 1234567891011121314151617181920212223242526272829public interface SysService &#123; /** * 根据roleid查找用户角色名，自定义Realm中调用 * @param roleid * @return roles */ String getRoleByRoleid(Integer roleid); /** * 根据roleid查找用户权限，自定义Realm中调用 * @param roleid * @return Set&lt;permissions&gt; */ Set&lt;String&gt; getPermissionsByRoleid(Integer roleid);&#125;/** * 实现类 */@Servicepublic class SysServiceImpl implements SysService &#123; @Override public String getRoleByRoleid(Integer roleid) &#123; return sysDao.getRoleByRoleid(roleid); &#125; @Override public Set&lt;String&gt; getPermissionsByRoleid(Integer roleid) &#123; return sysDao.getPermissionsByRoleid(roleid); &#125;&#125; Controller中使用@RequiresPermissions来控制权限 1234567891011121314151617181920212223@RestControllerpublic class UserApi &#123; /** * 获取所有用户信息 * @return */ @RequiresPermissions(&quot;user:list&quot;) @GetMapping(&quot;/user/list&quot;) public ResponseVo list() &#123; return userService.loadUser(); &#125; /** * 用户更新资料 * @param user * @return */ @RequiresPermissions(&quot;user:update&quot;) @PostMapping(&quot;/user/update&quot;) public ResponseVo update(User user, HttpServletRequest request) &#123; String token = request.getHeader(&quot;X-Token&quot;); return userService.modifyUser(token, user); &#125;&#125; 注：这里的登录认证+授权控制 在github源码tag的V2.0中，后续版本再加入前端动态路由控制等。源码地址: https://github.com/chaooo/springboot-vue-shiro.git仅下载后端认证+授权控制源码:git clone --branch V2.0 https://github.com/chaooo/springboot-vue-shiro.git","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"}]},{"title":"「安全认证」基于Shiro前后端分离的认证与授权(一.认证篇)","date":"2020-01-18T15:26:02.000Z","path":"2020/01/18/authenticate-shiro-login.html","text":"1. 开始之前1.1 技术选型选用SpringBoot+Shiro+JWT实现登录认证，结合Redis服务实现token的续签，前端选用Vue动态构造路由及更细粒度的操作权限控制。 前后端分离项目中，我们一般采用的是无状态登录：服务端不保存任何客户端请求者信息，客户端需要自己携带着信息去访问服务端，并且携带的信息可以被服务端辨认。 而Shiro默认的拦截跳转都是跳转url页面，拦截校验机制恰恰使用的session；而前后端分离后，后端并无权干涉页面跳转。 因此前后端分离项目中使用Shiro就需要对其进行改造，我们可以在整合Shiro的基础上自定义登录校验，继续整合JWT(或者 oauth2.0 等)，使其成为支持服务端无状态登录，即token登录。 在Vue项目中，只需要根据登录用户的权限信息动态的加载路由列表就可以动态的构造出访问菜单。 1.2 整体流程 首次通过post请求将用户名与密码到login进行登入，登录成功后返回token； 每次请求，客户端需通过header将token带回服务器做JWT Token的校验； 服务端负责token生命周期的刷新，用户权限的校验； 2. SpringBoot 整合 Shiro+JWT这里贴出主要逻辑，源码请移步文章末尾获取。 数据表 12345678910111213141516/** 系统用户表 */DROP TABLE IF EXISTS sys_user;CREATE TABLE sys_user( id INT AUTO_INCREMENT COMMENT &#x27;用户ID&#x27;, account VARCHAR(30) NOT NULL COMMENT &#x27;用户名&#x27;, PASSWORD VARCHAR(50) COMMENT &#x27;用户密码&#x27;, salt VARCHAR(8) COMMENT &#x27;随机盐&#x27;, nickname VARCHAR(30) COMMENT &#x27;用户昵称&#x27;, roleId INT COMMENT &#x27;角色ID&#x27;, createTime DATE COMMENT &#x27;创建时间&#x27;, updateTime DATE COMMENT &#x27;更新时间&#x27;, deleteStatus VARCHAR(2) DEFAULT &#x27;1&#x27; COMMENT &#x27;是否有效：1有效，2无效&#x27;, CONSTRAINT sys_user_id_pk PRIMARY KEY(id), CONSTRAINT sys_user_account_uk UNIQUE(account));COMMIT; pom.xml 123456789101112&lt;!-- JWT --&gt;&lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;/groupId&gt; &lt;artifactId&gt;java-jwt&lt;/artifactId&gt; &lt;version&gt;3.8.3&lt;/version&gt;&lt;/dependency&gt;&lt;!-- shiro --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.2&lt;/version&gt;&lt;/dependency&gt; shiro配置类：构建securityManager环境，及配置shiroFilter并将jwtFilter添加进shiro的拦截器链中，放行登录注册请求。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566@Configurationpublic class ShiroConfig &#123; @Bean(&quot;securityManager&quot;) public DefaultWebSecurityManager getManager(MyRealm myRealm) &#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 使用自己的realm securityManager.setRealm(myRealm); /* * 关闭shiro自带的session，详情见文档 * http://shiro.apache.org/session-management.html#SessionManagement-StatelessApplications%28Sessionless%29 */ DefaultSubjectDAO subjectDAO = new DefaultSubjectDAO(); DefaultSessionStorageEvaluator defaultSessionStorageEvaluator = new DefaultSessionStorageEvaluator(); defaultSessionStorageEvaluator.setSessionStorageEnabled(false); subjectDAO.setSessionStorageEvaluator(defaultSessionStorageEvaluator); securityManager.setSubjectDAO(subjectDAO); return securityManager; &#125; @Bean(&quot;shiroFilter&quot;) public ShiroFilterFactoryBean factory(DefaultWebSecurityManager securityManager) &#123; ShiroFilterFactoryBean factoryBean = new ShiroFilterFactoryBean(); factoryBean.setSecurityManager(securityManager); // 拦截器 Map&lt;String, String&gt; filterChainDefinitionMap = new LinkedHashMap&lt;String, String&gt;(); // 配置不会被拦截的链接 顺序判断，规则：http://shiro.apache.org/web.html#urls- filterChainDefinitionMap.put(&quot;/register&quot;, &quot;anon&quot;); filterChainDefinitionMap.put(&quot;/login&quot;, &quot;anon&quot;); filterChainDefinitionMap.put(&quot;/unauthorized&quot;, &quot;anon&quot;); // 添加自己的过滤器并且取名为jwt Map&lt;String, Filter&gt; filterMap = new HashMap&lt;&gt;(1); filterMap.put(&quot;jwt&quot;, new JwtFilter()); factoryBean.setFilters(filterMap); // 过滤链定义，从上向下顺序执行，一般将/**放在最为下边 filterChainDefinitionMap.put(&quot;/**&quot;, &quot;jwt&quot;); // 未授权返回 factoryBean.setUnauthorizedUrl(&quot;/unauthorized&quot;); factoryBean.setFilterChainDefinitionMap(filterChainDefinitionMap); return factoryBean; &#125; /** * 添加注解支持 */ @Bean @DependsOn(&quot;lifecycleBeanPostProcessor&quot;) public DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator() &#123; DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator = new DefaultAdvisorAutoProxyCreator(); // 强制使用cglib，防止重复代理和可能引起代理出错的问题 // https://zhuanlan.zhihu.com/p/29161098 defaultAdvisorAutoProxyCreator.setProxyTargetClass(true); return defaultAdvisorAutoProxyCreator; &#125; @Bean public LifecycleBeanPostProcessor lifecycleBeanPostProcessor() &#123; return new LifecycleBeanPostProcessor(); &#125; @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(DefaultWebSecurityManager securityManager) &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); advisor.setSecurityManager(securityManager); return advisor; &#125;&#125; 自定义Realm：继承AuthorizingRealm类，在其中实现登陆验证及权限获取的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Slf4j@Component(&quot;MyRealm&quot;)public class MyRealm extends AuthorizingRealm &#123; /** 注入SysService */ private SysService sysService; @Autowired public void setSysService(SysService sysService) &#123; this.sysService = sysService; &#125; /** * 必须重写此方法，不然Shiro会报错 */ @Override public boolean supports(AuthenticationToken token) &#123; return token instanceof JwtToken; &#125; /** * 用来进行身份认证，也就是说验证用户输入的账号和密码是否正确， * 获取身份验证信息，错误抛出异常 */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken auth) throws AuthenticationException &#123; log.info(&quot;————————身份认证——————————&quot;); String token = (String) auth.getCredentials(); if (null == token || !JwtUtil.isVerify(token)) &#123; throw new AuthenticationException(&quot;token无效!&quot;); &#125; // 解密获得username，用于和数据库进行对比 String account = JwtUtil.parseTokenAud(token); User user = sysService.selectByAccount(account); if (null == user) &#123; throw new AuthenticationException(&quot;用户不存在!&quot;); &#125; return new SimpleAuthenticationInfo(user, token,&quot;MyRealm&quot;); &#125; /** * 获取用户权限信息，包括角色以及权限。 * 只有当触发检测用户权限时才会调用此方法，例如checkRole,checkPermissionJwtToken */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; log.info(&quot;————权限认证 [ roles、permissions]————&quot;); SimpleAuthorizationInfo simpleAuthorizationInfo = new SimpleAuthorizationInfo(); /* 暂不编写，此处编写后，controller中可以使用@RequiresPermissions来对用户权限进行拦截 */ return simpleAuthorizationInfo; &#125;&#125; 鉴权登录过滤器：继承BasicHttpAuthenticationFilter类,该拦截器需要拦截所有请求除(除登陆、注册等请求)，用于判断请求是否带有token，并获取token的值传递给shiro的登陆认证方法作为参数，用于获取token； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Slf4jpublic class JwtFilter extends BasicHttpAuthenticationFilter &#123; @Override protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) &#123; try &#123; executeLogin(request, response); return true; &#125; catch (Exception e) &#123; unauthorized(response); return false; &#125; &#125; /** * 认证 */ @Override protected boolean executeLogin(ServletRequest request, ServletResponse response) &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; String authorization = httpServletRequest.getHeader(&quot;X-Token&quot;); JwtToken token = new JwtToken(authorization); // 提交给realm进行登入，如果错误他会抛出异常并被捕获 getSubject(request, response).login(token); return true; &#125; /** * 认证失败 跳转到 /unauthorized */ private void unauthorized(ServletResponse resp) &#123; try &#123; HttpServletResponse httpServletResponse = (HttpServletResponse) resp; httpServletResponse.sendRedirect(&quot;/unauthorized&quot;); &#125; catch (IOException e) &#123; log.error(e.getMessage()); &#125; &#125; /** * 对跨域提供支持 */ @Override protected boolean preHandle(ServletRequest request, ServletResponse response) throws Exception &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = (HttpServletResponse) response; httpServletResponse.setHeader(&quot;Access-control-Allow-Origin&quot;, httpServletRequest.getHeader(&quot;Origin&quot;)); httpServletResponse.setHeader(&quot;Access-Control-Allow-Methods&quot;, &quot;GET,POST,OPTIONS,PUT,DELETE&quot;); httpServletResponse.setHeader(&quot;Access-Control-Allow-Headers&quot;, httpServletRequest.getHeader(&quot;Access-Control-Request-Headers&quot;)); // 跨域时会首先发送一个option请求，给option请求直接返回正常状态 if (httpServletRequest.getMethod().equals(RequestMethod.OPTIONS.name())) &#123; httpServletResponse.setStatus(HttpStatus.OK.value()); return false; &#125; return super.preHandle(request, response); &#125;&#125; JwtToken 1234567891011121314public class JwtToken implements AuthenticationToken &#123; private String token; JwtToken(String token) &#123; this.token = token; &#125; @Override public Object getPrincipal() &#123; return token; &#125; @Override public Object getCredentials() &#123; return token; &#125;&#125; JWT工具类：利用登陆信息生成token，根据token获取username，token验证等方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class JwtUtil &#123; /** 设置过期时间: 30分钟 */ private static final long EXPIRE_TIME = 30 * 60 * 1000; /** 服务端的私钥secret,在任何场景都不应该流露出去 */ private static final String TOKEN_SECRET = &quot;zhengchao&quot;; /** * 生成签名，30分钟过期 */ public static String createToken(User user) &#123; try &#123; // 设置过期时间 Date date = new Date(System.currentTimeMillis() + EXPIRE_TIME); // 私钥和加密算法 Algorithm algorithm = Algorithm.HMAC256(TOKEN_SECRET); // 设置头部信息 Map&lt;String, Object&gt; header = new HashMap&lt;&gt;(2); header.put(&quot;typ&quot;, &quot;JWT&quot;); header.put(&quot;alg&quot;, &quot;HS256&quot;); // 返回token字符串 return JWT.create() .withHeader(header) .withClaim(&quot;aud&quot;, user.getAccount()) .withClaim(&quot;uid&quot;, user.getId()) .withExpiresAt(date) .sign(algorithm); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 检验token是否正确 */ public static boolean isVerify(String token)&#123; try &#123; Algorithm algorithm = Algorithm.HMAC256(TOKEN_SECRET); JWTVerifier verifier = JWT.require(algorithm).build(); verifier.verify(token); return true; &#125; catch (Exception e)&#123; return false; &#125; &#125; /** *从token解析出uid信息,用户ID */ public static int parseTokenUid(String token) &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(&quot;uid&quot;).asInt(); &#125; /** *从token解析出aud信息,用户名 */ public static String parseTokenAud(String token) &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(&quot;aud&quot;).asString(); &#125; /** *从token解析出过期时间 */ public static Date paraseExpiresTime(String token)&#123; DecodedJWT jwt = JWT.decode(token); return jwt.getExpiresAt(); &#125;&#125; MD5 加密工具类 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Md5Util &#123; /** * md5加密 * @param s：待加密字符串 * @return 加密后16进制字符串 */ public static String md5(String s) &#123; try &#123; //实例化MessageDigest的MD5算法对象 MessageDigest md = MessageDigest.getInstance(&quot;MD5&quot;); //通过digest方法返回哈希计算后的字节数组 byte[] bytes = md.digest(s.getBytes(&quot;utf-8&quot;)); //将字节数组转换为16进制字符串并返回 return toHex(bytes); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * 获取随即盐 */ public static String salt()&#123; //利用UUID生成随机盐 UUID uuid = UUID.randomUUID(); //返回a2c64597-232f-4782-ab2d-9dfeb9d76932 String[] arr = uuid.toString().split(&quot;-&quot;); return arr[0]; &#125; /** * 字节数组转换为16进制字符串 * @param bytes数组 * @return 16进制字符串 */ private static String toHex(byte[] bytes) &#123; final char[] HEX_DIGITS = &quot;0123456789ABCDEF&quot;.toCharArray(); StringBuilder ret = new StringBuilder(bytes.length * 2); for (int i=0; i&lt;bytes.length; i++) &#123; ret.append(HEX_DIGITS[(bytes[i] &gt;&gt; 4) &amp; 0x0f]); ret.append(HEX_DIGITS[bytes[i] &amp; 0x0f]); &#125; return ret.toString(); &#125;&#125; 3. 注册与登录主要逻辑这里只贴出主要逻辑，DAO和Mapper映射可查看源码，源码请移步文章末尾获取。 登录Controller 1234567891011121314151617181920212223242526272829303132333435363738@RestControllerpublic class SysApi &#123; /** * 注入服务类 */ private SysService sysService; @Autowired public void setSysService(SysService sysService) &#123; this.sysService = sysService; &#125; /** * 注册(用户名，密码) * @param account * @param password * @return */ @PostMapping(&quot;/register&quot;) public ResponseVo&lt;String&gt; register(String account, String password) &#123; return sysService.register(account, password); &#125; /** * 登录(用户名，密码) * @param account * @param password * @return */ @PostMapping(&quot;/login&quot;) public ResponseVo&lt;String&gt; login(String account, String password) &#123; return sysService.login(account, password); &#125; /** * 处理非法请求 */ @GetMapping(&quot;/unauthorized&quot;) public ResponseVo unauthorized(HttpServletRequest request) &#123; return new ResponseVo(-1, &quot;Token失效请重新登录!&quot;); &#125;&#125; Service 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798public interface SysService &#123; /** * 注册(用户名，密码) */ ResponseVo&lt;String&gt; register(String account, String password); /** * 登录(用户名，密码) */ ResponseVo&lt;String&gt; login(String account, String password); /** * 根据account查找用户，自定义Realm中调用 */ User selectByAccount(String account);&#125;/** * 实现类 */@Servicepublic class SysServiceImpl implements SysService &#123; private SysDao sysDao; /** * 注入DAO */ @Autowired public void setSysDao(SysDao sysDao) &#123; this.sysDao = sysDao; &#125; /** * 用户注册(用户名，密码) * * @param account 用户名 * @param password 密码 * @return token */ @Override public ResponseVo&lt;String&gt; register(String account, String password) &#123; //检查用户名是否被占用 User user = sysDao.selectByAccount(account); if(user!=null) &#123; return new ResponseVo&lt;&gt;( -1, &quot;用户名被占用&quot;); &#125; //添加用户信息 user = new User(); //设置用户名 user.setAccount(account); //密码加密后再保存 String salt = Md5Util.salt(); String md5Password = Md5Util.md5(password+salt); user.setPassword(md5Password); user.setSalt(salt); //设置注册时间 user.setCreatetime(new Date()); //添加到数据库 int row = sysDao.insertSelective(user); //返回信息 if(row&gt;0) &#123; //生成token给用户 String token = JwtUtil.createToken(user); return new ResponseVo&lt;&gt;(0,&quot;注册成功&quot;, token); &#125;else &#123; return new ResponseVo&lt;&gt;( -1, &quot;注册失败&quot;); &#125; &#125; /** * 用户登录(用户名，密码) * * @param account 用户名 * @param password 密码 * @return token */ @Override public ResponseVo&lt;String&gt; login(String account, String password) &#123; //处理比对密码 User user = sysDao.selectByAccount(account); if(user!=null) &#123; String salt = user.getSalt(); String md5Password = Md5Util.md5(password+salt); String dbPassword = user.getPassword(); if(md5Password.equals(dbPassword)) &#123; //生成token给用户 String token = JwtUtil.createToken(user); return new ResponseVo&lt;&gt;(0,&quot;登录成功&quot;, token); &#125; &#125; return new ResponseVo&lt;&gt;( -1, &quot;登录失败&quot;); &#125; /** * 根据account查找用户，自定义Realm中调用 * * @param account * @return User */ @Override public User selectByAccount(String account) &#123; return sysDao.selectByAccount(account); &#125;&#125; 统一接口返回格式 123456789101112131415161718192021222324252627282930313233343536public class ResponseVo&lt;T&gt; &#123; /** 状态码 */ private int code; /** 提示信息 */ private String msg; /** 返回的数据 */ private T data; public ResponseVo() &#123;&#125; public ResponseVo(Integer code, String msg) &#123; this.code = code; this.msg = msg; &#125; public ResponseVo(Integer code, String msg, T data) &#123; this.code = code; this.msg = msg; this.data = data; &#125; public int getCode() &#123; return code; &#125; public void setCode(int code) &#123; this.code = code; &#125; public String getMsg() &#123; return msg; &#125; public void setMsg(String msg) &#123; this.msg = msg; &#125; public T getData() &#123; return data; &#125; public void setData(T data) &#123; this.data = data; &#125;&#125; 注：这里的登录认证逻辑在github源码tag的V1.0中，后续版本再加入Token续签和shiro前后端权限管理等。源码地址: https://github.com/chaooo/springboot-vue-shiro.git仅下载认证逻辑源码:git clone --branch V1.0 https://github.com/chaooo/springboot-vue-shiro.git","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"}]},{"title":"「安全认证」Shiro安全框架入门","date":"2019-12-22T12:44:40.000Z","path":"2019/12/22/authenticate-shiro.html","text":"1. 初识 ShiroApache Shiro是一个强大易用的 Java 安全框架，提供了认证、授权、加密、会话管理、与 Web 集成、缓存等。 具体来说，满足对如下元素的支持： 用户，角色，权限(仅仅是操作权限，数据权限必须与业务需求紧密结合)，资源(url)。 用户分配角色，角色定义权限。 访问授权时支持角色或者权限，并且支持多级的权限定义。 Shiro 作为一个完善的权限框架，可以应用在多种需要进行身份认证和访问授权的场景，例如：独立应用、web应用、spring框架中集成等。 2. Shiro 整体架构在 shiro 架构中，有 3 个最主要的组件：Subject，SecurityManager，Realm。 Subject(如图上层部分)：”操作用户(主体)”，本质上就是当前访问用户的抽象描述。 SecurityManager(如图中层部分)：是 Shiro 架构中最核心的组件(控制器)，通过它可以协调其他组件完成用户认证和授权。 Authenticator：认证器，协调一个或者多个 Realm，从 Realm 指定的数据源取得数据之后进行执行具体的认证。 Authorizer：授权器，用户访问控制授权，决定用户是否拥有执行指定操作的权限。 Session Manager：Session 管理器，Shiro 自己实现了一套 Session 管理机制。 Session DAO：实现了 Session 的操作，主要有增删改查。 CacheManager：缓存管理器，缓存角色数据和权限数据等。 Pluggable Realms：数据库与数据源之间的一个桥梁。Shiro 获取认证信息、权限数据、角色数据 通过 Realms 来获取。 Cryptography：是用来做加解密，能非常快捷的实现数据加密。 Realm(如图下层部分)：定义了访问数据的方式，用来连接不同的数据源，如：LDAP，关系数据库，配置文件等等。 3. Shiro 认证与授权3.1 Shiro 认证「创建SecurityManager」&gt;「主体提交请求」&gt;「SecurityManager调用Authenticator去认证」&gt;「Realm验证」 操作用户（主体）提交请求到 Security Manager 调用 Authenticator 去认证,Authenticator 通过 Pluggable Realms 去获取认证信息，Pluggable Realms 是从下面的数据源（数据库）中去获取的认证信息，然后用通过 Pluggable Realms 从数据库中获取的认证信息和主体提交过来的认证数据做比对。 12345678910111213141516171819202122232425262728293031323334353637/** * Shiro认证 测试 */public class AuthenticationTest &#123; // 构建一个简单的数据源 SimpleAccountRealm simpleAccountRealm = new SimpleAccountRealm(); @Before public void addUser()&#123; // 参数分别为：用户名，密码，权限... simpleAccountRealm.addAccount(&quot;chaooo&quot;, &quot;123456&quot;, &quot;admin&quot;,&quot;user&quot;); &#125; /** * 认证测试方法 */ @Test public void testAuthentication()&#123; // 1.构建SecurityManager环境 DefaultSecurityManager defaultSecurityManager = new DefaultSecurityManager(); defaultSecurityManager.setRealm(simpleAccountRealm); // 2. 主体提交认证请求 SecurityUtils.setSecurityManager(defaultSecurityManager); Subject subject = SecurityUtils.getSubject(); // 3. 调用Subject.login(token)方法开始用户认证流程 UsernamePasswordToken token = new UsernamePasswordToken(&quot;chaooo&quot;, &quot;123456&quot;); try &#123; subject.login(token); &#125; catch (UnknownAccountException e) &#123; logger.error(String.format(&quot;用户不存在: %s&quot;, username), e); &#125; catch (IncorrectCredentialsException e) &#123; logger.error(String.format(&quot;密码不正确: %s&quot;, username), e); &#125; catch (ConcurrentAccessException e) &#123; logger.error(String.format(&quot;用户重复登录: %s&quot;, username), e); &#125; catch (AccountException e) &#123; logger.error(String.format(&quot;其他账户异常: %s&quot;, username), e); &#125; &#125;&#125; 3.2 Shiro 授权shiro 访问授权有 3 种实现方式：**api调用，java注解，jsp标签**。 通过 api 调用实现:「创建SecurityManager」&gt;「主体授权」&gt;「SecurityManager调用Authorizer授权」&gt;「Realm获取角色权限数据」 大体上和认证操作一样，也是通过 Pluggable Realms 从下面的数据源（数据库）中去获取权限数据,角色数据。 12345678910111213141516// 在执行访问授权验证之前，必须执行用户认证// 角色验证Subject subject = SecurityUtils.getSubject();if(subject.hasRole(&quot;admin&quot;)) &#123; //用户属于角色admin&#125;else&#123; //用户不属于角色admin&#125;// subject.checkRoles(&quot;admin&quot;,&quot;user&quot;);同时check多个角色// 权限验证String perm = &quot;log:manage:*&quot;;if(subject.isPermitted(perm)) &#123; logger.info(String.format(&quot;用户： %s 拥有权限：%s&quot;, name, perm));&#125;else &#123; logger.error(String.format(&quot;用户：%s 没有权限：%s&quot;, name, perm));&#125; 在 spring 框架中可以通过 java 注解 12345@RequiresPermissions(value=&#123;&quot;log:manage:*&quot;&#125;)public ModelAndView home(HttpServletRequest req) &#123; ModelAndView mv = new ModelAndView(&quot;home&quot;); return mv;&#125; 在 JSP 页面中还可以直接使用 jsp 标签 1234&lt;!-- 使用shiro标签 --&gt;&lt;shiro:hasPermission name=&quot;log:manage:*&quot;&gt; &lt;a href=&quot;&lt;%=request.getContextPath()%&gt;/user/home&quot;&gt;操作日志审计&lt;/a&gt;&lt;br/&gt;&lt;/shiro:hasPermission&gt; 3.3 Quickstart 新建一个Maven项目，pom导入jar包:shiro-all、slf4j-api、slf4j-log4j12、log4j; classpath下新建shiro.ini配置文件: shiro.ini123456789101112131415161718192021222324252627# -----------------------------------------------------------------------------# Users and their assigned roles## Each line conforms to the format defined in the# org.apache.shiro.realm.text.TextConfigurationRealm#setUserDefinitions JavaDoc# -----------------------------------------------------------------------------[users]# user &#x27;root&#x27; with password &#x27;secret&#x27; and the &#x27;admin&#x27; roleroot = secret, admin# user &#x27;guest&#x27; with the password &#x27;guest&#x27; and the &#x27;guest&#x27; roleguest = guest, guest# user &#x27;chaooo&#x27; with password &#x27;123456&#x27; and roles &#x27;user&#x27; and &#x27;guest&#x27;chaooo = 123456, user, guest# -----------------------------------------------------------------------------# Roles with assigned permissions## Each line conforms to the format defined in the# org.apache.shiro.realm.text.TextConfigurationRealm#setRoleDefinitions JavaDoc# -----------------------------------------------------------------------------[roles]# &#x27;admin&#x27; role has all permissions, indicated by the wildcard &#x27;*&#x27;admin = *# The &#x27;schwartz&#x27; role can do anything (*) with any lightsaber:user = user:*# The &#x27;goodguy&#x27; role is allowed to &#x27;query&#x27; (action) the user (type) with license plate &#x27;zhangsan&#x27; (instance specific id)guest = user:query:zhangsan 启动运行 Quickstart Quickstart.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596public class Quickstart &#123; private static final transient Logger log = LoggerFactory.getLogger(Quickstart.class); public static void main(String[] args) &#123; // 构建SecurityManager环境 DefaultSecurityManager securityManager = new DefaultSecurityManager(); IniRealm iniRealm = new IniRealm(&quot;classpath:shiro.ini&quot;); securityManager.setRealm(iniRealm); SecurityUtils.setSecurityManager(securityManager); // get the currently executing user: // 获取当前的 Subject Subject currentUser = SecurityUtils.getSubject(); // Do some stuff with a Session (no need for a web or EJB container!!!) // 测试使用 shiro的Session Session session = currentUser.getSession(); session.setAttribute(&quot;someKey&quot;, &quot;aValue&quot;); String value = (String) session.getAttribute(&quot;someKey&quot;); if (value.equals(&quot;aValue&quot;)) &#123; log.info(&quot;---&gt; Retrieved the correct value! [&quot; + value + &quot;]&quot;); &#125; // let&#x27;s login the current user so we can check against roles and permissions: // 测试当前的用户是否已经被认证. 即是否已经登录. // 调动 Subject 的 isAuthenticated() if (!currentUser.isAuthenticated()) &#123; // 把用户名和密码封装为 UsernamePasswordToken 对象 UsernamePasswordToken token = new UsernamePasswordToken(&quot;chaooo&quot;, &quot;123456&quot;); // rememberme token.setRememberMe(true); try &#123; // 执行登录. currentUser.login(token); &#125; // 若没有指定的账户, 则 shiro 将会抛出 UnknownAccountException 异常. catch (UnknownAccountException uae) &#123; log.info(&quot;----&gt; There is no user with username of &quot; + token.getPrincipal()); return; &#125; // 若账户存在, 但密码不匹配, 则 shiro 会抛出 IncorrectCredentialsException 异常。 catch (IncorrectCredentialsException ice) &#123; log.info(&quot;----&gt; Password for account &quot; + token.getPrincipal() + &quot; was incorrect!&quot;); return; &#125; // 用户被锁定的异常 LockedAccountException catch (LockedAccountException lae) &#123; log.info(&quot;The account for username &quot; + token.getPrincipal() + &quot; is locked. &quot; + &quot;Please contact your administrator to unlock it.&quot;); &#125; // ... catch more exceptions here (maybe custom ones specific to your application? // 所有认证时异常的父类. catch (AuthenticationException ae) &#123; //unexpected condition? error? &#125; &#125; //say who they are: //print their identifying principal (in this case, a username): log.info(&quot;----&gt; User [&quot; + currentUser.getPrincipal() + &quot;] logged in successfully.&quot;); //test a role: // 测试是否有某一个角色. 调用 Subject 的 hasRole 方法. if (currentUser.hasRole(&quot;admin&quot;)) &#123; log.info(&quot;----&gt; May the Admin be with you!&quot;); &#125; else &#123; log.info(&quot;----&gt; Hello, mere mortal.&quot;); return; &#125; //test a typed permission (not instance-level) // 测试用户是否具备某一个行为. 调用 Subject 的 isPermitted() 方法。 if (currentUser.isPermitted(&quot;user:query, edit&quot;)) &#123; log.info(&quot;----&gt; You are permitted to &#x27;query&#x27; and &#x27;edit&#x27; &#x27;user&#x27;&quot;); &#125; else &#123; log.info(&quot;Sorry, you don&#x27;t have permission&quot;); &#125; //a (very powerful) Instance Level permission: // 测试用户是否具备某一个行为. 资源标识符:操作:对象实例ID if (currentUser.isPermitted(&quot;user:query:zhangsan&quot;)) &#123; log.info(&quot;----&gt; You are permitted to &#x27;delete&#x27; &#x27;user&#x27; &#x27;zhangsan&#x27;&quot;); &#125; else &#123; log.info(&quot;Sorry, you don&#x27;t have permission!&quot;); &#125; //all done - log out! // 执行登出. 调用 Subject 的 Logout() 方法. System.out.println(&quot;----&gt;&quot; + currentUser.isAuthenticated()); currentUser.logout(); System.out.println(&quot;----&gt;&quot; + currentUser.isAuthenticated()); System.exit(0); &#125;&#125; 4. 在 SpringMVC 框架中集成 Shiro4.1 配置 Maven 依赖123456789101112131415161718192021222324252627282930&lt;!-- shiro配置 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Enables support for web-based applications. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-web&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Enables AspectJ support for Shiro AOP and Annotations. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-aspectj&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Enables Ehcache-based famework caching. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-ehcache&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Enables Spring Framework integration. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;$&#123;version.shiro&#125;&lt;/version&gt;&lt;/dependency&gt; Shiro使用了日志框架slf4j，因此需要对应配置指定的日志实现组件，如：log4j，logback等。 在此，以使用log4j为日志实现为例： 12345678910111213141516171819&lt;!--shiro使用slf4j作为日志框架，所以必需配置slf4j。同时，使用log4j作为底层的日志实现框架。--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 4.2 集成 Shiro在Spring框架中集成Shiro，本质上是与Spring IoC容器和Spring MVC框架集成。 4.2.1 Shiro与Spring IoC容器集成 Spring IoC容器提供了一个非常重要的功能，就是依赖注入，将Bean的定义以及Bean之间关系的耦合通过容器来处理。 也就是说，在Spring中集成Shiro时，Shiro中的相应Bean的定义以及他们的关系也需要通过Spring IoC容器实现。 Shiro提供了与Web集成的支持，其通过一个ShiroFilter入口来拦截需要安全控制的URL，然后进行相应的控制。 ShiroFilter类是安全控制的入口点，其负责读取配置（如ini配置文件），然后判断URL 是否需要登录&#x2F;权限等工作。 [urls] 部分的配置，其格式是：url = 拦截器[参数], 拦截器[参数] shiro中默认的过滤器： 默认拦截器名 拦截器类与说明（括号里的表示默认值） 身份验证相关 authc org.apache.shiro.web.filter.authc.FormAuthenticationFilter基于表单的拦截器；如”&#x2F;**&#x3D;authc”，如果没有登录会跳到相应的登录页面登录；主要属性：usernameParam：表单提交的用户名参数名（ username）； passwordParam：表单提交的密码参数名（password）； rememberMeParam：表单提交的密码参数名（rememberMe）； loginUrl：登录页面地址（&#x2F;login.jsp）；successUrl：登录成功后的默认重定向地址； failureKeyAttribute：登录失败后错误信息存储 key（shiroLoginFailure）； authcBasic org.apache.shiro.web.filter.authc.BasicHttpAuthenticationFilterBasic HTTP 身份验证拦截器，主要属性：applicationName：弹出登录框显示的信息（application）； logout org.apache.shiro.web.filter.authc.LogoutFilter退出拦截器，主要属性：redirectUrl：退出成功后重定向的地址（&#x2F;）;示例”&#x2F;logout&#x3D;logout” user org.apache.shiro.web.filter.authc.UserFilter用户拦截器，用户已经身份验证&#x2F;记住我登录的都可；示例”&#x2F;**&#x3D;user” anon org.apache.shiro.web.filter.authc.AnonymousFilter匿名拦截器，即不需要登录即可访问；一般用于静态资源过滤；示例”&#x2F;static&#x2F;**&#x3D;anon” 授权相关 roles org.apache.shiro.web.filter.authz.RolesAuthorizationFilter角色授权拦截器，验证用户是否拥有所有角色；主要属性：loginUrl：登录页面地址（&#x2F;login.jsp）；unauthorizedUrl：未授权后重定向的地址；示例”&#x2F;admin&#x2F;**&#x3D;roles[admin]” perms org.apache.shiro.web.filter.authz.PermissionsAuthorizationFilter权限授权拦截器，验证用户是否拥有所有权限；属性和 roles 一样；示例”&#x2F;user&#x2F;**&#x3D;perms[“user:create”]” port org.apache.shiro.web.filter.authz.PortFilter端口拦截器，主要属性：port（80）：可以通过的端口；示例”&#x2F;test&#x3D; port[80]”，如果用户访问该页面是非 80，将自动将请求端口改为 80 并重定向到该 80 端口，其他路径&#x2F;参数等都一样 rest org.apache.shiro.web.filter.authz.HttpMethodPermissionFilterrest 风格拦截器，自动根据请求方法构建权限字符串（GET&#x3D;read, POST&#x3D;create,PUT&#x3D;update,DELETE&#x3D;delete,HEAD&#x3D;read,TRACE&#x3D;read,OPTIONS&#x3D;read, MKCOL&#x3D;create）构建权限字符串；示例”&#x2F;users&#x3D;rest[user]”，会自动拼出”user:read,user:create,user:update,user:delete”权限字符串进行权限匹配（所有都得匹配，isPermittedAll）； ssl org.apache.shiro.web.filter.authz.SslFilterSSL 拦截器，只有请求协议是 https 才能通过；否则自动跳转会 https 端口（443）；其他和 port 拦截器一样； 其他 noSessionCreation org.apache.shiro.web.filter.session.NoSessionCreationFilter不创建会话拦截器，调用 subject.getSession(false)不会有什么问题，但是如果 subject.getSession(true)将抛出 DisabledSessionException 异常； URL匹配模式：url 模式使用 Ant 风格模式 Ant 路径通配符支持?、*、**，注意通配符匹配不包括目录分隔符“&#x2F;”： ?：匹配一个字符，如&#x2F;admin? 将匹配&#x2F;admin1，但不匹配&#x2F;admin 或&#x2F;admin&#x2F;； *：匹配零个或多个字符串，如&#x2F;admin 将匹配&#x2F;admin、&#x2F;admin123，但不匹配&#x2F;admin&#x2F;1； **：匹配路径中的零个或多个路径，如&#x2F;admin&#x2F;** 将匹配&#x2F;admin&#x2F;a 或&#x2F;admin&#x2F;a&#x2F;b URL匹配顺序：URL 权限采取第一次匹配优先的方式，即从头开始使用第一个匹配的 url 模式对应的拦截器链。如： &#x2F;bb&#x2F;**&#x3D;filter1 &#x2F;bb&#x2F;aa&#x3D;filter2 &#x2F;**&#x3D;filter3 如果请求的 url 是“&#x2F;bb&#x2F;aa”，因为按照声明顺序进行匹配，那么将使用 filter1 进行拦截，所以通配符一般写在靠后。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt; &lt;property name=&quot;loginUrl&quot; value=&quot;/index&quot;/&gt; &lt;property name=&quot;successUrl&quot; value=&quot;/home&quot;/&gt; &lt;property name=&quot;unauthorizedUrl&quot; value=&quot;/unauthorized.jsp&quot;/&gt; &lt;!-- The &#x27;filters&#x27; property is not necessary since any declared javax.servlet.Filter bean --&gt; &lt;!-- defined will be automatically acquired and available via its beanName in chain --&gt; &lt;!-- definitions, but you can perform instance overrides or name aliases here if you like: --&gt; &lt;!-- &lt;property name=&quot;filters&quot;&gt; &lt;util:map&gt; &lt;entry key=&quot;logout&quot; value-ref=&quot;logoutFilter&quot; /&gt; &lt;/util:map&gt; &lt;/property&gt; --&gt; &lt;property name=&quot;filterChainDefinitions&quot;&gt; &lt;value&gt; # some example chain definitions: # /admin/** = authc, roles[admin] # /docs/** = authc, perms[document:read] /login = anon /logout = anon /error = anon /** = user # more URL-to-FilterChain definitions here &lt;/value&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt; &lt;!-- Single realm app. If you have multiple realms, use the &#x27;realms&#x27; property instead. --&gt; &lt;property name=&quot;realm&quot; ref=&quot;myRealm&quot; /&gt; &lt;!-- By default the servlet container sessions will be used. Uncomment this line to use shiro&#x27;s native sessions (see the JavaDoc for more): --&gt; &lt;!-- &lt;property name=&quot;sessionMode&quot; value=&quot;native&quot;/&gt; --&gt;&lt;/bean&gt;&lt;bean id=&quot;lifecycleBeanPostProcessor&quot; class=&quot;org.apache.shiro.spring.LifecycleBeanPostProcessor&quot;/&gt;&lt;!-- Define the Shiro Realm implementation you want to use to connect to your back-end --&gt;&lt;!-- security datasource: --&gt;&lt;bean id=&quot;myRealm&quot; class=&quot;org.apache.shiro.realm.jdbc.JdbcRealm&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;property name=&quot;permissionsLookupEnabled&quot; value=&quot;true&quot;/&gt;&lt;/bean&gt;&lt;!-- Enable Shiro Annotations for Spring-configured beans. Only run after --&gt;&lt;!-- the lifecycleBeanProcessor has run: --&gt;&lt;bean class=&quot;org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator&quot; depends-on=&quot;lifecycleBeanPostProcessor&quot;/&gt;&lt;bean class=&quot;org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt; &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt;&lt;/bean&gt; 4.2.2 与Spring MVC集成 跟在普通Java Web应用中使用Shiro一样，集成Shiro到Spring MVC时，实际上就是通过在web.xml中添加指定Filter实现。配置如下： 1234567891011121314151617&lt;!-- The filter-name matches name of a &#x27;shiroFilter&#x27; bean inside applicationContext.xml --&gt;&lt;!-- DelegatingFilterProxy作用是自动到Spring 容器查找名字为shiroFilter（filter-name）的bean并把所有Filter 的操作委托给它。 --&gt;&lt;filter&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;targetFilterLifecycle&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;!-- Make sure any request you want accessible to Shiro is filtered. /* catches all --&gt;&lt;!-- requests. Usually this filter mapping is defined first (before all others) to --&gt;&lt;!-- ensure that Shiro works in subsequent filters in the filter chain: --&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; Spring中集成Shiro的原理就是：通过在web.xml中配置的Shiro Filter与Spring IoC中定义的相应的Shiro Bean定义建立关系，从而实现在Spring框架集成Shiro。 4.3 数据源配置在Shiro中，Realm定义了访问数据的方式，用来连接不同的数据源，如：LDAP，关系数据库，配置文件等。 以org.apache.shiro.realm.jdbc.JdbcRealm为例，将用户信息存放在关系型数据库中。 在使用JdbcRealm时，必须要在关系型数据库中存在 3 张表，分别是 users表，存放认证用户基本信息，在该表中必须存在 2 个字段：username，password。 roles_permissions表，存放角色和权限定义，在该表中必须存在 2 个字段：role_name，permission。 user_roles表，存放用户角色对应关系，在该表中必须存在 2 个字段：username，role_name。 实际上，在更加复杂的应用场景下，通常需要扩展JdbcRealm。 4.4 认证在Shiro中，认证即执行用户登录，读取指定Realm连接的数据源，以验证用户身份的有效性与合法性。 在 shiro 中，用户需要提供 principals （身份）和 credentials（证明）给 shiro，从而应用能验证用户身份： principals：身份，即主体的标识属性，可以是任何属性，如用户名、邮箱等，唯一即可。一个主体可以有多个 principals，但只有一个 Primary principals，一般是用户名&#x2F;邮箱&#x2F;手机号。 credentials：证明&#x2F;凭证，即只有主体知道的安全值，如密码&#x2F;数字证书等。 最常见的 principals 和 credentials 组合就是用户名&#x2F;密码了 身份认证流程： 首先调用 Subject.login(token) 进行登录，其会自动委托给 SecurityManager SecurityManager 负责真正的身份验证逻辑；它会委托给 Authenticator 进行身份验证； Authenticator 才是真正的身份验证者，ShiroAPI 中核心的身份认证入口点，此处可以自定义插入自己的实现； Authenticator 可能会委托给相应的 AuthenticationStrategy 进行多 Realm 身份验证，默认 ModularRealmAuthenticator 会调用 AuthenticationStrategy 进行多 Realm 身份验证； Authenticator 会把相应的 token 传入 Realm，从 Realm 获取身份验证信息，如果没有返回&#x2F;抛出异常表示身份验证失败了。此处可以配置多个 Realm，将按照相应的顺序及策略进行访问。 Realm：一般继承 AuthorizingRealm（授权）即可；其继承了 AuthenticatingRealm（即身份验证），而且也间接继承了 CachingRealm（带有缓存实现） 123456789101112131415Subject subject = SecurityUtils.getSubject();if(!subject.isAuthenticated()) &#123; UsernamePasswordToken token = new UsernamePasswordToken(name, password); try &#123; subject.login(token); &#125; catch (UnknownAccountException e) &#123; logger.error(String.format(&quot;用户不存在: %s&quot;, token.getPrincipal()), e); &#125; catch (IncorrectCredentialsException e) &#123; logger.error(String.format(&quot;密码不正确: %s&quot;, token.getPrincipal()), e); &#125; catch (ConcurrentAccessException e) &#123; logger.error(String.format(&quot;用户重复登录: %s&quot;, token.getPrincipal()), e); &#125; catch (AccountException e) &#123; logger.error(String.format(&quot;其他账户异常: %s&quot;, token.getPrincipal()), e); &#125;&#125; 4.5 授权Shiro 作为权限框架，仅仅只能控制对资源的操作权限，并不能完成对数据权限的业务需求。 而对于 Java Web 环境下 Shiro 授权，包含两个方面的含义。 其一，对于前端来说，用户只能看到他对应访问权限的元素。 其二，当用户执行指定操作（即：访问某个 uri 资源）时，需要验证用户是否具备对应权限。 对于第一点，在 Java Web 环境下，通过 Shiro 提供的 JSP 标签实现。 对于第二点，与在非 Java Web 环境下一样，需要在后端调用 API 进行权限（或者角色）检验。 在 Spring 框架中集成 Shiro，还可以直接通过 Java 注解方式实现 Permissions： 规则：资源标识符：操作：对象实例ID,即对哪个资源的哪个实例可以进行什么操作.其默认支持通配符权限字符串，: 表示资源&#x2F;操作&#x2F;实例的分割；, 表示操作的分割，* 表示任意资源&#x2F;操作&#x2F;实例。如：user:edit:manager 也可以使用通配符来定义，如：user:edit:*、user:*:*、user:*:manager 部分省略通配符：缺少的部件意味着用户可以访问所有与之匹配的值，比如：user:edit等价于user:edit:*、user等价于user:*:* 注意：通配符只能从字符串的结尾处省略部件，也就是说user:edit并不等价于user:*:edit 授权流程: 首先调用 Subject.isPermitted*&#x2F;hasRole* 接口，其会委托给 SecurityManager，而 SecurityManager 接着会委托给 Authorizer； Authorizer 是真正的授权者，如果调用如 isPermitted(“user:view”)，其首先会通过 PermissionResolver 把字符串转换成相应的 Permission 实例； 在进行授权之前，其会调用相应的 Realm 获取 Subject 相应的角色&#x2F;权限用于匹配传入的角色&#x2F;权限； Authorizer 会判断 Realm 的角色&#x2F;权限是否和传入的匹配，如果有多个 Realm，会委托给 ModularRealmAuthorizer 进行循环判断，如果匹配如 isPermitted*&#x2F;hasRole* 会返回 true，否则返回 false 表示授权失败。 ModularRealmAuthorizer进行多 Realm 匹配流程： 首先检查相应的 Realm 是否实现了实现了 Authorizer； 如果实现了 Authorizer，那么接着调用其相应的isPermitted*/hasRole*接口进行匹配； 如果有一个 Realm 匹配那么将返回 true，否则返回 false。 4.5.1 Shiro 标签 &lt;shiro:guest&gt;&lt;/shiro:guest&gt;:用户没有身份验证时显示相应信息，即游客访问信息 &lt;shiro:user&gt;&lt;/shiro:user&gt;:用户已经经过认证&#x2F;记住我登录后显示相应的信息。 &lt;shiro:authenticated&gt;&lt;/shiro:authenticated&gt;:用户已经身份验证通过，即 Subject.login 登录成功，不是记住我登录的 &lt;shiro:notAuthenticated&gt;&lt;/shiro:notAuthenticated&gt;标签：用户未进行身份验证，即没有调用 Subject.login 进行登录，包括记住我自动登录的也属于未进行身份验证。 &lt;shiro:pincipal&gt;&lt;/shiro:pincipal&gt;：显示用户身份信息，默认调用Subject.getPrincipal()获取，即 Primary Principal。 **&lt;shiro:hasRole&gt;&lt;/shiro:hasRole&gt;**标签：如果当前 Subject 有角色将显示 body 体内容 &lt;shiro:hasAnyRoles&gt;&lt;/shiro:hasAnyRoles&gt;标签：如果当前 Subject 有任意一个角色（或的关系）将显示 body 体内容 &lt;shiro:lacksRole&gt;&lt;/shiro:lacksRole&gt;：如果当前 Subject 没有角色将显示 body 体内容 **&lt;shiro:hasPermission&gt;&lt;/shiro:hasPermission&gt;**：如果当前 Subject 有权限将显示 body 体内容 &lt;shiro:lacksPermission&gt;&lt;/shiro:lacksPermission&gt;：如果当前 Subject 没有权限将显示 body 体内容 123456789&lt;!-- 在jsp页面中引入shiro标签库 --&gt;&lt;%@ taglib prefix=&quot;shiro&quot; uri=&quot;http://shiro.apache.org/tags&quot; %&gt;&lt;!-- 权限控制 --&gt;&lt;shiro:hasRole name=&quot;admin&quot;&gt; &lt;a&gt;用户管理&lt;/a&gt;&lt;/shiro:hasRole&gt;&lt;shiro:hasPermission name=&quot;winnebago:drive:eagle5&quot;&gt; &lt;a&gt;操作审计&lt;/a&gt;&lt;/shiro:hasPermission&gt; 4.5.2 调用 API 进行权限（或者角色）检验12345String roleAdmin = &quot;admin&quot;;Subject currentUser = SecurityUtils.getSubject();if(!currentUser.hasRole(roleAdmin)) &#123; //todo something&#125; 4.5.3 Shiro 权限注解 @RequiresAuthentication：表示当前 Subject 已经通过 login 进行了身份验证；即 Subject. isAuthenticated() 返回 true @RequiresUser：表示当前 Subject 已经身份验证或者通过记住我登录的。 @RequiresGuest：表示当前 Subject 没有身份验证或通过记住我登录过，即是游客身份。 @RequiresRoles(value=&#123;“admin”, “user”&#125;, logical= Logical.AND)：表示当前 Subject 需要角色 admin 和 user @RequiresPermissions(value=&#123;“user:a”, “user:b”&#125;, logical= Logical.OR)：表示当前 Subject 需要权限 user:a 或 user:b。 通过自定义拦截器可以扩展功能，例如：动态 url-角色&#x2F;权限访问控制的实现、根据 Subject 身份信息获取用户信息绑定到 Request（即设置通用数据）、验证码验证、在线用户信息的保存等 123456789@Controllerpublic class HomeController &#123; @RequestMapping(&quot;/home&quot;) @RequiresPermissions(value=&#123;&quot;log:manage:*&quot;&#125;) public ModelAndView home(HttpServletRequest req) &#123; ModelAndView mv = new ModelAndView(&quot;home&quot;); return mv; &#125;&#125; 4.6 Spring 集成 Shiro 注意事项 Spring 4.2.0 RELEASE+ 与 Spring 4.1.9 RELEASE**-**版本，配置方式有所不同。 虽然shiro的注解定义是在Class级别的，但是实际验证只能支持方法级别：@RequiresAuthentication、@RequiresPermissions、@RequiresRoles。 5. Shiro 会话管理Shiro 提供了完整的企业级会话管理功能，不依赖于底层容器（如 web 容器 tomcat），不管 JavaSE 还是 JavaEE 环境都可以使用，提供了会话管理、会话事件监听、会话存储&#x2F;持久化、容器无关的集群、失效&#x2F;过期支持、对 Web 的透明支持、SSO 单点登录的支持等特性。 5.1 会话相关的 API Subject.getSession()：即可获取会话；其等价于 Subject.getSession(true)，即如果当前没有创建 Session 对象会创建一个；Subject.getSession(false)，如果当前没有创建 Session 则返回 null session.getId()：获取当前会话的唯一标识 session.getHost()：获取当前 Subject 的主机地址 session.getTimeout() &amp; session.setTimeout(毫秒)：获取&#x2F;设置当前 Session 的过期时间 session.getStartTimestamp() &amp; session.getLastAccessTime()：获取会话的启动时间及最后访问时间；如果是 JavaSE 应用需要自己定期调用 session.touch() 去更新最后访问时间；如果是 Web 应用，每次进入 ShiroFilter 都会自动调用 session.touch() 来更新最后访问时间。 session.touch() &amp; session.stop()：更新会话最后访问时间及销毁会话；当 Subject.logout()时会自动调用 stop 方法来销毁会话。如果在 web 中，调用 HttpSession. invalidate()也会自动调用 Shiro Session.stop 方法进行销毁 Shiro 的会话 session.setAttribute(key, val) &amp; session.getAttribute(key) &amp; session.removeAttribute(key)：设置&#x2F;获取&#x2F;删除会话属性；在整个会话范围内都可以对这些属性进行操作 5.2 会话监听器会话监听器(SessionListiner):会话监听器用于监听会话创建、过期及停止事件 5.3 SessionDao AbstractSessionDAO 提供了 SessionDAO 的基础实现，如生成会话 ID 等 CachingSessionDAO 提供了对开发者透明的会话缓存的功能，需要设置相应的 CacheManager MemorySessionDAO 直接在内存中进行会话维护 EnterpriseCacheSessionDAO 提供了缓存功能的会话维护，默认情况下使用 MapCache 实现，内部使用 ConcurrentHashMap 保存缓存的会话。 5.4 数据表12345create table sessions ( id varchar(200), session varchar(2000), constraint pk_sessions primary key(id)) charset=utf8 ENGINE=InnoDB; 5.5 会话验证 Shiro 提供了会话验证调度器，用于定期的验证会话是否已过期，如果过期将停止会话 出于性能考虑，一般情况下都是获取会话时来验证会话是否过期并停止会话的；但是如在 web 环境中，如果用户不主动退出是不知道会话是否过期的，因此需要定期的检测会话是否过期，Shiro 提供了会话验证调度器 SessionValidationScheduler Shiro 也提供了使用 Quartz 会话验证调度器：QuartzSessionValidationScheduler 6. Shiro 缓存 CacheManagerAware 接口 Shiro 内部相应的组件（DefaultSecurityManager）会自动检测相应的对象（如 Realm）是否实现了 CacheManagerAware 并自动注入相应的 CacheManager。 Realm 缓存 + Shiro 提供了 CachingRealm，其实现了 CacheManagerAware 接口，提供了缓存的一些基础实现； + AuthenticatingRealm 及 AuthorizingRealm 也分别提供了对 AuthenticationInfo 和 AuthorizationInfo 信息的缓存。 Session 缓存 如 SecurityManager 实现了 SessionSecurityManager，其会判断 SessionManager 是否实现了 acheManagerAware 接口，如果实现了会把 CacheManager 设置给它。 SessionManager 也会判断相应的 SessionDAO（如继承自 CachingSessionDAO）是否实现了 CacheManagerAware，如果实现了会把 CacheManager 设置给它 设置了缓存的 SessionManager，查询时会先查缓存，如果找不到才查数据库。 RememberMe Shiro 提供了记住我（RememberMe）的功能，比如访问如淘宝等一些网站时，关闭了浏览器，下次再打开时还是能记住你是谁，下次访问时无需再登录即可访问，基本流程如下： 首先在登录页面选中 RememberMe 然后登录成功；如果是浏览器登录，一般会把 RememberMe 的 Cookie 写到客户端并保存下来； 关闭浏览器再重新打开；会发现浏览器还是记住你的； 访问一般的网页服务器端还是知道你是谁，且能正常访问； 但是比如我们访问淘宝时，如果要查看我的订单或进行支付时，此时还是需要再进行身份认证的，以确保当前用户还是你。 认证和记住我 subject.isAuthenticated() 表示用户进行了身份验证登录的，即使有 Subject.login 进行了登录； subject.isRemembered()：表示用户是通过记住我登录的，此时可能并不是真正的你（如你的朋友使用你的电脑，或者你的 cookie 被窃取）在访问的 两者二选一，即 subject.isAuthenticated()&#x3D;&#x3D;true，则 subject.isRemembered()&#x3D;&#x3D;false；反之一样。 建议 访问一般网页：如个人在主页之类的，我们使用 user 拦截器即可，user 拦截器只要用户登录(isRemembered() || isAuthenticated())过即可访问成功； 访问特殊网页：如我的订单，提交订单页面，我们使用 authc 拦截器即可，authc 拦截器会判断用户是否是通过 Subject.login（isAuthenticated()&#x3D;&#x3D;true）登录的，如果是才放行，否则会跳转到登录页面叫你重新登录。 实现 如果要自己做 RememeberMe，需要在登录之前这样创建 Token：UsernamePasswordToken(用户名，密码，是否记住我)，且调用 UsernamePasswordToken 的：token.setRememberMe(true); 方法 参考文章： 细说 shiro 之一：shiro 简介 细说 shiro 之五：在 spring 框架中集成 shiro","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"}]},{"title":"「安全认证」JSON Web Token 入门","date":"2019-11-18T07:50:53.000Z","path":"2019/11/18/authenticate-jwt.html","text":"JSON Web TokenJSON Web Token（缩写 JWT）基于JSON格式信息一种Token令牌，是目前最流行的跨域认证解决方案。 JWT 的原理是，服务器认证以后，生成一个 JSON 对象，发回给用户。 此后，用户与服务端通信的时候，都要发回这个 JSON 对象。服务器完全只靠这个对象认定用户身份。为了防止用户篡改数据，服务器在生成这个对象的时候，会加上签名。 服务器就不保存任何 session 数据了，也就是说，服务器变成无状态了，从而比较容易实现扩展。 1. JWT数据结构它是一个很长的字符串，中间用点（.）分隔成三个部分。 例如：eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJjaGFvIiwidWlkIjoyOSwiZXhwIjoxNTY3OTM2NzgwfQ.6zvimBNs_MCiov4MOkkUodgKmRFBS2dVhmhIb1MV6m4。 JWT 的三个部分(Header.Payload.Signature)依次如下: Header（头部） Payload（负载） Signature（签名） 1.1 Header（头部）Header 部分是一个 JSON 对象，描述 JWT 的元数据。 1234&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125; alg：签名的算法（algorithm），默认是 HMAC SHA256（写成HS256） typ：表示这个令牌（token）的类型（type），JWT令牌统一写为JWT。 最后，将上面的 JSON 对象使用 Base64URL算法转成字符串。 1.2 Payload（负载）Payload 部分也是一个 JSON 对象，用来存放实际需要传递的数据。JWT 规定了7个官方字段(Reserved claims)，供选用。标准中建议使用这些字段，但不强制。 iss (issuer)：签发人 exp (expiration time)：过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号，JWT唯一标识，能用于防止JWT重复使用 除了官方字段，还有公共声明的字段（见：http://www.iana.org/assignments/jwt/jwt.xhtml）也可以定义私有字段，如： 12345&#123; &quot;sub&quot;: &quot;1234567890&quot;, &quot;name&quot;: &quot;John Doe&quot;, &quot;admin&quot;: true&#125; 注意，JWT 默认是不加密的，任何人都可以读到，所以不要把秘密信息放在这个部分。 这个 JSON 对象也要使用 Base64URL算法转成字符串。 1.3 Signature（签名）Signature 部分是对前两部分的签名，防止数据篡改。该签名信息是通过header和payload，加上secret，通过算法加密生成。 首先，需要指定一个密钥（secret）。这个密钥只有服务器才知道，不能泄露给用户。然后，使用 Header 里面指定的签名算法（默认是 HMAC SHA256），按照下面的公式产生签名。 HMACSHA256(base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), secret) 算出签名以后，把 Header、Payload、Signature 三个部分拼成一个字符串，每个部分之间用”点”（.）分隔，就可以返回给用户。 2. Base64URL算法前面提到，Header 和 Payload 串型化的算法是 Base64URL。这个算法跟 Base64 算法基本类似，但有一些小的不同。 JWT 作为一个令牌（token），有些场合可能会放到 URL（比如 api.example.com&#x2F;?token&#x3D;xxx）。Base64 有三个字符+、/和=，在 URL 里面有特殊含义，所以要被替换掉：=被省略、+替换成-，/替换成_ 。这就是 Base64URL 算法。 3. JWT 的使用方式及特点 认证原理： 客户端向服务器申请授权，服务器认证以后，生成一个token字符串并返回给客户端，此后客户端在请求受保护的资源时携带这个token，服务端进行验证再从这个token中解析出用户的身份信息。 JWT的使用方式： 客户端收到服务器返回的JWT，存储在浏览器（Cookie或localStorage） 此后，客户端每次与服务器通信，都要带上这个JWT。 一种做法是放在HTTP请求的头信息Authorization字段里面，格式如下： Authorization: &lt;token&gt; 需要将服务器设置为接受来自所有域的请求，用Access-Control-Allow-Origin: * 另一种做法是，跨域的时候，JWT就放在POST请求的数据体里面。 对JWT实现token续签的做法： 额外生成一个refreshToken用于获取新token，refreshToken需存储于服务端，其过期时间比JWT的过期时间要稍长。 用户携带refreshToken参数请求token刷新接口，服务端在判断refreshToken未过期后，取出关联的用户信息和当前token。 使用当前用户信息重新生成token，并将旧的token置于黑名单中，返回新的token。 JWT 的几个特点 JWT 默认是不加密，但也是可以加密的。生成原始 Token 以后，可以用密钥再加密一次。 JWT 不加密的情况下，不能将秘密数据写入JWT。 JWT 不仅可以用于认证，也可以用于交换信息。有效使用 JWT，可以降低服务器查询数据库的次数。 JWT 的最大缺点是，由于服务器不保存 session 状态，因此无法在使用过程中废止某个 token，或者更改 token 的权限。也就是说，一旦 JWT 签发了，在到期之前就会始终有效，除非服务器部署额外的逻辑。 JWT 本身包含了认证信息，一旦泄露，任何人都可以获得该令牌的所有权限。为了减少盗用，JWT 的有效期应该设置得比较短。对于一些比较重要的权限，使用时应该再次对用户进行认证。 为了减少盗用，JWT 不应该使用 HTTP 协议明码传输，要使用 HTTPS 协议传输。 4. Java中JWT的使用java-jwt工具包提供了JWT算法的封装 导入java-jwt，选择一种算法（HMAC256为例） Algorithm algorithm = Algorithm.HMAC256(&quot;secret&quot;); 算法定义了一个令牌是如何被签名和验证的。 创建一个签名的JWT token（通过调用jwt.create()创建一个JWTCreator实例） String token = JWT.create().withIssuer(&quot;auth0&quot;).sign(algorithm); 如果Claim不能转换为JSON，或者在签名过程中使用的密钥无效，那么将会抛出JWTCreationException异常 验证令牌（调用jwt.require()和传递算法实例来创建一个JWTVerifier实例。方法build()返回的实例是可重用的，因此可以定义一次，并使用它来验证不同的标记。最后调用verifier.verify()来验证token） JWTVerifier verifier = JWT.require(algorithm).withIssuer(&quot;auth0&quot;).build(); verifier.verify(token); 如果令牌有一个无效的签名，或者没有满足Claim要求，那么将会抛出JWTVerificationException异常 jwt时间的验证（当验证一个令牌时，时间验证会自动发生；JWT令牌可能包括可用于验证的DateNumber字段） &quot;iat&quot; &lt; TODAY：这个令牌发布了一个过期的时间 &quot;exp&quot; &gt; TODAY：这个令牌还没过期 &quot;nbf&quot; &gt; TODAY：这个令牌已经被使用了 解码一个jwt令牌 DecodedJWT jwt = JWT.decode(token); jwt.getAlgorithm();:返回jwt的算法值,如果没有定义则返回null jwt.getType();:返回jwt的类型值，如果没有定义则返回null（多数情况类型值为jwt） 如果令牌有无效的语法，或者消息头或有效负载不是JSONs，那么将会抛出JWTDecodeException异常 5. Java中JWT的使用实例封装一个JWT工具类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import java.util.Date;import java.util.HashMap;import java.util.Map;import com.auth0.jwt.JWT; //导入java-jwtimport com.auth0.jwt.algorithms.Algorithm;import com.auth0.jwt.interfaces.DecodedJWT;import com.auth0.jwt.interfaces.JWTVerifier;import com.entity.User; //引入User实体类public class JwtUtil &#123; //设置过期时间，这里设置15分钟 private static final long EXPIRE_TIME = 15 * 60 * 1000; //服务端的私钥secret,在任何场景都不应该流露出去 private static final String TOKEN_SECRET = &quot;zhengchao&quot;; /** * 生成签名 * @param **User** * @param **password** * @return */ public static String createToken(User user) &#123; try &#123; // 设置过期时间 Date date = new Date(System.currentTimeMillis() + EXPIRE_TIME); // 私钥和加密算法 Algorithm algorithm = Algorithm.HMAC256(TOKEN_SECRET); // 设置头部信息 Map&lt;String, Object&gt; header = new HashMap&lt;&gt;(2); header.put(&quot;typ&quot;, &quot;JWT&quot;); header.put(&quot;alg&quot;, &quot;HS256&quot;); // 返回token字符串 return JWT.create() .withHeader(header) .withClaim(&quot;aud&quot;, user.getName()) .withClaim(&quot;uid&quot;, user.getId()) .withExpiresAt(date) .sign(algorithm); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 检验token是否正确 * @param **token** * @return */ public static boolean isVerify(String token)&#123; try &#123; Algorithm algorithm = Algorithm.HMAC256(TOKEN_SECRET); JWTVerifier verifier = JWT.require(algorithm).build(); verifier.verify(token); return true; &#125; catch (Exception e)&#123; return false; &#125; &#125; /** *从token解析出uid信息,用户ID * @param token * @param key * @return */ public static int parseTokenUid(String token) &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(&quot;uid&quot;).asInt(); &#125; /** *从token解析出aud信息,用户名 * @param token * @param key * @return */ public static String parseTokenAud(String token) &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim(&quot;aud&quot;).asString(); &#125;&#125; 登录成功后，生成token给浏览器，存储在浏览器（Cookie或localStorage） 1String token = JwtUtil.createToken(user); 此后，客户端每次与服务器通信（需权限的资源），都要带上这个JWT。 一种做法是放在HTTP请求的头信息Authorization字段里面，格式如下： Authorization: &lt;token&gt; 需要将服务器设置为接受来自所有域的请求，用Access-Control-Allow-Origin: * 另一种做法是，跨域的时候，JWT就放在POST请求的数据体里面。 jwt 适合做简单的 restful api 认证，颁发一个固定有效期的 jwt，降低 jwt 暴露的风险，尽量不要对 jwt 做服务端的状态管理，这样才能体现出 jwt 无状态的优势。 附：java-jwt已经实现的算法 JWS 算法 介绍 HS256 HMAC256 HMAC with SHA-256 HS384 HMAC384 HMAC with SHA-384 HS512 HMAC512 HMAC with SHA-512 RS256 RSA256 RSASSA-PKCS1-v1_5 with SHA-256 RS384 RSA384 RSASSA-PKCS1-v1_5 with SHA-384 RS512 RSA512 RSASSA-PKCS1-v1_5 with SHA-512 ES256 ECDSA256 ECDSA with curve P-256 and SHA-256 ES384 ECDSA384 ECDSA with curve P-384 and SHA-384 ES512 ECDSA512 ECDSA with curve P-521 and SHA-512","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"}]},{"title":"「安全认证」MD5算法加盐实现用户密码加密","date":"2019-11-15T13:49:28.000Z","path":"2019/11/15/authenticate-md5.html","text":"1. MD5加密算法介绍MD5的全称是Message-Digest Algorithm 5（信息-摘要算法 第五版），经MD2、MD3和MD4发展而来的一种加密算法，是典型的消息摘要算法，属Hash算法一类。作用是让大容量信息在用数字签名软件签署私人密匙前被”压缩”成一种保密的格式（就是把一个任意长度的字节串变换成一定长的大整数）。通过MD5算法进行加密获得一个随机长度的信息并产生一个128位的信息摘要。如果将这个128位的二进制摘要信息换算成十六进制，可以得到一个32位的字符串，因此我们加密完成后的16进制的字符串长度为32位。 2. MD5加密算法特点： 压缩性：任意长度的数据，算出的MD5值长度都是固定的。 容易计算：从原数据计算出MD5值很容易。 抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。 强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。 3. 盐（Salt）在密码学中，是指通过在密码任意固定位置插入特定的字符串，让散列后的结果和使用原始密码的散列结果不相符，这种过程称之为“加盐”。 4. java.security.MessageDigest类JDK中的java.security.MessageDigest用于为应用程序提供信息摘要算法的功能，如 MD5 或 SHA 算法。 MessageDigest 通过其getInstance系列静态函数来进行实例化和初始化。 MessageDigest 对象通过使用 update 方法处理数据。任何时候都可以调用 reset 方法重置摘要。一旦所有需要更新的数据都已经被更新了，应该调用 digest 方法之一完成哈希计算并返回结果。 对于给定数量的更新数据，digest 方法只能被调用一次。digest 方法被调用后，MessageDigest 对象被重新设置成其初始状态。 5. 封装一个MD5加密工具类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.security.MessageDigest;import java.util.UUID;public class MD5Util &#123; /** * md5加密 * @param s：待加密字符串 * @return 加密后16进制字符串 */ public static String md5(String s) &#123; try &#123; //实例化MessageDigest的MD5算法对象 MessageDigest md = MessageDigest.getInstance(&quot;MD5&quot;); //通过digest方法返回哈希计算后的字节数组 byte[] bytes = md.digest(s.getBytes(&quot;utf-8&quot;)); //将字节数组转换为16进制字符串并返回 return toHex(bytes); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; /** * 获取随即盐 * @return */ public static String salt()&#123; //利用UUID生成随机盐 UUID uuid = UUID.randomUUID(); //返回a2c64597-232f-4782-ab2d-9dfeb9d76932 String[] arr = uuid.toString().split(&quot;-&quot;); return arr[0]; &#125; /** * 字节数组转换为16进制字符串 * @param bytes数组 * @return 16进制字符串 */ private static String toHex(byte[] bytes) &#123; final char[] HEX_DIGITS = &quot;0123456789ABCDEF&quot;.toCharArray(); StringBuilder ret = new StringBuilder(bytes.length * 2); for (int i=0; i&lt;bytes.length; i++) &#123; ret.append(HEX_DIGITS[(bytes[i] &gt;&gt; 4) &amp; 0x0f]); ret.append(HEX_DIGITS[bytes[i] &amp; 0x0f]); &#125; return ret.toString(); &#125;&#125; 6. 使用封装的MD5工具类完成用户注册(主要代码)12345678910111213141516public Object register(String name, String password) &#123; //添加用户信息 user = new User(); //设置用户名 user.setName(name); //密码加密后再保存 String salt = MD5Util.salt(); String md5Password = MD5Util.md5(password+salt); //存入MD5加密后的密码 user.setPassword(md5Password); //随机盐存入数据库，用于登录校验 user.setSalt(salt); //最后将用户数据数据存入数据库 int row = userDao.insert(user); return ...&#125; 7. 使用封装的MD5工具类完成用户登录(主要代码)123456789101112public Object login(String name, String password) &#123; //根据用户名在数据库查找用户 User user = userDao.selectByName(name); //取出用户信息比对 String dbPassword = user.getPassword(); String salt = user.getSalt(); //通过密码+盐 重新生成 MD5密码 String md5Password = MD5Util.md5(password+salt); if(md5Password.equals(dbPassword)) &#123; //登录成功 &#125;&#125; 8. 扩展：MessageDigest类常用方法8.1 构造方法摘要MessageDigest(String algorithm) –创建具有指定算法名称的MessageDigest 实例对象。 MessageDigest类是一个工厂类，其构造器是受保护的，不允许直接使用new MessageDigist( )来创建对象，而必须通过其静态方法getInstance( )生成MessageDigest对象。其中传入的参数指定计算消息摘要所使用的算法，常用的有”MD5”，”SHA”等。 8.2 成员方法摘要： 返回值 方法名 描述 Object clone() 如果实现是可复制的，则返回一个副本。 byte[] digest() 通过执行诸如填充之类的最终操作完成哈希计算。 byte[] digest(byte[] input) 使用指定的字节数组对摘要进行最后更新，然后完成摘要计算。 int digest(byte[] buf, int offset, int len) 通过执行诸如填充之类的最终操作完成哈希计算。 String getAlgorithm() 返回标识算法的独立于实现细节的字符串。 int getDigestLength() 返回以字节为单位的摘要长度，如果提供程序不支持此操作并且实现是不可复制的，则返回 0。 static MessageDigest getInstance(String algorithm) 生成实现指定摘要算法的 MessageDigest 对象。 static MessageDigest getInstance(String algorithm, Provider provider) 生成实现指定提供程序提供的指定算法的 MessageDigest 对象，如果该算法可从指定的提供程序得到的话。 static MessageDigest getInstance(String algorithm, String provider) 生成实现指定提供程序提供的指定算法的 MessageDigest 对象，如果该算法可从指定的提供程序得到的话。 Provider getProvider() 返回此信息摘要对象的提供程序。 static boolean isEqual(byte[] digesta, byte[] digestb) 比较两个摘要的相等性。 void reset() 重置摘要以供再次使用。 String toString() 返回此信息摘要对象的字符串表示形式。 void update(byte input) 使用指定的字节更新摘要。 void update(byte[] input) 使用指定的字节数组更新摘要。 void update(byte[] input, int offset, int len) 使用指定的字节数组，从指定的偏移量开始更新摘要。 void update(ByteBuffer input) 使用指定的 ByteBuffer 更新摘要。 ★ 编程思路：java.security包中的MessageDigest类提供了计算消息摘要（即生成散列码）的方法，首先生成对象，执行其update( )方法可以将原始数据传递给该对象，然后执行其digest( )方法即可得到消息摘要。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"安全认证","slug":"safe","permalink":"http://chaooo.github.io/tags/safe/"}]},{"title":"「并发编程」NIO、Netty及websocket实现","date":"2019-10-20T15:34:31.000Z","path":"2019/10/20/concurrent-nio-websocket.html","text":"1. BIO&#x2F;NIO&#x2F;AIO演变Java IO 方式有很多种，基于不同的IO抽象模型和交互方式，可以进行简单区分。 IO类型 模型 客户端:线程数 API使用难度 调试难度 可靠性 吞吐量 BIO 流，同步阻塞 1:1 简单 简单 很差 非常低 伪异步IO 同步阻塞 M:N 简单 简单 较差 中等 NIO 同步非阻塞 M:1 复杂 复杂 较高 高 AIO 异步非阻塞 M:0,被动回调 复杂 复杂 高 高 区分同步(synchronous)或异步(asynchronous) 同步是一种可靠的有序运行机制，当我们进行同步操作时，后续的任务是等待当前调用返回，才会进行下一步； 异步则相反，其他任务不需要等待当前调用返回，通常依靠事件、回调等机制来实现任务间次序关系。 区分阻塞(blocking)与非阻塞(non-blocking) 在进行阻塞操作时，当前线程会处于阻塞状态，无法从事其他任务，只有当条件就绪才能继续，比如 ServerSocket 新连接建立完毕，或数据读取、写入操作完成； 非阻塞则是不管 IO 操作是否结束，直接返回，相应操作在后台继续处理 传统的java.io包，它基于流模型实现，同步阻塞的交互方式，如File抽象、输入输出流等。好处是代码简单、直观，缺点是IO效率和扩展性局限性 很多时候，也把java.net下面提供的部分网络API，比如Socket、ServerSocket、HttpURLConnection也归类到同步阻塞IO类库，因为网络通信同样是IO行为。 伪异步IO：后端通过维护一个消息队列和N个活跃线程, 通过一个线程池来处理多个客户端的请求接入，通过线程池，可以灵活地调配线程资源，设置线程的最大值，防止由于海量并发接入而导致的线程耗尽和宕机。 JDK4引入了NIO框架(java.nio)，提供了Channel、Selector、Buffer等新的抽象，可以构建多路复用的、同步非阻塞IO程序，同时提供了更接近操作系统底层的高性能数据操作方式。 JDK7中，NIO有了进一步的改进，引入了异步非阻塞IO方式，也叫AIO(Asynchronous IO)。异步IO操作基于事件和回调机制，可以简单理解为，应用操作直接返回，而不会阻塞在那里，当后台处理完成，操作系统会通知相应线程进行后续工作。 1.1 NIO的主要组成部分： Buffer(缓冲区)，高效的数据容器，除了布尔类型，所有原始数据类型都有相应的Buffer实现。 Buffer最常见的类型是ByteBuffer，另外还有CharBuffer，ShortBuffer，IntBuffer，LongBuffer，FloatBuffer，DoubleBuffer。 Channel(通道)，是NIO中被用来支持批量式IO操作的一种抽象。 和流不同，通道是双向的。数据可以从Channel读到Buffer中，也可以从Buffer 写到Channel中。 Selector(多路复用器)，是NIO实现多路复用的基础，它允许单线程处理多个Channel。 Selector是基于底层操作系统机制，不同模式、不同版本都存在区别。 要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。 1.2 NIO多路复用的过程 通过Selector.open()创建一个Selector，作为类似调度员的角色。 创建一个ServerSocketChannel，并绑定监听端口，设置为非阻塞模式 将Channel向Selector注册，通过指定SelectionKey.OP_ACCEPT，告诉调度员，它关注的是新的连接请求。 Selector循环阻塞在select操作，当有Channel发生接入请求，就会被唤醒。 调用selectedKeys方法获取就绪channel集合 通过SocketChannel和Buffer进行数据操作。 1.3 AIO AIO也叫NIO2.0 是一种非阻塞异步的通信模式。在NIO的基础上引入了新的异步通道的概念，并提供了异步文件通道和异步套接字通道的实现。 没有采用NIO的多路复用器，而是使用异步通道的概念。 其read，write方法的返回类型都是Future对象。而Future模型是异步的，其核心思想是：去主函数等待时间。 AIO模型中通过AsynchronousSocketChannel和AsynchronousServerSocketChannel完成套接字通道的实现。非阻塞，异步。 2. Netty框架Netty是一个高性能事件驱动，异步非阻塞的IO开源框架，由Jboss提供，用于建立Tcp等底层的链接，基于Netty可以建立高性能的Http服务器，快速开发高性能、高可靠的网络服务器和客户端程序。支持Http、websocket，tcp，udp等协议。 Netty使用场景：高性能领域（游戏，大数据分布式计算等）、多线程并发领域（多路复用模型，多线程模型，主从多线程模型）、异步通信领域 Netty 是一个吸收了多种协议（包括FTP、SMTP、HTTP等各种二进制文本协议）的实现经验，在保证易于开发的同时还保证了其应用的性能，稳定性和伸缩性。 2.1 Netty的核心概念 ServerBootstrap，服务器端程序的入口，这是 Netty 为简化网络程序配置和关闭等生命周期管理，所引入的 Bootstrapping 机制。我们通常要做的创建 Channel、绑定端口、注册 Handler 等，都可以通过这个统一的入口，以Fluent API等形式完成，相对简化了 API 使用。与之相对应， Bootstrap则是 Client 端的通常入口。 Channel，作为一个基于 NIO 的扩展框架，Channel 和 Selector 等概念仍然是 Netty 的基础组件，但是针对应用开发具体需求，提供了相对易用的抽象。 EventLoop，这是 Netty 处理事件的核心机制。例子中使用了 EventLoopGroup。我们在 NIO 中通常要做的几件事情，如注册感兴趣的事件、调度相应的 Handler 等，都是 EventLoop 负责。 ChannelFuture，这是 Netty 实现异步 IO 的基础之一，保证了同一个 Channel 操作的调用顺序。Netty 扩展了 Java 标准的 Future，提供了针对自己场景的特有Future定义。 ChannelHandler，这是应用开发者放置业务逻辑的主要地方，也是我上面提到的“Separation Of Concerns”原则的体现。 ChannelPipeline，它是 ChannelHandler 链条的容器，每个 Channel 在创建后，自动被分配一个 ChannelPipeline。在上面的示例中，我们通过 ServerBootstrap 注册了 ChannelInitializer，并且实现了 initChannel 方法，而在该方法中则承担了向 ChannelPipleline 安装其他 Handler 的任务。 2.2 对比 Java 标准 NIO 类库，Netty是如何实现更高性能的？单独从性能角度，Netty 在基础的 NIO 等类库之上进行了很多改进，例如： 更加优雅的 Reactor 模式实现、灵活的线程模型、利用 EventLoop 等创新性的机制，可以非常高效地管理成百上千的 Channel。 充分利用了 Java 的 Zero-Copy 机制，并且从多种角度，“斤斤计较”般的降低内存分配和回收的开销。例如，使用池化的 Direct Buffer 等技术，在提高 IO 性能的同时，减少了对象的创建和销毁；利用反射等技术直接操纵 SelectionKey，使用数组而不是 Java 容器等。 使用更多本地代码。例如，直接利用 JNI 调用 Open SSL 等方式，获得比 Java 内建 SSL 引擎更好的性能。 在通信协议、序列化等其他角度的优化。 Netty 的设计强调了 “Separation Of Concerns”，通过精巧设计的事件机制，将业务逻辑和无关技术逻辑进行隔离，并通过各种方便的抽象，一定程度上填补了了基础平台和业务开发之间的鸿沟，更有利于在应用开发中普及业界的最佳实践。另外，Netty &gt; java.nio + java. net！ 除了核心的事件机制等，Netty 还额外提供了很多功能，例如： 从网络协议的角度，Netty 除了支持传输层的 UDP、TCP、SCTP协议，也支持 HTTP(s)、WebSocket 等多种应用层协议，它并不是单一协议的 API。 在应用中，需要将数据从 Java 对象转换成为各种应用协议的数据格式，或者进行反向的转换，Netty 为此提供了一系列扩展的编解码框架，与应用开发场景无缝衔接，并且性能良好。 它扩展了 Java NIO Buffer，提供了自己的 ByteBuf 实现，并且深度支持 Direct Buffer 等技术，甚至 hack 了 Java 内部对 Direct Buffer 的分配和销毁等。同时，Netty 也提供了更加完善的 Scatter&#x2F;Gather 机制实现。 3. 基于Netty搭建简单的Http服务 环境准备：jdk1.8、Netty4.1.43.Final 代码编写：MyChannelInitializer.java、MyClientHandler.java、NettyServer.java MyChannelInitializer.java：添加了Http的处理协议 1234567891011public class MyChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel channel) &#123; // 数据解码操作 channel.pipeline().addLast(new HttpResponseEncoder()); // 数据编码操作 channel.pipeline().addLast(new HttpRequestDecoder()); // 在管道中添加我们自己的接收数据实现方法 channel.pipeline().addLast(new MyServerHandler()); &#125;&#125; MyServerHandler.java 123456789101112131415161718192021222324252627282930313233343536373839public class MyServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (msg instanceof HttpRequest) &#123; DefaultHttpRequest request = (DefaultHttpRequest) msg; System.out.println(&quot;URI:&quot; + request.getUri()); System.err.println(msg); &#125; if (msg instanceof HttpContent) &#123; LastHttpContent httpContent = (LastHttpContent) msg; ByteBuf byteData = httpContent.content(); if (!(byteData instanceof EmptyByteBuf)) &#123; //接收msg消息 byte[] msgByte = new byte[byteData.readableBytes()]; byteData.readBytes(msgByte); System.out.println(new String(msgByte, StandardCharsets.UTF_8)); &#125; &#125; String sendMsg = &quot;不平凡的岁月终究来自你每日不停歇的刻苦拼搏，每一次真正成长都因看清脚下路而抉择出的生活。&quot;; FullHttpResponse response = new DefaultFullHttpResponse( HttpVersion.HTTP_1_1, HttpResponseStatus.OK, Unpooled.wrappedBuffer(sendMsg.getBytes(StandardCharsets.UTF_8))); response.headers().set(HttpHeaderNames.CONTENT_TYPE, &quot;text/plain;charset=UTF-8&quot;); response.headers().set(HttpHeaderNames.CONTENT_LENGTH, response.content().readableBytes()); response.headers().set(HttpHeaderNames.CONNECTION, HttpHeaderValues.KEEP_ALIVE); ctx.write(response); ctx.flush(); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); cause.printStackTrace(); &#125;&#125; NettyServer.java 1234567891011121314151617181920212223242526public class NettyServer &#123; public static void main(String[] args) &#123; new NettyServer().bing(7397); &#125; private void bing(int port) &#123; //配置服务端NIO线程组 EventLoopGroup parentGroup = new NioEventLoopGroup(); EventLoopGroup childGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(parentGroup, childGroup) .channel(NioServerSocketChannel.class)//非阻塞模式 .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true) .childHandler(new MyChannelInitializer()); ChannelFuture f = b.bind(port).sync(); System.out.println(&quot;http-netty server start done. &quot;); f.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; childGroup.shutdownGracefully(); parentGroup.shutdownGracefully(); &#125; &#125;&#125; 启动NettyServer，Postman访问http://localhost:7397并设置参数 4. WebSocketWebSocket是一种H5协议规范，通过握手机制客户端与服务器之间就能够建立一个类似Tcp的连接，从而方便客户端与服务器之间的通信。 它是一种解决客户端与服务端实时通信而产生的技术：WebSocket本质是一种基于TCP协议，先通过Http&#x2F;Https发一个特殊的Http请求进行握手，握手后会创建一个用于交换数据的TCP链接，之后客户端和服务端使用该TCP链接进行实时通信。当WebSocket的客户端和服务端握手后 建立通信后，就不再需要之前的http请求参与。 4.1 WebSocket的优点： 节省通信开销，之前WebServer实现通信，都使用轮询，需要不停的向服务器发送请求，而HttpRequest的handler很长，请求包含真正的数据可能很小，会占用很多额外的带宽和服务器资源。 建立连接后，服务器可主动传数据给客户端，客户端也可以随意向服务端传数据。交换数据时所携带的头信息很小。浏览器（客户端）和服务器只需要做一个握手的动作。 实时通信：WebSocket不仅限于Ajax方式通信。ajax方式需要浏览器发起请求。而WebSocket技术 服务端和客户端可以彼此相互推送信息，从而实现实时通信。 4.2 WebSocket建立连接过程：客户端发起握手请求 ---&gt; 服务端响应请求 ---&gt; 建立连接 详细流程：建立一个WebSocket连接，客户端或浏览器首先向服务器发送一个特殊的Http请求(携带一些附加头信息)Upgrade:websocket，服务端解析附加头信息，产生应答消息，然后响应给客户端，之后客户端就与服务端建立响应的链接。 4.3 WebSocket生命周期： 打开事件：端点上建立新链接时，该事件是先于其他任何事件发生之前。该事件发生会产生三部分信息。 创建WebSocket Session对象：用于表示已经建立好的链接 配置对象：包含配置端点的信息。 一组路径参数，用于打开节点握手时，WebSocket端入栈匹配的URI 消息事件：主要是接收WebSocket对话中，另一端发送的消息。链接上的消息将会有三种形式抵达客户端。 文本消息 用String处理 二进制消息 用byteBuffer或者byte[]处理 pong消息 用Java WebSocket API中的pong.message接口的实例来处理 错误事件：WebSocket链接或者端点发生错误时产生。可以处理入栈消息时发生的各种异常。入栈消息可能产生的三种异常。 WebSocket建立链接时发生错误：SessionException类型 WebSocket试图将入栈消息解码成开发人员使用的对象时 EncodeException类型 WebSocket端点的其他方法运行时产生的错误，WebSocket实现将记录端点操作过程中产生的任何异常 关闭事件：WebSocket链接端点关闭，做一些清理工作，可以由参与连接的任意一个端点发出。 4.4 WebSocket如何关闭链接：流程：当服务器被指示关闭WebSocket链接时，服务端会发起一个TCP Close操作， 客户端应该等待服务器的TCP Close 关闭WebSocket连接，端点需关闭底层TCP连接。 底层TCP连接，在大多数正常情况下，应该首先被服务器关闭，服务器持有TIME_WAIT状态（因为这会防止它在2个报文最大生存时间（2MLS）内重新打开连接，然而当一个新的带有更高的seq number的SYN时没有对应的服务器影响TIME_WAIT连接被立即重新打开）。 在异常情况下（例如在一个合理的时间量后没有接收到服务器的TCP Close）,客户端可以发起TCP Close。 5. 基于Netty搭建WebSocket多人聊天室 使用SpringBoot+Netty+WebSocket搭建功能。 使用Netty提供的HttpServerCodec、HttpObjectAggregator、ChunkedWriteHandler进行编码解码处理。 环境准备：jdk1.8、Netty4.1.43.Final、spring-boot-starter-web 目录结构 12345678910111213141516171819202122└── src.main ├── java │ └── top.chaooo.hellonetty │ ├── domain │ │ ├── ClientMsgProtocol.java │ │ └── ServerMsgProtocol.java │ ├── server │ │ ├── MyChannelInitializer.java │ │ ├── MyServerHandler.java │ │ └── NettyServer.java │ ├── util │ │ ├── ChannelHandler.java │ │ └── MsgUtil.java │ ├── controller │ │ └── NettyController.java │ └── NettyApplication.java └── resources ├── static(js,img) ├── templates │ └── index.html └── application.yml resources&#x2F;application.yml：基础配置信息，包括了；应用端口、netty服务端端口等 12345678910111213server: port: 8080netty: host: 127.0.0.1 port: 7397spring: thymeleaf: mode: HTML5 encoding: UTF-8 content-type: text/html cache: false server&#x2F;MyChannelInitializer.java：websocket处理协议 12345678910public class MyChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel channel) &#123; channel.pipeline().addLast(&quot;http-codec&quot;, new HttpServerCodec()); channel.pipeline().addLast(&quot;aggregator&quot;, new HttpObjectAggregator(65536)); channel.pipeline().addLast(&quot;http-chunked&quot;, new ChunkedWriteHandler()); // 在管道中添加我们自己的接收数据实现方法 channel.pipeline().addLast(new MyServerHandler()); &#125;&#125; server&#x2F;MyServerHandler.java：处理websocket消息信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class MyServerHandler extends ChannelInboundHandlerAdapter &#123; private Logger logger = LoggerFactory.getLogger(MyServerHandler.class); private WebSocketServerHandshaker handshaker; /** * 当客户端主动链接服务端的链接后，这个通道就是活跃的了。 * 也就是客户端与服务端建立了通信通道并且可以传输数据 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; SocketChannel channel = (SocketChannel) ctx.channel(); logger.info(&quot;链接报告开始&quot;); logger.info(&quot;链接报告信息：有一客户端链接到本服务端&quot;); logger.info(&quot;链接报告IP:&#123;&#125;&quot;, channel.localAddress().getHostString()); logger.info(&quot;链接报告Port:&#123;&#125;&quot;, channel.localAddress().getPort()); logger.info(&quot;链接报告完毕&quot;); ChannelUtil.channelGroup.add(ctx.channel()); &#125; /** * 当客户端主动断开服务端的链接后，这个通道就是不活跃的。 * 也就是说客户端与服务端的关闭了通信通道并且不可以传输数据 */ @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; logger.info(&quot;客户端断开链接&#123;&#125;&quot;, ctx.channel().localAddress().toString()); ChannelUtil.channelGroup.remove(ctx.channel()); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; //http if (msg instanceof FullHttpRequest) &#123; FullHttpRequest httpRequest = (FullHttpRequest) msg; if (!httpRequest.decoderResult().isSuccess()) &#123; DefaultFullHttpResponse httpResponse = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.BAD_REQUEST); // 返回应答给客户端 if (httpResponse.status().code() != 200) &#123; ByteBuf buf = Unpooled.copiedBuffer(httpResponse.status().toString(), CharsetUtil.UTF_8); httpResponse.content().writeBytes(buf); buf.release(); &#125; // 如果是非Keep-Alive，关闭连接 ChannelFuture f = ctx.channel().writeAndFlush(httpResponse); if (httpResponse.status().code() != 200) &#123; f.addListener(ChannelFutureListener.CLOSE); &#125; return; &#125; WebSocketServerHandshakerFactory wsFactory = new WebSocketServerHandshakerFactory(&quot;ws:/&quot; + ctx.channel() + &quot;/websocket&quot;, null, false); handshaker = wsFactory.newHandshaker(httpRequest); if (null == handshaker) &#123; WebSocketServerHandshakerFactory.sendUnsupportedVersionResponse(ctx.channel()); &#125; else &#123; handshaker.handshake(ctx.channel(), httpRequest); &#125; return; &#125; //ws if (msg instanceof WebSocketFrame) &#123; WebSocketFrame webSocketFrame = (WebSocketFrame) msg; //关闭请求 if (webSocketFrame instanceof CloseWebSocketFrame) &#123; handshaker.close(ctx.channel(), (CloseWebSocketFrame) webSocketFrame.retain()); return; &#125; //ping请求 if (webSocketFrame instanceof PingWebSocketFrame) &#123; ctx.channel().write(new PongWebSocketFrame(webSocketFrame.content().retain())); return; &#125; //只支持文本格式，不支持二进制消息 if (!(webSocketFrame instanceof TextWebSocketFrame)) &#123; throw new Exception(&quot;仅支持文本格式&quot;); &#125; String request = ((TextWebSocketFrame) webSocketFrame).text(); System.out.println(&quot;服务端收到：&quot; + request); ClientMsgProtocol clientMsgProtocol = JSON.parseObject(request, ClientMsgProtocol.class); //1请求个人信息 if (1 == clientMsgProtocol.getType()) &#123; ctx.channel().writeAndFlush(MsgUtil.buildMsgOwner(ctx.channel().id().toString())); return; &#125; //群发消息 if (2 == clientMsgProtocol.getType()) &#123; TextWebSocketFrame textWebSocketFrame = MsgUtil.buildMsgAll(ctx.channel().id().toString(), clientMsgProtocol.getMsgInfo()); ChannelUtil.channelGroup.writeAndFlush(textWebSocketFrame); &#125; &#125; &#125; /** * 抓住异常，当发生异常的时候，可以做一些相应的处理，比如打印日志、关闭链接 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); logger.info(&quot;异常信息：\\r\\n&quot; + cause.getMessage()); &#125;&#125; server&#x2F;NettyServer.java：主服务 1234567891011121314151617181920212223242526272829303132333435363738@Component(&quot;nettyServer&quot;)public class NettyServer &#123; private Logger logger = LoggerFactory.getLogger(NettyServer.class); //配置服务端NIO线程组 private final EventLoopGroup parentGroup = new NioEventLoopGroup(); private final EventLoopGroup childGroup = new NioEventLoopGroup(); private Channel channel; public ChannelFuture bing(InetSocketAddress address) &#123; ChannelFuture channelFuture = null; try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(parentGroup, childGroup) .channel(NioServerSocketChannel.class) //非阻塞模式 .option(ChannelOption.SO_BACKLOG, 128) .childHandler(new MyChannelInitializer()); channelFuture = b.bind(address).syncUninterruptibly(); channel = channelFuture.channel(); &#125; catch (Exception e) &#123; logger.error(e.getMessage()); &#125; finally &#123; if (null != channelFuture &amp;&amp; channelFuture.isSuccess()) &#123; logger.info(&quot;demo-netty server start done&quot;); &#125; else &#123; logger.error(&quot;demo-netty server start error&quot;); &#125; &#125; return channelFuture; &#125; public void destroy() &#123; if (null == channel) return; channel.close(); parentGroup.shutdownGracefully(); childGroup.shutdownGracefully(); &#125; public Channel getChannel() &#123; return channel; &#125;&#125; util&#x2F;MsgUtil.java：消息构建工具类 123456789101112131415161718192021public class MsgUtil &#123; public static TextWebSocketFrame buildMsgAll(String channelId, String msgInfo) &#123; //模拟头像 int i = Math.abs(channelId.hashCode()) % 10; ServerMsgProtocol msg = new ServerMsgProtocol(); msg.setType(2); //链接信息;1自发信息、2群发消息 msg.setChannelId(channelId); msg.setUserHeadImg(&quot;head&quot; + i + &quot;.jpg&quot;); msg.setMsgInfo(msgInfo); return new TextWebSocketFrame(JSON.toJSONString(msg)); &#125; public static TextWebSocketFrame buildMsgOwner(String channelId) &#123; ServerMsgProtocol msg = new ServerMsgProtocol(); msg.setType(1); //链接信息;1链接信息、2消息信息 msg.setChannelId(channelId); return new TextWebSocketFrame(JSON.toJSONString(msg)); &#125;&#125; util&#x2F;ChannelUtil.java：存储每一个客户端接入进来时的channel对象 1234public class ChannelUtil &#123; //用于存放用户Channel信息，也可以建立map结构模拟不同的消息群 public static ChannelGroup channelGroup = new DefaultChannelGroup(GlobalEventExecutor.INSTANCE);&#125; domain&#x2F;*MsgProtocol.java：省略get&#x2F;set 123456789101112public class ServerMsgProtocol &#123; private int type; //链接信息;1:自发信息、2:群发消息 private String channelId; //通信管道ID，实际使用中会映射成用户名 private String userHeadImg; //用户头像[模拟分配] private String msgInfo; //通信消息 // ...&#125;public class ClientMsgProtocol &#123; private int type; //1:请求个人信息，2:发送聊天信息 private String msgInfo; //消息 // ...&#125; controller&#x2F;NettyController.java：路由控制层 12345678@Controllerpublic class NettyController &#123; @RequestMapping(value = &quot;/index&quot;) public String index(Model model) &#123; model.addAttribute(&quot;name&quot;, &quot;Dear&quot;); return &quot;index&quot;; &#125;&#125; js逻辑：依赖jquery.min.js、jquery.serialize-object.min.js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// JavaScript Documentvar socket;$(function()&#123; if(!window.WebSocket)&#123; window.WebSocket = window.MozWebSocket; &#125; if(!window.WebSocket)&#123; alert(&quot;您的浏览器不支持WebSocket协议！推荐使用谷歌浏览器进行测试。&quot;); return; &#125; socket = new WebSocket(&quot;ws://localhost:7397/websocket&quot;); socket.onmessage = function(event)&#123; var msg = JSON.parse(event.data); //链接信息;1自发信息、2群发消息 if(1 == msg.type)&#123; jQuery.data(document.body, &#x27;channelId&#x27;, msg.channelId); return; &#125; //链接信息;1自发信息、2群发消息 if(2 == msg.type)&#123; var channelId = msg.channelId; //自己 if(channelId == jQuery.data(document.body, &#x27;channelId&#x27;))&#123; var module = $(&quot;.msgBlockOwnerClone&quot;).clone(); module.removeClass(&quot;msgBlockOwnerClone&quot;).addClass(&quot;msgBlockOwner&quot;).css(&#123;display: &quot;block&quot;&#125;); module.find(&quot;.headPoint&quot;).attr(&quot;src&quot;, &quot;res/img/&quot;+msg.userHeadImg); module.find(&quot;.msgBlock_msgInfo .msgPoint&quot;).text(msg.msgInfo); $(&quot;#msgPoint&quot;).before(module); util.divScroll(); &#125; //好友 else&#123; var module = $(&quot;.msgBlockFriendClone&quot;).clone(); module.removeClass(&quot;msgBlockFriendClone&quot;).addClass(&quot;msgBlockFriend&quot;).css(&#123;display: &quot;block&quot;&#125;); module.find(&quot;.headPoint&quot;).attr(&quot;src&quot;, &quot;res/img/&quot;+msg.userHeadImg); module.find(&quot;.msgBlock_channelId&quot;).text(&quot;ID：&quot;+msg.channelId); module.find(&quot;.msgBlock_msgInfo .msgPoint&quot;).text(msg.msgInfo); $(&quot;#msgPoint&quot;).before(module); util.divScroll(); &#125; &#125; &#125;; socket.onopen = function(event)&#123; console.info(&quot;打开WebSoket 服务正常，浏览器支持WebSoket!&quot;); var clientMsgProtocol = &#123;&#125;; clientMsgProtocol.type = 1; clientMsgProtocol.msgInfo = &quot;请求个人信息&quot;; socket.send(JSON.stringify(clientMsgProtocol)); &#125;; socket.onclose = function(event)&#123; console.info(&quot;WebSocket 关闭&quot;); &#125;; document.onkeydown = function(e) &#123; if (13 == e.keyCode &amp;&amp; e.ctrlKey)&#123; util.send(); &#125; &#125;&#125;);util = &#123; send: function()&#123; if(!window.WebSocket)&#123;return;&#125; if(socket.readyState == WebSocket.OPEN)&#123; var clientMsgProtocol = &#123;&#125;; clientMsgProtocol.type = 2; clientMsgProtocol.msgInfo = $(&quot;#sendBox&quot;).val(); socket.send(JSON.stringify(clientMsgProtocol)); $(&quot;#sendBox&quot;).val(&quot;&quot;); &#125;else&#123; alert(&quot;WebSocket 连接没有建立成功！&quot;); &#125; &#125;, divScroll: function()&#123; var div = document.getElementById(&#x27;show&#x27;); div.scrollTop = div.scrollHeight; &#125; &#125;; 主要Html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot; xmlns:sec=&quot;http://www.thymeleaf.org/extras/spring-security&quot;&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/jquery.serialize-object.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/index.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=&quot;chatDiv&quot;&gt; &lt;div id=&quot;chat&quot; style=&quot;width:529px; height:667px; background-color:#F5F5F5; float:right;&quot;&gt; &lt;!-- 会话区域 begin --&gt; &lt;div id=&quot;show&quot; style=&quot;width:529px; height:450px; float:left;overflow-y:scroll;&quot;&gt; &lt;!-- 消息块；好友 --&gt; &lt;div class=&quot;msgBlockFriendClone&quot; style=&quot; display:none; margin-left:30px; margin-top:15px; width:340px; height:auto; margin-bottom:15px; float:left;&quot;&gt; &lt;div class=&quot;msgBlock_userHeadImg&quot; style=&quot;float:left; width:35px; height:35px;border-radius:3px;-moz-border-radius:3px; background-color:#FFFFFF;&quot;&gt; &lt;img class=&quot;headPoint&quot; src=&quot;/img/head5.jpg&quot; width=&quot;35px&quot; height=&quot;35px&quot; style=&quot;border-radius:3px;-moz-border-radius:3px;&quot;/&gt; &lt;/div&gt; &lt;div class=&quot;msgBlock_channelId&quot; style=&quot;float:left; width:100px; margin-top:-5px; margin-left:10px; padding-bottom:2px; font-size:10px;&quot;&gt; &lt;!-- 名称 --&gt; &lt;/div&gt; &lt;div class=&quot;msgBlock_msgInfo&quot; style=&quot;height:auto;width:280px;float:left;margin-left:12px; margin-top:4px;border-radius:3px;-moz-border-radius:3px; &quot;&gt; &lt;div style=&quot;width:4px; height:20px; background-color:#CC0000; float:left;border-radius:3px;-moz-border-radius:3px;&quot;&gt;&lt;/div&gt; &lt;div class=&quot;msgPoint&quot; style=&quot;float:left;width:260px; padding:7px; background-color:#FFFFFF; border-radius:3px;-moz-border-radius:3px; height:auto; font-size:12px;display:block;word-break: break-all;word-wrap: break-word;&quot;&gt; &lt;!-- 信息 --&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;!-- 消息块；自己 --&gt; &lt;div class=&quot;msgBlockOwnerClone&quot; style=&quot; display:none; margin-right:30px; margin-top:15px; width:340px; height:auto; margin-bottom:15px; float:right;&quot;&gt; &lt;div style=&quot;float:right; width:35px; height:35px;border-radius:3px;-moz-border-radius:3px; background-color:#FFFFFF;&quot;&gt; &lt;img class=&quot;headPoint&quot; src=&quot;/img/head3.jpg&quot; width=&quot;35px&quot; height=&quot;35px&quot; style=&quot;border-radius:3px;-moz-border-radius:3px;&quot;/&gt; &lt;/div&gt; &lt;div class=&quot;msgBlock_msgInfo&quot; style=&quot;height:auto;width:280px;float:left;margin-left:12px; margin-top:4px;border-radius:3px;-moz-border-radius:3px; &quot;&gt; &lt;div class=&quot;msgPoint&quot; style=&quot;float:left;width:260px; padding:7px; background-color:#FFFFFF; border-radius:3px;-moz-border-radius:3px; height:auto; font-size:12px;display:block;word-break: break-all;word-wrap: break-word;&quot;&gt; &lt;!-- 信息 --&gt; &lt;/div&gt; &lt;div style=&quot;width:4px; height:20px; background-color:#CC0000; float:right;border-radius:3px;-moz-border-radius:3px;&quot;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;span id=&quot;msgPoint&quot;&gt;&lt;/span&gt; &lt;/div&gt; &lt;!-- 会话区域 end --&gt; &lt;div style=&quot;width:100%; height:2px; float:left; background-color:#CCCCCC;&quot;&gt;&lt;/div&gt; &lt;div style=&quot;margin:0 auto; width:100%; height:149px; margin-top:5px; background-color:#FFFFFF; float:left;&quot;&gt; &lt;textarea id=&quot;sendBox&quot; style=&quot;font-size:14px; border:0; width:499px; height:80px; outline:none; padding:15px;font-family:”微软雅黑”;resize: none;&quot;&gt;&lt;/textarea&gt; &lt;div style=&quot;margin-top:20px; float:right; margin-right:35px; padding:5px; padding-left:15px; padding-right:15px; font-size:12px; background-color:#F5F5F5;border-radius:3px;-moz-border-radius:3px; cursor:pointer;&quot; onclick=&quot;javascript:util.send();&quot;&gt;发送(S)&lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 启动SpringBoot，Netty会随着启动； 用不同浏览器访问 http://localhost:8080/index 测试多人实时聊天。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"并发编程","slug":"concurrent","permalink":"http://chaooo.github.io/tags/concurrent/"}]},{"title":"「并发编程」阻塞队列 与 线程池","date":"2019-10-14T15:04:54.000Z","path":"2019/10/14/concurrent-blockingqueue-threadpool.html","text":"池和队列的关系 线程池或者数据库连接池，都有最大限制。如果超出了限制数量，则新进来的申请连接都要放入额外的队列里，等到池空出来时，从队列中取出连接放进池里。 1. BlockingQueue（阻塞队列）1234567Queue接口 |———— BlockingQueue接口 |———— ArrayBlockingQueue类 |———— DelayQueue类 |———— LinkedBlockingQueue类 |———— PriorityBlockingQueue类 |———— SynchronousQueue类 BlockingQueue继承了Queue接口，提供了一些阻塞方法，主要作用如下： 当线程向队列中插入元素时，如果队列已满，则阻塞线程，直到队列有空闲位置（非满）； 当线程从队列中取元素（删除队列元素）时，如果队列为空，则阻塞线程，直到队列有元素； BlockingQueue在Queue方法基础上增加了两类和阻塞相关的方法：put(e)、take()；offer(e, time, unit)、poll(time, unit)。 操作类型 抛出异常 返回特殊值 阻塞线程 超时 插入 add(e) offer(e) put(e) offer(e, time, unit) 删除 remove() poll() take() poll(time, unit) 读取 element() peek() &#x2F; &#x2F; **put(e)和take()**方法会一直阻塞调用线程，直到线程被中断或队列状态可用； **offer(e, time, unit)和poll(time, unit)**方法会限时阻塞调用线程，直到超时或线程被中断或队列状态可用。 阻塞队列主要用在生产者&#x2F;消费者的场景 1.1 ArrayBlockingQueueArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。 有边界的意思是它的容量是有限的，我们必须在其初始化的时候指定它的容量大小，容量大小一旦指定就不可改变。 ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。 1.2 DelayQueueDelayQueue阻塞的是其内部元素，DelayQueue中的元素必须实现java.util.concurrent.Delayed接口，Delayed接口继承了Comparable接口，这是因为DelayedQueue中的元素需要进行排序，一般情况，我们都是按元素过期时间的优先级进行排序。 DelayQueue应用场景：定时关闭连接、缓存对象，超时处理等 1.3 LinkedBlockingQueueLinkedBlockingQueue阻塞队列大小的配置是可选的，如果我们初始化时指定一个大小，它就是有边界的，如果不指定，它就是无边界的。 说是无边界，其实是采用了默认大小为Integer.MAX_VALUE的容量 。它的内部实现是一个链表。 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。 1.4 PriorityBlockingQueuePriorityBlockingQueue是一个没有边界的队列，它的排序规则和java.util.PriorityQueue一样。需要注意，PriorityBlockingQueue中允许插入null对象。 所有插入PriorityBlockingQueue的对象必须实现java.lang.Comparable接口，队列优先级的排序规则就是按照我们对这个接口的实现来定义的。 从PriorityBlockingQueue获得一个迭代器Iterator，但这个迭代器并不保证按照优先级顺序进行迭代。 1.5 SynchronousQueueSynchronousQueue队列内部仅允许容纳一个元素。 当一个线程插入一个元素后会被阻塞，除非这个元素被另一个线程消费。 2. Callable &amp; FutureCallable与Runnable的功能大致相似，Callable功能强大一些，就是被线程执行后，可以返回值，并且能抛出异常。 Runnable接口只有一个run()方法，实现类重写run方法，把一些费时操作写在其中，然后使用某个线程去执行该Runnable实现类即可实现多线程。 Callable是一个泛型接口只有一个call()方法，返回的类型就是创建Callable传进来的V类型。 1234@FunctionalInterfacepublic interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; Callable一般是和ExecutorService配合来使用的，在ExecutorService接口中声明了若干个submit方法的重载版本 2.1 Future &amp; FutureTaskFuture就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果。必要时可以通过get方法获取执行结果，该方法会阻塞直到任务返回结果。 也就是说Future提供了三种功能： 判断任务是否完成； 能够中断任务； 能够获取任务执行结果。 在Future接口中声明了5个方法：**cancel、isCancelled、isDone、get** boolean cancel(boolean mayInterruptIfRunning);&#x2F;&#x2F;用来取消任务，参数mayInterruptIfRunning表示是否允许取消正在执行却没有执行完毕的任务。 如果取消已经完成的任务会返回false；如果任务还没有执行会返回true； 如果任务正在执行，则返回mayInterruptIfRunning设置的值(true/false)； boolean isCancelled();&#x2F;&#x2F;任务是否被取消成功，如果在任务正常完成前被取消成功，则返回 true。 boolean isDone();&#x2F;&#x2F;任务是否已经完成，若任务完成，则返回true； V get();&#x2F;&#x2F;获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回； V get(long timeout, TimeUnit unit);&#x2F;&#x2F;获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null。 Future可以得到别的线程任务方法的返回值。Future是一个接口,引用对象指向的实际是FutureTask。 3. FutureTask**FutureTask**的父类是RunnableFuture，而RunnableFuture继承了Runnbale和Futrue这两个接口 从FutureTask构造方法可以了解到： FutureTask最终都是执行Callable类型的任务。 如果构造函数参数是Runnable，会被Executors.callable方法转换为Callable类型。 Executors.callable方法直接返回一个RunnableAdapter实例。 RunnableAdapter是FutureTask的一个静态内部类并且实现了Callable，也就是说RunnableAdapter是Callable子类。 RunnableAdapter的call方法实现代码是，执行Runnable的run方法，并返回构造FutureTask传入result参数。 FutureTask总结： FutureTask实现了两个接口，Runnable和Future，所以它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值，这个组合的好处：假设有一个很费时逻辑需要计算并且返回这个值，同时这个值不是马上需要，那么就可以使用这个组合，用另一个线程去计算返回值，而当前线程在使用这个返回值之前可以做其它的操作，等到需要这个返回值时，再通过Future得到！ 注意： 通过Executor执行线程任务都是以Callable形式，如果传入Runnable都会转化为Callable。 通过new Thread(runnable)，只能是Runnable子类形式。 4. Fork&#x2F;Join从JDK1.7开始，Java提供Fork/Join框架用于并行执行任务，它的思想就是讲一个大任务分割成若干小任务，最终汇总每个小任务的结果得到这个大任务的结果。 主要有两步：任务切分 -&gt; 结果合并 第一步**分割任务**。首先我们需要有一个 fork 类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。 第二步执行任务并**合并结果。分割的子任务分别放在双端队列**里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。 工作窃取算法（work-stealing）是指某个线程从其他队列里窃取任务来执行。 Fork/Join 使用两个类来完成以上两个步骤： **ForkJoinTask**：我们要使用 ForkJoin 框架，必须首先创建一个 ForkJoin 任务。它提供在任务中执行 fork() 和 join() 操作的机制，通常情况下我们不需要直接继承 ForkJoinTask 类，而只需要继承它的子类，Fork/Join 框架提供了以下两个子类： RecursiveAction：用于没有返回结果的任务。 RecursiveTask：用于有返回结果的任务。 **ForkJoinPool**：ForkJoinTask 需要通过 ForkJoinPool 来执行，任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进入队列的头部。当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务。 5. 线程池线程池可以看作是一个资源集，任何池的作用都大同小异，主要是用来减少资源创建、初始化的系统开销。 一个线程池包括以下四个基本组成部分： 线程池管理器（ThreadPool）：用于创建并管理线程池，包括 创建线程池，销毁线程池，添加新任务； 工作线程（PoolWorker）：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务； 任务接口（Task）：每个任务必须实现的接口，以供工作线程调度任务的执行，它主要规定了任务的入口，任务执行完后的收尾工作，任务的执行状态等； 任务队列（taskQueue）：用于存放没有处理的任务。提供一种缓冲机制。 12345678Executor接口 |———— ExecutorService接口 |———— AbstractExecutorService抽象类 |———— ForkJoinPool类 |———— ThreadPoolExecutor类 |———— ScheduledExecutorService接口 |———— ScheduledThreadPoolExecutor类Executors类 5.1 通过Executors工厂类中的六个静态方法创建线程池六大静态方法创建的ThreadPoolExecutor对象，返回的父接口的引用，即返回的ExecutorService的引用。六大静态方法内部都是直接或间接调用ThreadPoolExecutor类的构造方法创建线程池对象。 newCachedThreadPool(ThreadPoolExecutor)：创建一个可缓存的线程池 如果线程池的大小超过了处理任务所需要的线程,那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）,maximumPoolSize最大可以至(Integer.MAX_VALUE),若达到该上限,直接OOM。 newFixedThreadPool(ThreadPoolExecutor)：创建固定大小的线程池。 每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 newSingleThreadExecutor(ThreadPoolExecutor)：创建一个单线程的线程池。 这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务,保证按任务的提交顺序依次执行。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newScheduledThreadPool(ScheduledThreadPoolExecutor)：创建一个支持定时及周期性任务执行的线程池。 线程数最大至Integer.MAX_ VALUE,存在OOM风险,不回收工作线程. newSingleThreadScheduledExecutor(ScheduledThreadPoolExecutor)：创建一个单线程用于定时以及周期性执行任务的需求。 newWorkStealingPool(ForkJoinPool)：创建一个工作窃取 JDK8 引入,创建持有足够线程的线程池支持给定的并行度;并通过使用多个队列减少竞争; Executors返回的线程池对象的弊端： FixedThreadPool和SingleThreadExecutor： 允许的请求队列长度为Integer.MAX_VALUE，可能会堆积大量的请求，从而导致OOM。 CachedThreadPool： 允许的创建线程数量为Integer.MAX_VALUE，可能会创建大量的线程，从而导致OOM。 5.2 通过ThreadPoolExecutor构造方法创建线程池123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, //核心线程数，包括空闲线程 int maximumPoolSize,//最大线程数 long keepAliveTime, //线程空闲时间 TimeUnit unit, //时间单位 BlockingQueue&lt;Runnable&gt; workQueue,//缓存队列 ThreadFactory threadFactory, //线程工厂 RejectedExecutionHandler handler //拒绝策略) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 5.2.1 corePoolSize(核心线程数量) corePoolSize的设置非常关键： =0：则任务执行完之后,没有任何请求进入时销毁线程池的线程 &gt;0：即使本地任务执行完毕,核心线程也不会被销毁 设置过大会浪费资源; 设置过小会导致线程频繁地创建或销毁 若设置了allowCoreThreadTimeOut这个参数,当提交一个任务到线程池时,若线程数量(包括空闲线程)小于corePoolSize,线程池会创建一个新线程放入works(一个HashSet)中执行任务,等到需要执行的任务数大于线程池基本大小时就不再创建,会尝试放入等待队列workQueue；如果调用线程池的prestartAllCoreThreads(),线程池会提前创建并启动所有核心线程 5.2.2 maximumPoolSize（线程池最大线程数） maximumPoolSize表示线程池能够容纳同时执行的最大线程数,必须&gt;&#x3D;1. 若队列满,并且已创建的线程数小于最大线程数,则线程池会再创建新的线程放入works中执行任务,CashedThreadPool的关键,固定线程数的线程池无效 如果maximumPoolSize = corePoolSize,即是固定大小线程池. 若使用了无界任务队列,这个参数就没什么效果 5.2.3 keepAliveTime（线程池中的线程空闲时间） 线程没有任务执行时最多保持多久时间终止（线程池的工作线程空闲后，保持存活的时间) 如果任务很多，并且每个任务执行的时间比较短，可以调大时间，提高线程的利用率 当空闲时间达到keepAliveTime时,线程会被销毁,直到只剩下corePoolSize个线程;避免浪费内存和句柄资源. 在默认情况下,当线程池的线程数大于corePoolSize时,keepAliveTime才起作用. 但是当ThreadPoolExecutor的allowCoreThreadTimeOut=true时,核心线程超时后也会被回收. 5.2.4 TimeUnit（时间单位） keepAliveTime的时间单位通常是TimeUnit.SECONDS 可选的单位：天(DAYS)、小时(HOURS)、分钟(MINUTES)、毫秒(MILLISECONDS)、微秒(MICROSECONDS，千分之一毫秒) 和 纳秒(NANOSECONDS，千分之一微秒) 5.2.5 workQueue（缓存队列） 存储待执行任务的阻塞队列，这些任务必须是Runnable的对象（如果是Callable对象，会在submit内部转换为Runnable对象） 当请求的线程数大于maximumPoolSize时,线程进入BlockingQueue. 可以选择以下几个阻塞队列: LinkedBlockingQueue:一个基于链表结构的阻塞队列,此队列按FIFO排序元素,吞吐量通常要高于ArrayBlockingQueue.静态工厂方法Executors.newFixedThreadPool()使用了这个队列 SynchronousQueue:一个不存储元素的阻塞队列.每个插入操作必须等到另一个线程调用移除操作,否则插入操作一直处于阻塞状态,吞吐量通常要高于LinkedBlockingQueue,静态工厂方法Executors.newCachedThreadPool使用了这个队列 5.2.6 threadFactory （线程工厂） 用于设置创建线程的工厂; 线程池的命名是通过增加组名前缀来实现的，可以通过线程工厂给每个创建出来的线程设置更有意义的名字 在虚拟机栈分析时,就可以知道线程任务是由哪个线程工厂产生的. 5.2.7 RejectedExecutionHandler（拒绝策略） 当队列和线程池都满,说明线程池饱和,必须采取一种策略处理提交的新任务；策略默认**AbortPolicy**,表无法处理新任务时抛出异常 当超过参数workQueue的任务缓存区上限的时候,就可以通过该策略处理请求,这是一种简单的限流保护. 友好的拒绝策略可以是如下三种: 保存到数据库进行削峰填谷;在空闲时再提取出来执行 转向某个提示页面 打印日志 AbortPolicy：丢弃任务，抛出RejectedExecutionException CallerRunsPolicy：只用调用者所在线程来运行任务,有反馈机制，使任务提交的速度变慢）。 DiscardOldestPolicy：若没有发生shutdown,尝试丢弃队列里最近的一个任务,并执行当前任务, 丢弃任务缓存队列中最老的任务，并且尝试重新提交新的任务 DiscardPolicy:不处理,丢弃掉, 拒绝执行，不抛异常 当然,也可以根据应用场景需要来实现RejectedExecutionHandler接口自定义策略.如记录日志或持久化存储不能处理的任务 5.3 自定义一个ThreadPoolExecutor线程池12345678910111213141516171819202122232425262728ThreadPoolExecutor pool = new ThreadPoolExecutor( 5, //核心线程数 Runtime.getRuntime().availableProcessors() * 2,//最大线程数 60,//线程空闲时间 TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(200), new ThreadFactory() &#123; @Override public Thread newThread(Runnable r) &#123; Thread t = new Thread(r); t.setName(&quot;order-thread&quot;);//设置有意义的线程名字 if(t.isDaemon()) &#123;//若是守护线程将其释放 t.setDaemon(false); &#125; if(Thread.NORM_PRIORITY != t.getPriority()) &#123; //恢复线程优先级 t.setPriority(Thread.NORM_PRIORITY); &#125; return t; &#125; &#125;, new RejectedExecutionHandler() &#123; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; System.err.println(&quot;拒绝策略:&quot; + r); &#125; &#125; ); 5.3.1 线程池执行流程 要求 线程池有上限，使用有限队列 当线程池核心线程数量用完，先扔进队列 队列也用完后，看最大线程数量 最大线程数量用完后，走拒绝策略 拒绝策略可以打印一些日志，做一些补偿 线程池用完一定要优雅的关闭 线程池要统一管理，不要用Executors工厂类，要用ThreadPoolExecutor自定义线程池 5.3.2 线程池配置-核心线程数量线程CPU时间所占比例越高，需要越少线程(CPU密集)。线程等待时间所占比例越高，需要越多线程(IO密集)。 CPU密集型：内存运算、不涉及IO操作等 设置线程数为：CPU核数+1 IO密集型：数据读取、存取、数据库操作、持久化操作等 最佳线程数目：CPU核数/(1-阻塞系数) 这个阻塞系数一般为0.8~0.9之间，也可以取0.8或者0.9。 java.lang.Runtime.availableProcessors() 方法返回到Java虚拟机的可用的处理器数量(CPU核数)。此值可能会改变在一个特定的虚拟机调用。应用程序可用处理器的数量是敏感的，因此偶尔查询该属性，并适当地调整自己的资源使用情况.","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"并发编程","slug":"concurrent","permalink":"http://chaooo.github.io/tags/concurrent/"}]},{"title":"「并发编程」AQS框架 与 锁框架（JUC.locks）","date":"2019-10-10T15:00:24.000Z","path":"2019/10/10/concurrent-aqs-locks.html","text":"1. AQS（队列同步器）AbstractQueuedSynchronizer：队列同步器，简称AQS。 AQS维护了一个volatile int state(代表资源共享变量) 和一个**FIFO线程等待队列**(多线程争用资源被阻塞时会进入此队列)。 AQS定义了两种资源共享方式：Exclusive(独占)，Share(共享) isHeldExclusively方法：该线程是否正在独占资源 tryAcquire&#x2F;tryRelease：独占的方式尝试获取和释放资源 tryAcquireShared&#x2F;tryReleaseShared：共享的方式尝试获取和释放资源 整个框架的核心就是如何管理线程阻塞队列，该队列是严格的FIFO队列，因此不支持线程优先级的同步。 AQS只有一个同步队列，可以有多个条件队列。 同步队列的最佳选择是自身没有使用底层锁来构造的非阻塞数据结构，同步队列选择了**CLH**作为实现的基础。 条件队列：AQS框架提供了一个ConditionObject类，给维护独占同步的类以及实现Lock接口的类使用。 使用Node实现**FIFO双向队列**，可以用于构建锁 或 其他同步装置的基础框架 内部有一个int变量表示的**同步状态(同步状态通过getState、setState、compareAndSetState**来维护，同时这三个方法能够保证线程安全) AQS是个抽象类（但没有抽象方法），同步组件一般通过维护AQS的继承子类来实现。 AQS既支持独占地获取同步状态(排它锁)，又支持共享地获取同步状态(共享锁)，从而实现不同类型的组件。 AQS是基于模板方法，同步组件需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。 Synchronizer(同步器)：是一个对象，它根据本身的状态调节线程的控制流。常见类型的Synchronizer包括信号量、关卡和闭锁。 2. CountDownLatch（倒计时闭锁） 闭锁(latch)是一种Synchronizer，它可以延迟线程的进度直到线程达到终止状态。 CountDownLatch(倒计时闭锁)是一个灵活的闭锁实现。 CountDownLatch是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程执行完后再执行。 CountDownLatch原理：是通过一个计数器来实现的，计数器的初始化值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就相应得减1。当计数器到达0时，表示所有的线程都已完成任务，然后在闭锁上等待的线程就可以恢复执行任务。 await()，阻塞程序继续执行 countDown()，计数器的值减1，当计数器值减至零时，所有因调用await()方法而处于等待状态的线程就会继续往下执行。 计数器不能被重置，如果业务上需要一个可以重置计数次数的版本，可以考虑使用CycliBarrier CountDownLatch使用场景：应用初始化 3. Semaphore（信号量） Semaphore(信号量)：用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源。 Semaphore原理：线程需要通过acquire()方法获取许可，而release()释放许可。如果许可数达到最大活动数，那么调用acquire()之后，便进入等待队列，等待已获得许可的线程释放许可，从而使得多线程能够合理的运行。 acquire()：获取权限，其底层实现与CountDownLatch.countdown()类似; release()：释放权限，其底层实现与acquire()是一个互逆的过程。 Semaphore可以用于做流量控制，特别公用资源有限的应用场景，比如数据库连接。 4. CyclicBarrier（同步屏障） CyclicBarrier(同步屏障)：可以让一组线程达到一个屏障时被阻塞，直到最后一个线程达到屏障时，所有被阻塞的线程才能继续执行。 CyclicBarrier类似于CountDownLatch，它也是通过计数器来实现的。但是相比于CountDownLatch功能更加强大。 CyclicBarrier原理：当某个线程调用await方法时，该线程进入等待状态，且计数器加1，当计数器的值达到设置的初始值时，所有因调用await进入等待状态的线程被唤醒，继续执行后续操作。因为CycliBarrier在释放等待线程后可以重用，所以称为循环barrier。 4.1 CountDownLatch 和 CyclicBarrier 对比 CountDownLatch描述的是线程(1个或多个)等待其他线程的关系；CyclicBarrier描述的是多个线程相互等待的关系。 CountDownLatch的计数器只能使用一次。而CyclicBarrier的计数器可以使用reset()方法重置并复用。 CountDownLatch方法比较少，操作比较简单，而CyclicBarrier提供的方法更多，比如： getNumberWaiting()：获取阻塞的线程数量。 isBroken()：获取阻塞线程的状态，被中断返回true，否则返回false。 CyclicBarrier的构造方法可以传入barrierAction，指定当所有线程都到达时执行的业务功能； CyclicBarrier可以用于多线程计算数据，最后合并计算结果的应用场景 5. JUC.locks 锁框架123456789java.util.concurrent.locks |———— Lock接口 |———— ReentrantLock类 |———— ReentrantReadWriteLock.ReadLock内部类 |———— ReentrantReadWriteLock.WriteLock内部类 |———— Condition接口 |———— ReadWriteLock接口 |———— ReentrantReadWriteLock类 |———— LockSupport类 Lock接口核心方法：lock()，unlock()，lockInterruptibly()，newCondition()，tryClock() lock()方法类似于使用synchronized关键字加锁，如果锁不可用，出于线程调度目的，将禁用当前线程，并且在获得锁之前，该线程将一直处于休眠状态。 lockInterruptibly()方法顾名思义，就是如果锁不可用，那么当前正在等待的线程是可以被中断的，这比synchronized关键字更加灵活。 Condition接口核心方法：awit()，signal()，signalAll() 可以看做是Obejct类的wait()、notify()、notifyAll()方法的替代品，与Lock配合使用 ReadWriteLock接口核心方法：readLock()，writeLock() 获取读锁和写锁，注意除非使用Java8新锁，否则读读不互斥，读写是互斥的 6. ReentrantLock（可重入锁）ReentrantLock重入锁使用**AQS同步状态**来保存锁重复持有的次数 底层代码分析： **state**初始化为0，表示未锁定状态 A线程lock()时，会调用tryAcquire()独占该锁并将**state+1** 此后，其他线程再tryAcquire()时就会失败，直到A线程unlock()到state=0(即释放锁)为止，其他线程才有机会获取该锁 当然，锁释放之前，A线程自己是可以重复获取此锁的(state会累加)，这就是可重入的概念 synchronized实现的锁的重入依赖于JVM，是一种重量级锁。ReentrantLock实现了在内存语义上的synchronized，使用**AQS同步状态**来保存锁重复持有的次数。当锁被一个线程获取时，ReentrantLock也会记录下当前获得锁的线程标识，以便检查是否是重复获取，以及当错误的线程试图进行解锁操作时检测是否存在非法状态异常。 公平锁和非公平锁 公平锁还是非公平锁取决于ReentrantLock的构造方法，默认无参为非公平锁(NonfairSync)；含参构造方法，入参true为FairSync，入参false为NonfairSync。 非公平锁中，抢到AQS的同步状态的未必是同步队列的首节点，只要线程通过CAS抢到了同步状态或者在acquire中抢到同步状态，就优先占有锁（插队），而相对同步队列这个严格的FIFO队列来说，所以会被认为是非公平锁。 公平锁的实现直接调用AQS的acquire方法，acquire中调用tryAcquire。和非公平锁相比，这里不会执行一次CAS，接下来在tryAcquire去抢占锁的时候，也会先调用hasQueuedPredecessors看看前面是否有节点已经在等待获取锁了，如果存在则同步队列的前驱节点优先（排队FIFO）。 虽然公平锁看起来在公平性上比非公平锁好，但是公平锁为此付出了大量线程切换的代价，而非公平锁在锁的获取上不能保证公平，就有可能出现锁饥饿，即有的线程多次获取锁而有的线程获取不到锁，没有大量的线程切换保证了非公平锁的吞吐量。 7. 读写锁RRW（ReentrantReadWriteLock）ReentrantLock是独占锁，ReentrantReadWriteLock是读写锁。 独占锁通过state变量的0和1两个状态来控制是否有线程占有锁，共享锁通过state变量0或者非0来控制多个线程访问。 读写锁定义为：一个资源能够被多个读线程访问，或者被一个写线程访问，但是不能同时存在读写线程。 ReentrantReadWriteLock的特殊之处其实就是用一个int值表示两种不同的状态（低16位表示写锁的重入次数，高16位表示读锁的使用次数），并通过两个内部类同时实现了AQS的两套API，核心部分与共享&#x2F;独占锁并无什么区别。 ReentrantReadWriteLock也会发生写请求饥饿的情况，因为写请求一样会排队，不管是公平锁还是非公平锁，在有读锁的情况下，都不能保证写锁一定能获取到，这样只要读锁一直占用，就会发生写饥饿的情况。JDK8中新增的改进读写锁StampedLock可解决饥饿问题 8. LockSupport工具类归根结底，LockSupport调用的Unsafe中的native代码：park()，unpark()； park函数是将当前Thread阻塞，而unpark函数则是将另一个Thread唤醒。 与Object类的wait/notify机制相比，park/unpark有两个优点： 以thread为操作对象更符合阻塞线程的直观定义； 操作更精准，可以准确地唤醒某一个线程（Object类的notify随机唤醒一个线程，notifyAll唤醒所有等待的线程），增加了灵活性 park方法的调用一般要在方法一个循环判断体里面。之所以这样做，是为了防止线程被唤醒后，不进行判断而意外继续向下执行，这其实是一种的多线程设计模式-Guarded Suspension。 9. StampedLock（Java8新型锁）ReentrantReadWriteLock锁具有读写锁，问题在于ReentrantReadWriteLock使得多个读线程同时持有读锁（只要写锁未被占用），而写锁是独占的 ，很容易造成写锁获取不到资源(写请求饥饿)。 Java8引入了一个新的读写锁叫StampedLock. 不仅这个锁更快，而且它提供强大的乐观锁API。这种乐观策略的锁非常类似于无锁的操作，使得乐观锁完全不会阻塞写线程。 StampedLock的主要特点： 所有获取锁的方法，都返回一个邮戳（Stamp），Stamp为0表示获取失败，其余都表示成功； 所有释放锁的方法，都需要一个邮戳（Stamp），这个Stamp必须是和成功获取锁时得到的Stamp一致； StampedLock是不可重入的；（如果一个线程已经持有了写锁，再去获取写锁的话就会造成死锁） StampedLock有三种访问模式： Reading（读模式）：功能和ReentrantReadWriteLock的读锁类似 Writing（写模式）：功能和ReentrantReadWriteLock的写锁类似 Optimistic reading（乐观读模式）：这是一种优化的读模式。 StampedLock支持读锁和写锁的相互转换 RRW(ReentrantReadWriteLock)中，当线程获取到写锁后，可以降级为读锁，但是读锁是不能直接升级为写锁的；StampedLock提供了读锁和写锁相互转换的功能，使得该类支持更多的应用场景。 无论写锁还是读锁，都不支持Conditon等待","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"并发编程","slug":"concurrent","permalink":"http://chaooo.github.io/tags/concurrent/"}]},{"title":"「并发编程」JUC并发容器类","date":"2019-10-06T15:04:22.000Z","path":"2019/10/06/concurrent-collection.html","text":"在java.util.concurrent包中，提供了两种类型的并发集合：一种是阻塞式，另一种是非阻塞式。 阻塞式集合：当集合已满或为空时，被调用的添加（满）、移除（空）方法就不能立即被执行，调用这个方法的线程将被阻塞，一直等到该方法可以被成功执行 非阻塞式集合：当集合已满或为空时，被调用的添加（满）、移除（空）方法就不能立即被执行，调用这个方法的线程不会被阻塞，而是直接则返回null或抛出异常。 1. 线程安全相关容器1.1 线程安全-同步容器： ArrayList –&gt; Vector,Stack HashMap –&gt; HashTable(key、value不能为null) Collections.synchronizedXXX(List&#x2F;Set&#x2F;Map) &#x2F;&#x2F;本质是对相应的容器进行包装，通过在方法中加synchronized同步锁来实现 同步容器的同步原理就是在方法上用synchronized修饰。性能开销大。 在单独使用里面的方法的时候，可以保证线程安全，但是，复合操作需要额外加锁来保证线程安全。 1.2 线程安全-并发容器： ArrayList –&gt; **CopyOnWriteArrayList：保证最终一致性，写时复制，适用于读多写少**的并发场景 HashSet、TreeSet –&gt; CopyOnWriteArraySet、ConcurrentSkipListSet： HashMap、TreeMap –&gt; **ConcurrentHashMap**、ConcurrentSkipListMap： 1.3 安全共享对象策略 线程限制：一个被线程限制的对象，由线程独占，并且只能被占有者修改 共享只读：一个共享只读的对象，在没有额外同步的情况下，可以被多个线程并发访问，但不能修改 线程安全对象：一个线程安全的对象或者容器，在内部通过同步机制来保证线程安全，其他线程无需额外的同步就可以通过公共接口随意访问它 被守护对象：被守护对象只能通过获取特定的锁来访问 2. CopyOnWrite机制CopyOnWrite（简称COW），是计算机程序设计领域中的一种优化策略，也是一种思想–即写入时复制思想。 在CopyOnWrite中，对容器的修改操作加锁后，通过copy一个新的容器副本来进行修改，修改完毕后将容器替换为新的容器即可。 这种方式的好处显而易见：通过copy一个新的容器来进行修改，这样读操作就不需要加锁，可以并发读，因为在读的过程中是采用的旧的容器，即使新容器做了修改对旧容器也没有影响，同时也很好的解决了迭代过程中其他线程修改导致的并发问题。 从JDK1.5开始，java.util.concurrent包中提供了两个CopyOnWrite机制容器，分别为**CopyOnWriteArrayList和CopyOnWriteArraySet** CopyOnWriteArrayList通过使用**ReentrantLock锁**来实现线程安全： 在添加、获取元素时，使用getArray()获取底层数组对象，获取此时集合中的数组对象；使用setArray()设置底层数组，将原有数组对象指针指向新的数组对象—-实以此来实现CopyOnWrite副本概念 添加元素: 在添加元素之前进行加锁操作，保证数据的原子性。在添加过程中，进行数组复制，修改操作，再将新生成的数组复制给集合中的array属性。最后，释放锁； 由于array属性被volatile修饰，所以当添加完成后，其他线程就可以立刻查看到被修改的内容。 获取元素：在获取元素时，由于array属性被volatile修饰，所以每当获取线程执行时，都会拿到最新的数据。此外，添加线程在进行添加元素时，会将新的数组赋值给array属性，所以在获取线程中并不会因为元素的添加而导致本线程的执行异常。因为获取线程中的array和被添加后的array指向了不同的内存区域。 在执行add()时，为什么还要在加锁的同时又copy了一分新的数组对象? 因为，在add()时候加了锁，首先不会有多个线程同时进到add中去，这一点保证了数组的安全。当在一个线程执行add时，又进行了数组的复制操作，生成了一个新的数组对象，在add后又将新数组对象的指针指向了旧的数组对象指针，注意此时是指针的替换，原来旧的数组对象还存在。这样就实现了，添加方法无论如何操作数组对象，获取方法在获取到集合后，都不会受到其他线程添加元素的影响。 CopyOnWrite机制的优缺点 优点: CopyOnWriteArrayList保证了数据在多线程操作时的最终一致性。 缺点: 缺点也同样显著，那就是内存空间的浪费：因为在写操作时，进行数组复制，在内存中产生了两份相同的数组。如果数组对象比较大，那么就会造成频繁的GC操作，进而影响到系统的性能； 适用场景：读多写少的并发场景 3. ConcurrentHashMapConcurrentHashMap容器相较于CopyOnWrite容器在并发加锁粒度上有了更大一步的优化，它通过修改对单个hash桶元素加锁的达到了更细粒度的并发控制。 在底层数据结构上，ConcurrentHashMap和HashMap都使用了数组+链表+红黑树的方式，只是在HashMap的基础上添加了并发相关的一些控制。 JDK1.8中取消了segment分段锁，而采用CAS和synchronized来保证并发安全。synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。并且初始化操作大大简化，修改为lazy-load形式。 3.1 put方法过程put方法内部是一个 putVal 的调用： 判断键值是否为null，为null抛出异常 调用spread()方法计算key的hashCode()获得哈希地址 判断Node[]数组(table)是否为空，若空则进行初始化操作 需要注意的是这里并没有加synchronized，也就是允许多个线程去**尝试**初始化table，但是在初始化函数里面使用了CAS保证只有一个线程去执行初始化过程 使用(容量大小-1 &amp; 哈希地址)计算下标，如果没有碰撞，使用CAS原子性操作放入桶中；插入失败(被别的线程抢先插入了)则进入下次循环。 如果该下标上的节点(头节点)的哈希地址为-1，代表需要扩容，该线程执行helpTransfer()方法协助扩容。 如果碰撞了(bucket不为空)且又不需要扩容，则进入到bucket中，且锁住该bucket，其他bucket不影响。 进入到bucket里面，首先判断这个bucket存储的是红黑树(哈希地址小于0)还是链表。 如果是链表，则遍历链表，若节点已经存在(key相同)就覆盖旧值，没有找到相同的节点就将新增的节点插入到链表尾部。如果是红黑树，则将节点插入。到这里释放锁。 判断该bucket上的链表长度是否链表长度超过阀值（TREEIFY_THRESHOLD==8），大于则调用treeifyBin()方法将链表转成红黑树。 调用addCount()方法，作用是将ConcurrentHashMap的键值对数量+1，还有另一个作用是检查ConcurrentHashMap是否需要扩容。 总结： JDK8中的实现也是锁分离的思想，它把锁分的比segment（JDK1.5）更细一些，只要hash不冲突，就不会出现并发获得锁的情况。它首先使用无锁操作CAS插入头结点，如果插入失败，说明已经有别的线程插入头结点了，再次循环进行操作。如果头结点已经存在，则通过synchronized获得头结点锁，进行后续的操作。性能比segment分段锁又再次提升。 3.2 ConcurrentHashMap多线程环境下扩容 transfer()方法为ConcurrentHashMap扩容操作的核心方法。由于ConcurrentHashMap支持多线程扩容，而且也没有进行加锁，所以实现会变得有点儿复杂。整个扩容操作分为两步： 构建一个nextTable，其大小为原来大小的两倍，这个步骤是在单线程环境下完成的 将原来table里面的内容复制到nextTable中，这个步骤是允许多线程操作的，所以性能得到提升，减少了扩容的时间消耗。 扩容的时机： 如果新增节点之后，所在链表的元素个数达到了阈值 8，则会调用treeifyBin方法把链表转换成红黑树，不过在结构转换之前，会对数组长度进行判断： 如果数组长度n小于阈值MIN_TREEIFY_CAPACITY，默认是64，则会调用tryPresize方法把数组长度扩大到原来的两倍，并触发transfer方法，重新调整节点的位置。 新增节点之后，会调用addCount方法记录元素个数，并检查是否需要进行扩容，当数组元素个数达到阈值时，会触发transfer方法，重新调整节点的位置。 JDK8的源码里面就引入了一个**ForwardingNode**类，在一个线程发起扩容的时候，就会改变sizeCtl这个值，其含义如下： sizeCtl ：默认为0，用来控制table的初始化和扩容操作，具体应用在后续会体现出来。 -1 代表table正在初始化 -N 表示有N-1个线程正在进行扩容操作 其余情况： 如果table未初始化，表示table需要初始化的大小。 如果table初始化完成，表示table的容量，默认是table大小的0.75倍 扩容时候会判断sizeCtl的值，如果超过阈值就要扩容，首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素f，初始化一个forwardNode实例fwd，如果f == null，则在table中的i位置放入fwd，否则采用头插法的方式把当前旧table数组的指定任务范围的数据给迁移到新的数组中，然后给旧table原位置赋值fwd。直到遍历过所有的节点以后就完成了复制工作，把table指向nextTable，并更新sizeCtl为新数组大小的0.75倍 ，扩容完成。在此期间如果其他线程的有读写操作都会判断head节点是否为forwardNode节点，如果是就帮助扩容。 在扩容时读写操作如何进行 对于get读操作，如果当前节点有数据，还没迁移完成，此时不影响读，能够正常进行。如果当前链表已经迁移完成，那么头节点会被设置成fwd节点，此时get线程会帮助扩容。 对于put&#x2F;remove写操作，如果当前链表已经迁移完成，那么头节点会被设置成fwd节点，此时写线程会帮助扩容，如果扩容没有完成，当前链表的头节点会被锁住，所以写线程会被阻塞，直到扩容完成。 总结: ConcurrentHashMap扩容的原理是新生成原来2倍的数组，然后拷贝旧数组数据到新的数组里面，在多线程情况下，这里面如果注意线程安全问题，在解决安全问题的同时，我们也要关注其效率，这才是并发容器类的最出色的地方。 3.3 size、mappingCount方法 size和mappingCount方法都是用来统计table的size的 这两者不同的地方在size返回的是一个int类型，即可以表示size的范围是[-2^31，2^31-1]，超过这个范围就返回int能表示的最大值 mappingCount返回的是一个long类型，即可以表示size的范围是[-2^63，2^63-1]。 这两个方法都是调用的sumCount()方法实现统计。 对于size和迭代器是弱一致性 volatile修饰的数组引用是强可见的，但是其元素却不一定，所以，这导致size的根据sumCount的方法并不准确。 同理Iteritor的迭代器也一样，并不能准确反映最新的实际情况 4. ConcurrentSkipListMapConcurrentSkipListMap内部使用跳表（SkipList）这种数据结构来实现，他的结构相对红黑树来说非常简单理解，实现起来也相对简单，而且在理论上它的查找、插入、删除时间复杂度都为log(n)。在并发上，ConcurrentSkipListMap采用无锁的**CAS+自旋**来控制。 跳表简单来说就是一个多层的链表，底层是一个普通的链表，然后逐层减少，通常通过一个简单的算法实现每一层元素是下一层的元素的二分之一，这样当搜索元素时从最顶层开始搜索，可以说是另一种形式的二分查找。 ConcurrentSkipListMap的**put**(插入)： 调用doPut()方法，可以分为3大步来理解： 第一步获取前继节点后通过CAS来插入节点； 第二步对level层数进行判断，如果大于最大层数，则插入一层； 第三步插入对应层的数据。整个插入过程全部通过CAS自旋的方式保证并发情况下的数据正确性。 5. volatile &amp; Atmoic &amp; UnSafe **volatile**作用：①多线程间的可见性、②阻止指令重排序 **Atmoic系列类**提供了原子性操作，保障多线程下的安全 **UnSafe类**的作用：①内存操作、②字段的定位与修改(底层)、③线程挂起与恢复、④CAS操作(乐观锁)","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"并发编程","slug":"concurrent","permalink":"http://chaooo.github.io/tags/concurrent/"}]},{"title":"「深入JVM」内存模型JMM 与 锁机制","date":"2019-08-27T09:30:45.000Z","path":"2019/08/27/jvm-jmm.html","text":"Java 内存模型(Java Memory Model)Java Memory Model(JMM)描述了 Java 程序中各种变量(线程共享变量)的访问规则，以及在 JVM 中将变量存储到内存中和从内存中读取变量这样的底层细节(可见性,有序性,原子性)。 所有的变量都存储在主内存中 每个线程都有自己的独立的工作内存，里面保存该线程使用到的变量的副本(来自主内存的拷贝) JMM 规定： + 线程对共享变量的所有操作都必须在自己的工作内存中进行，不能直接从主内存中读写。 + 不同线程之间无法直接访问其他线程工作内存中的变量，线程间变量值的传递需要通过主内存来完成。 1. JMM-同步八种操作JMM 模型下,线程间通信必须要经过主内存。JMM 数据原子操作:lock -&gt; read -&gt; load -&gt; use -&gt; assign -&gt; store -&gt; write -&gt; unlock lock（锁定）：将主内存变量加锁，标识为线程独占状态 read（读取）：从主内存读取数据到工作内存 load（载入）：将读取的数据写入工作内存 use（使用）：将工作内存数据传递给执行引擎来计算 assign（赋值）：将计算好的值赋值给工作内存的变量 store（存储）：把工作内存数据存储到主内存 write（写入）：把 store 过来的变量值赋值给主内存的变量 unlock（解锁）：将主内存变量解锁，释放后的变量才可以被其他线程锁定。 在执行上述八种基本操作时，必须满足如下规则： 从主复制到工作,必须按顺序执行read-&gt;load操作; 从工作同步到主内存,必须按顺序执行store-&gt;write操作; 但不保证必须是连续执行 不允许read-&gt;load、store-&gt;write操作之一单独出现 assign 操作改变数据后必须同步到主内存,不允许把没有发生过 assign 操作的数据同步到主内存 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load 或 assign）的变量 一个变量在同一时刻只允许一条线程对其进行 lock 操作,lock 和 unlock 必须成对出现 lock 操作会清空工作内存中此变量的值，执行引擎使用前需要重新执行 load 或 assign 操作初始化变量的值 不允许去 unlock 一个未被锁定 或 被其他线程锁定的变量 unlock 之前，必须先同步到主内存中（执行 store 和 write 操作） 2. JMM-原子性和数据库事务中的原子性一样，满足原子性特性的操作是不可中断的，要么全部执行成功要么全部执行失败。Synchronized 能够实现：原子性(同步) 和 可见性 JMM 关于 synchronized 的两条规定： 线程解锁前，必须把共享变量的最新值刷新到主内存中 线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从内存中重新读取最新的值（注意：加锁与解锁需要是同一把锁） 线程执行互斥代码的过程： 获得互斥锁 清空工作内存 从主内存拷贝变量的最新副本到工作内存 执行代码 将更改后的共享变量的值刷新到主内存 释放互斥锁 2. JMM-可见性多个线程访问同一个共享变量时，其中一个线程对这个共享变量值的修改，其他线程能够立刻获得修改以后的值。volatile 能够实现可见性，但不保证原子性 深入来说：通过加入内存屏障和禁止重排序优化来实现的。 对 volatile 变量执行写操作时，会在写操作后加入一条 store 屏蔽指令 对 volatile 变量执行读操作时，会在读操作前加入一条 load 屏蔽指令 通俗地讲：volatile 变量在每次被线程访问时，都强迫从主内存中重读该变量的值，而当该变量发生变化时，又会强迫线程将最新的值刷新到主内存。这样任何时刻，不同的线程总能看到该变量的最新值。 线程写 volatile 变量的过程： 改变线程工作内存中 volatile 变量副本的值 将改变后的副本的值从工作内存刷新到主内存 线程读 volatile 变量的过程： 从主内存中读取 volatile 变量的最新值到线程的工作内存中 从工作内存中读取 volatile 变量的副本 2.1 happens-before 规则在 JMM 中，如果一个操作执行的结果需要对另一个操作可见，那么这 2 个操作之间必须要存在 happens-before 关系。 定义: 如果一个操作在另一个操作之前发生(happens-before),那么第一个操作的执行结果将对第二个操作可见, 而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在 happens-before 关系，并不意味着一定要按照 happens-before 原则制定的顺序来执行。如果重排序之后的执行结果与按照 happens-before 关系来执行的结果一致，那么这种重排序并不非法。 happens-before 规则： 程序次序规则：在一个线程内一段代码的执行结果是有序的。就是还会指令重排，但是随便它怎么排，结果是按照我们代码的顺序生成的不会变！ 锁定规则：一个 unLock 操作先行发生于后面对同一个锁的 lock 操作；论是单线程还是多线程，必须要先释放锁，然后其他线程才能进行 lock 操作 volatile 变量规则：就是如果一个线程先去写一个 volatile 变量，然后一个线程去读这个变量，那么这个写操作的结果一定对读的这个线程可见。 传递规则：如果操作 A 先行发生于操作 B，而操作 B 又先行发生于操作 C，则可以得出操作 A 先行发生于操作 C 线程启动规则：在主线程 A 执行过程中，启动子线程 B，那么线程 A 在启动子线程 B 之前对共享变量的修改结果对线程 B 可见 线程终止规则：在主线程 A 执行过程中，子线程 B 终止，那么线程 B 在终止之前对共享变量的修改结果在线程 A 中可见。 线程中断规则：对线程 interrupt()方法的调用先行发生于被中断线程代码检测到中断事件的发生，可以通过 Thread.interrupted()检测到是否发生中断 对象终结规则：这个也简单的，就是一个对象的初始化的完成，也就是构造函数执行的结束一定 happens-before 它的 finalize()方法。 3. JMM-有序性编译器和处理器为了优化程序性能而对指令序列进行重排序，也就是你编写的代码顺序和最终执行的指令顺序是不一致的，重排序可能会导致多线程程序出现内存可见性问题。 我们编写的源代码到最终执行的指令，会经过三种重排序: 源代码–&gt;编译器优化重排序–&gt;指令级并行重排序–&gt;内存系统重排序–&gt;最终执行的指令 3.1 as-if-serial 语义as-if-serial 语义：不管怎么重排序(编译器和处理器为了提高并行度做的优化),(单线程)程序的执行结果不会改变。编译器、runtime 和处理器都必须遵守 as-if-serial 语义。多线程中程序交错执行时, 重排序可能造成内存可见性问题, 可能会改变程序的执行结果。 有序性规则表现在以下两种场景: 线程内和线程间 线程内: 指令会按照一种“串行”(as-if-serial)的方式执行，此种方式已经应用于顺序编程语言。 线程间: 一个线程“观察”到其他线程并发地执行非同步的代码时，任何代码都有可能交叉执行。唯一起作用的约束是：对于同步方法，同步块以及 volatile 字段的操作仍维持相对有序。 As-if-serial 只是保障单线程不会出问题，所以有序性保障，可以理解为把 As-if-serial 扩展到多线程，那么在多线程中也不会出现问题 从底层的角度来看，是借助于处理器提供的相关指令内存屏障来实现的 对于 Java 语言本身来说，Java 已经帮我们与底层打交道，我们不会直接接触内存屏障指令，java 提供的关键字 synchronized 和 volatile，可以达到这个效果，保障有序性（借助于显式锁 Lock 也是一样的，Lock 逻辑与 synchronized 一致） 3.2 著名的双检锁(double-checked locking)模式实现单例1234567891011121314151617public class Singleton &#123; // volatile保证happens-before规则,重排序被禁止 private volatile static Singleton INSTANCE = null; private Singleton() &#123;&#125; public Singleton getInstance() &#123; // 实例没创建,才进入内部的synchronized代码块 if (null == INSTANCE) &#123; synchronized (Singleton.class) &#123; // 判断其他线程是否已经创建实例 if (null == INSTANCE) &#123; INSTANCE = new Singleton(); &#125; &#125; &#125; return INSTANCE; &#125;&#125; 如果不用 volatile 修饰 INSTANCE,可能造成访问的是一个初始化未完成的对象; 使用了 volatile 关键字后，重排序被禁止，所有的写（write）操作都将发生在读（read）操作之前。 4. 锁机制 锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁（但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级）。 锁的状态是通过对象监视器在对象头中的字段来表明的。 四种状态会随着竞争的情况逐渐升级，而且是不可逆的过程，即不可降级。 这四种状态都不是 Java 语言中的锁，而是 Jvm 为了提高锁的获取与释放效率而做的优化(使用 synchronized 时)。 4.1 对象头 Mark Mark Word,对象头的标记,32 位: 描述对象的 hash,锁信息,垃圾回收标记,分代年龄 指向锁记录的指针 指向 monitor 的指针 GC 标记 偏向锁线程 ID 4.2 偏向锁Java 偏向锁(Biased Locking)是 Java6 引入的一项多线程优化 大部分情况锁是没有竞争的,所以可以通过偏向锁来提高性能; 所谓偏向,就是偏心,即锁会偏向于当前已经占有锁的线程,总是由同一线程多次获得; 会在对象头和栈帧中的锁记录里存储锁偏向的线程 ID 只要没有竞争,获得偏向锁的线程,在将来进入同步块,不需要做同步 当其他线程请求相同的锁时,偏向模式结束 -XX:+UseBiasedLocking(默认开启) 在竞争激烈的场合,偏向锁会增加系统负担 4.3 轻量级锁轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。 普通的锁处理性能不够理想,轻量级锁是一种快速的锁定方法. 过程: 如果对象没有被锁定: 将对象头的 Mark 指针保存到锁对象中 将对象头设置为指向锁的指针(在线程栈空间中) 如果轻量级锁失败,表示存在竞争,升级为重量级锁(常规锁) 在没有锁竞争的情况下,减少传统锁使用 OS 互斥量产生的性能损耗 在竞争激烈的场合,轻量级锁会多做很多额外操作,导致性能下降 4.4 自旋锁自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗 CPU。 当竞争存在时,如果线程可以很快获得锁,那么可以不在 OS 层挂起线程,让线程做几个空操作(自旋) 如果同步块很长,自旋失败,会降低系统性能 如果同步块很短,自旋成功,节省线程挂起切换时间,提升系统性能 4.5 重量级锁重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。 4.6 synchronized 的执行过程： 检测 Mark Word 里面是不是当前线程的 ID，如果是，表示当前线程处于偏向锁 如果不是，则使用 CAS 将当前线程的 ID 替换 Mard Word，如果成功则表示当前线程获得偏向锁，置偏向标志位 1 如果失败，则说明发生竞争，撤销偏向锁，进而升级为轻量级锁。 当前线程使用 CAS 将对象头的 Mark Word 替换为锁记录指针，如果成功，当前线程获得锁 如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 如果自旋成功则依然处于轻量级状态。 如果自旋失败，则升级为重量级锁。 以上几种锁都是 JVM 自己内部实现，当我们执行 synchronized 同步块的时候 jvm 会根据启用的锁和当前线程的争用情况，决定如何执行同步操作； 5. Java 语言层面对锁的优化 减少锁持有时间 不需要同步执行的代码，能不放在同步快里面执行就不要放在同步快内，可以让锁尽快释放； 减少锁的粒度 它的思想是将物理上的一个锁，拆成逻辑上的多个锁，增加并行度，从而降低锁竞争。它的思想也是用空间来换时间； java 中很多数据结构都是采用这种方法提高并发操作的效率： ConcurrentHashMap: 使用 Segment 数组,Segment 继承自 ReenTrantLock，所以每个 Segment 就是个可重入锁，每个 Segment 有一个 HashEntry&lt; K,V &gt;数组用来存放数据，put 操作时，先确定往哪个 Segment 放数据，只需要锁定这个 Segment，执行 put，其它的 Segment 不会被锁定；所以数组中有多少个 Segment 就允许同一时刻多少个线程存放数据，这样增加了并发能力。 LongAdder:实现思路也类似 ConcurrentHashMap，LongAdder 有一个根据当前并发状况动态改变的 Cell 数组，Cell 对象里面有一个 long 类型的 value 用来存储值;开始没有并发争用的时候或者是 cells 数组正在初始化的时候，会使用 cas 来将值累加到成员变量的 base 上，在并发争用的情况下，LongAdder 会初始化 cells 数组，在 Cell 数组中选定一个 Cell 加锁，数组有多少个 cell，就允许同时有多少线程进行修改，最后将数组中每个 Cell 中的 value 相加，在加上 base 的值，就是最终的值；cell 数组还能根据当前线程争用情况进行扩容，初始长度为 2，每次扩容会增长一倍，直到扩容到大于等于 cpu 数量就不再扩容，这也就是为什么 LongAdder 比 cas 和 AtomicInteger 效率要高的原因，后面两者都是 volatile+cas 实现的，他们的竞争维度是 1，LongAdder 的竞争维度为“Cell 个数+1”为什么要+1？因为它还有一个 base，如果竞争不到锁还会尝试将数值加到 base 上； 拆锁的粒度不能无限拆，最多可以将一个锁拆为当前 CPU 数量即可； 锁粗化 大部分情况下我们是要让锁的粒度最小化，锁的粗化则是要增大锁的粒度(如:循环内的操作); 锁分离 使用读写锁: ReentrantReadWriteLock 是一个读写锁，读操作加读锁，可以并发读，写操作使用写锁，只能单线程写； 读写分离: CopyOnWriteArrayList 、CopyOnWriteArraySet CopyOnWrite 容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行 Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对 CopyOnWrite 容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以 CopyOnWrite 容器也是一种读写分离的思想，读和写不同的容器 CopyOnWrite 并发容器用于读多写少的并发场景，因为，读的时候没有锁，但是对其进行更改的时候是会加锁的，否则会导致多个线程同时复制出多个副本，各自修改各自的； LinkedBlockingQueue: LinkedBlockingQueue 也体现了这样的思想，在队列头入队，在队列尾出队，入队和出队使用不同的锁，相对于 LinkedBlockingArray 只有一个锁效率要高； 锁消除 在即时编译时,如果发现不可能被共享的对象,则可以消除对象的锁操作 无锁(如 CAS) 如果需要同步的操作执行速度非常快，并且线程竞争并不激烈，这时候使用 CAS 效率会更高，因为加锁会导致线程的上下文切换，如果上下文切换的耗时比同步操作本身更耗时，且线程对资源的竞争不激烈，使用 volatiled+CAS 操作会是非常高效的选择； 消除缓存行的伪共享 除了我们在代码中使用的同步锁和 jvm 自己内置的同步锁外，还有一种隐藏的锁就是缓存行，它也被称为性能杀手。在多核 cup 的处理器中，每个 cup 都有自己独占的一级缓存、二级缓存，甚至还有一个共享的三级缓存，为了提高性能，cpu 读写数据是以缓存行为最小单元读写的；32 位的 cpu 缓存行为 32 字节，64 位 cup 的缓存行为 64 字节，这就导致了一些问题。 6. CAS 与原子类CAS 即Compare and Swap翻译过来就是比较并替换, 它体现了一种乐观锁的思想 (synchronized 为悲观锁思想); 结合 CAS 和 volatile 可以实现无锁并发(非阻塞同步),适用于竞争不激烈,多核 CPU 的场景下(竞争激烈,重试频繁发生会影响效率); CAS 算法涉及到三个操作数: 内存值 V, 旧值 A, 新值 B; 当且仅当 V&#x3D;&#x3D;A 时，CAS 用新值 B 来更新 V，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 CAS 底层依赖一个 Unsafe 类来直接调用操作系统底层的 CAS 指令; 6.1 Unsafe 类java 中 CAS 操作依赖于 Unsafe 类，Unsafe 类所有方法都是 native 的，直接调用操作系统底层资源执行相应任务，它可以像 C 一样操作内存指针，是非线程安全的。 Unsafe 里的 CAS 操作相关实现: compareAndSwapObject,compareAndSwapInt,compareAndSwapLong 12345//第一个参数o为给定对象，offset为对象内存的偏移量，通过这个偏移量迅速定位字段并设置或获取该字段的值，//expected表示期望值，x表示要设置的值，下面3个方法都通过CAS原子指令执行操作。public final native boolean compareAndSwapObject(Object o, long offset,Object expected, Object x);public final native boolean compareAndSwapInt(Object o, long offset,int expected,int x);public final native boolean compareAndSwapLong(Object o, long offset,long expected,long x); 6.2 原子操作类并发包 JUC(java.util.concurrent)中的原子操作类(Atomic 系列),底层是基于CAS + volatile实现的. AtomicBoolean：原子更新布尔类型 AtomicInteger：原子更新整型 AtomicLong：原子更新长整型 下面看 AtomicInteger 类的部分源码： 1234567891011121314public class AtomicInteger extends Number implements java.io.Serializable&#123; //获取指针类Unsafe private static final Unsafe unsafe = Unsafe.getUnsafe(); //省略...获取内存偏移量等 //如果当前值为expect，则设置为update(当前值指的是value变量) public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; //当前值加1返回旧值，底层CAS操作 public final int getAndIncrement() &#123; return unsafe.getAndAddInt(this, valueOffset, 1); &#125; //省略...其他方法&#125; AtomicInteger 基本是基于 Unsafe 类中 CAS 相关操作实现的，是无锁操作。再看 Unsafe 类中的 getAndAddInt()方法，该方法执行一个 CAS 操作，保证线程安全。 12345678//Unsafe类中的getAndAddInt方法(JDK8)public final int getAndAddInt(Object o, long offset, int delta) &#123; int v; do &#123; v = getIntVolatile(o, offset); &#125; while (!compareAndSwapInt(o, offset, v, v + delta)); return v;&#125; 可看出 getAndAddInt 通过一个 while 循环不断的重试更新要设置的值，直到成功为止，调用的是 Unsafe 类中的 compareAndSwapInt 方法，是一个 CAS 操作方法。 6.3 CAS 操作中可能会带来的 ABA 问题ABA 问题是指在 CAS 操作时，其他线程将变量值 A 改为了 B，但是又被改回了 A，等到本线程使用期望值 A 与当前变量进行比较时，发现变量 A 没有变，于是 CAS 就将 A 值进行了交换操作，但是实际上该值已经被其他线程改变过，这与乐观锁的设计思想不符合。 无法正确判断这个变量是否已被修改过，一般称这种情况为 ABA 问题。 ABA 问题一般不会有太大影响，产生几率也比较小。但是并不排除极特殊场景下会造成影响，因此需要解决方法： AtomicStampedReference 类 AtomicMarkableReference 类 AtomicStampedReference 类: 一个带有时间戳的对象引用，每次修改时，不但会设置新的值，还会记录修改时间。在下一次更新时，不但会对比当前值和期望值，还会对比当前时间和期望值对应的修改时间，只有二者都相同，才会做出更新。解决了反复读写时，无法预知值是否已被修改的窘境。 底层实现为：一个键值对 Pair 存储数据和时间戳，并构造 volatile 修饰的私有实例；两者都符合预期才会调用 Unsafe 的 compareAndSwapObject 方法执行数值和时间戳替换。 AtomicMarkableReference 类: 一个 boolean 值的标识，true 和 false 两种切换状态表示是否被修改。不靠谱。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"JVM","slug":"JVM","permalink":"http://chaooo.github.io/tags/JVM/"}]},{"title":"「深入JVM」 类文件结构 与 类加载机制","date":"2019-08-25T11:29:48.000Z","path":"2019/08/25/jvm-class.html","text":"1. 类文件结构Class 文件是一组以 8 位字节为基础单位的二进制流，各个数据严格按照顺序紧凑的排列在 Class 文件中，中间无任何分隔符，这使得整个 Class 文件中存储的内容几乎全部都是程序运行的必要数据，没有空隙存在。当遇到需要占用 8 位字节以上空间的数据项时，会按照高位在前的方式分割成若干个 8 位字节进行存储。Java 虚拟机规范规定 Class 文件格式采用一种类似与 C 语言结构体的伪结构体来存储数据，这种伪结构体中只有两种数据类型：无符号数和表。 无符号数：属于基本数据类型，以 u1、u2、u4、u8 来代表 1 个字节、2 个字节、4 个字节、8 个字节的无符号数， 无符号数可以用来描述数字、索引引用、数量值或者按照 UTF-8 编码构成字符串值。 表：由多个无符号数或者其他表作为数据项构成的复合数据类型，所有表都习惯性地以「_info」结尾。表用于描述有层次关系的复合结构的数据，整个 Class 文件就是一张表。 根据 Java 虚拟机规范，类文件由单个 ClassFile 结构组成： 123456789101112131415161718ClassFile &#123; u4 magic; //Class文件的标志(魔数) u2 minor_version; //Class的小版本号 u2 major_version; //Class的大版本号 u2 constant_pool_count; //常量池的数量 cp_info constant_pool[constant_pool_count-1];//常量池 u2 access_flags; //Class的访问标记 u2 this_class; //当前类 u2 super_class; //父类 u2 interfaces_count; //接口 u2 interfaces[interfaces_count];//一个类可以实现多个接口 u2 fields_count; //Class文件的字段属性 field_info fields[fields_count]; //一个类会可以有个字段 u2 methods_count; //Class文件的方法数量 method_info methods[methods_count];//一个类可以有个多个方法 u2 attributes_count; //此类的属性表中的属性数 attribute_info attributes[attributes_count];//属性表集合&#125; 1.1 魔数 (Magic Number) Class 文件的0~3 字节(前四个字节: ca fe ba be) 作用: 确定这个文件是否为一个能被虚拟机接收的 Class 文件 1.2 Class 文件版本 4~7 字节, 其中 45 次版本号,67 主版本号(如 jdk8 主版本号是: 00 34) 1.3 常量池 8~9 字节表示 16 进制常量池数量,其后紧跟具体常量池, 常量池的数量是 constant_pool_count-1（常量池计数器是从 1 开始计数的，将第 0 项常量空出来是有特殊考虑的，索引值为 0 代表“不引用任何一个常量池项”） 常量池主要存放两大常量: 字面量和符号引用 字面量: Java 语言层面的常量概念(String,final 等) 符号引用: 编译原理方面的概念(类和接口的全限定名\\字段的名称和描述符\\方法的名称和描述符) 常量池中每一项常量都是一个表，这14 种表有一个共同的特点：开始的第一位是一个 u1 类型的标志位 -tag 来标识常量的类型，代表当前这个常量属于哪种常量类型 .class 文件可以通过javap -v class类名 指令来看一下其常量池中的信息(javap -v class类名-&gt; temp.txt ：将结果输出到 temp.txt 文件) 1.4 类的访问标志与继承信息 在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口，是否为 public 或者 abstract 类型，如果是类的话是否声明为 final 等等. access_flags 中一共有 16 个标志位可以使用，当前只定义了其中的 8 个，没有使用到的标志位要求一律为 0。 1.5 当前类索引(this),父类索引(super)与接口索引集合(interfaces) 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，由于 Java 语言的单继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。 接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按 implents(如果这个类本身是接口的话则是 extends) 后的接口顺序从左到右排列在接口索引集合中。 1.6 成员变量信息(Feild) 字段表（field info）用于描述接口或类中声明的变量。字段包括类级变量以及实例变量，但不包括在方法内部声明的局部变量。 字段信息包括：字段的作用域（public、private、protected 修饰符）、是实例变量还是类变量（static 修饰符）、可变性（final）、并发可见性（volatile 修饰符，是否强制从主内存读写）、可否被序列化（transient 修饰符）、字段数据类型（基本类型、对象、数组）、字段名称，以上修饰符都是布尔类型。 方法和字段的描述符作用是用来描述字段的数据类型、方法的参数列表（包括数量、类型以及顺序）和返回值。 根描述规则，基本数据类型（byte、char、double、float、int、long、short、boolean）以及代表无返回值的 void 类型都用一个大写字符来表示，对象类型使用字符 L 加对象的全限定名来表示。 B: 基本类型 byte C: 基本类型 char D: 基本类型 double F: 基本类型 float I: 基本类型 J: 基本类型 long S: 基本类型 short Z: 基本类型 boolean V: 特殊类型 void L: 对象类型，如 Ljava&#x2F;lang&#x2F;Object 1.7 方法信息(Method) methods_count 表示方法的数量，而 method_info 表示的方法表。 Class 文件存储格式中对方法的描述与对字段的描述几乎采用了完全一致的方式。方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项。 1.8 附加属性信息 attributes_count表示属性表中的属性个数, attribute_info 表示属性表 在 Class 文件，字段表，方法表中都可以携带自己的属性表集合，以用于描述某些场景专有的信息。与 Class 文件中其它的数据项目要求的顺序、长度和内容不同，属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写 入自己定义的属性信息，Java 虚拟机运行时会忽略掉它不认识的属性。 2. 字节码指令Java 字节码指令就是 Java 虚拟机能够识别、可执行的指令，可以说是 Jvm 的最小执行单元。javac 命令会将 Java 源文件编译成字节码文件，即.class 文件，其中就包含了大量的字节码指令，javap 命令可以解析字节码(.class 文件)，将字节码内部逻辑以可读的方式呈现出来 (javap -v -p HelloWorld)。 按指令的功能分为如下几类： 存储和加载类指令：主要包括 load 系列(将一个局部变量加载到操作数栈)、store 系列(将一个数值从操作数栈存储到局部变量表)和 ldc&#x2F;push&#x2F;const 系列(将一个常量加载到操作数栈)，主要用于在局部变量表、操作数栈和常量池三者之间进行数据调度； 例如: iload_0表示从当前栈帧局部变量表中 0 号位置取 int 类型的数值加载到操作数栈 对象操作指令（创建与读写访问）：比如我们刚刚的 putfield 和 getfield 就属于读写访问的指令，此外还有 putstatic&#x2F;getstatic，还有 new 系列指令，以及 instanceof 等指令。 操作数栈管理指令：如 pop 和 dup，他们只对操作数栈进行操作。 类型转换指令和运算指令：如 add(加)&#x2F;sub(减)&#x2F;mul(乘)&#x2F;div(除)&#x2F;l2i&#x2F;d2f 等系列指令，实际上这类指令一般也只对操作数栈进行操作。 控制跳转指令：这类里包含常用的 if 系列指令以及 goto 类指令。 方法调用和返回指令：主要包括 invoke 系列指令和 return 系列指令。这类指令也意味这一个方法空间的开辟和结束，即 invoke 会唤醒一个新的 java 方法小宇宙（新的栈和局部变量表），而 return 则意味着这个宇宙的结束回收。 从指令操作的数据类型来讲：指令开头或尾部的一些字母，就往往表明了它所能操作的数据类型： a 对应对象，表示指令操作对象性数据，比如 aload 和 astore、areturn 等等。 i 对应整形。也就有 iload，istore 等 i 系列指令。 f 对应浮点型，l 对应 long，b 对应 byte，d 对应 double，c 对应 char。 ia 对应 int array，aa 对应 object array，da 对应 double array。 3. 编译期处理(语法糖)语法糖: 指 Java 编译器把.java 源码编译为.class 字节码过程中,自动生成和转换的一些代码. 如:默认构造器,自动拆装箱等. 默认构造器: public class Candy&#123;&#125; 编译后为: public class Candy&#123;public Candy()&#123;super();&#125;&#125; 自动拆装箱: Integer x=1;int y=x; 编译后为: Integer x=Integer.valueOf(1);int y=x.intValue(); 泛型擦除: 擦除的是字节码上的泛型信息. 泛型反射: 通过反射获得泛型信息 可变参数: String... args 可以是一个String[] args foreach: 集合相当于获取迭代器 Iterator switch: Jdk7 开始可以配合 String 和枚举 switch-String: 执行了两遍 switch,第一遍根据字符串的 hashCode 和 equals 将字符串转换为相应的 byte 类型,第二遍利用 byte 执行比较. switch-枚举: 会为当前类生成一个静态内部类(合成类,仅 JVM 使用,对我们不可见),用来映射枚举类的枚举编号(从 0 开始)与数组元素的关系,数组大小即为枚举元素的个数,里面存储 case 用来对比的数字,根据这个数字执行 switch 枚举类: 继承 Enum 并且用 final 修饰类,构造方法私有,枚举量被编译成本类的 final 类变量,定义私有静态枚举量数组$VALUES,静态方法 values()用来返回定义的枚举量数组的 clone(),静态方法 valueOf()调用父类 valueOf(本类.class,名称)根据类型和名称得到相应实例 try-with-resources: 无论 try 块的异常还是关闭资源时的异常都不会丢。可以在 try-with-resources 语句中同时处理多个资源。 在 Java 7&#x2F;8 ，try-with-resources 语句中必须声明要关闭的资源。通过这种方式声明的资源属于隐式 final。 Java 9 中甚至能使用预先创建的资源，只要所引用的资源声明为 final 或者是 effective final。 在幕后施展魔法的是 AutoCloseable 或者 Closeable 接口，它们与 try-with-resources 语句协同工作。 重写桥接: 子类重写方法返回值可以是父类返回值的子类,JVM 内部使用了桥接方法(synthetic bridge 修饰)重写父类方法并返回子类重写的同名方法,并且没有命名冲突,仅对 jvm 可见. 匿名内部类: 内部创建了 final 修饰的实现类, 匿名内部类引用局部变量时,局部变量必须是 final 的:因为内部创建实现类时,将值赋给其对象的 valx 属性,valx 属性没有机会再跟着一起变化. 4. 类加载阶段 隐式加载：new 显式加载：loadClass、forName 等(需要调用 Class 的 newInstance 方法获取实例) 类的装载阶段：**加载 --&gt; 链接 --&gt; 初始化** 加载：通过 Classloader 加载 class 文件字节码，生成 class 对象 链接：校验–&gt;准备–&gt;解析 校验：检查加载的 Class 的正确性和安全性 准备：为变量分配存储空间并设置类变量初始值 解析：JVM 将常量池内的符号引用转换为直接引用 初始化：执行类变量赋值和静态代码块 4.1 加载 将类的字节码载入方法区中,内部采用 C++的 instanceKlass 描述 java 类, 它的重要 field 有: _java_mirror:Java 类的镜像, _super:父类, _field:成员变量, _methods:方法, _constants:常量池, _class_loader:类加载器, _vtable:虚方法表, _itable:接口方法表 如果这个类还有父类没加载,先加载父类 加载和链接可能是交替运行的 instanceKlass这样的元数据是存储在方法区(元空间),但_java_mirror存储在堆中; 可通过 HSDB 工具查看. 4.2 链接 验证: 验证类是否符合 JVM 规范,安全性检查 准备: 为 static 变量分配空间,设置默认值 jdk7 开始, static 变量存储于_java_mirror末尾, jdk7 之前是 instanceKlass 末尾. static 变量分配空间和赋值是两个步骤, 分配空间在准备阶段完成,赋值在初始化阶段完成 如果 static 变量是 final 的基本类型或字符串常量,那么编译阶段值就确定了,赋值在准备阶段完成 如果 static 变量是 final 的引用类型,那么赋值还是会在初始化阶段完成 解析: 将常量池中的符号引用解析为直接引用(确切知道类,方法,属性在内存中的位置) 4.3 初始化 初始化即调用&lt;cinit&gt;()V方法,虚拟机会保证这个类的[构造方法]线程安全 发生的时机: 概括的说,类初始化是[懒惰的] main 方法所在的类的,总会被首先初始化 首次访问这个类的静态变量或静态方法时 子类初始化, 如果父类没有初始化,会引发 子类访问父类静态变量, 只会触发父类的初始化 Class.forName 和 new 操作 导致初始化 不会导致类初始化的情况 访问类的 static final 静态常量(基本类型和字符串常量)不会触发初始化 类对象.class 不会 创建该类的数组 不会 类加载器的 loadClass 方法 不会 Class.forName 的第二个参数为 false 时 不会 4.4 应用实例-懒惰初始化单例模式(线程安全)1234567891011class Singleton&#123; private Singleton()&#123;&#125; // 内部类中保存单例 private static class LazyHolder&#123; private static final Singleton SINGLETON = new Singleton(); &#125; // 第一次调用getInstance,才会导致内部类加载和初始化其静态成员 public static Singleton getInstance()&#123; return LazyHolder.SINGLETON; &#125;&#125; 5. 类加载器以 JDK8 为例: 名称 加载哪的类 说明 Bootstrap ClassLoader JAVA_HOME&#x2F;jre&#x2F;lib 启动类加载器, 最顶层, 打印显示为 null Extension ClassLoader JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext 扩展类加载器, 第二级, 打印显示为$ExtClassLoader Application ClassLoader classpath 应用程序类加载器, 第三级, 打印显示为$AppClassLoader 自定义类加载器 自定义 上级为 Application 5.1 类加载器-双亲委派机制 类加载器在接到加载类的请求时，首先将加载任务委托给上级加载器，依次递归，如果上级加载器可以完成类加载任务，就成功返回；只有上级加载器无法完成此加载任务时，才自己去加载。 这种双亲委派模式的好处，一个可以避免类的重复加载，另外也避免了 java 的核心 API 被篡改。 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * loadClass方法的实现方式 */protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded //「1」 检查该类是否已经加载 Class c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; //「2」 有上级的话,委派上级 loadClass c = parent.loadClass(name, false); &#125; else &#123; //「3」 如果没有上级了(ExtClassLoader),则委派BootstrapClassLoader c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order to find the class. long t1 = System.nanoTime(); //「4」 每一级都找不到,调用findClass(每个类加载器自己扩展)来加载 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 5.2 线程上下文类加载器 Java 提供了很多服务提供者接口(Service Provider Interface，SPI),允许第三方为这些接口提供实现(常见的 SPI 有 JDBC、JCE、JNDI、JAXP 和 JBI 等)。 SPI 接口中的代码经常需要加载具体的实现类; SPI 的接口由 Java 核心库来提供，实现类可能是作为 Java 应用所依赖的 jar 包被包含进来，可以通过类路径（CLASSPATH）来找到。 SPI 的接口是 Java 核心库的一部分，是由引导类加载器来加载的；引导类加载器是无法找到 SPI 的实现类的,这时候需要抛弃双亲委派加载链模式，使用线程上下文里的类加载器加载类。 类 java.lang.Thread 中的方法 getContextClassLoader()和 setContextClassLoader(ClassLoader cl)用来获取和设置线程的上下文类加载器。 Java 默认的 线程上下文类加载器 是 应用程序类加载器(AppClassLoader)。 5.3 何时使用 Thread.getContextClassLoader()? 总的说来动态加载资源时，一般只有两种选择，当前类加载器和线程上下文类加载器。当前类加载器是指当前方法所在类的加载器。这个类加载器是运行时类解析使用的加载器，Class.forName(String)和 Class.getResource(String)也使用该类加载器。代码中 X.class 的写法使用的类加载器也是这个类加载器。 该如何选择类加载器？ 如若代码是限于某些特定框架，这些框架有着特定加载规则，则不要做任何改动，让框架开发者来保证其工作（比如应用服务器提供商，尽管他们并不能总是做对）。如在 Web 应用和 EJB 中，要使用 Class.gerResource 来加载资源。 在其他情况下，我们可以自己来选择最合适的类加载器。可以使用策略模式来设计选择机制。其思想是将“总是使用上下文类加载器”或者“总是使用当前类加载器”的决策同具体实现逻辑分离开。往往设计之初是很难预测何种类加载策略是合适的，该设计能够让你可以后来修改类加载策略。 一般来说，上下文类加载器要比当前类加载器更适合于框架编程，而当前类加载器则更适合于业务逻辑编程。 5.4 类加载器与 Web 容器以 Apache Tomcat 来说，每个 Web 应用都有一个对应的类加载器实例。该类加载器也使用代理模式，所不同的是它是首先尝试去加载某个类，如果找不到再代理给父类加载器。这与一般类加载器的顺序是相反的。这是 Java Servlet 规范中的推荐做法，其目的是使得 Web 应用自己的类的优先级高于 Web 容器提供的类。这种代理模式的一个例外是：Java 核心库的类是不在查找范围之内的。这也是为了保证 Java 核心库的类型安全。 绝大多数情况下，Web 应用的开发人员不需要考虑与类加载器相关的细节。下面给出几条简单的原则： 每个 Web 应用自己的 Java 类文件和使用的库的 jar 包，分别放在 WEB-INF&#x2F;classes 和 WEB-INF&#x2F;lib 目录下面。 多个应用共享的 Java 类文件和 jar 包，分别放在 Web 容器指定的由所有 Web 应用共享的目录下面。 当出现找不到类的错误时，检查当前类的类加载器和当前线程的上下文类加载器是否正确。 5.5 自定义类加载器 什么时候需要自定义类加载器 加载非 classpath 路径的任意路径类文件 都是通过接口来使用实现,希望解耦时,常用于框架设计 这些类希望予以隔离,不同应用的同名类都可以加载,不冲突,常见于 tomcat 容器 如何自定义类加载器 继承 ClassLoader 类 重写 findClass(String className)方法 读取(加载)类文件的字节码。 调用 ClassLoader 超类的 defineClass 方法，向虚拟机提供字节码。 使用者调用该自定义类加载器的 loadClass 方法 1234567891011121314151617181920212223import java.io.IOException;import java.nio.file.Files;import java.nio.file.Paths;public class MyClassLoader extends ClassLoader &#123; /** * @param name 类名称 */ @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; try &#123; String cname = &quot;E:\\\\myclasspath\\\\&quot; + name.replace(&#x27;.&#x27;, &#x27;/&#x27;) + &quot;.class&quot;; byte[] classBytes = Files.readAllBytes(Paths.get(cname)); Class&lt;?&gt; cl = defineClass(name, classBytes, 0, classBytes.length); if (cl == null) &#123; throw new ClassNotFoundException(name); &#125; return cl; &#125; catch (IOException e) &#123; System.out.print(e); throw new ClassNotFoundException(name); &#125; &#125;&#125; 6. 运行期 JVM 自动优化Java 程序最初是通过解释器进行解释执行的，当程序需要迅速启动和执行时，解释器可以首先发挥作用，省去编译时间，立即执行；当程序运行后，随着时间的推移，编译器逐渐发挥作用，把越来越多的代码编译成本地代码，获得更高的执行效率。解释执行节约内存，编译执行提升效率。 同时，解释器可以作为编译器激进优化时的一个“逃生门”，让编译器根据概率选择一些大多数时候都能提升运行速度的优化手段，当激进优化的假设不成立，则通过逆优化退回到解释状态继续执行。HotSpot 虚拟机中内置了两个即时编译器，分别称为**Client Compiler(C1 编译器)和Server Compiler(C2 编译器)**，默认采用 Interpreter(解释器)与其中一个编译器直接配合的方式工作，使用哪个编译器取决于虚拟机运行的模式，也可以自己去指定。 分层编译策略, JVM 将执行状态分成了 5 个层次: 1. 0 层, 解释执行 2. 1 层, 使用 C1 即时编译器编译执行(不带 profiling) 3. 2 层, 使用 C1 即时编译器编译执行(带基本的 profiling) 4. 3 层, 使用 C1 即时编译器编译执行(带完全的 profiling) 5. 4 层, 使用 C2 即时编译器编译执行 profiling 是指在运行过程中收集一些程序执行状态的数据,例如[方法的调用次数],[循环的回边次数]等 即时编译器(JIT)与解释器的区别 解释器是将字节码解释为机器码,下次即便遇到相同的字节码,仍会执行重复的解释 JIT 是将一些字节码编译为机器码并存入 CodeCache,下次遇到相同的代码,直接执行,无需再编译 解释器是将字节码解释为针对所有平台都通用的机器码 JIT 会根据平台类型,生成平台特定的机器码 对于占据大部分的不常用的代码,我们无需耗费时间将其编译成机器码,而是采用解释执行的方式运行;另一方面,对于占据小部分的热点代码,我们则可以将其编译成机器码,以达到理想的运行速度; 执行效率: Interpreter &lt; C1 &lt; C2, 总的目标是发现热点代码(hotpot 名称的由来)优化之. 6.1 公共子表达式消除如果一个表达式 E 已经计算过了，并且先前的计算到现在 E 中所有变量的值都没有发生变化，那么 E 的这次出现就成为了公共表达式，可以直接用之前的结果替换。例：int d &#x3D; (c _ b) _ 12 + a + (a + b _ c) &#x3D;&gt; int d &#x3D; E _ 12 + a + (a + E) 6.2 数组边界检查消除Java 语言中访问数组元素都要进行上下界的范围检查，每次读写都有一次条件判定操作，这无疑是一种负担。编译器只要通过数据流分析就可以判定循环变量的取值范围永远在数组长度以内，那么整个循环中就可以把上下界检查消除，这样可以省很多次的条件判断操作。 6.3 方法内联方法内联能去除方法调用的成本，同时也为其他优化建立了良好的基础，因此各种编译器一般会把内联优化放在优化序列的最靠前位置，然而由于 Java 对象的方法默认都是虚方法，在编译期无法确定方法版本，就无法内联。 因此方法调用都需要在运行时进行多态选择，为了解决虚方法的内联问题，Java 虚拟机团队引入了“类型继承关系分析(CHA)”的技术。 在内联时，若是非虚方法，则可以直接内联 遇到虚方法，首先根据 CHA 判断此方法是否有多个目标版本，若只有一个，可以直接内联，但是需要预留一个“逃生门”，称为守护内联，若在程序的后续执行过程中，加载了导致继承关系发生变化的新类，就需要抛弃已经编译的代码，退回到解释状态执行，或者重新编译。 若 CHA 判断此方法有多个目标版本，则编译器会使用“内联缓存”，第一次调用缓存记录下方法接收者的版本信息，并且每次调用都比较版本，若一致则可以一直使用，若不一致则取消内联，查找虚方法表进行方法分派。 6.4 逃逸分析分析对象动态作用域，当一个方法被定以后，它可能被外部方法所引用，称为方法逃逸，甚至还有可能被外部线程访问到，称为线程逃逸。 若能证明一个对象不会逃逸到方法或线程之外，这可以通过栈上分配、同步消除、标量替换来进行优化。 栈上分配：如果确定一个对象不会逃逸，则可以让它分配在栈上，对象所占用的内存空间就可以随栈帧出栈而销毁。这样可以减小垃圾收集系统的压力。 同步消除：线程同步相对耗时，如果确定一个变量不会逃逸出线程，那这个变量的读写不会有竞争，则对这个变量实施的同步措施也就可以消除掉。 标量替换：如果逃逸分析证明一个对象不会被外部访问，并且这个对象可以被拆散的话，那么程序真正执行的时候可以不创建这个对象，改为直接创建它的成员变量，这样就可以在栈上分配。 7. 反射机制简单说，反射机制是程序在运行时能够获取自身的信息。在 java 中，只要给定类的名字，那么就可以通过反射机制来获得类的所有信息。Class 反射对象描述的是类的语义结构，通过 class 对象，可以获取构造器，成员变量，方法等类元素的反射对象，并且可以用编程的方法通过这些反射对象对目标对象进行操作。这些反射类在 java.lang.reflect 包中定义，下面是最主要的三个类： Constructor：类的构造函数反射类： 通过 Class#getConstructors()方法可以获得类的所有构造函数的反射对象数组。 其中最主要的方法是 newInstance(Object[] args),通过该方法可以创建一个对象类的实例，功能和 new 一样。在 jdk5.0 之后，提供了 newInstance(Object…args)更为灵活。 Method：类方法的反射类。 通过 Class#getDeclaredMethods()方法可以获取所有方法的反射类对象数组 Method[].其中最主要的方法是: invoke(String name,class parameterTypes),和 invoke(Object obj,Object…args)。同时也还有很多其他方法 Class getReturnType（）：获取方法的返回值类型 Class[] getParameterTypes（）：获取方法的参数数组 Field：类成员变量的反射类， 通过 Class#getDeclareFields（）可以获取类成员变量反射的数组。 Class#getDeclareField（String name）获取某特定名称的反射对象。 最主要的方法是：set(Object obj,Object value),为目标对象的成员变量赋值。如果是基础类型还可以这样赋值 setInt(),setString()… java 还提供了包的反射类和注解的反射类。 总结:java 反射体系保证了通过程序化的方式访问目标对象的所有元素，对于 private 和 protected 成员变量或者方法，也是可以访问的。 7.1 反射中，Class.forName 和 classloader 的区别 Class.forName()得到的 Class 是完成初始化的 而 ClassLoader.loadClass()得到的 Class 是还没有链接的。 Spring IoC 为了加快初始化速度，因此大量使用了延时加载技术。而使用 classloader 不需要执行类中的初始化代码，可以加快加载速度，把类的初始化工作留到实际使用到这个类的时候。 7.2 哪里用到反射机制？ JDBC 中，利用反射动态加载了数据库驱动程序。 Web 服务器中利用反射调用了 Sevlet 的服务方法。 Eclispe 等开发工具利用反射动态刨析对象的类型与结构，动态提示对象的属性和方法。 很多框架都用到反射机制，注入属性，调用方法，如 Spring。 7.3 反射机制的优缺点？优点：可以动态执行，在运行期间根据业务功能动态执行方法、访问属性，最大限度发挥了 java 的灵活性。缺点：对性能有影响，这类操作总是慢于直接执行 java 代码。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"JVM","slug":"JVM","permalink":"http://chaooo.github.io/tags/JVM/"}]},{"title":"「深入JVM」 运行时数据区 与 垃圾回收机制","date":"2019-08-23T15:15:19.000Z","path":"2019/08/23/jvm-stack.html","text":"Java 虚拟机运行时数据区 程序计数器（Program Counter Register） 本地方法栈（Native Method Stack） Java 虚拟机栈（VM Stack） Java 堆（Heap）（线程共享） 方法区（Method Area）（线程共享） Java 运行过程 Java源代码 经过Javac编译成 字节码（bytecode).class文件; 在运行时，通过 虚拟机(JVM)内嵌的解释器 将字节码转换成为最终的机器码。 常见的 JVM，都提供了 JIT(Just-In-Time)编译器，也就是通常所说的动态编译器，JIT 能够在运行时将热点代码编译成机器码，所以准确的说 Java 代码会解释执行或编译执行。 1.程序计数器（Program Counter Register） 线程私有 不会内存溢出 作用：记住下一条 JVM 指令的执行地址。 2.Java 虚拟机栈（VM Stack） 线程私有 LIFO（后进先出） 存储栈帧，支撑 Java 方法的调用、执行和退出 可能出现 OutOfMemoryError 异常（如果被设计成动态扩展，而扩展又未申请到足够的内存抛出）和 StackOverflowError 异常（如线程请求的栈深度大于最大深度抛出） 2.1 栈帧（Frame） Java 虚拟机栈中存储的内容，它被用于存储数据和部分过程结构的数据结构，同时也被用来处理动态链接、方法返回值 和 异常分派 一个完整的栈帧包含：局部变量表、操作数栈、动态连接信息、方法正常完成和异常完成的信息 每个栈由多个栈帧组成，对应着每次方法调用时所占用的内存 每个线程只能有一个活动栈帧，对应着当前正在执行的那个方法 2.2 局部变量表 由若干个 Slot 组成，长度由编译期决定 单个 Slot 可以储存一个类型为 boolean、byte、char、short、float、reference、returnAddress 的数据，两个 Slot 可以存储一个类型为 long 或 double 的数据 局部变量表用于方法间参数的传递，以及方法执行过程中存储基础数据类型的值和对象的引用 2.3 操作数栈 一个后进先出栈，由若干个 Entry 组成，长度由编译期决定 单个 Entry 即可以存储一个 Java 虚拟机中定义的任意数据类型的值，包括 long 和 double 类型，但是存储 long 和 double 类型的 Entry 深度为 2，其他类型深度为 1 在方法执行过程中，栈帧用于存储计算参数和计算结果；在方法调用时，操作数栈也用来准备调用方法的参数以及接收方法返回结果 2.4 栈的内存溢出（StackOverflowError） 栈帧过多导致内存溢出（方法的递归调用） 栈帧过大导致内存溢出 JSON 数据转换可能导致内存溢出（可用@JsonIgnore 忽略不能转换的属性） 2.5 线程诊断 案例 1：cpu 占用过高 Linux 下，top打印所有进程，筛选 cpu 占用高的进程号，如：32655 用ps H -eo pid,tid,%cpu | grep 32655打印 32655 的所有线程，定位到具体 cpu 占用过高的线程 jstack 进程id打印该线程的所有线程详情 将线程 id 换算成 16 进制，对比打印出的线程详情，定位到具体线程，进一步定位到源代码具体代码行号。 案例 2：程序运行很长时间没有结果 前面步骤同上，jstack 进程id打印该线程的所有线程详情 在最后一段找到了 Found one Java-level deadlock，定位死锁的具体行号。 3.本地方法栈（Native Method Stack） 线程私有 LIFO（后进先出） 支撑 Native 方法的调用、执行和退出 可能出现 OutOfMemoryError 异常 和 StackOverflowError 异常 有一些虚拟机（如 HotSpot）将 Java 虚拟机栈和本地方法栈合并实现 3.1 Java 虚拟机栈和本地方法栈可能发生的异常情况： 如果线程请求分配的栈容量超过 Java 虚拟机栈允许的最大容量时，Java 虚拟机将会抛出一个 StackOverflowError 异常 如果 Java 虚拟机栈可以动态扩展，并且扩展的动作已经尝试过，但是目前无法申请到足够的内存去完成扩展，或者在建立新的线程时没有足够的内存去创建对应的虚拟机栈，那么 Java 虚拟机将会抛出一个 OutOfMemoryError 异常。 4.Java 堆（Heap） 全局共享 通常是 Java 虚拟机中最大的一块内存区域 作用是作为 Java 对象的主要存储区域（通过 new 创建的对象都会使用堆内存） 有垃圾回收机制 4.1 Java 堆可能发生的异常 如果实际所需的堆超过了自动内存管理系统能提供的最大容量，那 Java 虚拟机将会抛出一个 OutOfMemoryError 异常。 4.2 堆内存诊断 jps 工具：查看当前系统中有哪些 Java 进程 jmap 工具：查看堆内存占用情况jmap -head 进程id jconsole 工具：图形界面，多功能的监测工具，可以连续监测 案例：垃圾回收后，内存占用仍然很高 jps 工具定位进程，jmap -head 进程id查看堆使用情况， 可以用 jconsole 工具手动执行 GC 用 jvirsualvm 抓取堆 dump(快照，抓取堆里面有哪些类型的对象及个数等信息) 4.3 字符串常量池 (StringTable) 在 JDK6.0 及之前版本，字符串常量池是放在 Perm Gen 区(也就是方法区)中； 在 JDK7.0 版本，字符串常量池被移到了堆中 字符串手动入池: 调用String.intern() 5.方法区（Method Area） 全局共享 作用是存储 Java 类的结构信息 JVMS 不要求该区域实现自动内存管理，但是商用 Java 虚拟机都能够自动管理该区域内存 在 JDK1.8 后，方法区由元空间实现 方法区内存溢出场景：spring、mabatis 等动态加载类的场景使用不当会导致方法区内存溢出 5.1 运行时常量池 全局共享 是方法区的一部分 作用是存储 Java 类文件常量池中的符号信息 可能出现 OutOfMemoryError 异常 5.2 永久代与方法区 在 JDK1.2~6，HotSpot 使用永久代实现方法区 在 JDK1.7，开始移除（符号表被到 Native Heap，字符串常量和类的静态引用被移到 Java Head 中） 在 JDK1.8，永久代被元空间（Metaspace）所替代 6.直接内存 全局共享 并非 JVMS 定义的标准 Java 运行时内存区域, 属于操作系统内存 JDK1.4 引入 NIO，目的是避免 Java 堆 和 Native 堆 中来回 复制数据 带来的性能损耗。 能被自动管理，但是在检测手段上可能会由一些简陋 可能出现 OutOfMemoryError 异常 常用于 NIO 操作时，用于数据缓冲区 分配回收成本高，但读写性能高，不受 JVM 内存回收管理 7.可回收对象的判定 引用计数法：给对象添加一个引用计数器，每当有一个地方引用它时，计数器就+1，当引用失效就-1，任何时候计数器为 0 时就是可回收对象。 可达性分析：通过一系列名为 GC Roots 的对象作为起始点，从这些根节点开始向下搜索，搜索所走过的路径称为引用链(Reference Chain)，当一个对象到 GC Roots 没有任何引用链相连时，则称该对象是不可达的。 目前主流 Java 虚拟机中并没有选用引用计数法，其中最重要的原因是它很难解决循环引用问题 7.1 Java 语言中的 GC Roots 在虚拟机栈（栈帧中的本地变量表）中的引用的对象。 在方法区中的类静态属性引用的对象。 在方法区中的常量引用的对象。 在本地方法栈中 JNI（即一般说的 Native 方法）的引用对象。 7.2 Java 引用类型 强引用：Java 中默认声明的就是强引用 垃圾回收器将永远不会回收被「强引用」对象，哪怕内存不足时，JVM 也会直接抛出 OutOfMemoryError，不会去回收。可以赋值为 null 中断强引用。 软引用（SoftReference）：用来描述一些非必需但仍有用的对象，用 java.lang.ref.SoftReference 类来表示软引用 垃圾回收后，在内存不足时会再次触发垃圾回收，回收「软引用」对象，仍不足，才会抛出内存溢出异常。可以配合引用队列来释放软引用自身。 弱引用（WeakReference）：用 java.lang.ref.WeakReference 来表示弱引用 垃圾回收器将永远都会回收被「弱引用」对象，无论内存是否足够。可以配合引用队列来释放弱引用自身。 虚引用（PhantomReference）：最弱的一种引用关系，用 PhantomReference 类来表示 必须配合引用队列使用，主要配合 ByteBuffer 使用，被引用对象回收时，会将虚引用入队，由 Reference Handler 线程调用虚引用相关方法释放直接内存。 8.垃圾回收算法 标记清除算法（Mark-Sweep） 标记整理算法(Mark-Compact) 复制算法（copying） 8.1 分代垃圾回收（Java 堆分为新生代和老年代） 对象首先分配在新生代的Eden区 新生代空间不足时，触发 Minor GC，Eden 区和 From 幸存区(Survivor)存活的对象使用 coping 复制到 To 幸存区中，存活的年龄+1 并且交换 From 和 To。 Minor GC 会引发 STW(Stop the world)，暂停其他用户的线程，等垃圾回收结束后，用户线程才恢复运行 当对象寿命超过阈值时，会晋升至老年代，最大寿命 15(4bit) 当老年代空间不足，会先尝试触发 Minor GC，如果之后空间仍不足，那么触发 Full GC，STW 的时间更长 8.2 相关 JVM 参数 堆初始大小： -Xms 堆最大大小： -Xmx 或 -XX:MaxHeapSize=size 新生代大小： -Xmn 或 (-XX:NewSize=size + -XX:MaxNewSize=size) 幸存区比例（动态）： -XX:InitialSurvivorRatio=ratio 和 -XX:+UseAdaptiveSizePolicy 幸存区比例： -XX:SurvivorRatio=ratio 晋升阈值： -XX:MaxTenuringThreshold=threshold 晋升详情： -XX:+PrintTenuringDistribution GC 详情： -XX:+PrintGCDetils -verbose:gc FullGC 前 MinorGC： -XX:+ScavengeBeforeFullGC 9.垃圾回收器 串行（开启：-XX:+UseSerialGC=Serial + SerialOld） 单线程 适合堆内存较小，适合个人电脑 吞吐量优先 多线程 堆内存较大，多核 CPU 让单位时间内，总 STW 的时间最短 响应时间优先 多线程 堆内存较大，多核 CPU 尽量让单次 STW 的时间最短 9.1 吞吐量优先（并行）回收器 开启(默认开启)： -XX:+UseParallelGC ~ -XX:+UseParallelOldGC 动态调整堆大小：-XX:+UseAdaptiveSizePolicy 目标吞吐量：-XX:GCTimeRatio=ratio 最大暂停时间的目标值：-XX:MaxGCPauseMillis=ms 线程数：-XX:ParallelGCThreads=n 9.2 响应时间优先（并发）回收器可以和用户线程并发执行，工作在老年代 开启：-XX:+UseConcMarkSweepGC 配合 -XX:UseParNewGC ~ SerialOld 并行和并发线程数：-XX:ParallelGCThreads=n ~ -XX:ConsGCThreads=threads 回收时机（内存占比）:-XX:CMSInitiatingOccupancyFraction=percent 重新标记前对新生代先做垃圾回收：-XX:+CMSScavengeBeforeRemark 9.3 G1（Garbage First）（并发） G1 回收器 适用场景 同时注重 吞吐量(Throughput)和低延迟(Low latency)，默认暂停目标是 200ms 超大堆内存，会将堆划分为多个大小相等的区域(Region) 整体上是标记+整理算法，两个区域之间是复制算法 相关 JVM 参数 开启（JDK9 默认）：-XX:+UseG1GC 区域大小：-XX:G1HeapRegionSize=size 最大暂停时间：-XX:MaxGCPauseMillis=time G1 垃圾回收阶段（三个阶段循环） **Young Collection**：新生代 GC（会 STW） **Young Collection + Concurrent Mark**： 在 YoungGC 时会进行GC Root的初始标记 老年代占用堆空间比例达到阈值值，进行并发标记(不会 STW)，由下面的 JVM 参数决定 -XX:InitiatingHeadOccupancyPercent=percent(默认 45%) **Mixed Collection**：会对 Eden、Survivor、Old 进行全面垃圾回收 最终标记(Remark)会 STW 拷贝存活(Evacuation)会 STW 为达到最大暂停时间短的目标，Old 区是优先回收垃圾最多的区域 9.4 Minor GC 和 Full GC SerialGC 新生代内存不足：Minor GC 老年代内存不足：Full GC ParallelGC 新生代内存不足：Minor GC 老年代内存不足：Full GC CMS 新生代内存不足：Minor GC 老年代内存不足：分两种情况（回收速度高于内存产生速度不会触发 Full GC） G1 新生代内存不足：Minor GC 老年代内存不足：分两种情况（回收速度高于内存产生速度不会触发 Full GC） Minor GC：当 Eden 区满时，触发 Minor GC Full GC： System.gc()方法的调用 老年代空间不足 方法区空间不足 通过 Minor GC 后进入老年代的平均大小大于老年代的可用内存 由 Eden 区、From 幸存区 向 To 幸存区 复制时，对象大小大于 To 区可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 10.垃圾回收调优 调优领域：内存、锁竞争、CPU 占用、IO 调优目标：「低延迟」还是「高吞吐量」（高吞吐量:ParallelGC，低延迟:CMS,G1,ZGC） 最快的 GC 是不发生 GC：查看 Full GC 前后的内存占用（内存数据太多？数据表示太臃肿？内存泄漏？） 新生代调优：new 操作内存分配廉价、死亡对象回收代价是零、大部分对象用过即死、MinorGC 时间远低于 FullGC 10.1 新生代调优 理想情况：新生代能容纳所有「并发量*(请求-响应)」的数据 幸存区大到能够保留「当前活跃对象+需要晋升对象」 「晋升阈值配置」得当，让长时间存活的对象尽快晋升 调整最大晋升阈值：-XX:MaxTenuringThreshold=threshold 打印晋升详情：-XX:+PrintTenuringDistribution 10.2 老年代调优以 CMS 为例： CMS 的老年代内存越大越好（避免浮动垃圾引起的并发失败） 先尝试不做调优，如果没有 FullGC 那么已经 OK，否则先尝试调优新生代 观察发生 Full GC 时老年代内存占用，将老年代内存预设调大 1&#x2F;4~1&#x2F;3 -XX:CMSInitiatingOccupancyPercent=percent 10.3 调优案例 案例 1：FullGC 和 MinorGC 频繁 可能原因：空间紧张，若业务高峰期时，新生代空间紧张，幸存区的晋升阈值会降低，大量本来生存短对象晋升老年区，进一步触发老年代 FullGC 的频繁发生 解决方法：经过分析，观察堆空间大小，先试着增大新生代内存，同时增大幸存区的空间以及晋升阈值。 案例 2：请求高峰期发生了 FullGC，单次暂停时间特别长（CMS） 查看日志，看 CMS 哪个阶段暂停时间长（重新标记阶段），解决：打开开关参数 CMSScavengeBeforeRemark 重新标记前对新生代先做垃圾回收：-XX:+CMSScavengeBeforeRemark 10.4 G1 调优最佳实践 不要设置新生代和老年代的大小 G1 收集器在运行的时候会调整新生代和老年代的大小。通过改变代的大小来调整对象晋升的速度以及晋升年龄，从而达到我们为收集器设置的暂停时间目标。设置了新生代大小相当于放弃了 G1 为我们做的自动调优。我们需要做的只是设置整个堆内存的大小，剩下的交给 G1 自己去分配各个代的大小。 不断调优暂停时间指标 通过 XX:MaxGCPauseMillis&#x3D;x 可以设置启动应用程序暂停的时间，G1 在运行的时候会根据这个参数选择 CSet 来满足响应时间的设置。一般情况下这个值设置到 100ms 或者 200ms 都是可以的(不同情况下会不一样)，但如果设置成 50ms 就不太合理。暂停时间设置的太短，就会导致出现 G1 跟不上垃圾产生的速度。最终退化成 Full GC。所以对这个参数的调优是一个持续的过程，逐步调整到最佳状态。 关注 Evacuation Failure Evacuation Failure 类似于 CMS 里面的晋升失败，堆空间的垃圾太多导致无法完成 Region 之间的拷贝，于是不得不退化成 Full GC 来做一次全局范围内的垃圾收集。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"JVM","slug":"JVM","permalink":"http://chaooo.github.io/tags/JVM/"}]},{"title":"「数据结构」常见JAVA集合类的数据结构分析","date":"2019-07-28T14:53:06.000Z","path":"2019/07/28/data-structure-java.html","text":"集合(Collection&#x2F;Map)1234567891011121314151617Collection接口 |———— List接口 |———— ArrayList类 |———— Vector类 |———— LinkedList类 |———— Stack类 |———— Set接口 |———— HashSet类 |———— TreeSet类 |———— LinkedHashSet类 |———— Queue接口 |———— LinkedList类Map接口 |———— HashMap类 |———— TreeMap类 |———— LinkedHashMap类 |———— Hashtable类 0.1 List Arraylist： 动态数组 Vector： 动态数组(线程安全) LinkedList： 双向链表(JDK1.6之前为循环链表，JDK1.7取消了循环) 0.2 Set HashSet（无序，唯一）：基于 HashMap 实现的，底层采用 HashMap 来保存元素 LinkedHashSet： LinkedHashSet 继承于 HashSet，并且其内部是通过 LinkedHashMap 来实现的。 TreeSet（有序，唯一）：红黑树(自平衡的排序二叉树) 0.3 Map HashMap： JDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间 LinkedHashMap： LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。 Hashtable： 数组+链表(线程安全)，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap： 红黑树（自平衡的排序二叉树） 0.4 如何选用集合主要根据集合的特点来选用： 键值对就选用Map接口下的集合，需要排序时选择TreeMap，不需要排序时就选择HashMap,需要保证线程安全就选用ConcurrentHashMap. 只需要存放元素值时，就选用Collection接口下的集合，需要保证元素唯一时选择实现Set接口的集合（TreeSet或HashSet），不需要就选择实现List接口的ArrayList或LinkedList 0.5 对数公式log 与 时空复杂度 若a^n = b (a&gt;0,a≠1) 则 n = log(a)b , 如log(2)8 = 3; Java数据结构中log默认以2为底(个人理解,有待考证) 常用O(1), O(n), O(logn)表示对应算法的时间复杂度, 也用于表示空间复杂度。 O(1): 最低的时空复杂度, 无论数据规模多大，都可以在一次计算后找到目标 O(n): 数据量增大n倍时，耗时增大n倍; 比如常见的遍历算法 O(n^2): 数据量增大n倍时，耗时增大n的平方倍; 比如冒泡排序，对n个数排序，需要扫描n×n次 o(logn): 当数据增大n倍时，耗时增大logn倍; 二分查找就是O(logn)的算法，每找一次排除一半的可能，256个数据中查找只要找8次就可以找到目标(2^8&#x3D;256) O(nlogn): 同理，就是n乘以logn，当数据增大256倍时，耗时增大256*8&#x3D;2048倍。这个复杂度高于线性低于平方。归并排序就是O(nlogn)的时间复杂度 0.6 移位运算符按照平移的方向和填充数字的规则分为三种:&lt;&lt;(左移)、&gt;&gt;(带符号右移) 和 &gt;&gt;&gt;(无符号右移) 左移 &lt;&lt; : 丢弃最高位,0补最低位；左移n位就相当于乘以2的n次方 右移 &gt;&gt; : 符号位不变,高位补上符号位(正数0, 负数1)；右移n位相当于除以2的n次方 无符号右移 &gt;&gt;&gt; : 忽略符号位，0补最高位(补码移位所得) 正数的左移与右移，负数的无符号右移，就是相应的补码移位所得，在高位补0即可。 负数的右移，就是补码高位补1,然后按位取反加1即可。 运算规则： 左移：高位移出(舍弃)，低位的空位补零；int类型时，每移动1位它的第31位就要被移出并且丢弃；long类型时，每移动1位它的第63位就要被移出并且丢弃；byte和short类型时，将自动把这些类型扩大为int型。 右移：低位移出(舍弃)，高位的空位补符号位，即正数补0，负数补1；当右移的运算数是byte 和short类型时，将自动把这些类型扩大为 int 型。 无符号右移：补码移位，高位补0；正数和右移表现一致，负数变成了很大的正数； 1. ArraylistArrayList的底层是数组队列，相当于动态数组。与数组相比，它的容量能动态增长。在添加大量元素前，应用程序使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。它继承于 AbstractList，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。 数组时间复杂度: 插入&#x2F;删除:O(n)，增加(末尾)&#x2F;随机访问: O(1) ArrayList 继承了AbstractList，实现了List。它是一个数组队列，提供了相关的添加、删除、修改、遍历等功能 ArrayList 实现了RandomAccess 接口， RandomAccess 是一个标志接口，表明实现这个这个接口的 List 集合是支持快速随机访问的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问 ArrayList 实现了Cloneable 接口，即覆盖了函数 clone()，能被克隆。 ArrayList 实现java.io.Serializable 接口，这意味着ArrayList支持序列化，能通过序列化去传输。 和 Vector 不同，ArrayList 中的操作不是线程安全的！所以，建议在单线程中才使用 ArrayList，而在多线程中可以选择 Vector 或者 CopyOnWriteArrayList。 1.1 ArrayList扩容机制*（重点） 以无参数构造方法创建ArrayList时，实际上初始化赋值的是一个空数组；当add第一个元素时，才真正分配容量(默认10) ArrayList在每次增加元素(1个或一组)时，都要调用ensureCapacityInternal()方法来确保足够的容量 当容量不足以容纳当前的元素个数时，进入grow()方法进行扩容，首先设置新的容量为旧容量的1.5倍 若设置后的新容量还不够，则设置新容量为minCapacity(所需最小容量) 比较新容量是否大于MAX_ARRAY_SIZE(Integer最大值减8)，若大于，再比较minCapacity是否大于MAX_ARRAY_SIZE，若大于，设置新的容量为Integer.MAX_VALUE(Integer最大值)，否则设置新的容量为MAX_ARRAY_SIZE(Integer最大值减8) 最后用Arrays.copyof()方法将元素拷贝到新的数组 (第Integer.MAX_VALUE+1次添加元素时，抛出OutOfMemoryError异常) System.arraycopy()和Arrays.copyOf()方法通过源码发现这两个实现数组复制的方法被广泛使用, 比如插入操作add(int index, E element)方法就很巧妙的用到了 System.arraycopy()方法让数组自己复制自己实现让index开始之后的所有成员后移一个位置 Arrays.copyOf()内部也是调用了System.arraycopy()方法 Arrays.copyOf()是系统自动在内部新建一个数组，并返回该数组 System.arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置 1.2 ensureCapacityArrayList对外提供了一个ensureCapacity(int n)方法 最好在add大量元素之前用ensureCapacity方法，以减少增量重新分配的次数 ensureCapacity一次性扩容到位，否则在添加大量元素的过程中，一点一点的进行扩容 1.3 内部类1234private class Itr implements Iterator&lt;E&gt;&#123;...&#125;private class ListItr extends Itr implements ListIterator&lt;E&gt;&#123;...&#125;private class SubList extends AbstractList&lt;E&gt; implements RandomAccess&#123;...&#125;static final class ArrayListSpliterator&lt;E&gt; implements Spliterator&lt;E&gt;&#123;...&#125; ArrayList有四个内部类 Itr 实现了Iterator接口，同时重写了里面的hasNext()， next()， remove() 等方法； ListItr 继承 Itr，实现了ListIterator接口，同时重写了hasPrevious()， nextIndex()， previousIndex()， previous()， set(E e)， add(E e) 等方法 Iterator和ListIterator的区别: ListIterator在Iterator的基础上增加了添加对象，修改对象，逆向遍历等方法，这些是Iterator不能实现的。 2. LinkedListLinkedList是基于双向链表实现的, 可以在任何位置进行高效地插入和移除操作的有序序列。 复杂度: 增加(末尾)&#x2F;删除:O(1)，插入&#x2F;获取: O(n) LinkedList 继承AbstractSequentialList的双向链表。它也可以被当作堆栈、队列或双端队列进行操作。 LinkedList 实现 List 接口，能对它进行队列操作。 LinkedList 实现 Deque 接口，即能将LinkedList当作双端队列使用。 LinkedList 实现了Cloneable接口，即覆盖了函数clone()，能克隆。 LinkedList 实现java.io.Serializable接口，这意味着LinkedList支持序列化，能通过序列化去传输。 LinkedList 不是线程安全的，如果想使LinkedList变成线程安全的，可以调用静态类Collections类中的synchronizedList方法 2.1 LinkedList底层分析:LinkedList的底层是一个双向链表，链表中挂载着一个个的Node元素；可以从LinkedList的Node内部类看出奥秘： 1234567891011121314transient Node&lt;E&gt; first; //头指针transient Node&lt;E&gt; last; //尾指针//内部类private static class Node&lt;E&gt; &#123; E item; // 数据域（当前节点的值） Node&lt;E&gt; next; // 后继（指向当前一个节点的后一个节点） Node&lt;E&gt; prev; // 前驱（指向当前节点的前一个节点） // 构造函数，赋值前驱后继 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; LinkedList 是基于链表结构实现，所以在类中包含了 first 和 last 两个指针(Node)。 Node 中包含了上一个节点和下一个节点的引用，这样就构成了双向的链表。 2.2 LinkedList增删改查 链表批量增加，是靠for循环遍历原数组，依次执行插入节点操作。增加一定会修改modCount。 通过下标获取某个node的时候(add select)，会根据index处于前半段还是后半段进行一个折半，以提升查询效率 删也一定会修改modCount。 按下标删，也是先根据index找到Node，然后去链表上unlink掉这个Node。 按元素删，会先去遍历链表寻找是否有该Node，如果有，去链表上unlink掉这个Node。 改也是先根据index找到Node，然后替换值。不修改modCount。 CRUD操作里，都涉及到根据index去找到Node的操作。 2.2 unlink原理 先判断该节点是否存在上一个节点，即是否有前驱节点。 无前驱节点则说明要删除的节点为链表的第一节点，那么只需要把该节点的下一个节点设置为链表的第一个节点。 有前驱节点则需要把前驱节点的尾部引用指向该节点的下一个节点。 再判断该节点是否存在下一个节点，即是否有后继节点。 无后继节点则说明该节点是链表的最后一个节点，那么只需要把该节点前驱节点设置成链表的最后一个节点即可。 有后继节点则需要把后继节点的头部引用指向该节点的上一个节点。 核心就是在于将要删除的节点的前驱节点尾部指向该节点的后继节点，将要删除的节点的后继节点的头部指向该节点的前驱节点。这样便完成了链表的删除操作。 删除和新增方法的实现基本是对该节点的上一个节点和下一个节点的引用设置，不需要操作其他节点，效率相对较高 2.3 offer与add的区别 offer属于 offer in interface Deque。 add 属于 add in interface Collection。 当队列为空时候，使用add方法会报错，而offer方法会返回false。 作为List使用时,一般采用add &#x2F; get方法来 压入&#x2F;获取对象。 作为Queue使用时,才会采用 offer&#x2F;poll&#x2F;take等方法作为链表对象时,offer等方法相对来说没有什么意义这些方法是用于支持队列应用的。 2.2 对比Vector、ArrayList、LinkedList有何区别这三者都是实现集合框架中的 List，也就是所谓的有序集合，因此具体功能也比较近似，比如都提供按照位置进行定位、添加或者删除的操作，都提供迭代器以遍历其内容等。但因为具体的设计区别，在行为、性能、线程安全等方面，表现又有很大不同。 Vector 是 Java 早期提供的线程安全的动态数组，如果不需要线程安全，并不建议选择，毕竟同步是有额外开销的。Vector 内部是使用对象数组来保存数据，可以根据需要自动的增加容量，当数组已满时，会创建新的数组，并拷贝原有数组数据扩容为旧容量的2倍。 ArrayList 是应用更加广泛的动态数组实现，它本身不是线程安全的，所以性能要好很多。ArrayList 也是可以根据需要调整容量，在扩容为旧容量的1.5倍。 LinkedList 顾名思义是 Java 提供的双向链表，不需要扩容，它也不是线程安全的。LinkedList不支持高效的随机元素访问。 3. HashMapHashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的, 用于存储Key-Value键值对的集合，每一个键值对也叫做一个Entry。这些Entry分散存储在一个数组当中，这个数组就是HashMap的主干。 HashMap继承了AbstractMap类，实现了Map，Cloneable，Serializable接口 继承 abstractMap，也就是用来减轻实现Map接口的编写负担。 实现 Cloneable：能够使用Clone()方法，在HashMap中，实现的是浅层次拷贝，即对拷贝对象的改变会影响被拷贝的对象。 实现 Serializable：能够使之序列化，即可以将HashMap对象保存至本地，之后可以恢复状态。 JDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以实现O(logn)时间复杂读查找。 HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。 HashMap的实例有两个参数影响其性能: 初始容量(默认16)：哈希表中桶的数量 加载因子(默认0.75)：哈希表在其容量自动增加之前可以达到多满的一种尺度 当哈希表中条目数超出了当前容量*加载因子(其实就是HashMap的实际容量)时，则对该哈希表进行rehash操作，将哈希表扩充至两倍的桶数。 3.1 HashMap的 put 方法过程*（重点）put方法内部是一个 putVal 的调用： 对 Key 求 Hash 值，然后再计算下标。 如果没有碰撞，直接放入桶中， 如果碰撞了，若是树节点，就putTreeVal添加元素，若不是就遍历链表插入。 如果链表长度超过阀值（TREEIFY_THRESHOLD&#x3D;&#x3D;8），就把链表转成红黑树。 如果节点已经存在就替换旧值，若未找到则继续 如果桶满了（容量 * 加载因子），就需要 resize(扩容为原来2倍并重新散列,元素的下标要么不变，要么变为「原下标+原容量」)。 3.2 HashMap 桶下标计算 下标：hash(key) &amp; (table.length - 1) 扰动函数**hash(key)**：(key&#x3D;&#x3D;null) ? 0 : (key.hashCode()^(key.hashCode() &gt;&gt;&gt; 16)) 低16位 和 高 16位 做了一个异或得到 hash值 与 (容器长度-1)进行**取模(%)**运算,得到下标。 利用位运算代替取模运算，提高程序的计算效率：（当 b&#x3D;2^n 时，a%b &#x3D; a &amp; (b-1) ），也是因此，HashMap 才将初始长度设置为 16，且扩容只能是以 2 的倍数（2^n）扩容。 有些数据计算出的哈希值差异主要在高位，而HashMap里的哈希寻址是忽略容量以上的高位的，那么这种处理就可以尽可能有效的避免哈希碰撞。 HashMap 的性能表现非常依赖于哈希码的有效性: equals相等，hashCode一定要相等。重写了 hashCode 也要重写 equals。hashCode 需要保持一致性，状态改变返回的哈希值仍然要一致。 3.3 HashMap 容量、负载因子和树化 容量和负载系数决定了可用的桶的数量，空桶太多会浪费空间，如果使用的太满则会严重影响操作的性能。 如果能够知道 HashMap 要存取的键值对数量，可以考虑预先设置合适的容量大小。 计算条件：负载因子 * 容量 &gt; 元素数量；所以，预先设置的容量需要满足，大于“预估元素数量&#x2F;负载因子”，同时它是2的幂数 容量理论最大极限由 MAXIMUM_CAPACITY 指定，数值为 1&lt;&lt;30，也就是2的30次方 3.3.1 HashMap 负载因子loadFactor loadFactor加载因子是控制数组存放数据的疏密程度，越大越密，越小越稀疏。 loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给出的一个比较好的临界值。 给定的默认容量为16，负载因子为0.75。当数量达到了 16*0.75 &#x3D; 12 就需要将当前16的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。 而对于负载因子，建议： 如果没有特别需求，不要轻易进行更改，因为 JDK 自身的默认负载因子是非常符合通用场景的需求的。 如果确实需要调整，建议不要设置超过 0.75 的数值，因为会显著增加冲突，降低 HashMap 的性能。 如果使用太小的负载因子，按照上面的公式，预设容量值也进行调整，否则可能会导致更加频繁的扩容，增加无谓的开销，本身访问性能也会受影响。 3.3.2 HashMap 门限值thresholdthreshold = capacity * loadFactor，当Size&gt;=threshold的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 衡量数组是否需要扩增的一个标准。 门限值等于(负载因子 x 容量)，如果构建 HashMap 的时候没有指定它们，那么就是依据相应的默认常量值。 门限通常是以倍数进行调整 （newThr &#x3D; oldThr &lt;&lt; 1），根据 putVal 中的逻辑，当元素个数超过门限大小时，则调整 Map 大小。 扩容后，需要将老的数组中的元素重新放置到新的数组，这是扩容的一个主要开销来源。 3.3.2 HashMap 树化改造树化改造逻辑主要在 putVal 和 treeifyBin 中。 链表结构（这里叫 bin）的数量大于 TREEIFY_THRESHOLD(默认为8) 时： 如果容量小于 MIN_TREEIFY_CAPACITY(默认为64) ，只会进行简单的扩容。 如果容量大于 MIN_TREEIFY_CAPACITY(默认为64)，则会进行树化改造。 3.4 HashMap 扩容resize HashMap扩容条件： 元素个数超出了加载因子与当前容量的乘积，并且发生了Hash碰撞 HashMap扩容步骤： 创建一个新的Entry空数组，长度是原来的2倍。 遍历原Entry数组，把所有的Entry重新Hash到新数组里。 重新散列的元素下标要么「不变」，要么变为「原下标+原容量」，取决于位运算((n - 1) &amp; hash) 经过一次扩容处理后，元素会更加均匀的分布在各个桶中，会提升访问效率。但会遍历所有的元素，时间复杂度很高；遍历元素所带来的坏处大于元素在桶中均匀分布所带来的好处。尽量避免进行扩容处理。 3.5 常见的hash算法及冲突的解决hash函数，即散列函数。它可以将不定长的输入，通过散列算法转换成一个定长的输出，这个输出就是散列值(不保证唯一)。 常见Hash算法： 直接定址法：直接以关键字k或者k加上某个常数（k+c）作为哈希地址（H(k)&#x3D;ak+b）。 数字分析法：提取关键字中取值比较均匀的数字作为哈希地址（如一组出生日期，相较于年-月，月-日的差别要大得多，可以降低冲突概率） 分段叠加法：按照哈希表地址位数将关键字分成位数相等的几部分，其中最后一部分可以比较短。然后将这几部分相加，舍弃最高进位后的结果就是该关键字的哈希地址。 平方取中法：如果关键字各个部分分布都不均匀的话，可以先求出它的平方值，然后按照需求取中间的几位作为哈希地址。 伪随机数法：选择一随机函数，取关键字的随机值作为散列地址，通常用于关键字长度不同的场合。 除留余数法：用关键字k除以某个不大于哈希表长度m的数p，将所得余数作为哈希表地址（H(k)&#x3D;k%p, p&lt;&#x3D;m; p一般取m或素数）。 常见解决hash冲突的方法 链地址法：将哈希表的每个单元作为链表的头结点，所有哈希地址为 i 的元素构成一个同义词链表。即发生冲突时就把该关键字链在以该单元为头结点的链表的尾部。 开放定址法：即发生冲突时，去寻找下一个空的哈希地址。只要哈希表足够大，总能找到空的哈希地址。 再哈希法：即发生冲突时，由其他的函数再计算一次哈希值。 建立公共溢出区：将哈希表分为基本表和溢出表，发生冲突时，将冲突的元素放入溢出表。 HashMap就是使用链地址法来解决冲突的（JDK1.8增加了红黑树） 3.6 对比Hashtable、HashMap、TreeMap有什么不同Hashtable、HashMap、TreeMap 都是最常见的一些 Map 实现，是以键值对的形式存储和操作数据的容器类型。 Hashtable 是早期 Java 类库提供的一个哈希表实现，本身是同步的，不支持 null 键和值，由于同步导致的性能开销，所以已经很少被推荐使用。 HashMap 是应用更加广泛的哈希表实现，行为上大致上与 HashTable 一致，主要区别在于 HashMap 不是同步的，支持 null 键和值等。通常情况下，HashMap 进行 put 或者 get 操作，可以达到常数时间的性能，所以它是绝大部分利用键值对存取场景的首选，比如，实现一个用户 ID 和用户信息对应的运行时存储结构。 TreeMap 则是基于红黑树的一种提供顺序访问的 Map，和 HashMap 不同，它的 get、put、remove 之类操作都是 O(log(n))的时间复杂度，具体顺序可以由指定的 Comparator 来决定，或者根据键的自然顺序来判断。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"数据结构","slug":"data-structure","permalink":"http://chaooo.github.io/tags/data-structure/"}]},{"title":"「网络协议」网络协议基础","date":"2019-07-15T13:04:33.000Z","path":"2019/07/15/network-protocol.html","text":"1. 分层网络协议 OSI七层网络协议：物理层，数据链路层，网络层，传输层(TCP&#x2F;UDP)，会话层，表示层，应用层 TCP&#x2F;IP协议分层(可以理解为OSI的一种实现)：网络接口层，网络层，传输层(TCP&#x2F;UDP)，应用层 2. TCP通信协议简介： 面向连接的、可靠的、基于字节流的 传输层通信协议 将应用层的数据流分割成报文段并发送给目标节点的TCP层 数据包都有序号，对方收到则发送ACK确认，未收到则重传 使用校验和来校验数据在传输过程中是否有误 报文头中的ACK(确认序号标志)，SYN(同步序号，用于建立连接过程) 3. TCP建立连接的三次握手 第一次：建立连接时，客户端发送SYN包(syn=j)到服务器，并进入SYS_SEND状态，等待服务器确认； 第二次：服务器收到SYN包，必须确认客户的SYN(ack=j+1)，同时自己也发送一个SYN包(syn=k)，即 SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次：客户端收到SYN+ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务端进入ESTABLISHED状态，完成三次握手。 4. 为什么需要三次握手 为了初始化Sequence Number的初始值（通信双方要互相通知对方自己的Sequence Number，要作为以后数据通信的序号，以保证接收到的数据不会因为网络传输问题而乱序，TCP会用这个序号拼接数据） 5. 首次握手的隐患—SYN超时 服务端收到客户端的SYN，回复SYN-ACK的时候未收到ACK确认 服务端不断尝试(重发SYN-ACK)直至超时，Linux默认等待63秒才断开连接(默认重试5次，重试间隔1s开始，每次翻倍，即1+2+4+8+16+32&#x3D;63) 可能遭受SYN Flood的风险(syn攻击，又称为ddos攻击) 6. 什么是SYN Flood攻击 客户端恶意的向某个服务器端口发送大量的SYN包，则可以使服务器打开大量的半开连接，分配TCB，从而消耗大量的服务器资源，同时也使得正常的连接请求无法被相应。而攻击发起方的资源消耗相比较可忽略不计。 SYN Flood是当前最流行的DoS（拒绝服务攻击）与DDoS（分布式拒绝服务攻击）的方式之一。 7. Linux针对SYN Flood的防护措施 SYN队列满后，通过tcp_syncookies参数回发SYN Cookies 若为正常连接则客户端会回发SYN Cookies，直接建立连接 8. 建立连接后，客户端出现故障怎么办（保活机制） 向对方发送保活探测报文，如果未收到响应则继续发送 尝试次数达到保活探测树仍未收到响应则中断连接 9. TCP终止连接的四次挥手（以客户端主动为例） 第一次：客户端发送一个FIN(seq=u)，用来关闭客户端到服务器的数据传送，客户端进入FIN_WAIT_1状态； 第二次：服务器收到FIN，发回一个ACK(ack=u+1)，确认序号为收到的序号+1(和SYN一样，一个FIN将占用一个序号)，服务端进入CLOSE_WAIT状态； 第三次：服务端发送一个FIN(seq=w)，用来关闭服务端到客户端的数据传送，服务端进入LAST_ACK状态； 第四次：客户端收到FIN，发回一个ACK(ack=w+1)，将确认序号设置为收到序号+1，客户端进入TIME_WAIT状态，服务端进入CLOSED状态，完成四次挥手。 10. 存在TIME_WAIT状态的原因 保证TCP全双工连接的可靠释放，确保有足够时间让对方收到ACK包 避免新旧来凝结混淆，使旧数据包在网络中因过期而失效 11. 为什么需要四次挥手 因为全双工，发送方和接收方都需要FIN报文和ACK报文 12. 服务器出现大量CLOSE_WAIT状态的原因 对方关闭socket连接，我方忙于读或写，没有及时关闭连接 检查代码，特别是释放资源的代码 检查配置，特别是处理请求的线程配置 13. UDP简介 面向非连接 不维护连接状态，支持同时向多个客户端传输相同消息 数据包报头只有8个字节，额外开销小 吞吐量只受限于数据生成速率、传输速率以及机器性能 尽最大努力交付，不保证可靠性，不需要维持复杂的链接状态表 面向报文，不对应用程序提交的报文信息进行拆分或者合并 14. TCP和UDP的区别 面向连接 vs 无连接 可靠性和有序性 vs 不保证 全双工的字节流 vs 全双工的数据报 效率低 vs 速度快 重量级 vs 轻量级 15. Http协议简介 基于TCP&#x2F;IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等），Hyper Text Transfer Protocol（超文本传输协议）的缩写。 简单快速：客户向服务器请求服务时，只需传送请求方法(GET、HEAD、POST等)和路径。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。 灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。 无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快 支持B&#x2F;S及C&#x2F;S模式。 16. HTTP 请求&#x2F;响应的步骤 客户端连接到Web服务器 一个HTTP客户端，通常是浏览器，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。例如，https://chaooo.github.io。 发送HTTP请求 通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。 服务器接受请求并返回HTTP响应 Web服务器解析请求，定位请求资源。服务器将资源复本写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。 释放连接TCP连接 若connection 模式为close，则服务器主动关闭TCP连接，客户端被动关闭连接，释放TCP连接;若connection 模式为keepalive，则该连接会保持一段时间，在该时间内可以继续接收请求; 客户端浏览器解析HTML内容 客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据HTML，根据HTML的语法对其进行格式化，并在浏览器窗口中显示。 17. 在浏览器地址栏键入URL，按下回车之后会经历以下流程： 浏览器向 DNS 服务器请求解析该 URL 中的域名所对应的 IP 地址; 解析出 IP 地址后，根据该 IP 地址和默认端口 80，和服务器建立TCP连接; 浏览器发出读取文件(URL 中域名后面部分对应的文件)的HTTP 请求，该请求报文作为 TCP 三次握手的第三个报文的数据发送给服务器; 服务器对浏览器请求作出响应，并把对应的 html 文本发送给浏览器; 释放 TCP连接; 浏览器将该 html 文本并显示内容; 18. HTTP之状态码 状态代码有三位数字组成，第一个数字定义了响应的类别，共分五种类别: 1xx：指示信息–表示请求已接收，继续处理 2xx：成功–表示请求已被成功接收、理解、接受 3xx：重定向–要完成请求必须进行更进一步的操作 4xx：客户端错误–请求有语法错误或请求无法实现 5xx：服务器端错误–服务器未能实现合法的请求 常见状态码： 200 OK &#x2F;&#x2F;客户端请求成功 400 Bad Request &#x2F;&#x2F;客户端请求有语法错误，不能被服务器所理解 401 Unauthorized &#x2F;&#x2F;请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden &#x2F;&#x2F;服务器收到请求，但是拒绝提供服务 404 Not Found &#x2F;&#x2F;请求资源不存在，eg：输入了错误的URL 500 Internal Server Error &#x2F;&#x2F;服务器发生不可预期的错误 503 Server Unavailable &#x2F;&#x2F;服务器当前不能处理客户端的请求，一段时间后可能恢复正常 19. HTTPS和HTTP的区别： https协议需要到CA申请证书(收费)，http不需要。 https密文传输，http明文传输。 http使用80端口，https默认使用443端口。 https &#x3D; http + 加密 + 认证 + 完整性保护 20. Socket简介 Socket是对TCP&#x2F;IP协议的抽象，是操作系统对外开发的接口 基于tcp协议的编程模型 服务器： 创建ServerSocket类型的对象并提供端口号； 等待客户端的连接请求，调用accept方法； 使用输入输出流进行通信； 关闭Socket； 客户端： 创建Socket类型的对象并提供服务器的通信地址和端口号； 使用输入输出流进行通信； 关闭Socket； 基于udp协议的编程模型 主机A(接收方): 创建DatagramSocket类型的对象，并提供端口号； 创建DatagramPacket类型的对象，用于接收发来的数据； 从Socket中接收数据，调用**receive()**方法； 关闭Socket并释放有关的资源； 主机B(发送方) 创建DatagramSocket类型的对象； 创建DatagramPacket类型的对象，并提供接收方的IP地址和端口号； 通过Socket发送数据，调用**send()**方法； 关闭Socket并释放有关的资源；","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"java","slug":"java","permalink":"http://chaooo.github.io/tags/java/"},{"name":"网络协议","slug":"network-protocol","permalink":"http://chaooo.github.io/tags/network-protocol/"}]},{"title":"「Redis」基于Redis的分布式锁实现","date":"2019-04-08T08:04:23.000Z","path":"2019/04/08/redis-lock.html","text":"SETNX命令简介 SETNX key value返回(1:key的值被设置，0:key的值没被设置)，将key的值设为value，并且仅当key不存在。 锁的key为目标数据的唯一键，value为锁的期望超时时间点； 基于Redis实现的分布式锁，主要基于redis的setnx（set if not exist）命令； 1. jedis实现分布式锁12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt;&lt;/dependency&gt; 1.1 实现示例:1234567public static boolean correctGetLock(String lockKey, String requestId, int expireTime) &#123; String result = jedis.set(lockKey, requestId, &quot;NX&quot;, &quot;PX&quot;, expireTime); if (&quot;OK&quot;.equals(result)) &#123; return true; &#125; return false;&#125; jedis.set(String key, String value, String nxxx, String expx, int time) - **key**：保证唯一，用来当锁（redis记录的key） - **value**：redis记录的value，目的是为了标志锁的所有者（竞争锁的客户端），保证解锁时只能解自己加的锁。requestId可以使用UUID.randomUUID().toString()方法生成 - **nxxx**：&quot;NX&quot;意思是SET IF NOT EXIST，即当key不存在时，我们进行set操作，若key已经存在，则不做任何操作 - **expx**：&quot;PX&quot;意思是要给这个key加一个过期的设置（单位毫秒），过期时间由第五个参数决定 - **time**：expx设置为&quot;PX&quot;时，redis key的过期时间 1.2 解锁示例:12345678public boolean correctReleaseLock(String lockKey, String requestId) &#123; String script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;; Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId)); if (RELEASE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; eval命令执行Lua代码的时候，Lua代码将被当成一个命令去执行，并且直到eval命令执行完成，Redis才会执行其他命令，所以保证了检查和删除操作都是原子的。 1.3 这类琐最大的缺点加锁时只作用在一个Redis节点上，即使Redis通过sentinel保证高可用，如果这个master节点由于某些原因发生了主从切换，那么就会出现锁丢失的情况： 在Redis的master节点上拿到了锁； 但是这个加锁的key还没有同步到slave节点； master故障，发生故障转移，slave节点升级为master节点； 导致锁丢失。 因此，Redis作者antirez基于分布式环境下提出了一种更高级的分布式锁的实现方式：Redlock。基于Redis的Redisson实现了Redlock。 2. Redisson实现普通分布式锁普通分布式实现非常简单，无论是那种架构，向Redis通过EVAL命令执行LUA脚本即可。 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.3.2&lt;/version&gt;&lt;/dependency&gt; 单机模式: 123456789101112131415161718192021// 构造redisson实现分布式锁必要的ConfigConfig config = new Config();config.useSingleServer().setAddress(&quot;redis://172.29.1.180:5379&quot;) .setPassword(&quot;a123456&quot;).setDatabase(0);// 构造RedissonClientRedissonClient redissonClient = Redisson.create(config);// 设置锁定资源名称, 还可以getFairLock(), getReadWriteLock()RLock lock = redissonClient.getLock(&quot;DISLOCK&quot;);boolean isLock;try &#123; // 尝试获取分布式锁 // 500ms拿不到锁, 就认为获取锁失败。10000ms即10s是锁失效时间。 isLock = lock.tryLock(500, 10000, TimeUnit.MILLISECONDS); if (isLock) &#123; //TODO if get lock success, do something; &#125;&#125; catch (Exception e) &#123;&#125; finally &#123; // 无论如何, 最后都要解锁 lock.unlock();&#125; 哨兵模式:即Sentinel模式，实现代码和单机模式几乎一样，唯一的不同就是Config的构造： 1234Config config = new Config();config.useSentinelServers().addSentinelAddress( &quot;redis://172.29.3.245:26378&quot;,&quot;redis://172.29.3.245:26379&quot;, &quot;redis://172.29.3.245:26380&quot;) .setMasterName(&quot;mymaster&quot;).setPassword(&quot;a123456&quot;).setDatabase(0); 集群模式:即Cluster模式，集群模式构造Config如下： 12345Config config = new Config();config.useClusterServers().addNodeAddress( &quot;redis://172.29.3.245:6375&quot;,&quot;redis://172.29.3.245:6376&quot;, &quot;redis://172.29.3.245:6377&quot;, &quot;redis://172.29.3.245:6378&quot;,&quot;redis://172.29.3.245:6379&quot;, &quot;redis://172.29.3.245:6380&quot;) .setPassword(&quot;a123456&quot;).setScanInterval(5000); 3. Redisson实现Redlock分布式锁3.1 Redlock算法大概原理： 在Redis的分布式环境中，我们假设有N个Redis master。这些节点完全互相独立，不存在主从复制或者其他集群协调机制。我们确保将在N个实例上使用与在Redis单实例下相同方法获取和释放锁。 为了取到锁，客户端应该执行以下操作: 获取当前Unix时间，以毫秒为单位。 依次尝试从N个实例，使用相同的key和具有唯一性的value（例如UUID）获取锁。 客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。 当且仅当(N&#x2F;2+1)的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功，例如3个节点至少需要3/2+1=22个。 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。 若获取锁失败，客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功）。 3.2 使用Redlock单机模式Redis为例: 1234567891011121314151617181920212223242526272829303132333435Config config = new Config();config.useClusterServers().addNodeAddress( &quot;redis://127.0.0.1:6379&quot;,&quot;redis://127.0.0.1:6369&quot;, &quot;redis://127.0.0.1:6359&quot;, &quot;redis://127.0.0.1:6349&quot;,&quot;redis://127.0.0.1:6339&quot;) .setPassword(&quot;******&quot;);// 节点1Config config1 = new Config();config1.useSingleServer().setAddress(&quot;redis://127.0.0.1:6379&quot;);RedissonClient redissonClient1 = Redisson.create(config1);// 节点2Config config2 = new Config();config2.useSingleServer().setAddress(&quot;redis://127.0.0.1:6378&quot;);RedissonClient redissonClient2 = Redisson.create(config2);// 节点3Config config3 = new Config();config3.useSingleServer().setAddress(&quot;redis://127.0.0.1:6377&quot;);RedissonClient redissonClient3 = Redisson.create(config3);// 设置锁定资源名称String resourceName = &quot;REDLOCK&quot;;RLock lock1 = redissonClient1.getLock(resourceName);RLock lock2 = redissonClient2.getLock(resourceName);RLock lock3 = redissonClient3.getLock(resourceName);// 实例化RedissonRedLockRedissonRedLock redLock = new RedissonRedLock(lock1, lock2, lock3);try &#123; boolean isLock = redLock.tryLock(500, 30000, TimeUnit.MILLISECONDS); if (isLock) &#123; //TODO if get lock success, do something; Thread.sleep(30000); &#125;&#125; catch (Exception e) &#123;&#125; finally &#123; //解锁 redLock.unlock();&#125; 最核心的变化就是 RedissonRedLock redLock&#x3D;**new RedissonRedLock(lock1,lock2,lock3);**，因为我这里是以三个节点为例。 如果是主从Redis架构、哨兵Redis架构、集群Redis架构实现Redlock，只需要改变上述config1、config2、config3为主从模式、哨兵模式、集群模式配置即可，但相应需要3个独立的Redis主从集群、3个Redis独立的哨兵集群、3个独立的Cluster集群。 以sentinel模式架构为例，3个sentinel模式集群，如果要获取分布式锁，那么需要向这3个sentinel集群通过EVAL命令执行LUA脚本，需要3/2+1=2，即至少2个sentinel集群响应成功，才算成功的以Redlock算法获取到分布式锁。 4. Redlock问题合集4.1 N个节点的理解假设我们用N(&gt;=3)个节点实现Redlock算法的分布式锁。不是一个有N个主节点的cluster集群；而是要么是N个redis单实例，要么是N个sentinel集群，要么是N个cluster集群。 4.2 失效时间如何设置这个问题的场景是，假设设置失效时间10秒，如果由于某些原因导致10秒还没执行完任务，这时候锁自动失效，导致其他线程也会拿到分布式锁。这确实是Redis分布式最大的问题，不管是普通分布式锁，还是Redlock算法分布式锁，都没有解决这个问题。也有一些文章提出了对失效时间续租，即延长失效时间，很明显这又提升了分布式锁的复杂度（没有现成的框架有实现）。 4.3 redis分布式锁的高可用关于Redis分布式锁的安全性问题，在分布式系统专家Martin Kleppmann和Redis的作者Antirez之间已经发生过一场争论。有兴趣的同学，搜索”基于Redis的分布式锁到底安全吗”就能得到你想要的答案，需要注意的是，有上下两篇（这应该就是传说中的神仙打架吧）。 4.4 使用Zookeeper还是Redis实现分布式锁没有绝对的好坏，只有更适合自己的业务。就性能而言，Redis很明显优于Zookeeper；就分布式锁实现的健壮性(高可用)而言，Zookeeper很明显优于Redis。至于如何选择，还要看具体业务场景。 参考：https://mp.weixin.qq.com/s/8uhYult2h_YUHT7q7YCKYQ","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"}]},{"title":"「Redis」Redis穿透、击穿、雪崩和数据一致性","date":"2019-03-27T11:15:31.000Z","path":"2019/03/27/redis-consistency.html","text":"1. 缓存穿透访问一个不存在的key，缓存不起作用，请求会穿透到DB，流量大时DB会挂掉。 解决方案： 采用布隆过滤器（bloomfilter就类似于一个hash set），使用一个足够大的bitmap，用于存储可能访问的key，不存在的key直接被过滤； 访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。 接口限流与熔断、降级 使用互斥锁排队（分布式环境中要使用分布式锁，单机的话用普通的锁（synchronized、Lock）） 2. 缓存雪崩大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。 解决方案 可以给缓存设置过期时间时加上一个随机值时间，使得每个key的过期时间分布开来，不会集中在同一时刻失效。 建立备份缓存，缓存A和缓存B，A设置超时时间，B不设值超时时间，先从A读缓存，A没有读B，并且更新A缓存和B缓存; 加锁排队，实现同上; 3. 缓存击穿一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。 解决方案 在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。 4. 缓存并发竞争多个redis的client同时set key引起的并发问题（例如：多客户端同时并发写一个key，一个key的值是1，本来按顺序修改为2,3,4，最后是4，但是顺序变成了4,3,2，最后变成了2） 解决方案 如果对这个key操作，不要求顺序：准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可。 如果对这个key操作，要求顺序： 分布式锁+时间戳（假设系统B先抢到锁，将key1设置为{ValueB 7:05}。接下来系统A抢到锁，发现自己的key1的时间戳早于缓存中的时间戳（7:00&lt;7:05），那就不做set操作了） 利用消息队列（把Redis.set操作放在队列中使其串行化,必须的一个一个执行） 5. 缓存和数据库一致性解决方案5.1 并发量、一致性要求都不是很高的场景 写流程：先淘汰缓存，再写数据库，之后再异步将数据刷回缓存 读流程：先读缓存，如果缓存没读到，则去读DB，之后再异步将数据刷回缓存 优点：实现起来简单，异步刷新，补缺补漏 缺点：容灾不足，并发问题，一个比较大的缺陷在于刷新缓存有可能会失败，而失败之后缓存中数据就一直会处于错误状态，所以它并不能保证数据的最终一致性 5.2 业务简单，读写QPS比较低的场景（QPS每秒查询率(Query Per Second)） 写流程：先淘汰缓存，再写数据库，监听从库binlog，通过解析binlog来刷新缓存 读流程：第一步先读缓存，如果缓存没读到，则去读DB，之后再异步将数据刷回缓存 优点：容灾 缺点：只适合简单业务，复杂业务容易发生并发问题（例如：读&#x2F;写的时候，缓存中的数据已失效，此时又发生了更新） 5.3 业务只需要达到“最终一致性”要求的场景 写流程：先淘汰缓存，再写数据库，监听从库binlog，通过分析binlog我们解析出需要需要刷新的数据标识，然后将数据标识写入MQ，接下来就消费MQ，解析MQ消息来读库获取相应的数据刷新缓存。 读流程：第一步先读缓存，如果缓存没读到，则去读DB，之后再异步将数据标识写入MQ（这里MQ与写流程的MQ是同一个），接下来就消费MQ，解析MQ消息来读库获取相应的数据刷新缓存。 优点：容灾完善，无并发问题 缺点：只能达到”最终一致性” 5.4 强一致性的场景 写流程：我们把修改的数据通过Cache_0标记“正在被修改”，如果标记成功，写数据库，删除缓存，监听从库binlog，通过分析binlog我们解析出需要需要刷新的数据标识，然后将数据标识写入MQ，接下来就消费MQ，解析MQ消息来读库获取相应的数据刷新缓存； 那如果标记失败，则要放弃这次修改。 读流程：先读Cache_0，看看要读的数据是否被标记，如果被标记，则直接读主库；如果没有被标记，读缓存，如果缓存没读到，则去读DB，之后再异步将数据标识写入MQ（这里MQ与写流程的MQ是同一个），接下来就消费MQ，解析MQ消息来读库获取相应的数据刷新缓存。 优点：容灾完善，无并发问题 缺点：增加Cache_0强依赖，复杂度是比较高的（涉及到Databus、MQ、定时任务等等组件）","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"}]},{"title":"「Redis」深入学习Redis及集群","date":"2019-03-20T08:19:26.000Z","path":"2019/03/20/redis-cluster.html","text":"Redis本质上是一个Key-Value类型的内存数据库，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过10万次读写操作，是已知性能最快的Key-Value DB。 Redis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB。另外Redis也可以对存入的Key-Value设置expire时间。 Redis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 1. Redis数据结构及命令操作1.1 基本概念及操作 默认16个数据库，类似数组下表从零开始，初始默认使用零号库； 统一密码管理，16个库都是同样密码，要么都OK要么一个也连接不上，redis默认端口是6379； select命令切换数据库：select 0-15； dbsize：查看当前数据库的key的数量； flushdb：清空当前库； flushall；通杀全部库； 1.2 Redis数据结构redis存储的是：key-value格式的数据，其中key都是字符串，value有5种不同的数据结构:String、Hash、List、Set、Zset(Sorted Set) String：set, get, del, append, strlen Hash：hset, hget, hdel, hmset(批量设值), hmget, hgetall List：lpush, rpush, lrange, lpop(删除), rpop, lindex Set：sadd, smembers, srem(根据可以移除member), sismember(判断是否为key的成员) ZSet：zadd, zrange, zrem 1.3 Redis键(key)–常用命令介绍 keys *：查看所有 key ； exists key的名字：判断某个 key 是否存在； move key dbID（0-15）： 当前库就没有了，被移除了； expire key 秒钟： 为给定的 key 设置过期时间； ttl key： 查看还有多少秒过期，-1表示永不过期，-2表示已过期； type key： 查看你的 key 是什么类型； 2. Redis持久化Redis作为一个键值对内存数据库(NoSQL)，数据都存储在内存当中，在处理客户端请求时，所有操作都在内存当中进行，为了避免内存中数据丢失，Redis提供了RDB和AOF两种不同的数据持久化方式。 2.1 RDB（Redis DataBase）RDB是一种快照存储持久化方式，具体就是将Redis某一时刻的内存数据保存到硬盘的文件当中，默认保存的文件名为dump.rdb，而在Redis服务器启动时，会重新加载dump.rdb文件的数据到内存当中恢复数据。 开启RDB持久化方式一：save命令，或bgsave(异步) 开启方式二：在Redis配置文件redis.conf配置，配置完后启动时加载：redis-server redis.conf 123save 900 1 # 900s内至少达到一条写命令save 300 10 # 300s内至少达至10条写命令save 60 10000 # 60s内至少达到10000条写命令 RDB的几个优点 与AOF方式相比，通过rdb文件恢复数据比较快。 rdb文件非常紧凑，适合于数据备份。 通过RDB进行数据备，由于使用子进程生成，所以对Redis服务器性能影响较小。 RDB的几个缺点 如果服务器宕机的话，采用RDB的方式会造成某个时段内数据的丢失，比如我们设置10分钟同步一次或5分钟达到1000次写入就同步一次，那么如果还没达到触发条件服务器就死机了，那么这个时间段的数据会丢失。 使用save命令会造成服务器阻塞，直接数据同步完成才能接收后续请求。 使用bgsave命令在forks子进程时，如果数据量太大，forks的过程也会发生阻塞，另外，forks子进程会耗费内存。 2.2 AOF(Append-only file)与RDB存储某个时刻的快照不同，AOF持久化方式会记录客户端对服务器的每一次写操作命令（以日志的形式），并将这些写操作以Redis协议追加保存到以后缀为aof文件末尾，在Redis服务器重启时，会加载并运行aof文件的命令，以达到恢复数据的目的。 开启方式：在Redis配置文件redis.conf配置 123456appendonly yes # 开启aof机制appendfilename &quot;appendonly.aof&quot; # aof文件名# 写入策略,always表示每个写操作都保存到aof文件中,也可以是everysec(每秒写入一次)或no(操作系统处理)appendfsync alwaysno-appendfsync-on-rewrite no # 默认不重写aof文件dir ~/redis/ # 保存目录 aof文件太大，加载aof文件恢复数据时，就会非常慢，为了解决，Redis通过重写aof，可以生成一个恢复当前数据的最少命令集，两种方式：配置no-appendfsync-on-rewrite(默认no)，或者客户端向服务器发送bgrewriteaof命令 AOF的优点：AOF只是追加日志文件，因此对服务器性能影响较小，速度比RDB要快，消耗的内存较少。 AOF的缺点：AOF方式生成的日志文件太大，即使通过AFO重写，文件体积仍然很大。恢复数据的速度比RDB慢。 当RDB与AOF两种方式都开启时，Redis会优先使用AOF日志来恢复数据，因为AOF保存的文件比RDB文件更完整。 2.2.1 AOF文件修复 备份被写坏的AOF文件 运行redis-check-aof –fix进行修复 用diff -u来看下两个文件的差异，确认问题点 重启redis，加载修复后的AOF文件 3. Redis的高并发和快速原因 redis是基于内存的，内存的读写速度非常快； redis是单线程的，省去了很多上下文切换线程的时间； redis使用多路复用技术，可以处理并发的连接。非阻塞IO 内部实现采用epoll，采用了epoll+自己实现的简单的事件框架。epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性，绝不在io上浪费一点时间。 另外，数据结构也帮了不少忙，Redis全程使用hash结构，读取速度快，还有一些特殊的数据结构，对数据存储进行了优化，如压缩表，对短数据进行压缩存储，再如，跳表，使用有序的数据结构加快读取的速度。 还有一点，Redis采用自己实现的事件分离器，效率比较高，内部采用非阻塞的执行方式，吞吐能力比较大。 4. Redis利用哨兵(Sentinel)，复制(Replication)这两个功能来保证高可用 哨兵(Sentinel)：可以管理多个Redis服务器，它提供了监控，提醒以及自动的故障转移的功能。 集群监控：负责监控Redis master和slave进程是否正常工作 消息通知：如果某个Redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员 故障转移：如果master node挂掉了，会自动转移到slave node上 配置中心：如果故障转移发生了，通知client客户端新的master地址 复制(Replication)：则是负责让一个Redis服务器可以配备多个备份的服务器。 从数据库向主数据库发送sync(数据同步)命令。 主数据库接收同步命令后，会保存快照，创建一个RDB文件。 当主数据库执行完保持快照后，会向从数据库发送RDB文件，而从数据库会接收并载入该文件。 主数据库将缓冲区的所有写命令发给从服务器执行。 以上处理完之后，之后主数据库每执行一个写命令，都会将被执行的写命令发送给从数据库。 5. Redis 主从复制、哨兵和集群这三个有什么区别主从复制是为了数据备份，哨兵是为了高可用，Redis主服务器挂了哨兵可以切换，集群则是因为单实例能力有限，搞多个分散压力。 主从模式：读写分离，备份，一个Master可以有多个Slaves。 哨兵entinel：监控，自动转移，哨兵发现主服务器挂了后，就会从slave中重新选举一个主服务器。 集群Cluster：为了解决单机Redis容量有限的问题，将数据按一定的规则分配到多台机器，内存&#x2F;QPS不受限于单机，可受益于分布式集群高扩展性。 6. Redis Cluster集群Redis Cluster，是Redis 3.0开始引入的分布式存储方案。集群由多个节点(Node)组成，Redis的数据分布在这些节点中。集群中的节点分为主节点和从节点：只有主节点负责读写请求和集群信息的维护；从节点只进行主节点数据和状态信息的复制。 集群的作用： 数据分区：数据分区(或称数据分片)是集群最核心的功能。 高可用：集群支持主从复制和主节点的自动故障转移（与哨兵类似）；当任一节点发生故障时，集群仍然可以对外提供服务。 6.1 Redis Cluster集群的搭建可以分为四步： 启动节点：将节点以集群模式启动，此时节点是独立的，并没有建立联系； 节点握手：让独立的节点连成一个网络； 分配槽：将16384个槽分配给主节点； 指定主从关系：为从节点指定主节点。 6.2 Redis Cluster工作原理 客户端与Redis节点直连,不需要中间Proxy层，直接连接任意一个Master节点 根据公式HASH_SLOT=CRC16(key) mod 16384，计算出映射到哪个分片上，然后Redis会去相应的节点进行操作 123456 CRC16(key) | 0~5460 | &lt;--Slot--|Redis(M)|&lt;---|Redis(S可多个从) mode 16384 |Client --------------&gt; | 5461~10922| &lt;--Slot--|Redis(M)|&lt;---|Redis(S可多个从) | |10923~10383| &lt;--Slot--|Redis(M)|&lt;---|Redis(S可多个从) 6.3 Redis Cluster优点: 无需Sentinel哨兵监控，如果Master挂了，Redis Cluster内部自动将Slave切换Master 可以进行水平扩容 支持自动化迁移，当出现某个Slave宕机了，那么就只有Master了，这时候的高可用性就无法很好的保证了，万一master也宕机了，咋办呢？ 针对这种情况，如果说其他Master有多余的Slave ，集群自动把多余的Slave迁移到没有Slave的Master 中。 6.4 Redis Cluster缺点: 批量操作是个坑（不同的key会划分到不同的slot中，因此直接使用mset或者mget等操作是行不通） 资源隔离性较差，容易出现相互影响的情况。 6.5 Redis Cluster总结： Redis Cluster集群架构，不同的key是有可能分配在不同的Redis节点上的，在这种情况下Redis的事务机制是不生效。 单机下的redis可以支持16个数据库（db0 ~ db15），在Redis Cluster集群架构下只有一个数据库空间，即db0。 不同的key会划分到不同的slot中，因此直接使用mset或者mget等操作是行不通。 如果Hash对象非常大，是不支持映射到不同节点的！只能映射到集群中的一个节点上。 Redis集群模式下进行批量操作：如果执行的key数量比较少，就用串行get操作； 如果需要执行的key很多，就使用Hashtag保证这些key映射到同一台redis节点上。 Redis Cluster的架构，是属于分片集群的架构，不做读写分离，因为redis本身在内存上操作，不会涉及IO吞吐，即使读写分离也不会提升太多性能，Redis在生产上的主要问题是考虑容量，单机最多10-20G，key太多降低redis性能.因此采用分片集群结构，已经能保证了我们的性能。其次，用上了读写分离后，还要考虑主从一致性，主从延迟等问题，徒增业务复杂度。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Redis","slug":"Redis","permalink":"http://chaooo.github.io/tags/Redis/"}]},{"title":"「MySQL」MySQL慢日志查询分析","date":"2019-02-14T16:26:43.000Z","path":"2019/02/15/mysql-slow-log.html","text":"同大多数关系型数据库一样，日志文件是MySQL数据库的重要组成部分。MySQL有几种不同的日志文件，通常包括错误日志文件，二进制日志，通用日志，慢查询日志，等等。这些日志可以帮助我们定位mysqld内部发生的事件，数据库性能故障，记录数据的变更历史，用户恢复数据库等等。 错误日志：记录启动、运行或停止mysqld时出现的问题。 通用日志：记录建立的客户端连接和执行的语句。 更新日志：记录更改数据的语句。该日志在MySQL 5.1中已不再使用。 二进制日志：记录所有更改数据的语句。还用于复制。 慢查询日志：记录所有执行时间超过long_query_time秒的所有查询或不使用索引的查询 Innodb日志：InnoDB redo log(记录了事务的行为，可以很好的通过其对页进行“重做”操作) 1. 开启慢查询日志开启慢查询日志，可以让MySQL记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能。通过show variables like &#39;slow_query%&#39;;查询是否开了慢查询(默认禁用OFF) 1234567mysql&gt; show variables like &#x27;%slow_query_log%&#x27;;+---------------------+------------------------------------------------------+| Variable_name | Value |+---------------------+------------------------------------------------------+| slow_query_log | OFF || slow_query_log_file | D:\\mysql-5.7.27-winx64\\data\\DESKTOP-E9F062A-slow.log |+---------------------+------------------------------------------------------+ slow_query_log 慢查询开启状态 OFF 未开启 ON 为开启slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录） 开启慢查询，需要设置slow_query_log参数。当然，如果不是调优需要的话，一般不建议开启该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志写入文件。 12mysql&gt; set global slow_query_log = 1; //设置开启或者关闭，0为关闭，1为开启mysql&gt; set global long_query_time = 3; //设置慢的阙值时间，默认10秒 如果通过终端命令设定的话，需要重新连接或新开一个会话才能看到修改值 使用set global slow_query_log 命令开启慢查询日志，只对当前数据库生效，如果Mysql重启后则会失效。如果要永久生效，必须修改my.cnf配置文件(其他系统变量也是如此) 123456[mysqld]slow_query_log = 1 #开启slow_query_log_file = /mysql-5.7.27-winx64/data/mysql-slow.log #默认host_name_show.loglong_query_time = 3 #默认10秒（查询超过多少秒才记录）log-queries-not-using-indexes = on #如果值设置为ON，则会记录所有没有利用索引的查询，一般在性能调优的时候会暂时开启。log_output = &#x27;FILE,TABLE&#x27; #输出的格式(FILE:文本, TABLE:表中, FILE,TABLE:同时输出到文本和表中) 插入一条测试慢查询 1mysql&gt; select sleep(5); 通过MySQL命令查看有多少慢查询 123456mysql&gt; show global status like &#x27;%Slow_queries%&#x27;;+---------------+-------+| Variable_name | Value |+---------------+-------+| Slow_queries | 1 |+---------------+-------+ 2. 慢查询日志分析工具 工具 一般统计 高级统计 语言 优势 针对log mysqldumpslow √ × perl mysql官方自带 slow myprofi √ × php 简单 slow mysql-log-filter √ 部分√ python 简单 slow mysql-explain-slow-log √ × perl 无 slow mysqlbinlog √ × 二进制 mysql官方自带 binary log mysqlsla √ √ perl 总能强大，使用简单，自定义能力强 所有日志，包括自定义日志 pt-query-digest √ √ perl 总能强大，使用简单，自定义能力强 所有日志，包括自定义日志 2.1 mysqldumpslow MySQL自带的慢查询日志分析工具mysqldumpslow主要功能是, 统计不同慢sql的: 出现次数(Count), 执行最长时间(Time), 累计总耗费时间(Time), 等待锁的时间(Lock), 发送给客户端的行总数(Rows), 扫描的行总数(Rows), 用户以及sql语句本身(抽象了一下格式, 比如 limit 1, 20 用 limit N,N 表示).安装后基本使用：1234mysqldumpslow -s r -t 10 /data/mysql/mysql-slow.log //得到返回记录集最多的10个SQLmysqldumpslow -s c -t 10 /data/mysql/mysql-slow.log //得到访问次数最多的10个SQL mysqldumpslow -s t -t 10 -g &quot;left join&quot; /data/mysql/mysql-slow.log //得到按照时间排序的前10条里面含有做了连接的查询SQLmysqldumpslow -s r -t 10 /data/mysql/mysql-slow.log | more //另外建议在使用这些命令时结合|和more使用，否则有可能出现爆屏情况 2.2 mysqlslahackmysql.com推出的一款日志分析工具(该网站还维护了 mysqlreport, mysqlidxchk 等比较实用的mysql工具) 整体来说, 功能非常强大. 数据报表,非常有利于分析慢查询的原因, 包括执行频率, 数据量, 查询消耗等. 安装后基本使用方法： 1mysqlsla -lt slow -sort t_sum -top 1000 /tmp/slow_query.log 结果选项说明： 总查询次数 (queries total), 去重后的sql数量 (unique), 输出报表的内容排序(sorted by), 最重大的慢sql统计信息(包括 平均执行时间, 等待锁时间, 结果行的总数, 扫描的行总数) Count, sql的执行次数及占总的slow log数量的百分比. Time, 执行时间, 包括总时间, 平均时间, 最小, 最大时间, 时间占到总慢sql时间的百分比. 95% of Time, 去除最快和最慢的sql, 覆盖率占95%的sql的执行时间. Lock Time, 等待锁的时间. 95% of Lock , 95%的慢sql等待锁时间. Rows sent, 结果行统计数量, 包括平均, 最小, 最大数量. Rows examined, 扫描的行数量. Database, 属于哪个数据库 Users, 哪个用户,IP, 占到所有用户执行的sql百分比 Query abstract, 抽象后的sql语句 Query sample, sql语句 mysqlsla常用参数说明： -log-type (-lt) type logs:通过这个参数来制定log的类型，主要有slow, general, binary, msl, udl,分析slow log时通过制定为slow -sort:t_sum:按总时间排序(默认)，c_sum:按总次数排序c_sum_p: sql语句执行次数占总执行次数的百分比。 -top:显示sql的数量，默认是10,表示按规则取排序的前多少条 –statement-filter (-sf) [+-][TYPE]:过滤sql语句的类型，比如select、update、drop，[TYPE] 有SELECT, CREATE, DROP, UPDATE, INSERT，例如”+SELECT,INSERT”，不出现的默认是-，即不包括。 -db：要处理哪个库的日志： 123# 举个例子，只取funsion数据库的select语句，并按照总时间排序，取前1000条数据# 保存到当前目录下的 slow_query.pretty.log文件中mysqlsla -lt slow -sort t_sum -sf &quot;+select&quot; -db funsion -top 1000 /tmp/slow_query.log &gt; ./slow_query.pretty.log 深度使用可参考： MySQL日志分析神器之mysqlsla 2.3 pt-query-digestpt-query-digest是用于分析mysql慢查询的一个工具，它可以分析binlog、General log、slowlog，也可以通过SHOWPROCESSLIST或者通过tcpdump抓取的MySQL协议数据来进行分析。可以把分析结果输出到文件中，分析过程是先对查询语句的条件进行参数化，然后对参数化以后的查询进行分组统计，统计出各查询的执行时间、次数、占比等，可以借助分析结果找出问题进行优化。 12# 分析最近12小时内的查询：pt-query-digest --since=12h slow.log &gt; slow_report2.log pt-query-digest语法及重要选项12345678910111213pt-query-digest [OPTIONS] [FILES] [DSN] --create-review-table 当使用--review参数把分析结果输出到表中时，如果没有表就自动创建。 --create-history-table 当使用--history参数把分析结果输出到表中时，如果没有表就自动创建。 --filter 对输入的慢查询按指定的字符串进行匹配过滤后再进行分析 --limit 限制输出结果百分比或数量，默认值是20,即将最慢的20条语句输出，如果是50%则按总响应时间占比从大到小排序，输出到总和达到50%位置截止。 --host mysql服务器地址 --user mysql用户名 --password mysql用户密码 --history 将分析结果保存到表中，分析结果比较详细，下次再使用--history时，如果存在相同的语句，且查询所在的时间区间和历史表中的不同，则会记录到数据表中，可以通过查询同一CHECKSUM来比较某类型查询的历史变化。 --review 将分析结果保存到表中，这个分析只是对查询条件进行参数化，一个类型的查询一条记录，比较简单。当下次使用--review时，如果存在相同的语句分析，就不会记录到数据表中。 --output 分析结果输出类型，值可以是report(标准分析报告)、slowlog(Mysql slow log)、json、json-anon，一般使用report，以便于阅读。 --since 从什么时间开始分析，值为字符串，可以是指定的某个”yyyy-mm-dd [hh:mm:ss]”格式的时间点，也可以是简单的一个时间值：s(秒)、h(小时)、m(分钟)、d(天)，如12h就表示从12小时前开始统计。 --until 截止时间，配合—since可以分析一段时间内的慢查询。 分析pt-query-digest输出结果 总体统计结果 Overall：总共有多少条查询 Time range：查询执行的时间范围 unique：唯一查询数量，即对查询条件进行参数化以后，总共有多少个不同的查询 total：总计 min：最小 max：最大 avg：平均 95%：把所有值从小到大排列，位置位于95%的那个数，这个数一般最具有参考价值 median：中位数，把所有值从小到大排列，位置位于中间那个数 查询分组统计结果 Rank：所有语句的排名，默认按查询时间降序排列，通过–order-by指定 Query ID：语句的ID，（去掉多余空格和文本字符，计算hash值） Response：总的响应时间 time：该查询在本次分析中总的时间占比 calls：执行次数，即本次分析总共有多少条这种类型的查询语句 R&#x2F;Call：平均每次执行的响应时间 V&#x2F;M：响应时间Variance-to-mean的比率 Item：查询对象 每一种查询的详细统计结果 由下面查询的详细统计结果，最上面的表格列出了执行次数、最大、最小、平均、95%等各项目的统计。 ID：查询的ID号，和上图的Query ID对应 Databases：数据库名 Users：各个用户执行的次数（占比） Query_time distribution ：查询时间分布, 长短体现区间占比，本例中1s-10s之间查询数量是10s以上的两倍。 Tables：查询中涉及到的表 Explain：SQL语句 3. explain查看执行计划在上面的慢查询中，我们已经将查询时间超过阀值的sql语句过滤了出来，explain+查询语句具体分析是哪里出了问题。MySQL 提供了一个 Explain 命令, 它可以对 select 语句进行分析, 并输出 select 执行的详细信息, 以供开发人员针对性优化. 123456789101112131415mysql&gt; explain select * from user_info where id = 2\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user_info partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.06 sec) 各列的含义如下: id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_type: SELECT 查询的类型. SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. table: 查询的是哪个表 partitions: 匹配的分区 type: join 类型 type字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过type字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可. eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 &#x3D;, 查询效率较高. ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询. range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 &#x3D;, &lt;&gt;, &gt;, &gt;&#x3D;, &lt;, &lt;&#x3D;, IS NULL, &lt;&#x3D;&gt;, BETWEEN, IN() 操作中. index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. possible_keys: 此次查询中可能选用的索引 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key: 此字段是 MySQL 在当前查询时所真正使用到的索引. key_len: 表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到. ref: 哪个字段或常数与 key 一起被使用 rows: 显示此查询一共扫描了多少行. 这个是一个估计值. rows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数, 这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. filtered: 表示此查询条件所过滤的数据的百分比 extra: EXplain 中的很多额外的信息会在 Extra 字段显示 Using filesort: 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. Using index: “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary: 查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化. type 类型的性能比较通常来说, 不同的 type 类型的性能关系如下: ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的. 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快. 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. 4. MySQL性能分析语句show profileQuery Profile是MySQL自带的一种Query诊断分析工具，可以完整的显示一条sql执行的各方面的详细信息，默认关闭; 看看当前的MySQL版本是否支持: show variables like &#39;profiling&#39;;或show variables like &#39;profiling%&#39;; 1234567mysql&gt; show variables like &#x27;profiling%&#x27;;+------------------------+-------+| Variable_name | Value |+------------------------+-------+| profiling | OFF || profiling_history_size | 15 |+------------------------+-------+ 使用前需要开启: set profiling = 1; (1:开 / 0:关) 1mysql&gt; set profiling = 1; 运行sql后，查询结果show profiles; 1234567891011121314151617mysql&gt; SHOW PROFILES\\G*************************** 1. row ***************************Query_ID: 1Duration: 0.02949950 Query: explain select * from user*************************** 2. row ***************************Query_ID: 2Duration: 0.03405350 Query: select * from housedemo*************************** 3. row ***************************Query_ID: 3Duration: 0.07813800 Query: select * from house*************************** 4. row ***************************Query_ID: 4Duration: 0.00018150 Query: show prifiles 诊断SQL, show profile Type io for query Query_ID LIMIT部分的用法与SELECT中LIMIT子句一致，不赘述。 Type是可选的，取值范围可以如下： ALL 显示所有性能信息 BLOCK IO 显示块IO操作的次数 CONTEXT SWITCHES 显示上下文切换次数，不管是主动还是被动 CPU 显示用户CPU时间、系统CPU时间 IPC 显示发送和接收的消息数量 MEMORY [暂未实现] PAGE FAULTS 显示页错误数量 SOURCE 显示源码中的函数名称与位置 SWAPS 显示SWAP的次数 1234567891011121314151617181920mysql&gt; show profile cpu,block io for query 3;+----------------------+----------+----------+------------+--------------+---------------+| Status | Duration | CPU_user | CPU_system | Block_ops_in | Block_ops_out |+----------------------+----------+----------+------------+--------------+---------------+| starting | 0.000077 | 0.000000 | 0.000000 | NULL | NULL || checking permissions | 0.000013 | 0.000000 | 0.000000 | NULL | NULL || Opening tables | 0.031992 | 0.000000 | 0.000000 | NULL | NULL || init | 0.000059 | 0.000000 | 0.000000 | NULL | NULL || System lock | 0.000016 | 0.000000 | 0.000000 | NULL | NULL || optimizing | 0.000007 | 0.000000 | 0.000000 | NULL | NULL || statistics | 0.000017 | 0.000000 | 0.000000 | NULL | NULL || preparing | 0.008535 | 0.000000 | 0.000000 | NULL | NULL || executing | 0.000016 | 0.000000 | 0.000000 | NULL | NULL || Sending data | 0.037234 | 0.000000 | 0.000000 | NULL | NULL || end | 0.000011 | 0.000000 | 0.000000 | NULL | NULL || query end | 0.000012 | 0.000000 | 0.000000 | NULL | NULL || closing tables | 0.000014 | 0.000000 | 0.000000 | NULL | NULL || freeing items | 0.000108 | 0.000000 | 0.000000 | NULL | NULL || cleaning up | 0.000030 | 0.000000 | 0.000000 | NULL | NULL |+----------------------+----------+----------+------------+--------------+---------------+ 从图中可以看到开始，打开表，加载，关闭表，释放资源、记录日志，清理的你工作，在这完全可以看到一条SQL的完整生命周期。 日常开发需要注意 如果show profile … for query id；出现了如下四个，则必须优化这条sql。 converting HEAP to MyISAM 查询结果太大， 内存都不够用了网磁盘上搬了 Creating tmp table 创建临时表 拷贝数据到临时表：假设要查询两百万数据，刚好匹配的条件有一百万，恰巧要把这一百万的数据拷贝到临时表，然后再把数据推送给用户，最后再把临时表删掉，这个时候就是导致SQL变慢的罪魁祸首 用完再删除 Copying to tmp table on disk 把内存中临时表复制到磁盘，危险！！！ locked 5. 全局查询日志切记：永远不要再生产环境开启这个功能。全局查询日志有时也能帮助我们来调SQL。但是，切记，这家伙只能在测试环境使用，绝不可以在生产环境使用。 命令启用 12mysql&gt; set global general_log=1; #开启后会把所有的SQL偷偷的记录mysql&gt; set global log_output=&#x27;TABLE&#x27;; 配置启用, 在MySQL的my.cnf中，设置如下： 123456#开启general_log=1#记录日志文件的路径general_log_file=/path/logfile#输出格式log_output=file 此后，你所编写的SQL语句，将会记录到MySQL库里的general_log表，可以用下面的命令查看。 1select * from mysql.general_log; 场景：如果需要做系统的定案分析(今天下午2点-3点出的故障），如果要观察和复现的话，可以在测试环境下模拟一遍，然后把所有的问题复现一下。那么用general_log这个表来收集什么时间段发生了什么样的SQL，帮助我们定位收集。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"「MySQL」MySQL事务处理与并发控制","date":"2019-02-09T15:31:48.000Z","path":"2019/02/09/mysql-transaction.html","text":"1. MySQL事务 事务: 数据库操作的最小工作单元，是作为单个逻辑工作单元执行的一系列操作；事务是一组不可再分割的操作集合(工作逻辑单元)； 事务的特性(ACID)： 原子性（Atomicity，或称不可分割性）：最小的工作单元，整个工作单元要么一起提交成功，要么全部失败回滚 一致性（Consistency）：事务中操作的数据及状态改变是一致的，即写入资料的结果必须完全符合预设的规则， 不会因为出现系统意外等原因导致状态的不一致 隔离性（Isolation，又称独立性）：一个事务所操作的数据在提交之前，对其他事务的可见性设定（一般设定为不可见） 持久性（Durability）：事务所做的修改就会永久保存，不会因为系统意外导致数据的丢失 事务的开启与提交模式 若参数autocommit&#x3D;0，自动开启手动提交 若参数autocommit&#x3D;1（系统默认值），又分为两种状态： 自动开启自动提交：用户的每一个操作都是一个完整的事务周期。 手动开启手动提交：从用户执行start transaction命令到用户执行commit命令之间的一系列操作为一个完整的事务周期。若不执行commit命令，系统则默认事务回滚。 begin 或者 start transaction – 开启事务 commit 或者 rollback – 事务提交或回滚 1.1 事务的隔离级别 查看&#x2F;设置隔离级别 查看：SELECT @@tx_isolation 设置：set tx_isolation=&#39;xxx&#39; 读未提交（Read Uncommitted） 事务未提交对其他事务也是可见的，脏读（dirty read） 读提交（Read Committed）–解决脏读问题 一个事务开始之后，只能看到自己提交的事务所做的修改，不可重复读（nonrepeatable read） 可重复读（Repeatable Read）–解决不可重复读问题 在同一个事务中多次读取同样的数据结果是一样的，这种隔离级别未定义解决幻读的问题 串行化（Serializable）–解决所有问题 最高的隔离级别，通过强制事务的串行执行，但是会导致大量超时以及锁争用问题 Mysql默认采用REPEATABLE_READ隔离级别，Oracle默认采用READ_COMMITTED隔离级别。事务的隔离级别的实现：锁、MVCC（多版本并发控制 Multiversion Currency Control）。 1.2 事务的七大传播行为Spring在TransactionDefinition接口中规定了7种类型的事务传播行为。事务传播行为是Spring框架独有的事务增强特性，他不属于的事务实际提供方数据库行为。 事务传播行为用来描述由某一个事务传播行为修饰的方法被嵌套进另一个方法的时事务如何传播。 @Transactional(propagation = Propagation.REQUIRED) 第一类：运行在同一个事务 **REQUIRED**（required）：默认，支持当前事务，如果当前没有事务，就新建一个事务。 SUPPORTS（supports）：支持当前事务，如果当前没有事务，就不使用事务(以非事务方式执行) MANDATORY（mandatory）：支持当前事务，如果当前没有事务，就抛出异常 第二类：运行在不同事务 **REQUIRES_NEW**（requires new）：新建事务，如果当前存在事务，把当前事务挂起 NOT_SUPPORTED(not supported)：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起 NEVER（never）：以非事务方式执行，如果当前存在事务，则抛出异常 第三类：嵌套执行–即外层事务如果失败，内层事务要么回滚到保存点要么回滚到初始状态 **NESTED**（nested）：如果当前事务存在，则嵌套事务执行 2. 锁锁是用于管理不同事务对共享资源的并发访问，InnoDB存储引擎支持行锁和表锁（InnoDB表锁是另类的行锁） InnoDB行锁 共享锁（读锁）：Shared Locks 排它锁（写锁）：Exclusive Locks InnoDB表锁 意向锁共享锁（IS）：Intention Shared Locks 意向锁排它锁（IX）：Intention Exclusive Locks 自增锁：AUTO-INC Locks 行锁的算法 记录锁 Record Locks 间隙锁 Gap Locks 临键锁 Next-key Locks 2.1 共享锁(Shared) &amp; 排他锁(Exclusive)它们都是标准的行级锁。 共享锁（S锁）：读锁，读锁允许多个连接可以同一时刻并发的读取同一资源,互不干扰，但是只能读不能修改; 加锁： select * from users WHERE id=1 LOCK IN SHARE MODE; 解锁：**commit或rollback** 排他锁（X锁）：写锁，一个写锁会阻塞其他的写锁或读锁，保证同一时刻只有一个连接可以写入数据，同时防止其他用户对这个数据的读写。 加锁： select * from users WHERE id=1 FOR UPDATE; delete/update/insert 默认上 X 锁 解锁：**commit或rollback** 注意：所谓共享锁、排他锁其实均是锁机制本身的策略，通过这两种策略对锁做了区分。 InnoDB的行锁是通过给索引上的索引项加锁来实现的。 只有通过索引条件进行数据检索，InnoDB才使用行级锁，否则，InnoDB 将使用表锁（锁住索引的所有记录） 2.2 意向锁(Intention) &amp; 自增锁(AUTO-INC)它们都是标准的表级锁。 意向锁（Intention Locks）：表级别的锁。先提前声明一个意向，并获取表级别的意向锁（IS或IX），如果获取成功，才被允许对该表加行锁(S或X)。(即一个数据行加锁前必须先取得该表的意向锁) 意向锁(IS、IX)是InnoDB数据操作之前自动加的，不需要用户干预 意义：当事务想去进行锁表时，可以先判断意向锁是否存在，存在时则可快速返回该表不能启用表锁 自增锁（AUTO-INC Locks）：针对自增列自增长的一个特殊的表级别锁 show variables like &#39;innodb_autoinc_lock_mode&#39;; 默认取值1，代表连续，事务未提交ID永久丢失 2.3 记录锁(Record) &amp; 间隙锁(Gap) &amp; 临键锁(Next-key) 临键锁 Next-key locks： 锁住记录+区间（左开右闭） Innodb默认行锁算法 当sql执行按照索引进行数据的检索时,查询条件为范围查找（between and、&lt;、&gt;等）并有数 据命中则此时SQL语句加上的锁为Next-key locks，锁住索引的记录+区间（左开右闭） 间隙锁 Gap locks： 锁住数据不存在的区间（左开右开） 当记录不存在，临键锁退化为Gap锁 当sql执行按照索引进行数据的检索时，查询条件的数据不存在，这时SQL语句加上的锁即为 Gap locks，Gap只在RR事务隔离级别存在，锁住索引不存在的区间（左开右开） 记录锁 Record locks： 锁住具体的索引项 唯一性(主键&#x2F;唯一)索引，条件为精准匹配，退化成Record锁 当sql执行按照唯一性（Primary key、Unique key）索引进行数据的检索时，查询条件等值匹 配且查询的数据是存在，这时SQL语句加上的锁即为记录锁Record locks，锁住具体的索引项 2.4 死锁的产生与避免 死锁 在InnoDB中，锁是逐步获得的，就造成了死锁的可能（2个或以上并发事务） 每个事务都持有锁（或者是已经在等待锁）; 每个事务都需要再继续持有锁；事务之间产生加锁的循环等待，形成死锁。 死锁的产生与避免 类似的业务逻辑以固定的顺序访问表和行。 大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概 率。 降低隔离级别，如果业务允许，将隔离级别调低也是较好的选择 为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添 加上锁（或者说是表锁） 3. MVCC(多版本并发控制)Multiversion concurrency control (多版本并发控制)： MVCC 就是 同一份数据临时保留多版本的一种方式，进而实现并发控制 是行级锁的变种，它在普通读情况下避免了加锁操作，因此开销更低。 MVCC 提供了时点（point in time）一致性视图。MVCC 并发控制下的读事务一般使用时间戳或者事务ID去标记当前读的数据库的状态（版本），读取这个版本的数据。读、写事务相互隔离，不需要加锁。读写并存的时候，写操作会根据目前数据库的状态，创建一个新版本，并发的读则依旧访问旧版本的数据。 3.1 MVCC逻辑流程 在MySQL中建表时，每个表都会有三列隐藏记录，其中和MVCC有关系的有两列 DB_TRX_ID：数据行的版本号 DB_ROLL_PT：删除版本号 MVCC逻辑流程-插入 在插入数据的时候，会把全局事务ID记录到列DB_TRX_ID中去 MVCC逻辑流程-删除 执行完删除SQL之后数据并没有被真正删除，而是对删除版本号(DB_ROLL_PT)做改变 MVCC逻辑流程-修改 修改数据的时候 会先复制一条当前记录行数据，同时标记这条数据的数据行版本号为当前事务ID，最后把旧数据的删除版本号标记为新数据行版本号的值(即当前事务ID)。 MVCC逻辑流程-查询 查找数据行版本号早于当前事务ID的数据行记录 也就是说，数据行的版本号要小于或等于 当前事务ID，这样也就确保了读取到的数据是当前事务开始前已经存在的数据，或者是自身事务改变过的数据 查找删除版本号要么为NULL，要么大于当前事务版本号的记录 这样确保查询出来的数据行记录在事务开启之前没有被删除 MySQL解决不可重复读和脏读并不是单纯利用 MVCC 机制来实现的。 4. MySQL事务日志(Undo Log和Redo Log)innodb事务日志包括redo log和undo log。redo log是重做日志，提供前滚操作，undo log是回滚日志，提供回滚操作。undo log不是redo log的逆向过程，其实它们都算是用来恢复的日志： redo log通常是物理日志，记录的是数据页的物理修改，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页(恢复数据页，且只能恢复到最后一次提交的位置)。 undo用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。 s 4.1 Undo Log Undo Log定义： undo意为取消，以撤销操作为目的，返回指定某个状态的操作 undo log指事务开始之前，在操作任何数据之前,首先将需操作的数据备份到一个地方 (Undo Log) UndoLog是为了实现事务的原子性而出现的产物 Undo Log实现事务原子性： 事务处理过程中如果出现了错误或者用户执行了 ROLLBACK语句,Mysql可以利用Undo Log中的备份 将数据恢复到事务开始之前的状态 UndoLog在Mysql innodb存储引擎中用来实现多版本并发控制 Undo log实现多版本并发控制： 事务未提交之前，Undo保存了未提交之前的版本数据，Undo中的数据可作为数据旧版本快照供 其他并发事务进行快照读 4.2 当前读 &amp; 快照读 快照读： SQL读取的数据是快照版本，也就是历史版本，普通的SELECT就是快照读 innodb快照读，数据的读取将由 cache(原本数据) + undo(事务修改过的数据) 两部分组成 当前读： SQL读取的数据是最新版本。通过锁机制来保证读取的数据无法通过其他事务进行修改 UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE都是当前读 4.3 Redo Log Undo Log定义： Redo，顾名思义就是重做。以恢复操作为目的，重现操作； Redo log指事务中操作的任何数据,将最新的数据备份到一个地方 (Redo Log) Redo log的持久： 不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入redo 中。具体 的落盘策略可以进行配置 RedoLog是为了实现事务的持久性而出现的产物 Redo Log实现事务持久性： 防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的未入磁盘数据进行持久化这一特性。 一旦事务成功提交且数据持久化落盘之后，此时Redo log中的对应事务数据记录就失去了意义，所 以Redo log的写入是日志文件循环写入的 附: 58同城数据库设计30条军规 军规适用场景：并发量大、数据量大的互联网业务 解读：讲解原因，解读比军规更重要 一、基础规范 必须使用InnoDB存储引擎 解读：支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 必须使用UTF8字符集 UTF-8MB4 解读：万国码，无需转码，无乱码风险，节省空间 数据表、数据字段必须加入中文注释 解读：N年后谁tm知道这个r1,r2,r3字段是干嘛的 禁止使用存储过程、视图、触发器、Event 解读：高并发大数据的互联网业务，架构设计思路是“解放数据库CPU，将计算转移到服务 层”，并发量大的情况下，这些功能很可能将数据库拖死，业务逻辑放到服务层具备更好的 扩展性，能够轻易实现“增机器就加性能”。数据库擅长存储与索引，CPU计算还是上移吧 禁止存储大文件或者大照片 解读：为何要让数据库做它不擅长的事情？大文件和照片存储在文件系统，数据库里存URI 多好 二、命名规范 只允许使用内网域名，而不是ip连接数据库 线上环境、开发环境、测试环境数据库内网域名遵循命名规范 业务名称：xxx，线上环境：xxx.db，开发环境：xxx.rdb，测试环境：xxx.tdb 从库在名称后加-s标识，备库在名称后加-ss标识 线上从库：xxx-s.db 线上备库：xxx-sss.db 库名、表名、字段名：小写，下划线风格，不超过32个字符，必须见名知意，禁止 拼音英文混用 表名t_xxx，非唯一索引名idx_xxx，唯一索引名uniq_xxx 三、表设计规范 单实例表数目必须小于500 单表列数目必须小于30 表必须有主键，例如自增主键 解读： 主键递增，数据行写入可以提高插入性能，可以避免page分裂，减少表碎片提升空间和 内存的使用 主键要选择较短的数据类型， Innodb引擎普通索引都会保存主键的值，较短的数据类 型可以有效的减少索引的磁盘空间，提高索引的缓存效率 无主键的表删除，在row模式的主从架构，会导致备库夯住 禁止使用外键，如果有外键完整性约束，需要应用程序控制 解读：外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响 sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能，大数据高并发业务场景 数据库使用以性能优先 四、字段设计规范 必须把字段定义为NOT NULL并且提供默认值 解读： null的列使索引&#x2F;索引统计&#x2F;值比较都更加复杂，对MySQL来说更难优化 null这种类型MySQL内部需要进行特殊处理，增加数据库处理记录的复杂性；同等条 件下，表中有较多空字段的时候，数据库的处理性能会降低很多 null值需要更多的存储空，无论是表还是索引中每行中的null的列都需要额外的空间来标 识 对null 的处理时候，只能采用is null或is not null，而不能采用&#x3D;、in、&lt;、&lt;&gt;、!&#x3D;、 not in这些操作符号。如：where name!&#x3D;’shenjian’，如果存在name为null值的记 录，查询结果就不会包含name为null值的记录 禁止使用TEXT、BLOB类型 解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内 存命中率急剧降低，影响数据库性能 禁止使用小数存储货币 解读：使用整数吧，小数容易导致钱对不上 必须使用varchar(20)存储手机号 解读： 涉及到区号或者国家代号，可能出现+-() 手机号会去做数学运算么？ varchar可以支持模糊查询，例如：like“138%” 禁止使用ENUM，可使用TINYINT代替 解读： 增加新的ENUM值要做DDL操作 ENUM的内部实际存储就是整数，你以为自己定义的是字符串？ 五、索引设计规范 单表索引建议控制在5个以内 单索引字段数不允许超过5个 解读：字段超过5个时，实际已经起不到有效过滤数据的作用了 禁止在更新十分频繁、区分度不高的属性上建立索引 解读： 更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能 “性别”这种区分度不大的属性，建立索引是没有什么意义的，不能有效过滤数据，性 能与全表扫描类似 建立组合索引，必须把区分度高的字段放在前面 解读：能够更加有效的过滤数据 六、SQL使用规范 禁止使用SELECT *，只获取必要的字段，需要显示说明列属性 解读： 读取不需要的列会增加CPU、IO、NET消耗 不能有效的利用覆盖索引 禁止使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性 解读：容易在增加或者删除字段后出现程序BUG 禁止使用属性隐式转换 解读：SELECT uid FROM t_user WHERE phone&#x3D;13812345678 会导致全表扫描，而不 能命中phone索引 禁止在WHERE条件的属性上使用函数或者表达式 解读：SELECT uid FROM t_user WHERE from_unixtime(day)&gt;&#x3D;’2017-02-15’ 会导致全 表扫描 正确的写法是：SELECT uid FROM t_user WHERE day&gt;&#x3D; unix_timestamp(‘2017-02-15 00:00:00’) 禁止负向查询，以及%开头的模糊查询 解读： 负向查询条件：NOT、!&#x3D;、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描 %开头的模糊查询，会导致全表扫描 禁止大表使用JOIN查询，禁止大表使用子查询 解读：会产生临时表，消耗较多内存与CPU，极大影响数据库性能 禁止使用OR条件，必须改为IN查询 解读：旧版本Mysql的OR查询是不能命中索引的，即使能命中索引，为何要让数据库耗费 更多的CPU帮助实施查询优化呢？ 应用程序必须捕获SQL异常，并有相应处理","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"},{"name":"MySQL","slug":"MySQL","permalink":"http://chaooo.github.io/tags/MySQL/"}]},{"title":"「MySQL」MySQL索引的使用及优化","date":"2019-02-05T09:04:12.000Z","path":"2019/02/05/mysql-index.html","text":"1. 索引的基本概念索引是为了加速对表中数据行的检索而创建的一种分散存储的数据结构。 索引意义： 索引能极大的减少存储引擎需要扫描的数据量 索引可以把随机IO变成顺序IO 索引可以帮助我们在进行分组、排序等操作时，避免使用临时表 增加索引会有利于查询效率，但会降低insert，update，delete的效率，但实际上往往不是这样的，过多的索引会不但会影响使用效率，同时会影响查询效率，这是由于数据库进行查询分析时，首先要选择使用哪一个索引进行查询，如果索引过多，分析过程就会越慢，这样同样的减少查询的效率，因此我们要知道如何增加，有时候要知道维护和删除不需要的索引。 2. 索引的适用场景2.1 适合建索引的场景 表的主键自动建立唯一索引 表的字段唯一约束 直接条件查询的字段（在SQL中用于条件约束的字段） 查询中与其它表关联的字段 查询中排序的字段（排序的字段如果通过索引去访问那将大大提高排序速度） 查询中统计或分组统计的字段 表记录太少（如果一个表只有5条记录，采用索引去访问记录的话，那首先需访问索引表，再通过索引表访问数据表，一般索引表与数据表不在同一个数据块） 经常插入、删除、修改的表（对一些经常处理的业务表应在查询允许的情况下尽量减少索引） 数据重复且分布平均的表字段（假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率大约为50%，那么对这种表A字段建索引一般不会提高数据库的查询速度。） 经常和主字段一块查询但主字段索引值比较多的表字段 对千万级MySQL数据库建立索引的事项及提高性能的手段 2.2 不适合建索引的场景 表记录太少（300万左右性能开始逐渐下降，虽然官方文档说撑得住5-8百万以上，但是根本也不能等到这个时候再去优化，性能肯定会受到影响） 经常增删改的表（why：提高了查询速度，同事却会降低了更新表的速度，入队表进行INSERT,UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存下索引文件）。 数据重复切分布平均的表字段，因此应该只为最经常查询和最经常排序的数据建立索引。注意，如果某个数据列包括许多重复的内容，为他建立索引就没有太大的实际效果了。（加入一个表有10万行的记录，有一个字段A只有True和False两个值，且每个值的分布概率大约为50%，那么对这种表的A字段建立索引一般不会提高数据库的查询速度。再比如对银行卡建立索引，毕竟银行卡没有重复的。索引的选择性是指索引列中不同值的数据与表中的记录数的比，如果一个表中有2000条记录，表索引列就有1980个不同的值，那么这个索引的选择性就是1980&#x2F;2000&#x3D;0.99。一个索引的选择性越接近于1，这个索引的效率就越高。） 3. MySQl中索引的结构（B+树）3.1 基本概念： 二叉树：一个节点最多两个子节点，一个节点只存储一个关键字，等于则命中，小于走左节点，大于走右节点； B树：多路搜索树，每个节点存储M&#x2F;2到M个关键字，所有关键字在整颗树中出现，且只出现一次，非叶子节点可以命中； B+树：在B树基础上，为叶子节点增加链表指针，所有关键字都在叶子节点中出现(有序)，叶子节点才命中； B*树：在B+树基础上，为非叶子节点也增加兄弟链表指针，将节点的最低利用率从1&#x2F;2提高到2&#x2F;3； 3.2 B+树的特性： 所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的； 不可能在非叶子结点命中； 非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层； 更适合文件索引系统； 3.3 B+树的三个特点： 关键字数和子树相同 在 B+ 树中，节点的关键字代表子树的最大值，因此关键字数等于子树数。 非叶子节点仅用作索引，它的关键字和子节点有重复元素 除叶子节点外的所有节点的关键字，都在它的下一级子树中同样存在，最后所有数据都存储在叶子节点中。 根节点的最大关键字其实就表示整个 B+ 树的最大元素。 叶子节点用指针连在一起 叶子节点包含了全部的数据，并且按顺序排列，B+ 树使用一个链表将它们排列起来，这样在查询时效率更快。 由于 B+ 树的中间节点不含有实际数据，只有子树的最大数据和子树指针，因此磁盘页中可以容纳更多节点元素，也就是说同样数据情况下，B+ 树会 B 树更加“矮胖”，因此查询效率更快。B+ 树的查找必会查到叶子节点，更加稳定。有时候需要查询某个范围内的数据，由于 B+ 树的叶子节点是一个有序链表，只需在叶子节点上遍历即可，不用像 B 树那样挨个中序遍历比较大小。 3.4 B+ 树的三个优点： 层级更低，IO 次数更少 每次都需要查询到叶子节点，查询性能稳定 叶子节点形成有序链表，范围查询方便 4. 索引的优化4.1 优化法则（口诀）： 全值匹配我最爱，最左前缀要遵守 带头大哥不能死，中间兄弟不能断 索引列上无计算，范围之后全失效 like百分写最右，覆盖索引不写星 不等控制还有or，索引失效要少用 var引号不能丢，SQL优化也不难 4.2 具体描述 全值匹配 怎么建索引就怎么用索引，where后面的条件越来越多精度越来越高，精度越来越高带来的就是长度和花费的代价也就越来越多 最佳左前缀法则 指的是查询从索引的最左前列开始并且不跳过索引中的列。 例如：复合索引A-&gt;B-&gt;C，如果把开头A去掉的话，B，C也就都失效了（带头大哥不能死）；如果把中间B去掉的话，则只会走索引A，而C就失效了（中间兄弟不能断）。 不在索引列上做任何操作(计算、函数、(自动or手动)类型转换)，会导致索引失效而转向全表扫描 存储引擎不能使用索引中范围条件右面的列 例如：select t from test where A=1 and B&gt;2 and C=3，那么B&gt;2后的查询条件失效。 尽量使用覆盖索引————只访问索引的查询（索引列和查询列一致），减少 SELECT *。 MySQL在使用不等于(!= 或&lt;&gt;)的时候无法使用索引会导致全表扫描 is null，is not null 也无法使用索引 like以通配符开头(&#39;%abc...&#39;)mysql索引失效回变成全表扫描的操作（使用覆盖索引可解决），只有通配符在右面(&#39;abc...%&#39;)的才能避免索引失效。 字符串不加单引号索引失效 少用or，用它来连接时会索引失效 5. in 和 exists区别及应用场景5.1 in 和 exists的区别: 如果子查询得出的结果集记录较少，主查询中的表较大且又有索引时应该用in, 反之如果外层的主查询记录较少，子查询中的表大，又有索引时使用exists。 其实我们区分in和exists主要是造成了驱动顺序的改变(这是性能变化的关键)，如果是exists，那么以外层表为驱动表，先被访问，如果是IN，那么先执行子查询，所以我们会以驱动表的快速返回为目标，那么就会考虑到索引及结果集的关系了 ，另外IN时不对NULL进行处理。 in 是把外表和内表作hash 连接，而exists是对外表作loop循环，每次loop循环再对内表进行查询。一直以来认为exists比in效率高的说法是不准确的。 5.2 not in 和not exists 如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引； 而not extsts 的子查询依然能用到表上的索引。 所以无论那个表大，用not exists都比not in要快 6. order by 和 group by 优化索引的主要作用就是查找和排序，ORDER BY 子句尽量使用Index方式排序，能避免使用FileSort方式排序，尽可能在索引列上外城排序操作，遵照索引键的最佳左前缀。 6.1 提高ORDER BY速度的技巧 ORDER BY时不要使用SELECT *，只查需要的字段。 增大sort_buffer_size参数大小（根据系统能力去提高，因为这个参数是针对每个进程的） 增大max_length_for_sort_data参数大小 6.2 GROUP BY的优化 GROUP BY实质上是先排序后进行分组，遵照索引的最佳左前缀。 当无法使用索引列，考虑增大max_length_for_sort_data和sort_buffer_size的参数设置。 WHERE 高于 HAVING，能写在WHERE解决的条件就不要去HAVING限定了。 注意：group by 表面上叫分组，但是分组之前比排序。所以说group by和order by两者排序的法则和索引优化的原则几乎是一致的。当然也有不一样的地方，group by 还有having的存在。如果group by错乱，会导致临时表的产生。(就是说group by的顺序不对，建好的索引我用不上，我内部使用了内排序产生了filesort，为了把这些数据挪出来内部建了一张临时表来进行分组) 一般性建议： 对于单值索引，尽量选择针对query过滤性更好的索引 在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段的顺序中，位置越靠左越好。 在选择组合索引的时候，尽量选择可能包含当前query中的where子句中更多字段的索引 尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的。 参考链接：https://www.zhihu.com/people/hen-six-49/activities","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"},{"name":"MySQL","slug":"MySQL","permalink":"http://chaooo.github.io/tags/MySQL/"}]},{"title":"「MySQL」MySQL性能优化基础","date":"2019-02-01T13:36:46.000Z","path":"2019/02/01/mysql-base.html","text":"1. MySQL基础操作1.1 MySQL备份与恢复 备份：在mysql的安装目录的bin目录下有mysqldump命令，可以完成对数据库的备份。 语法：mysqldump -u 用户名 -p 数据库名&gt; 磁盘SQL文件路径 由于mysqldump命令不是sql命令，需要在dos窗口下使用。 仅仅只会备份数据库中的表和数据，恢复时需要先手动创建数据库。 恢复：先手动创建数据库：create database 数据库名 然后dos窗口：mysql -u 用户名-p 导入库名&lt; 磁盘SQL文件绝对路径 1.2 MySQL事务基础 特性(ACID)：原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。 事务隔离级别：读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 MYSQL事务处理主要有两种方法： 用 BEGIN, ROLLBACK, COMMIT来实现: BEGIN 开始一个事务 COMMIT 事务确认 ROLLBACK 事务回滚 直接用 SET 来改变 MySQL 的自动提交模式: SET AUTOCOMMIT&#x3D;0 禁止自动提交 SET AUTOCOMMIT&#x3D;1 开启自动提交 事务并发操作出现几种问题: 丢失修改数据、读“脏”数据、数据不一致 1.3 查看设置MySQL编码 查看：mysql&gt; show variables like &#39;character%&#39; 设置: # vi /etc/my.cnf：123456789101112[mysqld] character‐set‐server=utf8collation‐server=utf8_general_cisql_mode=&#x27;NO_ENGINE_SUBSTITUTION&#x27;[mysql] default‐character‐set = utf8 [mysql.server] default‐character‐set = utf8 [mysqld_safe] default‐character‐set = utf8 [client] default‐character‐set = utf8 mysql的主配置文件: /etc/my.cnf 数据库文件存放位置: /var/lib/mysql 数据库的日志输出存放位置: /var/log/mysql 端口: Netstat –nltp 看是否能找到3306的端口 1.4 范式概念：范式就是符合某一规范级别的关系模式的集合。共有7种范式：1NF⊃2NF⊃3NF⊃BCNF⊃4NF⊃5NF⊃6NF 第一范式(1NF, First Normal Form)：字段值具有原子性,不能再分(所有关系型数据库系统都满足第一范式); 例如：姓名字段,其中姓和名是一个整体,如果区分姓和名那么必须设立两个独立字段; 第二范式(2NF, Second Normal Form)：一个表必须有主键,即每行数据都能被唯一的区分(2NF必须先满足第一范式); 第三范式(3NF, Third Normal Form)：一个表中不能包涵其他相关表中非关键字段的信息,即数据表不能有冗余字段(3NF必须先满足第二范式); 备注：往往我们在设计表中不能遵守第三范式,因为合理的沉余字段将会给我们减少join的查询; 例如：相册表中会添加图片的点击数字段,在相册图片表中也会添加图片的点击数字段; 2. SQL语句优化2.1 通过慢查日志发现有问题的SQL 查询次数多且每次查询占用时间长的sql 通常为pt-query-digest分析的前几个查询；该工具可以很清楚的看出每个SQL执行的次数及百分比等信息，执行的次数多，占比比较大的SQL IO大的sql 注意pt-query-digest分析中的Rows examine项。扫描的行数越多，IO越大。 未命中的索引的SQL 注意pt-query-digest分析中的Rows examine 和Rows Send的对比。说明该SQL的索引命中率不高，对于这种SQL，我们要重点进行关注。 通过explain查询分析SQL的执行计划, SQL的执行计划侧面反映出了SQL的执行效率， 2.2 常见SQL优化手段 函数Max()的优化 在求max的字段建索引 函数Count()的优化： Count(*):是包含null值；Count(id)：不包含null值 子查询的优化 子查询是我们在开发过程中经常使用的一种方式，在通常情况下，需要把子查询优化为join查询但在优化是需要注意关联键是否有一对多的关系，要注意重复数据(distinct去重)。 在用Join进行多表联合查询时，我们通常使用On来建立两个表的关系。其实还有一个更方便的关键字，那就是Using（如果两个表的关联字段名是一样）。 group by的优化: 最好使用同一表中的列，在子查询中分组 Limit查询的优化：Limit常用于分页处理，时常会伴随order by从句使用，因此大多时候会使用Filesorts这样会造成大量的IO问题。 优化步骤1：使用有索引的列或主键进行order by操作，因为大家知道，innodb是按照主键的逻辑顺序进行排序的。可以避免很多的IO操作。 优化步骤2：记录上次返回的主键， 在下次查询时使用主键过滤。（说明：避免了数据量大时扫描过多的记录） 注意事项：主键要顺序排序并连续的，如果主键中间空缺了某一列，或者某几列，会出现列出数据不足一页的数据；如果不连续的情况，建立一个附加的列index_id列，保证这一列数据要自增的，并添加索引即可。 3. 索引的优化3.1 索引基础概念索引是为了加速对表中数据行的检索而创建的一种分散存储的数据结构。 索引的建立是表中比较有指向性的字段，相当于目录，比如说行政区域代码，同一个地域的行政区域代码都是相同的，那么给这一列加上索引，避免让它重复扫描，从而达到优化的目的！ 创建索引：在执行create table语句时可以创建索引，也可以单独用create index或alter index来为表增加索引。不能用create index语句创建primary key索引。 在创建索引时，可以规定索引能否包含重复值。如果不包含，则索引应该创建为primary key或unique索引。对于单列惟一性索引，这保证单列不包含重复的值。对于多列惟一性索引，保证多个值的组合不重复。 primary key索引和unique索引非常类似。事实上，primary key索引仅是一个具有名称PRIMARY的unique索引。 查看索引：show index from tblname;或show keys from tblname; 3.2 索引的创建与删除 创建普通索引： 方式1：create index 索引名 on 表名(列名) 方式2：alter table 表名 add index 索引名(列名) 方式3：创建表的时候直接指定: ,index [索引名] (列名) 删除索引： drop index [索引名] on 表名 唯一索引：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。 创建唯一索引： 方式1：create unique index 索引名 on 表名(列名) 方式2：alter table 表名 add unique 索引名(列名) 方式3：创建表的时候直接指定: ,unique [索引名] (列名) 3.3 使用索引的场景 表的主键自动建立唯一索引 表的字段唯一约束 直接条件查询的字段（在SQL中用于条件约束的字段） 查询中与其它表关联的字段 查询中排序的字段（排序的字段如果通过索引去访问那将大大提高排序速度） 查询中统计或分组统计的字段 表记录太少（如果一个表只有5条记录，采用索引去访问记录的话，那首先需访问索引表，再通过索引表访问数据表，一般索引表与数据表不在同一个数据块） 经常插入、删除、修改的表（对一些经常处理的业务表应在查询允许的情况下尽量减少索引） 数据重复且分布平均的表字段（假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率大约为50%，那么对这种表A字段建索引一般不会提高数据库的查询速度。） 经常和主字段一块查询但主字段索引值比较多的表字段 对千万级MySQL数据库建立索引的事项及提高性能的手段 3.4 索引的维护及优化（重复及冗余索引）增加索引会有利于查询效率，但会降低insert，update，delete的效率，但实际上往往不是这样的，过多的索引会不但会影响使用效率，同时会影响查询效率，这是由于数据库进行查询分析时，首先要选择使用哪一个索引进行查询，如果索引过多，分析过程就会越慢，这样同样的减少查询的效率，因此我们要知道如何增加，有时候要知道维护和删除不需要的索引 重复索引：重复索引是指相同的列以相同的顺序建立的同类型的索引，如在primary key再建立唯一索引就是重复索引 冗余索引：冗余索引是指多个索引的前缀列相同，或是在联合索引中包含了主键的索引，如对于innodb来说，每一个索引后面，实际上都会包含主键，这时候我们建立的联合索引，又人为的把主键包含进去，那么这个时候就是一个冗余索引。 工具：使用**pt-duplicate-key-checker工具检查重复及冗余索引**: pt-duplicate-key-checker -uroot -padmin -h 127.0.0.1 索引维护的方法: 由于业务变更，某些索引是后续不需要使用的，就要进行删除。 在mysql中，目前只能通过慢查询日志配合pt-index-usage工具来进行索引使用情况的分析；pt-index-usage -uroot -padmin /var/lib/mysql/mysql-host-slow.log 3.5 设计MySql索引的注意事项设计好MySql的索引可以让你的数据库飞起来，大大的提高数据库效率。设计MySql索引的时候有一下几点注意： 创建索引 对于查询占主要的应用来说，索引显得尤为重要。很多时候性能问题很简单的就是因为我们忘了添加索引而造成的，或者说没有添加更为有效的索引导致。如果不加索引的话，那么查找任何哪怕只是一条特定的数据都会进行一次全表扫描，如果一张表的数据量很大而符合条件的结果又很少，那么不加索引会引起致命的性能下降。 但是也不是什么情况都非得建索引不可，比如性别可能就只有两个值，建索引不仅没什么优势，还会影响到更新速度，这被称为过度索引。 复合索引 比如有一条语句是这样的：select * from users where area&#x3D;’beijing’ and age&#x3D;22; 如果我们是在area和age上分别创建单个索引的话，由于mysql查询每次只能使用一个索引，所以虽然这样已经相对不做索引时全表扫描提高了很多效率，但是如果在area、age两列上创建复合索引的话将带来更高的效率。如果我们创建了(area, age,salary)的复合索引，那么其实相当于创建了(area,age,salary)、(area,age)、(area)三个索引，这被称为最佳左前缀特性。 因此我们在创建复合索引时应该将最常用作限制条件的列放在最左边，依次递减。 索引不会包含有NULL值的列 只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 使用短索引 对字符串列进行索引，如果可能应该指定一个前缀长度。 例如，如果有一个CHAR(255)的 列，如果在前10 个或20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I&#x2F;O操作。 排序的索引问题 mysql查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。 like语句操作 一般情况下不鼓励使用like操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引，而like “aaa%”可以使用索引。 不要在列上进行运算 select * from users where YEAR(adddate) 不使用NOT IN操作 NOT IN操作都不会使用索引将进行全表扫描。NOT IN可以NOT EXISTS代替 4. MYSQL数据库设计规范与原则4.1 设计规范 命名规范 采用26个英文字母(区分大小写)和0-9的自然数(经常不需要)加上下划线’_’组成 命名简洁明确,多个单词用下划线’_’分隔,长度不超过30个字符 除非是备份数据库可以加0-9的自然数,如：&#39;user_db_20191210&#39; 表前缀可以有效的把相同关系的表显示在一起,如：&#39;user_&#39; 每个表中必须有自增主键 表与表之间的相关联字段名称要求尽可能的相同 字段类型规范 用尽量少的存储空间来存数一个字段的数据, 例如：能使用int就不要使用varchar、char,能用varchar(16)就不要使用varchar(256); IP地址最好使用int类型; 固定长度的类型最好使用char,例如：邮编; 能使用tinyint就不要使用smallint,int; 最好给每个字段一个默认值, 最好不能为null; 索引规范 命名简洁明确,例如：user_login表user_name字段的索引应为user_name_index唯一索引; 为每个表创建一个主键索引; 为每个表创建合理的索引; 建立复合索引请慎重; 4.2 设计原则 核心原则 不在数据库做运算; cpu计算务必移至业务层; 控制列数量(字段少而精,字段数建议在20以内); 平衡范式与冗余(效率优先；往往牺牲范式) 拒绝3B(拒绝大sql语句：big sql、拒绝大事务：big transaction、拒绝大批量：big batch); 字段类原则 用好数值类型(用合适的字段类型节约空间); 字符转化为数字(能转化的最好转化,同样节约空间、提高查询性能); 避免使用NULL字段(NULL字段很难查询优化、NULL字段的索引需要额外空间、NULL字段的复合索引无效); 少用text类型(尽量使用varchar代替text字段); 索引类原则 合理使用索引(改善查询,减慢更新,索引一定不是越多越好); 字符字段必须建前缀索引; 不在索引做列运算; innodb主键推荐使用自增列(主键建立聚簇索引,主键不应该被修改,字符串不应该做主键)(理解Innodb的索引保存结构就知道了); 不用外键(由程序保证约束); sql类原则 sql语句尽可能简单(一条sql只能在一个cpu运算,大语句拆小语句,减少锁时间,一条大sql可以堵死整个库); 简单的事务; 避免使用trig&#x2F;func(触发器、函数不用客户端程序取而代之); 不用select *(消耗cpu,io,内存,带宽,这种程序不具有扩展性); OR改写为IN(or的效率是n级别); OR改写为UNION(mysql的索引合并很弱智); 避免负向%; 慎用count(*); limit高效分页(limit越大，效率越低); 使用union all替代union(union有去重开销); 少用连接join; 使用group by; 请使用同类型比较; 打散批量更新; 5. 数据库结构的优化5.1 选择合适的数据类型数据类型的选择，重点在于“合适”二字 使用可以存下你的数据的最小的数据类型。（时间类型数据：可以使用varchar类型，可以使用int类型，也可以使用时间戳类型） 使用简单的数据类型，int要比varchar类型在mysql处理上简单。（int类型存储时间是最好的选择） 尽可能的使用not null定义字段。（innodb的特性所决定，非not null的值，需要额外的在字段存储，同时也会增加IO和存储的开销） 尽量少用text类型，非用不可时最好考虑分表。 5.2 数据库表的范式化优化 表范式化 范式化是指数据库设计的规范，目前说道范式化一般是指第三设计范式。也就是要求数据表中不存在非关键字段对任意候选关键字段的传递函数依赖则符合第三范式。 反范式化 反范式化是指为了查询效率的考虑把原本符合第三范式的表“适当”的增加冗余，以达到优化查询效率的目的，反范式化是一种以空间来换取时间的操作。 5.3 数据库表的垂直拆分所谓的垂直拆分，就是把原来一个有很多列的表拆分成多个表，这解决了表的宽度问题。 垂直拆分原则 把不常用的字段表单独存放到一个表中。 把大字段独立存放到一个表中。 把经常一起使用的字段放到一起。 5.4 数据库表的水平拆分表的水平拆分是为了解决单表数据量过大的问题，水平拆分的表每一个表的结构都是完全一致的 水平拆分原因 如果单表的数据量达到上亿条，那么这时候我们尽管加了完美的索引，查询效率低，写入的效率也相应的降低。 如何将数据平均分为N份 对customer_id进行hash运算，如果要拆分为5个表则使用mod（customer_id，5）取出0-4个值。 针对不动的hashid把数据存储到不同的表中。 水平拆分面临的挑战 夸分区表进行数据查询 前端业务统计：业务上给不同的用户返回不同的业务信息，对分区表没有大的挑战。 统计及后台报表操作 但是对后台进行报表统计时，数据量比较大，后台统计时效性比较低，后台就用汇总表，将前后台的表拆分开。 6. 数据库系统配置优化数据库是基于操作系统的，目前大多数MySQL都是安装在linux系统之上，所以对于操作系统的一些参数配置也会影响到MySQL的性能 6.1 操作系统的优化网络方面的配置，要修改/etc/sysctl.conf 增加tcp支持的队列数 net.ipv4.tcp_max_syn_backlog = 65535 减少断开连接时，资源回收(tcp有连接状态) net.ipv4.tcp_max_tw_buckets = 8000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_fin_timeout = 10 说明： TCP是有连接状态，通过netstat查看连接状态，经常会看到timeout状态或者timewait状态连接，为了加快timewait状态的连接回收，就需要调整上面的四个参数，保持TCP连接数在一个适当的状态。 6.2 打开文件数的限制打开文件数的限制，可以使用ulimit –a查看目录的各个限制，可以修改/etc/security/limits.conf文件 limits.conf中增加以下内容以修改打开文件数量的限制（永久生效） *Soft nofile 65535 *Hard nofile 65535 如果一次有效，就要使用ulimit –n 65535即可。（默认情况是1024） 除此之外最好在MySQL服务器上关闭iptables，selinux等防火墙软件 6.3 MySQL配置文件优化Mysql可以通过启动时指定参数和使用配置文件两种方法进行配置，在大多数情况下配置文件位于/etc/my.cnf或/etc/mysql/my.cnf MySQL查找配置文件的顺序可以通过以下命令获得： /usr/sbin/mysqld --verbose --help | grep -A 1 &#39;default options&#39; 注意：如果存在多个位置存在配置文件，则后面的会覆盖前面的。 6.3.1 my.cnf常用 连接请求 参数 max_connections：最大连接数 如果服务器的并发连接请求量比较大，建议调高此值，以增加并行连接数量，MySQL会为每个连接提供连接缓冲区，连接数越多就会开销越多的内存，所以要适当调整该值，不能盲目提高设值。 数值过小会经常出现ERROR 1040: Too many connections错误，可以过’conn%’通配符查看当前状态的连接数量，以定夺该值的大小。 max_used_connections / max_connections * 100% （**理想值≈ 85%**）: 响应的连接数&#x2F;最大连接数 back_log：能暂存的连接数量 如果MySQL的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。 默认数值是50，可调优为128，对于Linux系统设置范围为小于512的整数。 interactive_timeout：服务器关闭交互式连接前等待活动的秒数 默认数值是28800，可调优为7200。 wait_timeout：服务器关闭非交互连接之前等待活动的秒数 指定一个请求的最大连接时间，对于4GB左右内存的服务器可以设置为5-10。 6.3.1 my.cnf常用 缓冲区 参数 key_buffer_size: 指定索引缓冲区的大小 它决定索引处理的速度，尤其是索引读的速度。 它只对MyISAM表起作用。即使你不使用MyISAM表，但是内部的临时磁盘表是MyISAM表，也要使用该值 query_cache_size ：查询缓存的内存 使用查询缓冲，MySQL将查询结果存放在缓冲区中，今后对于同样的SELECT语句（区分大小写），将直接从缓冲区中读取结果。 通过检查状态值Qcache_*，可以知道query_cache_size设置是否合理（上述状态值可以使用SHOW STATUS LIKE ‘Qcache%’获得）。如果Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲不够的情况，如果Qcache_hits的值也非常大，则表明查询缓冲使用非常频繁，此时需要增加缓冲大小；如果Qcache_hits的值不大，则表明你的查询重复率很低，这种情况下使用查询缓冲反而会影响效率，那么可以考虑不用查询缓冲。此外，在SELECT语句中加入SQL_NO_CACHE可以明确表示不使用查询缓冲。 与查询缓冲有关的参数还有query_cache_type、query_cache_limit、query_cache_min_res_unit。 query_cache_type指定是否使用查询缓冲，可以设置为0、1、2，该变量是SESSION级的变量。 query_cache_limit指定单个查询能够使用的缓冲区大小，缺省为1M。 query_cache_min_res_unit指定分配缓冲区空间的最小单位，缺省为4K。检查状态值Qcache_free_blocks，如果该值非常大，则表明缓冲区中碎片很多，这就表明查询结果都比较小，此时需要减小query_cache_min_res_unit。 record_buffer_size：顺序扫描缓冲区大小 每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，你可能想要增加该值。 默认数值是131072(128K)，可改为16773120 (16M) read_rnd_buffer_size：随机读缓冲区大小 当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySQL会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySQL会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。一般可设置为16M sort_buffer_size：排序扫描缓冲区大小 每个需要进行排序的线程分配该大小的一个缓冲区。增加这值加速ORDER BY或GROUP BY操作。 默认数值是2097144(2M)，可改为16777208 (16M)。 join_buffer_size：联合查询缓冲区大小 联合查询操作所能使用的缓冲区大小。 record_buffer_size，read_rnd_buffer_size，sort_buffer_size，join_buffer_size为每个线程独占，也就是说，如果有100个线程连接，则占用为16M*100 table_cache：表高速缓存的大小 表高速缓存的大小。每当MySQL访问一个表时，如果在表缓冲区中还有空间，该表就被打开并放入其中，这样可以更快地访问表内容。通过检查峰值时间的状态值Open_tables和Opened_tables，可以决定是否需要增加table_cache的值。如果你发现open_tables等于table_cache，并且opened_tables在不断增长，那么你就需要增加table_cache的值了（上述状态值可以使用SHOW STATUS LIKE ‘Open%tables’获得）。注意，不能盲目地把table_cache设置成很大的值。如果设置得太高，可能会造成文件描述符不足，从而造成性能不稳定或者连接失败。 1G内存机器，推荐值是128－256。内存在4GB左右的服务器该参数可设置为256M或384M。 max_heap_table_size：用户可以创建的内存表(memory table)的大小 这个值用来计算内存表的最大行数值。这个变量支持动态改变，即set @max_heap_table_size&#x3D;# 这个变量和tmp_table_size一起限制了内部内存表的大小。如果某个内部heap（堆积）表大小超过tmp_table_size，MySQL可以根据需要自动将内存中的heap表改为基于硬盘的MyISAM表。 tmp_table_size：临时表的大小 通过设置tmp_table_size选项来增加一张临时表的大小，例如做高级GROUP BY操作生成的临时表。如果调高该值，MySQL同时将增加heap表的大小，可达到提高联接查询速度的效果，建议尽量优化查询，要确保查询过程中生成的临时表在内存中，避免临时表过大导致生成基于硬盘的MyISAM表。 每次创建临时表，Created_tmp_tables增加，如果临时表大小超过tmp_table_size，则是在磁盘上创建临时表，Created_tmp_disk_tables也增加,Created_tmp_files表示MySQL服务创建的临时文件文件数 比较理想的配置是：Created_tmp_disk_tables / Created_tmp_tables * 100% &lt;= 25%比如上面的服务器Created_tmp_disk_tables / Created_tmp_tables * 100% ＝1.20%，应该相当好了 默认为16M，可调到64-256最佳，线程独占，太大可能内存不够I&#x2F;O堵塞 thread_cache_size：可以复用的保存在中的线程的数量 可以复用的保存在中的线程的数量。如果有，新的线程从缓存中取得，当断开连接的时候如果有空间，客户的线置在缓存中。如果有很多新的线程，为了提高性能可以这个变量值。 通过比较 Connections和Threads_created状态的变量，可以看到这个变量的作用。默认值为110，可调优为80。 thread_concurrency：同一时间运行的线程系统提示所需数量的线程 推荐设置为服务器 CPU核数的2倍， 例如双核的CPU, 那么thread_concurrency的应该为4；2个双核的cpu, thread_concurrency的值应为8。默认为8 这个参数已经在5.7.2版本的MySQL中被移除 6.3.1 my.cnf常用 配置InnoDB的 参数 innodb_buffer_pool_size：缓冲池大小 对于InnoDB表来说，innodb_buffer_pool_size的作用就相当于key_buffer_size对于MyISAM表的作用一样。 InnoDB使用该参数指定大小的内存来缓冲数据和索引。对于单独的MySQL数据库服务器，最大可以把该值设置成物理内存的80%。 根据MySQL手册，对于2G内存的机器，推荐值是1G（50%）。 innodb_flush_log_at_trx_commit：主要控制了innodb将log buffer中的数据写入日志文件并flush磁盘的时间点，取值分别为0、1、2三个 设置为0，表示当事务提交时，不做日志写入操作，而是每秒钟将log buffer中的数据写入日志文件并flush磁盘一次； 设置为1，则在每秒钟或是每次事物的提交都会引起日志文件写入、flush磁盘的操作，确保了事务的ACID； 设置为2，每次事务提交引起写入日志文件的动作，但每秒钟完成一次flush磁盘操作。 实际测试发现，该值对插入数据的速度影响非常大，设置为2时插入10000条记录只需要2秒，设置为0时只需要1秒，而设置为1时则需要229秒。因此，MySQL手册也建议尽量将插入操作合并成一个事务，这样可以大幅提高速度。 根据MySQL手册，在允许丢失最近部分事务的危险的前提下，可以把该值设为0或2。 innodb_log_buffer_size：log缓存大小 一般为1-8M，默认为1M，对于较大的事务，可以增大缓存大小。可设置为4M或8M。 innodb_additional_mem_pool_size：内存池大小 该参数指定InnoDB用来存储数据字典和其他内部数据结构的内存池大小。缺省值是1M。通常不用太大，只要够用就行，应该与表结构的复杂度有关系。如果不够用，MySQL会在错误日志中写入一条警告信息。 根据MySQL手册，对于2G内存的机器，推荐值是20M，可适当增加。 innodb_thread_concurrency=8，推荐设置为 2*(NumCPUs+NumDisks)，默认一般为8 skip-name-resolve：禁止域名解析 禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。 但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求 7. MySQL的执行顺序MySQL的语句一共分为11步，最先执行的总是FROM操作，最后执行的是LIMIT操作。 其中每一个操作都会产生一张虚拟的表，这个虚拟的表作为一个处理的输入，只是这些虚拟的表对用户来说是透明的，但是只有最后一个虚拟的表才会被作为结果返回。 如果没有在语句中指定某一个子句，那么将会跳过相应的步骤。 ⑧select ⑨distinct &lt;字段名&gt; ①from &lt;表名&gt; ③&lt;连接类型&gt;join &lt;表名&gt; ②on&lt;连接条件&gt; ④where&lt;查询条件&gt; ④group by&lt;分组字段&gt; ⑥with&#123;cube|rollup&#125; ⑦having&lt;查询条件&gt; ⑩order by&lt;排序字段&gt; ⑪limit&lt;分页数量&gt; 查询处理的每一个阶段分析： FORM: 对FROM的左边的表和右边的表计算笛卡尔积。产生虚表VT1 ON: 对虚表VT1进行ON筛选，只有那些符合&lt;连接条件&gt;的行才会被记录在虚表VT2中。 JOIN：如果指定了OUTER JOIN（比如left join、 right join），那么保留表中未匹配的行就会作为外部行添加到虚拟表VT2中，产生虚拟表VT3, rug from子句中包含两个以上的表的话，那么就会对上一个join连接产生的结果VT3和下一个表重复执行步骤1~3这三个步骤，一直到处理完所有的表为止。 WHERE：对虚拟表VT3进行WHERE条件过滤。只有符合&lt;where查询条件&gt;的记录才会被插入到虚拟表VT4中。 GROUP BY: 根据group by子句中的列，对VT4中的记录进行分组操作，产生VT5. CUBE | ROLLUP: 对表VT5进行cube或者rollup操作，产生表VT6. HAVING： 对虚拟表VT6应用having过滤，只有符合&lt;having查询条件&gt;的记录才会被 插入到虚拟表VT7中。 8. MySQL执行引擎8.1 MyISAM存储引擎 不支持事务、也不支持外键，优势是访问速度快，对事务完整性没有 要求或者以select，insert为主的应用基本上可以用这个引擎来创建表 支持3种不同的存储格式，分别是：静态表；动态表；压缩表 静态表： 表中的字段都是非变长字段，这样每个记录都是固定长度的，优点存储非常迅速，容易缓存，出现故障容易恢复；缺点是占用的空间通常比动态表多（因为存储时会按照列的宽度定义补足空格）ps：在取数据的时候，默认会把字段后面的空格去掉，如果不注意会把数据本身带的空格也会忽略。 动态表： 记录不是固定长度的，这样存储的优点是占用的空间相对较少；缺点：频繁的更新、删除数据容易产生碎片，需要定期执行OPTIMIZE TABLE或者myisamchk-r命令来改善性能 压缩表： 因为每个记录是被单独压缩的，所以只有非常小的访问开支 8.2 InnoDB存储引擎 提供了具有提交、回滚和崩溃恢复能力的事务安全。但是对比MyISAM引擎，写的处理效率会差一些，并且会占用更多的磁盘空间以保留数据和索引。 InnoDB存储引擎的特点：支持自动增长列，支持外键约束 8.3 MEMORY存储引擎 Memory存储引擎使用存在于内存中的内容来创建表。每个memory表只实际对应一个磁盘文件，格式是.frm。memory类型的表访问非常的快，因为它的数据是放在内存中的，并且默认使用HASH索引，但是一旦服务关闭，表中的数据就会丢失掉。 MEMORY存储引擎的表可以选择使用BTREE索引或者HASH索引，两种不同类型的索引有其不同的使用范围 Hash索引优点： Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。 Hash索引缺点： 那么不精确查找呢，也很明显，因为hash算法是基于等值计算的，所以对于“like”等范围查找hash索引无效，不支持； Memory类型的存储引擎主要用于哪些内容变化不频繁的代码表，或者作为统计操作的中间结果表，便于高效地对中间结果进行分析并得到最终的统计结果，。对存储引擎为memory的表进行更新操作要谨慎，因为数据并没有实际写入到磁盘中，所以一定要对下次重新启动服务后如何获得这些修改后的数据有所考虑。 8.4 MERGE存储引擎 Merge存储引擎是一组MyISAM表的组合，这些MyISAM表必须结构完全相同，merge表本身并没有数据，对merge类型的表可以进行查询，更新，删除操作，这些操作实际上是对内部的MyISAM表进行的。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"},{"name":"MySQL","slug":"MySQL","permalink":"http://chaooo.github.io/tags/MySQL/"}]},{"title":"「ElasticStack」Beats+Logstash+Elasticsearch+Kibana基础整合","date":"2018-11-28T14:29:59.000Z","path":"2018/11/28/elastic-stack.html","text":"1. ElasticStack 的组成 **Beats**：数据采集 LogStash: 数据处理 ElasticSearch(核心引擎): 数据存储、查询和分析 Kibana: 数据探索与可视化分析 2. FilebeatFilebeat是本地文件的轻量型日志数据采集器。Beats可以直接（或者通过Logstash）将数据发送到Elasticsearch，在那里你可以进一步处理和增强数据，然后在Kibana中将其可视化。 2.1 Filebeat 工作原理Filebeat 由两个主要组件组成：prospector和harvester。这些组件一起工作来读取文件（tail file）并将事件数据发送到您指定的输出 **Harvester**： 负责读取单个文件的内容 如果文件在读取时被删除或重命名，Filebeat将继续读取文件 **prospector**： prospector负责**管理harvester**并找到所有要读取的文件来源 如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester Filebeat目前支持两种prospector类型：log和stdin Filebeat如何保持文件的状态 Filebeat 保存每个文件的状态并经常将状态刷新到磁盘上的注册文件中 该状态用于记住harvester正在读取的最后偏移量，并确保发送所有日志行 如果输出（例如Elasticsearch或Logstash）无法访问，Filebeat会跟踪最后发送的行，并在输出再次可用时继续读取文件。 在Filebeat运行时，每个prospector内存中也会保存的文件状态信息，当重新启动Filebeat时，将使用注册文件的数据来重建文件状态，Filebeat将每个harvester在从保存的最后偏移量继续读取 Filebeat存储唯一标识符以检测文件是否先前已采集过 Filebeat如何确保至少一次交付 Filebeat保证事件至少会被传送到配置的输出一次，并且不会丢失数据。 Filebeat能够实现此行为，因为它将每个事件的传递状态存储在注册文件中。 在输出阻塞或未确认所有事件的情况下，Filebeat将继续尝试发送事件，直到接收端确认已收到。 如果Filebeat在发送事件的过程中关闭，它不会等待输出确认所有收到事件。 发送到输出但在Filebeat关闭前未确认的任何事件在重新启动Filebeat时会再次发送。 这可以确保每个事件至少发送一次，但最终会将重复事件发送到输出。 也可以通过设置shutdown_timeout选项来配置Filebeat以在关闭之前等待特定时间 2.2 Filebeat 安装与配置安装Filebeat，创建配置文件itcast.yml，控制台运行测试 12345678910#创建如下配置文件 itcast.ymlfilebeat.inputs:- type: stdin # 标准输入 enabled: trueoutput.console: # 输出到控制台 pretty: true enable: true#启动filebeat./filebeat -e -c itcast.yml 输入 hello 运行结果如下： 1234567891011121314151617181920212223242526hello&#123; &quot;@timestamp&quot;: &quot;2019-11-23T09:21:19.213Z&quot;, &quot;@metadata&quot;: &#123; #元数据信息 &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;version&quot;: &quot;7.4.2&quot; # beat版本 &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;chaooo&quot; &#125;, &quot;agent&quot;: &#123; &#125;, &quot;log&quot;: &#123; &quot;offset&quot;: 0, &quot;file&quot;: &#123; &quot;path&quot;: &quot;&quot; &#125; &#125;, &quot;message&quot;: &quot;hello&quot;, # 输入的内容 &quot;input&quot;: &#123; # 控制台标准输入 &quot;type&quot;: &quot;stdin&quot; &#125;, &quot;ecs&quot;: &#123; &quot;version&quot;: &quot;1.1.0&quot; &#125;&#125; 2.3 读取文件创建配置文件itcast-log.yml 12345678910filebeat.inputs:- type: log enabled: true paths: - /test/*.log # 可以使用单个路径output.console: # 输出到控制台 pretty: true enable: true#启动filebeat./filebeat -e -c itcast-log.yml 在/test/下创建a.log文件，并输入如下内容hello world,观察filebeat输出: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; &quot;@timestamp&quot;: &quot;2019-11-23T09:45:56.379Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;version&quot;: &quot;7.4.2&quot; &#125;, &quot;log&quot;: &#123; &quot;offset&quot;: 0, &quot;file&quot;: &#123; &quot;path&quot;: &quot;/test/a.log&quot; &#125; &#125;, &quot;message&quot;: &quot;hello&quot;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;ecs&quot;: &#123; &quot;version&quot;: &quot;1.1.0&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;chaooo&quot; &#125;, &quot;agent&quot;: &#123; &#125;&#125;&#123; &quot;@timestamp&quot;: &quot;2019-11-23T09:45:56.379Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;version&quot;: &quot;7.4.2&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;chaooo&quot; &#125;, &quot;agent&quot;: &#123; &#125;, &quot;log&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;/test/a.log&quot; &#125;, &quot;offset&quot;: 7 &#125;, &quot;message&quot;: &quot;world&quot;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;ecs&quot;: &#123; &quot;version&quot;: &quot;1.1.0&quot; &#125;&#125; 2.4 自定义字段123456789101112filebeat.inputs:- type: log enabled: true paths: - /test/*.log # 可以使用单个路径 tags: [&quot;web&quot;] #添加自定义tag，便于后续的处理 fields: #添加自定义字段 from: itcast-im fields_under_root: true #true为添加到根节点，false为添加到子节点中output.console: # 输出到控制台 pretty: true enable: true 2.5 输出到 Elasticsearch12345678910111213filebeat.inputs:- type: log enabled: true paths: - /test/*.log # 可以使用单个路径 tags: [&quot;web&quot;] #添加自定义tag，便于后续的处理 fields: #添加自定义字段 from: itcast-im fields_under_root: falsesetup.template.settings: index.number_of_shards: 3 #指定索引的分区数output.elasticsearch: #指定ES的配置 hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] 2.6 读取 Nginx 日志文件12345678910filebeat.inputs:- type: log enabled: true paths: - /usr/local/nginx/logs/*.log tags: [&quot;nginx&quot;]setup.template.settings: index.number_of_shards: 3 #指定索引的分区数output.elasticsearch: #指定ES的配置 hosts: [&quot;192.168.1.7:9200&quot;,&quot;192.168.1.7:9201&quot;,&quot;192.168.1.7:9202&quot;] 2.7 Filebeat 的 Module日志数据的读取与处理可以不用手动配置的，在Filebeat中，有大量的Module，可以直接使用简化配置。 12345678910111213141516171819202122232425262728293031323334353637./filebeat modules listEnabled:Disabled:apacheauditdawscefciscocorednselasticsearchenvoyproxygooglecloudhaproxyibmmqicingaiisiptableskafkakibanalogstashmongodbmssqlmysqlnatsnetflownginxosquerypanwpostgresqlrabbitmqredissantasuricatasystemtraefikzeek 可以看到，内置了很多的module，但都没有启用，如果需要启用需要进行enable操作： 12/filebeat modules enable nginx #启动./filebeat modules disable nginx #禁用 2.8 nginx module 与 filebeat 配置1234567- module: nginx access: # Access logs enabled: true var.paths: [&quot;/usr/local/nginx/logs/access.log*&quot;] error: # Error logs enabled: true var.paths: [&quot;/usr/local/nginx/logs/error.log*&quot;] 1234567891011121314#vim itcast-nginx.ymlfilebeat.inputs:#- type: log# enabled: true# paths:# - /usr/local/nginx/logs/*.log# tags: [&quot;nginx&quot;]setup.template.settings: index.number_of_shards: 3output.elasticsearch: hosts: [&quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;]filebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: false 若启动报错，需要在Elasticsearch中安装ingest-user-agent、ingest-geoip插件 3. Metricbeat用于从系统和服务收集指标。Metricbeat和Filebeat一样，是一个轻量级的采集器，Metricbeat由模块(Module)和度量集(Metricset)组成。Metricbeat模块定义了从特定服务（如 Redis，MySQL 等）收集数据的基本逻辑。该模块指定有关服务的详细信息，包括如何连接，收集指标的频率以及要收集的指标。 Metricbeat有 2 部分组成，一部分是Module，另一部分为Metricset。 Module 收集的对象，如：mysql、redis、nginx、操作系统等； Metricset 收集指标的集合，如：cpu、memory、network等； 以Redis Module为例： 3.1 安装配置安装 Metricbeat，根据实际情况配置文件metricbeat.yml 123456789metricbeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: falsesetup.kibana: host: &quot;192.168.56.13:5601&quot;output.elasticsearch: hosts: [&quot;192.168.56.13:9200&quot;] username: &quot;elastic&quot; password: &quot;qiuyuetao&quot; 启动：./metricbeat -e查看module列表：./metricbeat modules list 12345678910Enabled:system #默认启用Disabled:apacheelasticsearchnginxmysqlredis... system module默认启用的，其配置: 123456789101112131415161718192021222324252627282930cat system.yml# Module: system# Docs: https://www.elastic.co/guide/en/beats/metricbeat/6.5/metricbeat-modulesystem.html- module: system period: 10s metricsets: - cpu - load - memory - network - process - process_summary #- core #- diskio #- socket process.include_top_n: by_cpu: 5 # include top 5 processes by CPU by_memory: 5 # include top 5 processes by memory- module: system period: 1m metricsets: - filesystem - fsstat processors: - drop_event.when.regexp: system.filesystem.mount_point: &#x27;^/(sys|cgroup|proc|dev|etc|host|lib)($|/)&#x27;- module: system period: 15m metricsets: - uptime 3.2 Nginx Module在 nginx 中，需要开启状态查询，才能查询到指标数据。 12345# 配置nginxlocation /nginx-status &#123; stub_status on; access_log off;&#125; 通过192.168.56.13/nginx-status查看nginx-status Active connections：正在处理的活动连接数 server accepts handled requests（连接数，握手数，处理请求总数） Reading: 0 Writing: 1 Waiting: 1（ 读取到客户端的 Header 信息数，返回给客户端 Header 信息数，已经处理完正在等候下一次请求指令的驻留链接） 配置Nginx Module（metricbeat/modules.d/nginx.yml） 1234567- module: nginx metricsets: [&quot;stubstatus&quot;] period: 10s # Nginx hosts hosts: [&quot;http://192.168.56.11&quot;] # Path to server status. Default server-status server_status_path: &quot;nginx_status&quot; 启动：./metricbeat -e 4. LogstashLogstash是ElasticStack中的实时数据采集引擎，可以采集来自不同数据源的数据，并对数据进行处理后输出到多种输出源，是Elastic Stack的重要组成部分。 4.1 Logstash 的数据处理过程 Inputs(Codecs)--&gt;Filters--&gt;Outputs(Codecs) 用户通过定义pipeline配置文件，设置需要使用的input，filter，output, codec插件，以实现特定的数据采集，数据处理，数据输出等功能 Inputs：用于从数据源获取数据，常见的插件如file, syslog, redis, beats等 Filters：用于处理数据如格式转换，数据派生等，常见的插件如grok, mutate, drop, clone, geoip等 Outputs：用于数据输出，常见的插件如elastcisearch，file, graphite, statsd等 Codecs：Codecs不是一个单独的流程，而是在输入和输出等插件中用于数据转换的模块，用于对数据进行编码处理，常见的插件如json，multiline 4.2 执行模型 每个Input启动一个线程，从对应数据源获取数据 Input会将数据写入一个队列：默认为内存中的有界队列（意外停止会导致数据丢失）。为了防止数丢失Logstash提供了两个特性： Persistent Queues：通过磁盘上的queue来防止数据丢失 Dead Letter Queues：保存无法处理的event（仅支持Elasticsearch作为输出源） Logstash会有多个pipeline worker, 每一个pipeline worker会从队列中取一批数据，然后执行filter和output（worker数目及每次处理的数据量均由配置确定） 4.3 安装配置下载Logstash并解压，配置有三部分，如下： 123456789input &#123; #输入stdin &#123; ... &#125; #标准输入&#125;filter &#123; #过滤，对数据进行分割、截取等处理...&#125;output &#123; #输出stdout &#123; ... &#125; #标准输出&#125; 4.4 读取自定义日志 日志结构：2019-11-23 21:21:21|ERROR|读取数据出错|参数：id=1002，日志中的内容是使用“|”进行分割的，使用，我们在处理的时候，也需要对数据做分割处理。 编写配置文件 123456789101112131415#vim itcast-pipeline.confinput &#123; file &#123; path =&gt; &quot;/itcast/logstash/logs/app.log&quot; start_position =&gt; &quot;beginning&quot; &#125;&#125;filter &#123; mutate &#123; split =&gt; &#123;&quot;message&quot;=&gt;&quot;|&quot;&#125; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125;&#125; 启动测试 1234567891011121314151617#启动./bin/logstash -f ./itcast-pipeline.conf#写日志到文件echo &quot;2019-11-23 21:21:21|ERROR|读取数据出错|参数：id=1002&quot; &gt;&gt; app.log#输出的结果&#123; &quot;@timestamp&quot; =&gt; 2019-03-15T08:44:04.749Z, &quot;path&quot; =&gt; &quot;/itcast/logstash/logs/app.log&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;node01&quot;, &quot;message&quot; =&gt; [ [0] &quot;2019-11-23 21:21:21&quot;, [1] &quot;ERROR&quot;, [2] &quot;读取数据出错&quot;, [3] &quot;参数：id=1002&quot; ]&#125; 输出到Elasticsearch配置 12345output &#123; elasticsearch &#123; hosts =&gt; [ &quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;] &#125;&#125; 5. Elasticsearch + Logstash + Beats + Kibana 基础整合123 (读取) (发送) (写入) (读取)「日志文件」&lt;----「FileBeat」----&gt;「Logstash」----&gt;「Elasticsearch」&lt;----「Kibana」 Filebeat配置与启动： 12345678910111213#vim itcast-dashboard.ymlfilebeat.inputs:- type: log enabled: true paths: - /itcast/logs/*.logsetup.template.settings: index.number_of_shards: 3output.logstash: hosts: [&quot;192.168.40.133:5044&quot;] #Logstash端口号#启动./filebeat -e -c itcast-dashboard.yml Logstash配置与启动： 123456789101112131415161718192021222324252627282930313233#vim itcast-dashboard.confinput &#123; beats &#123; port =&gt; &quot;5044&quot; &#125;&#125;filter &#123; mutate &#123; split =&gt; &#123;&quot;message&quot;=&gt;&quot;|&quot;&#125; &#125; mutate &#123; add_field =&gt; &#123; &quot;userId&quot; =&gt; &quot;%&#123;message[1]&#125;&quot; &quot;visit&quot; =&gt; &quot;%&#123;message[2]&#125;&quot; &quot;date&quot; =&gt; &quot;%&#123;message[3]&#125;&quot; &#125; &#125; mutate &#123; convert =&gt; &#123; &quot;userId&quot; =&gt; &quot;integer&quot; &quot;visit&quot; =&gt; &quot;string&quot; &quot;date&quot; =&gt; &quot;string&quot; &#125; &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; [ &quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;] &#125;&#125;#启动./bin/logstash -f itcast-dashboard.conf ElasticSearch启动与Kibana启动： 123456# ElasticSearch默认端口:9200bin/elasticsearch# kibana默认端口:5601bin/kibana#通过浏览器进行访问,添加Logstash索引到Kibana中http://192.168.40.133:5601","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://chaooo.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://chaooo.github.io/tags/Kibana/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"http://chaooo.github.io/tags/ElasticStack/"},{"name":"LogStash","slug":"LogStash","permalink":"http://chaooo.github.io/tags/LogStash/"}]},{"title":"「ElasticStack」ElasticSearch聚合分析与数据建模","date":"2018-11-21T14:30:47.000Z","path":"2018/11/21/elastic-aggregation.html","text":"1. ElasticSearch 中的聚合分析聚合分析，英文Aggregation，是 ES 除了搜索功能之外提供的针对 ES 数据进行统计分析的功能。 特点： ① 功能丰富，可满足大部分分析需求； ② 实时性高，所有计算结果实时返回。 基于分析规则的不同，ES 将聚合分析主要划分为以下 4 种： Metric: 指标分析类型，如：计算最值，平均值等； Bucket: 分桶类型，类似于group by语法，根据一定规则划分为若干个桶分类； Pipeline: 管道分析类型，基于上一级的聚合分析结果进行再分析； Matrix: 矩阵分析类型。 12345678910111213# 聚合分析格式：GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; # 关键词 &quot;&lt;aggregation_name&gt;&quot;:&#123; # 自定义聚合分析名称，一般起的有意义 &quot;&lt;aggregation_type&gt;&quot;:&#123; # 聚合分析类型 &quot;&lt;aggregation_body&gt;&quot; # 聚合分析主体 &#125; &#125; [,&quot;aggs&quot;:&#123;[&lt;svb_aggregation&gt;]+&#125;] # 可包含多个子聚合分析 &#125;&#125; 1.1 Metric 聚合分析主要分为两类：单值分析（输出单个结果）和多值分析（输出多个结果）。 1.1.1 单值分析 min：返回数值类型字段的最小值 max：返回数值类型字段的最大值 avg：返回数值类型字段的平均值 sum：返回数值类型字段值的总和 cardinality：返回字段的基数 使用多个单值分析关键词，返回多个结果 123456789101112131415161718192021222324252627282930313233343536373839GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;min_age&quot;:&#123; &quot;min&quot;:&#123; # 关键字min/max/avg/sum/cardinality &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125;&#125;## 使用多个单值分析关键词，返回多个分析结果GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;min_age&quot;:&#123; &quot;min&quot;:&#123; # 求最小年龄 &quot;field&quot;:&quot;age&quot; &#125; &#125;, &quot;max_age&quot;:&#123; &quot;max&quot;:&#123; # 求最大年龄 &quot;field&quot;:&quot;age&quot; &#125; &#125;, &quot;avg_age&quot;:&#123; &quot;avg&quot;:&#123; # 求平均年龄 &quot;field&quot;:&quot;age&quot; &#125; &#125;, &quot;sum_age&quot;:&#123; &quot;sum&quot;:&#123; # 求年龄总和 &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125;&#125; 1.1.2 多值分析 stats：返回所有单值结果 extended_stats：对stats进行扩展，包含更多，如：方差，标准差，标准差范围等 Percentile：百分位数统计 Top hits：一般用于分桶之后获取该桶内最匹配的定不稳当列表，即详情数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;stats_age&quot;:&#123; &quot;stats&quot;:&#123; # 关键字stats/extended_stats/percentiles &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125;&#125;## 使用percentiles关键词进行百分位数预测。GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;per_age&quot;:&#123; &quot;percentiles&quot;:&#123; # 关键字 &quot;field&quot;:&quot;age&quot;, &quot;values&quot;:[20, 25] # 判断20和25分别在之前的年轻区间的什么位置，以百分数显示 &#125; &#125; &#125;&#125;## 使用top_hits关键词GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; &quot;match&quot;:&#123; &quot;field&quot;:&quot;job.keyword&quot;, # 按job.keyword进行分桶聚合 &quot;size&quot;:10 &#125;, &quot;aggs&quot;:&#123; &quot;top_employee&quot;:&#123; &quot;top_hits&quot;:&#123; &quot;size&quot;:10, # 返回文档数量 &quot;sort&quot;:[ &#123; &quot;age&quot;:&#123; &quot;order&quot;:&quot;desc&quot; # 按年龄倒叙排列 &#125; &#125; ] &#125; &#125; &#125; &#125; &#125; &#125;&#125; 1.2 Bucket 聚合分析Bucket，意为桶。即：按照一定规则，将文档分配到不同的桶中，达分类的目的。常见的有以下五类： Terms: 直接按term进行分桶，如果是text类型，按分词后的结果分桶 Range: 按指定数值范围进行分桶 Date Range: 按指定日期范围进行分桶 Histogram: 直方图，按固定数值间隔策略进行数据分割 Date Histogram: 日期直方图，按固定时间间隔进行数据分割 1.2.1 TermsTerms: 直接按term进行分桶，如果是text类型，按分词后的结果分桶 12345678910111213# 使用terms关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;terms_job&quot;:&#123; &quot;terms&quot;:&#123; # 关键字 &quot;field&quot;:&quot;job.keyword&quot;, # 按job.keyword进行分桶 &quot;size&quot;:5 # 返回五个文档 &#125; &#125; &#125;&#125; 1.2.2 RangeRange: 按指定数值范围进行分桶： 123456789101112131415161718192021222324252627# 使用range关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;number_ranges&quot;:&#123; &quot;range&quot;:&#123; # 关键字 &quot;field&quot;:&quot;age&quot;, # 按age进行分桶 &quot;ranges&quot;:[ &#123; &quot;key&quot;:&quot;&gt;=19 &amp;&amp; &lt; 25&quot;, # 第一个桶： 19&lt;=年龄&lt;25 &quot;from&quot;:19, &quot;to&quot;:25 &#125;, &#123; &quot;key&quot;:&quot;&lt; 19&quot;, # 第二个桶： 年龄&lt;19 &quot;to&quot;:19 &#125;, &#123; &quot;key&quot;:&quot;&gt;= 25&quot;, # 第三个桶： 年龄&gt;=25 &quot;from&quot;:25 &#125; ] &#125; &#125; &#125;&#125; 1.2.3 Date RangeDate Range: 按指定日期范围进行分桶 12345678910111213141516171819202122232425262728# 使用date_range关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;date_ranges&quot;:&#123; &quot;date_range&quot;:&#123; # 关键字 &quot;field&quot;:&quot;birth&quot;, # 按age进行分桶 &quot;format&quot;:&quot;yyyy&quot;, &quot;ranges&quot;:[ &#123; &quot;key&quot;:&quot;&gt;=1980 &amp;&amp; &lt; 1990&quot;, # 第一个桶： 1980&lt;=出生日期&lt;1990 &quot;from&quot;:&quot;1980&quot;, &quot;to&quot;:&quot;1990&quot; &#125;, &#123; &quot;key&quot;:&quot;&lt; 1980&quot;, # 第二个桶： 出生日期&lt;1980 &quot;to&quot;:1980 &#125;, &#123; &quot;key&quot;:&quot;&gt;= 1990&quot;, # 第三个桶： 出生日期&gt;=1990 &quot;from&quot;:1990 &#125; ] &#125; &#125; &#125;&#125; 1.2.4 HistogramHistogram: 直方图，按固定数值间隔策略进行数据分割 1234567891011121314151617# 使用histogram关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;age_hist&quot;:&#123; &quot;histogram&quot;:&#123; # 关键词 &quot;field&quot;:&quot;age&quot;, &quot;interval&quot;:3, # 设定间隔大小为2 &quot;extended_bounds&quot;:&#123; # 设定数据范围 &quot;min&quot;:0, &quot;max&quot;:30 &#125; &#125; &#125; &#125;&#125; 1.2.5 Date HistogramDate Histogram: 日期直方图，按固定时间间隔进行数据分割 123456789101112131415161718# 使用date_histogram关键词GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;birth_hist&quot;:&#123; &quot;date_histogram&quot;:&#123; # 关键词 &quot;field&quot;:&quot;birth&quot;, &quot;interval&quot;:&quot;year&quot;, # 设定间隔大小为年year &quot;format&quot;:&quot;yyyy&quot;, &quot;extended_bounds&quot;:&#123; # 设定数据范围 &quot;min&quot;:&quot;1980&quot;, &quot;max&quot;:&quot;1990&quot; &#125; &#125; &#125; &#125;&#125; 1.3 Bucket+Metric 聚合分析Bucket 聚合分析允许通过添加子分析来进一步进行分析，该子分析可以是 Bucket，也可以是 Metric。 分桶之后再分桶（Bucket+Bucket），在数据可视化中一般使用千层饼图进行显示。 分桶之后再数据分析（Bucket+Metric） 123456789101112131415161718192021222324252627# 分桶之后再分桶——Bucket+BucketGET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 第一层Bucket &quot;match&quot;:&#123; &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10 &#125;, &quot;aggs&quot;:&#123; &quot;age_range&quot;:&#123; &quot;range&quot;:&#123; # 第二层Bucket &quot;field&quot;:&quot;age&quot;, &quot;ranges&quot;:[ &#123;&quot;to&quot;:20&#125;, &#123;&quot;from&quot;:20,&quot;to&quot;:30&#125;, &#123;&quot;from&quot;:30&#125; ] &#125; &#125; &#125; &#125; &#125; &#125;&#125; 12345678910111213141516171819202122# 分桶之后再数据分析——Bucket+MetricGET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 第一层Bucket &quot;match&quot;:&#123; &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10 &#125;, &quot;aggs&quot;:&#123; &quot;stats_age&quot;:&#123; &quot;stats&quot;:&#123; # 第二层Metric &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 1.4 Pipeline 聚合分析针对聚合分析的结果进行再分析，且支持链式调用： 12345678910111213141516171819202122232425# 使用pipeline聚合分析,计算订单月平均销售额。GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;sales_per_month&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;date&quot;, &quot;interval&quot;:&quot;month&quot; &#125;, &quot;aggs&quot;:&#123; &quot;sales&quot;:&#123; &quot;sum&quot;:&#123; &quot;field&quot;:&quot;price&quot; &#125; &#125; &#125; &#125;, &quot;avg_monthly_sales&quot;:&#123; &quot;avg_bucket&quot;:&#123; # bucket类型 &quot;buckets_path&quot;:&quot;sales_per_month&gt;sales&quot; # 使用buckets_path参数，表明是pipeline &#125; &#125; &#125;&#125; pipeline的分析结果会输出到原结果中，由输出位置不同，分为两类：Parent和Sibling。 Sibling。结果与现有聚合分析结果同级，如：Max&#x2F;Min&#x2F;Sum&#x2F;Avg Bucket、Stats&#x2F;Extended Stats Bucket、Percentiles Bucket Parent。结果内嵌到现有聚合分析结果中，如：Derivate、Moving Average、Cumulative Sum 123456789101112131415161718192021222324# Sibling聚合分析(min_bucket)GET my_index/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 根据job.keyword进行分桶 &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10 &#125;, &quot;aggs&quot;:&#123; &quot;avg_salary&quot;:&#123; &quot;avg&quot;:&#123; # 之后Metric中求工资的平均数 &quot;field&quot;:&quot;salary&quot; &#125; &#125; &#125; &#125;, &quot;min_salary_by_job&quot;:&#123; &quot;min_bucket&quot;:&#123; # 关键词 &quot;buckets_path&quot;:&quot;jobs&gt;avg_salary&quot; # 按工资平均数，排列每个桶中的job &#125; &#125;&#125; 1234567891011121314151617181920212223242526# Parent聚合分析(Derivate)GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;bitrh&quot;:&#123; &quot;date_histogram&quot;:&#123; &quot;field&quot;:&quot;birth&quot;, &quot;interval&quot;:&quot;year&quot;, &quot;min_doc_count&quot;:0 &#125;, &quot;aggs&quot;:&#123; &quot;avg_salary&quot;:&#123; &quot;avg&quot;:&#123; &quot;field&quot;:&quot;salary&quot; &#125; &#125;, &quot;derivative_avg_salary&quot;:&#123; &quot;derivative&quot;:&#123; # 关键词 &quot;buckets_path&quot;:&quot;avg_salary&quot; &#125; &#125; &#125; &#125; &#125;&#125; 1.5 聚合分析的作用范围ES 聚合分析默认作用范围是query的结果集 1234567891011121314151617181920# ES中聚合分析的默认作用范围是query的结果集GET my_index/_search&#123; &quot;size&quot;:0, &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; &quot;match&quot;:&#123; # 此时，只在username字段中包含alfred的文档中进行分桶 &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10 &#125; &#125; &#125; &#125;&#125; 可通过以下方式修改：filter、post_filter、global filter: 为某个结合分析设定过滤条件，从而在不改变整体 query 语句的情况下修改范围 post_filter，作用于文档过滤，但在聚合分析之后才生效 global，无视 query 条件，基于所有文档进行分析 1234567891011121314151617181920212223# 使用filter进行过滤GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs_salary_small&quot;:&#123; &quot;filter&quot;:&#123; &quot;range&quot;:&#123; &quot;salary&quot;:&#123; &quot;to&quot;:10000 &#125; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 在salary小于10000的文档中对工作进行分桶 &quot;field&quot;:&quot;job.keyword&quot; &#125; &#125; &#125; &#125; &#125;&#125; 1234567891011121314151617# 使用post_filter进行过滤GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; # 在salary小于10000的文档中对工作进行分桶 &quot;field&quot;:&quot;job.keyword&quot; &#125; &#125; &#125;, &quot;post_filter&quot;:&#123; # 在集合分析之后才生效 &quot;match&quot;:&#123; &quot;job.keyword&quot;:&quot;java engineer&quot; &#125; &#125;&#125; 123456789101112131415161718192021222324252627# 使用global进行过滤GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;job.keyword&quot;:&quot;java engineer&quot; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;java_avg_salary&quot;:&#123; &quot;avg&quot;:&#123; &quot;field&quot;:&quot;salary&quot; &#125; &#125;, &quot;all&quot;:&#123; &quot;global&quot;:&#123; # 关键词 &quot;aggs&quot;:&#123; &quot;avg_salary&quot;:&#123; &quot;avg&quot;:&#123; &quot;field&quot;:&quot;salary&quot; # 依然是对所有的文档进行查询，而不会去管query &#125; &#125; &#125; &#125; &#125; &#125;&#125; 1.6 聚合分析中的排序 可使用自带的关键数据排序，如：_count文档数、_key按 key 值 也可使用聚合结果进行排序 123456789101112131415161718192021# 使用自带的数据进行排序GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;jobs&quot;:&#123; &quot;terms&quot;:&#123; &quot;field&quot;:&quot;job.keyword&quot;, &quot;size&quot;:10, &quot;order&quot;:[ &#123; &quot;_count&quot;:&quot;asc&quot; # 默认按_count倒叙排列 &#125;, &#123; &quot;_key&quot;:&quot;desc&quot; 使用多个排序值，从上往下的顺序进行排列 &#125; ] &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627# 使用聚合结果进行排序GET my_index/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;salary_hist&quot;:&#123; &quot;histogram&quot;:&#123; &#125;, &quot;aggs&quot;:&#123; &quot;age&quot;:&#123; &quot;filter&quot;:&#123; &quot;range&quot;:&#123; &quot;age&quot;:&#123; &quot;gte&quot;:10 &#125; &#125; &#125;, &quot;aggs&quot;:&#123; &quot;avg_age&quot;:&#123; &quot;field&quot;:&quot;age&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 1.7 计算精准度问题ES 聚合的执行流程：每个Shard上分别计算，由coordinating Node做聚合。 Terms计算不准确原因：数据分散在多个Shard上，coordinating Node无法得悉数据全貌，那么在取数据的时候，造成精准度不准确。 如下图：正确结果应该为a,b,c,而返回的是 a,b,d 解决办法有两种： 直接设置shard数量为 1；消除数据分散问题，但无法承载大数据量。 设置shard_size大小，即每次从shard上额外多获取数据，从而提升精准度 terms 聚合返回结果中有两个统计值： doc_count_error_upper_bound：被遗漏的 term 可能的最大值； sum_other_doc_count：返回结果 bucket 的 term 外其他 term 的文档总数。 设定show_term_doc_count_error可以查看每个 bucket 误算的最大值(doc_count_error_upper_bound,为0表示计算准确) Shard_Size 默认大小：(size*1.5)+10 通过调整 Shard_Size 的大小降低doc_count_error_upper_bound来提升准确度 增大了整体的计算量，从而降低了响应时间 权衡 海量数据、精准度、实时性 三者只能取其二。 Elasticsearch 目前支持两种近似算法：cardinality(度量) 和 percentiles(百分位数度量) 结果近似准确，但不一定精准 可通过参数的调整使其结果精准，但同时消耗更多时间和性能 2. ElasticSearch 的数据建模数据建模(Data Modeling)大致分为三个阶段：概念建模、逻辑建模、物理建模 概念模型：时间占比10% 基础。确定系统的核心需求和范围边界，实际实体与实体之间的关系。 逻辑模型：时间占比60-70% 核心。确定系统的核心需求和范围边界，实际实体与实体之间的关系。 物理模型：时间占比20-30% 落地实现。结合具体的数据库产品，在满足业务读写性能等需求的前提下确定最终的定义。 2.1 ES 中的数据建模ES 是基于 Luence 以倒排索引为基础实现的存储体系，不遵循关系型数据库中的范式约定。 2.2 Mapping 字段相关设置 enabled:true/false。false表示 仅存储，不做搜索或聚合分析。 **index:true/false。是否构建倒排索引。不需进行字段的检索的时候设为 false。 index_options:docs/freqs/positions/offsets。确定存储倒排索引的哪些信息。 norms:true/false。是否存储归一化相关系数，若字段仅用于过滤和聚合分析，则可关闭。 doc_values:true/false。是否启用 doc_values，用于排序和聚类分析。默认开启。 field_data:true/false。是否设 text 类型为 fielddata，实现排序和聚合分析。默认关闭。 store:true/false。是否存储该字段。 coerce:true/false。 是否开启数值类型转换功能，如：字符串转数字等。 multifields:多字段。灵活使用多字段特性来解决多样业务需求。 dynamic:true/false/strict。控制 mapping 自动更新。 date_detection:true/false。是否启用自定识别日期类型，一般设为 false，避免不必要的识别字符串中的日期。 2.3 Mapping 字段属性设定流程判断类型—&gt;是否需要检索—&gt;是否需要排序和聚合分析—&gt;是否需要另行存储 判断类型 字符串类型：需要分词，则设为 text，否则设为 keyword。 枚举类型：基于性能考虑，设为 keyword，即便该数据为整型。 数值类型：尽量选择贴近的类型，如 byte 即可表示所有数值时，即用 byte，而不是所有都用 long。 其他类型：布尔型，日期类型，地理位置类型等。 是否需要检索 完全不需要检索、排序、聚合分析的字段enabled设为false。 不需检索的字段index设为false。 需检索的字段，可通过如下配置设定需要的存储粒度: index_options 结合需要设定。 norms 不需归一化数据时可关闭。 是否需要排序和聚合分析 当不需要排序和聚合分析功能时： doc_values设为false。 field_data设为false。 是否需要另行存储 store设为true即可存储该字段的原始内容(且与_source无关)，一般结合_source的enabled设为false时使用。 2.4 ES 建模实例 针对博客文章设定索引 blog_index，包含字段： 标题：title 发布日期：publish_data 作者：author 摘要：abstract 网址：url 简易的数据模型： 123456789101112131415161718192021222324# 简易模型blog_indexPUT blog_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;title&quot;:&#123; #title设为text，包含自字段keyword。支持检索、排序、聚合分析 &quot;type&quot;:&quot;text&quot;, &quot;fields&quot;:&#123; &quot;keyword&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125; &#125; &#125;,#publish_data设为date，支持检索、排序、聚合分析 &quot;publish_data&quot;:&#123;&quot;type&quot;:&quot;date&quot;&#125;, # author设为keyword，支持检索、排序、聚合分析 &quot;author&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;, # abstract设为text，支持检索、排序、聚合分析 &quot;abstract&quot;:&#123;&quot;type&quot;:&quot;text&quot;&#125;, # url设为date，不需进行检索 &quot;url&quot;:&#123;&quot;enabled&quot;:false&#125; &#125; &#125; &#125;&#125; 如果在blog_index中加入一个内容字段content 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 为blog_index增加content字段PUT blog_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; #关闭，不存原始内容到_source &quot;_source&quot;:&#123;&quot;enabled&quot;:false&#125;, &quot;properties&quot;:&#123; #title设为text，包含自字段keyword。支持检索、排序、聚合分析 &quot;title&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;fields&quot;:&#123; &quot;keyword&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125;, &quot;store&quot;:true #对数据进行存储 &#125;,#publish_data设为date，支持检索、排序、聚合分析 &quot;publish_data&quot;:&#123; &quot;type&quot;:&quot;date&quot;, &quot;store&quot;:true # 对数据进行存储 &#125;, &quot;author&quot;:&#123;# author设为keyword，支持检索、排序、聚合分析 &quot;type&quot;:&quot;keyword&quot;, &quot;store&quot;:true # 对数据进行存储 &#125;, &quot;abstract&quot;:&#123;# abstract设为text，支持检索、排序、聚合分析 &quot;type&quot;:&quot;text&quot;, &quot;store&quot;:true # 对数据进行存储 &#125;, &quot;content&quot;:&#123;# content设为text，支持检索、排序、聚合分析 &quot;type&quot;:&quot;text&quot;, &quot;store&quot;:true # 对数据进行存储 &#125;, &quot;url&quot;:&#123; &quot;type&quot;:&quot;keyword&quot;, # url设为keyword &quot;doc_values&quot;:false, # url不支持排序和聚合分析 &quot;norms&quot;:false, # url也不需要归一化数据 &quot;ignore_above&quot;:100, # 预设内容长度为100 &quot;store&quot;:true # 对数据进行存储 &#125; &#125; &#125; &#125;&#125; 在搜索时增加高亮: 在此时，content里面的数据会存储大量的内容数据，数据量可能达到上千、上万，甚至几十万。那么在搜索的时候，根据search机制，如果还是像之前一样进行_search搜索，并只显示其他字段的话，其实依然还是每次获取了content字段的内容，影响性能，所以，使用stored_fields参数，控制返回的字段。节省了大量资源： 123456789101112131415# 使用stored_fields返回指定的存储后的字段GET blog_index/_search&#123; &quot;stored_fields&quot;:[&quot;title&quot;,&quot;publish_data&quot;,&quot;author&quot;,&quot;Abstract&quot;,&quot;url&quot;], &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;content&quot;:&quot;world&quot;#依然进行content搜索，但是不返回所有的content字段 &#125; &#125;, &quot;highlight&quot;:&#123; #针对content字段进行高亮显示 &quot;fields&quot;:&#123; &quot;content&quot;:&#123;&#125; &#125; &#125;&#125; 注意：GET blog_index/_search?_source=title 虽然只显示了title，但是search机制决定了，会把所有_source内容获取到，但只是显示title。 2.5 ES 中关联关系处理ES不擅长处理关系型数据库中的关联关系，因为底层使用的倒排索引，如：文章表blog和评论表comment之间通过blog_id关联。目前 ES 主要有以下 4 种常用的方法来处理关联关系： Nested Object:嵌套文档 Parent/Child:父子文档 Data denormalization:数据的非规范化 Application-side joins:服务端 Join 或客户端 Join 2.5.1 Application-side joins（服务端 Join 或客户端 Join）索引之间完全独立（利于对数据进行标准化处理，如便于上述两种增量同步的实现），由应用端的多次查询来实现近似关联关系查询。 适用于第一个实体只有少量的文档记录的情况（使用ES的terms查询具有上限，默认1024，具体可在elasticsearch.yml中修改），并且最好它们很少改变。这将允许应用程序对结果进行缓存，并避免经常运行第一次查询。 2.5.2 Data denormalization（数据的非规范化）通俗点就是通过字段冗余，以一张大宽表来实现粗粒度的index，这样可以充分发挥扁平化的优势。但是这是以牺牲索引性能及灵活度为代价的。 使用的前提：冗余的字段应该是很少改变的；比较适合与一对少量关系的处理。当业务数据库并非采用非规范化设计时，这时要将数据同步到作为二级索引库的 ES 中，就很难使用上述增量同步方案，必须进行定制化开发，基于特定业务进行应用开发来处理join关联和实体拼接。 宽表处理在处理一对多、多对多关系时，会有字段冗余问题，适合“一对少量”且这个“一”更新不频繁的应用场景。 2.5.3 Nested objects（嵌套文档）索引性能和查询性能二者不可兼得，必须进行取舍。嵌套文档将实体关系嵌套组合在单文档内部（类似与 json 的一对多层级结构），这种方式牺牲索引性能（文档内任一属性变化都需要重新索引该文档）来换取查询性能，可以同时返回关系实体，比较适合于一对少量的关系处理。 当使用嵌套文档时，使用通用的查询方式是无法访问到的，必须使用合适的查询方式（nested query、nested filter、nested facet 等），很多场景下，使用嵌套文档的复杂度在于索引阶段对关联关系的组织拼装。 2.5.4 Parent&#x2F;Child（父子文档）父子文档牺牲了一定的查询性能来换取索引性能，适用于一对多的关系处理。其通过两种 type 的文档来表示父子实体，父子文档的索引是独立的。父-子文档 ID 映射存储在 Doc Values 中。当映射完全在内存中时， Doc Values 提供对映射的快速处理能力，另一方面当映射非常大时，可以通过溢出到磁盘提供足够的扩展能力。 在查询 parent-child 替代方案时，发现了一种 filter-terms 的语法，要求某一字段里有关联实体的 ID 列表。基本的原理是在 terms 的时候，对于多项取值，如果在另外的 index 或者 type 里已知主键 id 的情况下，某一字段有这些值，可以直接嵌套查询。具体可参考官方文档的示例：通过用户里的粉丝关系，微博和用户的关系，来查询某个用户的粉丝发表的微博列表。 父子文档相比嵌套文档较灵活，但只适用于“一对大量”且这个“一”不是海量的应用场景，该方式比较耗内存和 CPU，这种方式查询比嵌套方式慢 5~10 倍，且需要使用特定的 has_parent 和 has_child 过滤器查询语法，查询结果不能同时返回父子文档（一次 join 查询只能返回一种类型的文档）。 而受限于父子文档必须在同一分片上，ES 父子文档在滚动索引、多索引场景下对父子关系存储和联合查询支持得不好，而且子文档 type 删除比较麻烦（子文档删除必须提供父文档 ID）。 如果业务端对查询性能要求很高的话，还是建议使用宽表化处理的方式，这样也可以比较好地应对聚合的需求。在索引阶段需要做 join 处理，查询阶段可能需要做去重处理，分页方式可能也得权衡考虑下。 2.6 ES 中的 reindexreindex：指重建所有数据的过程，一般发生在一下情况： mapping设置变更，如：字段类型变化，分词器字典更新等； index设置变更，如：分片数变化； 迁移数据。 ES 提供了线程的 api 用于完成数据重建： _update_by_query：在现有索引上重建； _reindex：在其他索引上重建。 1234# 将blog_index中所有文档重建一遍：# 如果遇到版本冲突，依然执行。POST blog_index/_update_by_query?conflicts=proceed# 此时如果blog_index中没有store的数据，则会报错 2.6.1 使用_update_by_query，更新文档的字段值和部分文档：12345678910111213# 更新文档的字段值及部分文档POST blog_index/_update_by_query&#123; &quot;script&quot;:&#123; # 更新文档的字段值 &quot;source&quot;:&quot;ctx._source.likes++&quot;, # 代码 &quot;lang&quot;:&quot;painless&quot; # ES自带script语法 &#125;, &quot;query&quot;:&#123; # 更新部分文档 &quot;term&quot;:&#123; &quot;user&quot;:&quot;tom&quot; &#125; &#125;&#125; 在 reindex 发起后进入的文档，不会参与重建，类似于快照的机制。因此：一般在文档不再发生变更时，进行文档的 reindex。 2.6.2 使用_reindex，重建数据：12345678910# 使用_reindex：POST _reindex&#123; &quot;source&quot;:&#123; # 被重建索引 &quot;index&quot;:&quot;blog_index&quot; &#125;, &quot;dest&quot;:&#123; # 目标索引 &quot;index&quot;:&quot;blog_new_index&quot; &#125;&#125; 数据重建时间，受到索引文档规模的影响，此时设定url参数wait_for_completion为false，来异步执行。 ES通过task来描述此类执行任务，并提供了task api来查看任务的执行进度和相关数据： 1234# 使用task apiPOST blog_index/_update_by_query?comflicts=proceed&amp;wait_for_completion=false# 使用返回的taskid，查看任务的执行进度和相关数据GET _tasks/&lt;返回的task id&gt; 2.7 其他建议： 对 mapping 进行版本管理： 要么写文件&#x2F;注释，加入到Git仓库，一眼可见； 要么增加metadata字段，维护版本，并在每次更新mapping设置的时候加1。 123&quot;metadata&quot;:&#123; &quot;version&quot;:1&#125; 防止字段过多： index.mapping.total_fields_limit，默认1000个。一般是因为没有高质量的数据建模导致，如：dynamic设为true。此时考虑查分多个索引来解决问题。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://chaooo.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://chaooo.github.io/tags/Kibana/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"http://chaooo.github.io/tags/ElasticStack/"},{"name":"LogStash","slug":"LogStash","permalink":"http://chaooo.github.io/tags/LogStash/"}]},{"title":"「ElasticStack」ElasticSearch分布式特性 与 Search机制","date":"2018-11-17T12:44:13.000Z","path":"2018/11/17/elastic-search.html","text":"1. ElasticSearch 的分布式特性1.1 分布式介绍 ES支持集群模式，即一个分布式系统。其好处主要有以下 2 个: 可增大系统容量。比如：内存、磁盘的增加使得ES能够支持PB级别的数据； 提高了系统可用性。即使一部分节点停止服务，集群依然可以正常对外服务。 ES集群由多个ES实例构成。 + 不同集群通过集群名字来区分，通过配置文件elasticsearch.yml中的cluster.name可以修改，默认为elasticsearch + 每个ES实例的本质，其实是一个JVM进程，且有自己的名字，通过配置文件中的node.name可以修改。 1.2 构建 ES 集群1234# 创建一个本地化集群my_clusterbin/elasticsearch -Epath.data=node1 -Ecluster.name=my_cluster -Enode.name=node1 -dbin/elasticsearch -Ehttp.port=8200 -Epath.data=node2 -Ecluster.name=my_cluster -Enode.name=node2 -dbin/elasticsearch -Ehttp.port=7200 -Epath.data=node3 -Ecluster.name=my_cluster -Enode.name=node3 -d 可以通过 cerebro 插件可以看到，集群my_cluster中存在三个节点，分别为：node1、node2、node3 **Cluster State**：ES 集群相关的数据，主要记录如下信息： 节点信息：如节点名称、连接地址等 索引信息：如索引名称、配置等 Master Node：主节点，可修改cluster state的节点。一个集群只能有一个。 cluster state存储于每个节点上，master维护最新版本并向其他从节点同步。 master 节点是通过集群中所有节点选举产生的，可被选举的节点称为**master-eligible节点** 通过配置node.master:true设置节点为可被选举节点(默认为 true) **Cordinating Node**：处理请求的节点。是所有节点的默认角色，且不能取消。 路由请求到正确的节点处理，如：创建索引的请求到 master 节点。 **Data Node**：存储数据的节点，默认节点都是data类型。配置node.data:true。 1.3 副本与分片 提高系统可用性： 服务可用性：集群 数据可用性：副本(Replication) 增大系统容量：分片(Shard) 分片是ES能支持PB级别数据的基石：可在创建索引时指定 分片存储部分数据，可以分布于任意节点； 分片数在索引创建时指定，且后续不能更改，默认为 5 个； 有主分片和副本分片之分，以实现数据的高可用； 副本分片由主分片同步数据，可以有多个，从而提高数据吞吐量。 分片数的设定很重要，需要提前规划好 过小会导致后续无法通过增加节点实现水平扩容 过大会导致一个节点分片过多，造成资源浪费，同时会影响查询性能 例如：在 3 个节点的集群中配置索引指定 3 个分片和 1 个副本（index.number_of_shards:3,index.number_of_replicas:1），分布如下： 怎样增加节点或副本提高索引的吞吐量 同时增加新的节点和加新的副本，这样把新的副本放在新的节点上，进行索引数据读取的时候，并且读取，就会提升索引数据读取的吞吐量。 1.4 ES 集群状态 与 故障转移 ES 的**健康状态(Cluster Health)**分为三种： Greed，绿色。表示所有主分片和副本分片都正常分配； Yellow，黄色。表示所有主分片都正常分配，但有副本分片未分配； Red，红色。表示有主分片未分配。 可通过GET _cluster/health查看集群状态 返回集群名称，集群状态，节点数，活跃分片数等信息。 如果此时磁盘空间不够，name 在创建新的索引的时候，主副分片都不会再分配，此时的集群状态会直接飙红，但此时依然可以访问集群和索引，也可以正常进行搜索。 所以：ES 的集群状态为红色，不一定就不能正常服务。 故障转移 Failover 当其余节点发现定时 ping 主节点 master 无响应的时候，集群状态转为 Red。此时会发起 master 选举。 新 master 节点发现若有主分片未分配，会将副本分片提升为主分片，此时集群状态转为 Yellow。 新 master 节点会将提升后的主分片生成新的副本，此时集群状态转为 Green。整个故障转移过程结束。 1.5 文档分布式存储通过文档到分片的映射算法，使文档均匀分布到所有分片上，以充分利用资源。 文档对应分片计算公式：shard = hash(routing)%number_of_primary_shards hash保证数据均匀分布在分片中 routing作为关键参数，默认为文档 ID，也可自行指定 number_of_primary_shards为主分片数 主分片数一旦设定，不能更改：为了保证文档对应的分片不会发生改变。 文档创建流程: 文档读取流程 文档批量创建流程 文档批量读取流程 1.6 脑裂问题 在分布式系统中一个经典的网络问题 当一个集群在运行时，作为master节点的node1的网络突然出现问题，无法和其他节点通信，出现网络隔离情况。那么node1自己会组成一个单节点集群，并更新cluster state；同时作为data节点的node2和node3因为无法和node1通信，则通过选举产生了一个新的master节点node2，也更新了cluster state。那么当node1的网络通信恢复之后，集群无法选择正确的master。 解决方案也很简单： 仅在可选举的master-eligible节点数&gt;=quorum的时候才进行master选举。 quorum(至少为2)=master-eligible数量/2 + 1。 通过discovery.zen.minimum_master_nodes为quorum即可避免脑裂。 1.7 Shards 分片详解 倒排索引一旦生成，不能更改。 优点： 不用考虑并发写文件的问题，杜绝了锁机制带来的性能问题 文件不在更改，则可以利用文件系统缓存，只需载入一次，只要内存足够，直接从内存中读取该文件，性能高； 利于生成缓存数据(且不需更改)； 利于对文件进行压缩存储，节省磁盘和内存存储空间。 缺点：在写入新的文档时，必须重构倒排索引文件，然后替换掉老倒排索引文件后，新文档才能被检索到，导致实时性差。 解决文档搜索的实时性问题的方案： 新文档直接生成新待排索引文件，查询时同时查询所有倒排索引文件，然后做结果的汇总即可，从而提升了实时性。 Segment Lucene就采用了上述方案，构建的单个倒排索引称为Segment，多个Segment合在一起称为Index(Lucene中的Index)。在ES中的一个shard分片，对应一个Lucene中的Index。且Lucene有一个专门记录所有Segment信息的文件叫做Commit Point。 Segment写入磁盘的过程依然很耗时，可以借助文件系统缓存的特性。「先将Segment在内存中创建并开放查询，来进一步提升实时性」，这个过程在ES中被称为：refresh。 在refresh之前，文档会先存储到一个缓冲队列buffer中，refresh发生时，将buffer中的所有文档清空，并生成Segment。 ES默认每1s执行一次refresh操作，因此实时性提升到了1s。这也是ES被称为近实时的原因（Near Real Time）。 translog文件 translog机制：当文档写入buffer时，同时会将该操作写入到translog中，这个文件会即时将数据写入磁盘，在 6.0 版本之后默认每个要求都必须落盘，这个操作叫做fsync操作。这个时间也是可以通过配置：index.translog.*进行修改的。比如每五秒进行一次fdync操作，那么风险就是丢失这5s内的数据。 文档搜索实时性——flush(十分重要) flush的功能，就是：将内存中的Segment写入磁盘，主要做如下工作： 将translog写入磁盘； 将index bufffer清空，其中的文档生成一个新的Segment，相当于触发一次refresh； 更新Commit Point文件并写入磁盘； 执行fsync落盘操作，将内存中的Segment写入磁盘； 删除旧的translog文件。 refresh与flush的发生时机 refresh：发生时机主要有以下几种情况： 间隔时间达到。 通过index.settings.refresh_interval设置，默认为1s。 index.buffer占满时。 通过indices.memory.index_buffer_size设置，默认JVM heap的10%，且所有shard共享。 flush发生时。会触发一次refresh。 flush：发生时机主要有以下几种情况： 间隔时间达到。 5.x 版本之前，通过index.translog.flush_threshold_period设置，默认 30min。 5.x 版本之后，ES 强制每 30min 执行一次 flush，不能再进行更改。 translog占满时。 通过index.translog.flush_threshold_size设置，默认512m。且每个Index有自己的translog。 删除和更新文档： 删除： Segment一旦生成，就不能更改，删除的时候，Lucene专门维护一个.del文件，记录所有已删除的文档。 .del文件上记录的是文档在Lucene中的ID，在查询结果返回之前，会过滤掉.del文件中的所有文档。 更新： 先删除老文档，再创建新文档，两个文档的ID在Lucene中的ID不同，但是在ElasticSearch中ID相同。 Segment Merging(合并) 随着Segment的增多，由于每次查询的Segment数量也增多，导致查询速度变慢； ES会定时在后台进行Segment merge的操作，减少Segment数量； 通过force_merge api可以手动强制做Segment的合并操作。 2. ElasticSearch 的集群优化2.1 生产环境部署 遵照官方建议设置所有系统参数。 在 ES 的配置文件中 elasticsearch.yml 中，尽量只写必备的参数，其他可通过 api 进行动态设置，随着 ES 版本的不断升级，很多网上流传的参数，现在已经不再适用，所以不要胡乱复制。 建议设置的基本参数有： cluster.name node.name node.master/node.data/node.ingest network.host: 建议显示指定为服务器的内网ip，切勿直接指定0.0.0.0，很容易直接从外部被修改ES数据。 discovery.zen.ping.unicast.hosts: 设置集群其他节点地址，一般设置选举节点即可 discovery.zen.minimum_master_nodes: 一般设置为2，有3个即可。 path.data/path.log 除上述参数外，再根据需要增加其他的静态配置参数，如：refresh优化参数，indices.memory.index_buffer_size。 动态设定的参数有 transient(短暂的)和 persistent(持续的)两种，前者在集群重启后会丢失，后者在集群重启后依然# 生效。二者都覆盖了 yml 中的配置，举例： 12345678910# 使用transient和persistent动态设置ES集群参数PUT /_cluster/Settings&#123; &quot;persistent&quot;:&#123; # 永久 &quot;discovery.zen.minimum_master_nodes:2 &#125;, &quot;transient&quot;:&#123; # 临时 &quot;indices.store.throttle.max_bytes_per_sec&quot;:&quot;50mb&quot; &#125;&#125; 关于 JVM 内存设定 每个节点尽量不要超多31GB。 预留一半内存给操作系统，用来做文件缓存。ES 的具体内存大小根据 node 要存储的数据量来估算，为了保证性能 搜索类项目中：内存：数据量 &#x3D;&#x3D;&#x3D;&gt; 1：16； 日志类项目中：内存：数据量 &#x3D;&#x3D;&#x3D;&gt; 1：48&#x2F;96。 123456假设现有数据1TB，3个node，1个副本，那么：每个node存储(1+1)*1024 / 3 = 666GB,即700GB左右，做20%预留空间，每个node约存850GB数据。此时：如果是搜索类项目，每个node内存约为850/16=53GB，已经超过31GB最大限制；而：31*16 = 496，意味着每个node最大只能存496GB的数据，则：2024/496=4.08...即至少需要5个节点。如果是日志类项目，每个node最大能存:31*48=1488GB,则：2024/1488=1.36...，则三个节点已经够了。 2.2 写性能优化在写上面的优化，主要是增大写的吞吐量——EPS(Event Per Second) 优化方案： Client：多线程写，批量写bulk； ES：在高质量数据建模的前提下，主要在refresh、translig和flush之间做文章。 降低refresh写入内存的频率： 增大refresh_interval，降低实时性，增大每次refresh处理的文件数，默认 1s。可以设为-1s，禁止自动refresh。 增大index buffer大小，参数为：indices.memory.index_buffer_size。此为静态参数，需设定在elasticsea.yml中，默认10% 降低 translog 写入磁盘频率，同时会降低容灾能力： index.translog.durability：设为async； index.translog.sync_interval。设置需要的大小如：120s &#x3D;&gt; 每 120s 才写一次磁盘。 index.translog.flush_threshold_size。默认 512m。即当translog大小超过此值，会触发一次flush，可以调大避免flush过早触发。 在flush方面，从 6.x 开始，ES 固定每 30min 执行一次，所以优化点不多，一般都是 ES 自动完成。 其他： 将副本数设置为 0，在文档全部写完之后再加副本； 合理设计shard数，保证shard均匀地分布在所有node上，充分利用node资源： index.routing.allocation.total_shards_per_node：限定每个索引在每个node上可分配的主副分片数， 如：有5个node，某索引有10个主分片，1个副本(10个副分片)，则：20/5=45,但是实际要设置为5，预防某个node下线后分片迁移失败。 写性能优化，主要还是 index 级别的设置优化。一般在 refresh、translog、flush 三个方面进行优化； 2.3 读性能优化 主要受以下几方面影响： 数据模型是否符合业务模型？ 数据规模是否过大？ 索引配置是否优化？ 查询运距是否优化？ 高质量的数据建模 将需通过cripte脚本动态计算的值，提前计算好作为字段存入文档中； 尽量使数据模型贴近业务模型 根据不同数据规模设定不同的SLA(服务等级协议)，万级数据和千万级数据和亿万级数据性能上肯定有差异； 索引配置优化 根据数据规模设置合理的分片数，可通过测试得到最适合的分片数； 分片数并不是越多越好 查询语句优化 尽量使用Filter上下文，减少算分场景(Filter有缓存机制，能极大地提升查询性能)； 尽量不用cript进行字段计算或算分排序等； 结合profile、explain API分析慢查询语句的症结所在，再去优化数据模型。 2.4 其他优化点 如何设定shard数？ ES的性能基本是线性扩展的，因此，只需测出一个shard的性能指标，然后根据实际的性能需求就可算出所需的shard数。 测试一个shard的流程如下： 搭建与生产环境相同配置的单节点集群； 设定一个单分片0副本的索引； 写入实际生产数据进行测试，获取（写性能指标）； 针对数据进行查询操作，获取（读性能指标）。 压力测试工具，可以采用ES自带的esrally，从经验上讲： 如果是搜索引擎场景，单shard大小不超过15GB； 如果是日志分析场景，单shard大小不超过50GB； 估算索引的总数据大小，除以上述单shard大小，也可得到经验上的分片数。 2.5 ES 集群监控使用官方免费插件X-pack。 安装与启动： 123456# X-pack的安装cd ~/elasticsearch-6.1.1bin/elasticsearch-plugin install x-pack#cd ~/kibana-6.1.1bin/kibana-plugin indtall x-pack 之后重启ES集群即可。在kibana的界面可以看到新增了工具，使用Monitoring进行集群监控。 3. ElasticSearch 中 Search 的运行机制 Search执行的时候，实际分为两个步骤执行： Query阶段：搜索 Fetch阶段：获取 3.1 Query—Then—Fetch：若集群my_cluster中存在三个节点 node1、node2、node3，其中master为 node1，其余的为data节点。 Query阶段: Fetch阶段: 3.2 相关性算分：相关性算分在shard和shard之间是相互独立的。也就意味着：同一个单词term在不同的shard上的TDF等值也可能是不同的。得分与shard有关。当文档数量不多时，会导致相关性算分严重不准的情况发生。 解决方案： 设置分片数为1个，从根本上排除问题。（此方案只适用于百万&#x2F;少千万级的少量数据） 使用DFS Query-then-Fetch查询方式。 DFS Query-then-Fecth： 在拿到所有文档后，再重新进行完整的计算一次相关性得分，耗费更多的 CPU 和内存，执行性能也较低。所以也不推荐。 123456789# 使用DLS Query-then-Fetch进行查询：GET my_index/_search？search_type=dfs_query_then_fetch&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; ... &#125; &#125;&#125; 3.3 排序相关：默认采用相关性算分结果进行排序。可通过sort参数自定义排序规则，如： 1234567891011121314151617181920212223# 使用sort关键词进行排序GET my_index/_search&#123; &quot;sort&quot;:&#123; # 关键词 &quot;birth&quot;:&quot;desc&quot; &#125;&#125;# 或使用数组形式定义多字段排序规则GET my_index/_search&#123; &quot;sort&quot;:[ # 使用数组 &#123; &quot;birth&quot;:&#123; &quot;order&quot;:&quot;asc&quot; &#125; &#125;, &#123; &quot;age&quot;:&#123; &quot;order&quot;:&quot;desc&quot; &#125; &#125; ]&#125; 直接按数字&#x2F;日期排序，如上例中birth 按字符串进行排序：字符串排序较特殊，因为在ES中有keyword和text两种： 123456789101112131415# 直接对text类型进行排序GET my_index/_search&#123; &quot;sort&quot;:&#123; &quot;username&quot;:&quot;desc&quot; # 针对username字段进行倒序排序 &#125;&#125;## 针对keyword进行排序GET my_index/_search&#123; &quot;sort&quot;:&#123; &quot;username.keyword&quot;:&quot;desc&quot; # 针对username的子类型keyword类型进行倒叙排序 &#125;&#125; 3.3.1 关于 fielddata 和 docvalues:排序的实质是对字段的原始内容排序的过程，此过程中倒排索引无法发挥作用，需要用到正排索引。即：通过文档ID和字段得到原始内容。 ES 提供 2 中实现方式： Fielddata。 默认禁用。 DocValues。 默认启用，除了 text 类型。 对比 Fielddata DocValues 创建时机 搜索时即时创建 创建索引时创建，和倒排索引创建时间一致 创建位置 JVM Heap 磁盘 优点 不占用额外磁盘空间 不占用 Heap 内存 缺点 文档较多时，同时创建会花费过多时间，占用过多 Heap 内存 减慢索引的速度，占用额外的磁盘空间 3.3.2 Fielddata 的开启:Fielddata默认关闭，可通过如下 api 进行开启，且在后续使用时随时可以开启&#x2F;关闭： 使用场景：一般在对分词做聚合分析的时候开启。 12345678910# 开启字段的fielddata设置PUT my_index/_mapping/doc&#123; &quot;properties&quot;:&#123; &quot;username&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;fielddata&quot;:true # 关键词 &#125; &#125;&#125; 3.3.3 Docvalues 的关闭Docvalues默认开启，可在创建索引时关闭，且之后不能再打开，要打开只能做 reindex 操作。 使用场景：当明确知道，不会使用这个字段排序或者不做聚合分析的时候，可关闭 doc_values，减少磁盘空间的占用。 1234567891011121314# 关闭字段的docvalues设置PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;username&quot;:&#123; &quot;type&quot;:&quot;keyword&quot;, &quot;doc_values&quot;:false # 关键词 &#125; &#125; &#125; &#125;&#125; 3.4 分页与遍历ES 提供了三种方式来解决分页和遍历的问题： from/size，scroll，search_after。 3.4.1 from&#x2F;size from：指明开始位置； size：指明获取总数 123456# 使用from——sizeGET my_index/_search&#123; &quot;from&quot;:1, # 从第2个开始搜索 &quot;size&quot;:2 # 获取2个长度&#125; 经典问题：深度分页。 问题：如何在数据分片存储的情况下， 获取前 1000 个文档？ 答案： 先从每个分片上获取前 1000 个文档， 然后由处理节点聚合所有分片的结果之后，再排序获取前 1000 个文档。 此时页数越深，处理的文档就越多，占用的内存就越大，耗时就越长。这就是深度分页问题。 为了尽量避免深度分页为题，ES 通过设定index.max_result_window限定最多到 10000 条数据。 在设计分页系统时，有一个分页数十分重要： total_page=(total + page_size -1) / page_size 总分页数&#x3D; (文档总数+认为设定的文档大小-1) &#x2F; 人为设定的文档大小 但是在搜索引擎中的意义并不大，因为如果排在前面的结果都不能让用户满意，那么越往后，越不能让用户满意。 3.4.2 scroll 遍历文档集的API，以快照的方式来避免深度分页问题。 不能用来做实时搜索，因为数据不是实时的； 尽量不用复杂的sort条件，使用_doc最高效； 使用比较复杂。 步骤： 发起一个scroll search，会返回后续会用到的_scroll_id 调用scroll search的api，获取文档集合，不断迭代至返回hits数组为空时停止 之后不断返回新的_scroll_id，使用新的_scroll_id进行查询，直到返回数组为空。 当不断的进行迭代，会产生很多scroll，导致大量内存被占用，可以通过clear api进行删除 12345678910111213141516171819202122232425# 发起一个scroll searchGET my_index/_search?scroll=5m # 该快照的有效时间为5min&#123; &quot;size&quot;1 # 指明每次scroll返回的文档数&#125;## 调用scroll search 的api，获取文档集合POST _search/scroll&#123; &quot;scroll&quot;:&quot;5m&quot;, # 指明有效时间 &quot;scroll_id&quot;:&quot;xxxxxx&quot; # 上一步返回的_scroll_id&#125;## 使用clear api对scroll进行删除DELETE /_search/scroll&#123; &quot;scroll_id&quot;:[ &quot;xxxxxx&quot;, # _scroll_id &quot;xxxxxx&quot;, # _scroll_id ...... ]&#125;## 删除所有的scrollDELETE /_search/scroll/_all 3.4.3 search_after避免深度分页的性能问题，提供实时的下一页文档获取功能。 缺点：不能使用 from 参数，即：不能指定页数。且只能下一页，不能上一页。 使用步骤： 第一步：正常搜索，但是要指定 sort 值，并保证值唯一： 第二步：使用上一步最后一个文档的 sort 值进行查询： 1234567891011121314151617181920# 第一步，正常搜索GET my_index/_search&#123; &quot;size&quot;:1, &quot;sort&quot;:&#123; &quot;age&quot;:&quot;desc&quot;, &quot;_id&quot;:&quot;desc&quot; &#125;&#125;## 第二步，使用sort值进行查询GET my_index/_search&#123; &quot;size&quot;:1, &quot;search_after&quot;:[28,&quot;2&quot;],# 28,&quot;2&quot;，是上一次搜索返回的sort值 &quot;sort&quot;:&#123; &quot;age&quot;:&quot;desc&quot;, &quot;_id&quot;:&quot;desc&quot; &#125;&#125; 3.4.4 如何避免深度分页问题:这个问题目前连 google 都没能解决，所以只能最大程度避免，通过唯一排序值定位每次要处理的文档数都控制在 size 内： 应用场景： from&#x2F;size:需实时获取顶部的部分文档，且需自由翻页（实时）； scroll:需全部文档，如：导出所有数据的功能（非实时）； search_after:需全部文档，不需自由翻页（实时）。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://chaooo.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://chaooo.github.io/tags/Kibana/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"http://chaooo.github.io/tags/ElasticStack/"},{"name":"LogStash","slug":"LogStash","permalink":"http://chaooo.github.io/tags/LogStash/"}]},{"title":"「ElasticStack」ElasticSearch入门","date":"2018-11-12T12:42:59.000Z","path":"2018/11/12/elastic-base.html","text":"1. 概述1.1 ElasticStack特点 使用门槛低，开发周期短，上线快 性能好，查询快，实时展示结果 扩容方便，快速支撑增长迅猛的数据 1.2 ElasticStack各组件作用 **Beats**：数据采集 LogStash: 数据处理 ElasticSearch(核心引擎): 数据存储、查询和分析 Kibana: 数据探索与可视化分析 1.3 ElasticStack使用场景 搜索引擎、日志分析、指标分析 1.4 ElasticStack安装启动 ElasticSearch启动：解压到安装目录，启动bin/elasticsearch（默认端口:http:\\\\localhost:9200, 加参数-d后台启动） ElasticSearch集群： 123bin/elasticsearch -d bin/elasticsearch -Ehttp.port=8200 -Epath.data=node2 -dbin/elasticsearch -Ehttp.port=7200 -Epath.data=node3 -d Kibana启动：解压到安装目录，启动bin/kibana（默认端口:http:\\\\localhost:5601） 1.5 ElasticSearch常见术语 Document(文档)：用户存储在ES中的数据文档 Index(索引)：由具有相同字段的文档列表组成 field(字段)：包含具体数据 Node(节点)：一个ES的实例，构成clister的单元 Cluster(集群)：对外服务的一个&#x2F;多个节点 1.6 Document介绍 常用数据类型：字符串、数值型、布尔型、日期型、二进制、范围类型 每个文档都有一个唯一ID标识。（可以自行指定，也可由ES自动生成） 元数据，用于标注文档的相关信息： _index：文档所在的索引名 _type：文档所在的类型名(后续的版本中type这个概念将会被移除，也不允许一个索引中有多个类型) _id：文档唯一标识 _source：文档的原始JSON数据，可从这获取每个字段的内容 _all：整合所有字段内容到该字段。（默认禁用） _version：文档字段版本号，标识被操作了几次 Index介绍： 索引中存储相同结构的文档，且每个index都有自己的Mapping定义，用于定义字段名和类型； 一个集群中可以有多个inex，类似于可以有多个table。 RESTful API两种交互方式： CURL命令行：curl -XPUT xxx Kibana DevTools————PUT xxx{ } Index API： 用户创建、删除、获取索引配置等。 创建索引： PUT /test_index #创建一个名为test_index的索引 查看索引： GET _cat/indices #查看所有的索引 删除索引： DELETE /test_index #删除名为test_index的索引 1.7 CRUD操作（交互基于Kibana DevTools） 创建文档 12345678910111213# 创建ID为1的DocumentPUT /test_index/doc/1&#123; &quot;username&quot;:&quot;alfred&quot;, &quot;age&quot;:&quot;24&quot;&#125;## 不指定ID创建Document(ID会自动生成)POST /test_index/doc&#123; &quot;username&quot;:&quot;buzhiding&quot;, &quot;age&quot;:&quot;1&quot;&#125; 查询文档： 1234567891011121314# 查看名为test_index的索引中id为1的文档GET /test_index/doc/1## 查询所有文档：# 查询名为test_index的索引中所有文档,用到endpoint：_search，默认返回符合的前10条# term和match的区别：term完全匹配，不进行分词器分析；match模糊匹配，进行分词器分析，包含即返回GET /test_index/doc/_search&#123; &quot;query&quot;:&#123; &quot;term&quot;:&#123; &quot;_id&quot;:&quot;1&quot; &#125; &#125;&#125; 批量操作文档： 12345678910111213141516171819202122232425# 批量创建文档，用到endpoint：_bulk# index和create的区别，如果文档存在时，使用create会报错，而index会覆盖POST _bulk&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;doc&quot;,&quot;_id&quot;:&quot;3&quot;&#125;&#125;&#123;&quot;username&quot;:&quot;alfred&quot;,&quot;age&quot;:&quot;20&quot;&#125;&#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;doc&quot;,&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;update&quot;:&#123;&quot;_id&quot;:&quot;2&quot;,&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;doc&quot;&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;age&quot;:&quot;30&quot;&#125;&#125;## 批量查询文档，使用endpoint:_mgetGET _mget&#123; &quot;doc&quot;:[ &#123; &quot;_index&quot;:&quot;test_index&quot;, &quot;_type&quot;:&quot;doc&quot;, &quot;_id&quot;:&quot;1&quot; &#125;, &#123; &quot;_index&quot;:&quot;test_index&quot;, &quot;_type&quot;:&quot;doc&quot;, &quot;_id&quot;:&quot;2&quot; &#125; ]&#125; 删除文档： 1234567891011121314151617# 根据搜索内容删除文档,使用endpoint:_delete_by_queryPOST /test_index/doc/_delete_by_query&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;buzhiding&quot; &#125; &#125;&#125;# # 删除整个test_index的索引中的文档,依然使用endpoint:_delete_by_queryPOST /test_index/doc/_delete_by_query&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125; 2. ElasticSearch倒排索引与分词2.1 倒排索引 正排索引和倒排索引 正排索引：文档ID —&gt; 文档内容 倒排索引：单词—&gt; 文档ID列表 倒排索引组成：（单词词典，倒排列表） 单词词典（Term Dictionary） 记录所有文档的单词，记录了单词到倒排列表的关联信息，一般使用B+Tree实现。 倒排列表（Posting List） 记录单词对应的文档集合，由倒排索引项Posting List组成。 倒排索引项： 文档ID：用于获取原始信息。 词频TF：记录该单词在该文档中的出现次数，用于计算相关性得分。 位置Position：记录单词在文档中的分词位置(多个)，用于词语搜索。 偏移Offset：记录单词在文档的开始和结束位置，用于高亮显示。 2.2 分词Analysis分词：将文本转换成一系列单词Term/Token的过程，也可称作文本分析，ES中叫作：Analysis。 一些概念： Token(词元)：全文搜索引擎会用某种算法对要建索引的文档进行分析， 从文档中提取出若干Token(词元)。 Tokenizer(分词器)：这些算法叫做Tokenizer(分词器) Token Filter(词元处理器)：这些Token会被进一步处理， 比如转成小写等， 这些处理算法被称为TokenFilter(词元处理器) Term(词)：被处理后的结果被称为Term(词) Character Filter(字符过滤器)：文本被Tokenizer处理前可能要做一些预处理， 比如去掉里面的HTML标记， 这些处理的算法被称为Character Filter(字符过滤器) Analyzer(分析器)：这整个的分析算法被称为Analyzer(分析器)，由Tokenizer(分词器)和Filter(过滤器)组成 ES有很多内置Analyzer,比如： standard：按单词边界划分、支持多语言、小写处理、移除大部分标点符号，支持停用词 whitespace：空格为分隔符 simple：按非字母划分、小写处理 stop：类似简单分词器，同时支持移除停用词(the、an、的、这等) keyword：不分词 pattern：通过正则表达式自定义分隔符，默认\\w+，即：非字词的符号作为分隔符 第三方analyzer插件：常用的中文分词器有： IK：实现中英文分词，支持多模式，可自定义词库，支持热更新分词词典。 jieba。python中流行，支持繁体分词、并行分词，可自定义词典、词性标记等。 ES提供了一个测试分词的API接口，使用endpoint：_analyze，不指定分词时，会使用默认的standard 123456789101112131415161718192021# 指定分词器进行分词测试POST _analyze&#123; &quot;analyzer&quot;:&quot;standard&quot;, &quot;text&quot;:&quot;hello world!&quot;&#125;# # 直接指定索引中字段：使用username字段的分词方式对text进行分词。POST test_index/_analyze&#123; &quot;field&quot;:&quot;username&quot;, &quot;text&quot;:&quot;hello world!&quot;&#125;# # 自定义分词器，自定义Tokenizer、filter、等进行分词：POST _analyze&#123; &quot;tokenizer&quot;:&quot;standard&quot;, &quot;filter&quot;:[&quot;lowercase&quot;], &quot;text&quot;:&quot;Hello World!&quot;&#125; 3. ElasticSearch的Mapping3.1 Mapping简介Mapping：类似于数据库中的表结构 主要作用如下： 定义Index下的Field Name； 定义Field的类型，如：数值型、字符串型、布尔型等； 定义倒排索引的相关配置，如：是否有索引，记录position等。 获取一个mapping，使用endpoint：_mapping，例如： GET /test_index/_mapping 3.2 自定义Mapping 使用mappings进行自定义mapping。 Mapping中的字段类型一旦设定之后，禁止直接修改。 因为Luence事先的倒排索引生成后不能修改。 如果一定要改，可以重新建立新的索引，然后对应修改mapping，之后将之前的数据进行reindex操作，导入新的文档。 自定义mapping时允许新增字段。通过dynamic参数进行控制字段的新增，dynamic有三种配置： true：默认配置，允许自动新增字段； false：不允许自动新增字段，文档可以正常写入，但不能进行查询等操作； strict：严格模式。文档不能写入，写入会报错。 123456789101112131415161718192021# 创建名为my_index的索引，并自定义mapping# 使用dynamic参数控制字段的新增PUT my_index&#123; &quot;mappings&quot;:&#123; # 关键字 &quot;doc&quot;:&#123; # 类型名 &quot;dynamic&quot;:false, # 设置为false，索引不允许新增字段 &quot;properties&quot;:&#123; # 字段名称及类型定义 &quot;title&quot;:&#123; &quot;type&quot;:&quot;text&quot; # 字段类型 &#125;, &quot;name&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125; &#125; &#125; &#125;&#125; 3.3 copy_to的使用将该字段的值复制到目标字段，类似于6.0版本之前的_all的作用。且不会出现在_source，一般只用来进行搜索。 1234567891011121314151617181920212223242526272829303132333435363738394041# copy_to的使用PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;first_name&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;copy_to&quot;:&quot;full_name&quot; &#125;, &quot;last_name&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;copy_to&quot;:&quot;full_name&quot; &#125;, &quot;full_name&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125; &#125; &#125; &#125;&#125;# # 向索引写入数据PUT my_index/doc/1&#123; &quot;first_name&quot;:&quot;John&quot;, &quot;last_name&quot;:&quot;Smith&quot;&#125;# # 查询索引my_index中full_name同时包含John 和 Smith的数据GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;full_name&quot;:&#123; &quot;query&quot;:&quot;John Smith&quot;, &quot;operator&quot;:&quot;and&quot; &#125; &#125; &#125;&#125; 3.4 index参数的使用控制当前字段是否为索引，默认true，当设置为false的时候，不进行记录，此时该字段不能被搜索 1234567891011121314# index参数的使用PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;cookie&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;index&quot;:false # 设置为false，该字段不能被搜索 &#125; &#125; &#125; &#125;&#125; 此时在进行数据写入和查询，不能进行该字段搜索。一般用来进行不想被查询的私密信息设置，如身份证号，电话号码等： 12345# 向使用了index参数的字段写入信息PUT my_index/doc/1&#123; &quot;cookie&quot;:&quot;name=alfred&quot;&#125; 3.5 index_options参数的使用：控制倒排索引记录的内容，有如下四种配置： docs：只记录文档ID freqs：记录文档ID和词频TF positions：记录文档ID、词频TF和分词位置 offsets：记录文档ID、词频TF、分词位置和偏移 其中：text类型默认的配置是positions，其他的比如integer等类型默认为docs，目的是为了节省空间。 1234567891011121314# index_options参数的使用PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;cookie&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;index_options&quot;:&quot;offsets&quot; # 记录文档ID、词频TF、分词位置和偏移 &#125; &#125; &#125; &#125;&#125; 3.6 null_value参数的使用：当字段遇到空值null时的处理策略。默认为null，即跳过。此时ES会忽略该值，可通过修改进行默认值的修改： 1234567891011121314# 使用null_value修改ES遇到null值时的默认返回值PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;properties&quot;:&#123; &quot;cookie&quot;:&#123; &quot;type&quot;:&quot;keyword&quot;, &quot;null_value&quot;:&quot;NULL&quot; # 当遇到空值null的时候，返回一个字符串形式的NULL &#125; &#125; &#125; &#125;&#125; 3.7 Field字段的数据类型： 核心数据类型 字符串型：text(分词)，keyword(不分词) 数值型：long,integer,short,byte,double,float,half_float,scaled_float 日期类型：date 布尔类型：boolean 二进制类型：binary 范围类型：integer_range,float_range,long_range,double_range,date_range 复杂数据类型 数组类型：array 对象类型：object 嵌套类型：nested object 地理位置数据类型 点：geo-point 形状：geo-shape 专用类型 记录ip地址：ip 实现自动补全：completion 记录分词数：token_count 记录字符串hash值：murmur3 perclator join 多字段特性： ES允许对同一个字段采用不同的配置，如：分词。举例：对一个人名实现拼音搜索，只需要在人名字段中新增一个子字段pinyin即可。 3.8 ES的自动类型识别： Dynamic Mapping： ES可以自动识别文档字段类型，从而降低用户使用成本。 123456# ES的自动类型识别PUT my_index/doc/1&#123; &quot;username&quot;:&quot;alfred&quot;, # username字段自动识别为text类型 &quot;age&quot;:20 # age字段自动识别为long类型&#125; ES依靠JSON文档的字段类型实现自动识别字段类型： JSON类型 ElasticSearch类型 null 忽略 boolean boolean 浮点类型 float 整数类型 long object object array 由第一个非null的值的类型决定 String 匹配为日期，则为date类型(默认开启)匹配为数字，则为long类型&#x2F;float类型(默认关闭)都未匹配，则设为text类型，并附带keyword子字段 验证ES的字段类型自动识别： 123456789101112# 验证ES的字段类型自动识别PUT my_index/doc/1&#123; &quot;username&quot;:&quot;alfred&quot;, # 字符串类型text &quot;age&quot;:20, # 整数long &quot;bitrh&quot;:&quot;1998-10-10&quot;, # 默认识别日期date &quot;married&quot;:false, # 布尔类型boolean &quot;year&quot;:&quot;18&quot; # 默认不识别数字text &quot;tags&quot;:[&quot;boy&quot;,&quot;fashion&quot;],# 数组中第一个不为null的元素为字符串类型，所以为text &quot;money&quot;:100.1 # 浮点类型float&#125;# 再对my_index进行mapping查询，就会获得每个字段的类型： 3.9 ES中日期类型和数字的自动识别：ES中可自行配置日期的格式，默认：[“strict_date_optional_time“,”yyyy/MM/dd HH:mm:ss Z|| yyyy/MM/dd z“] 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 1. 使用dynamic_date_formats自定义日期格式PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;dynamic_date_formats&quot;:[&quot;MM/dd/yyyy&quot;] &#125; &#125;&#125;# 写入符合自定义格式的日期数据，可识别为date类型PUT my_index/doc/1&#123; &quot;create_time&quot;:&quot;01/01/2019&quot; # create_time字段识别为date类型&#125;# # 2. 使用date_detection可以关闭自动识别日期格式：PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;date_detection&quot;:false &#125; &#125;&#125;# PUT my_index/doc/1&#123; &quot;create_time&quot;:&quot;01/01/2019&quot; # create_time字段是text类型&#125;# # ES中可配置数字是否识别，默认关闭：PUT my_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;numeric_detection&quot;:true # 开启数字自动识别 &#125; &#125;&#125;# 写入数字数据，ES可以自动识别其类型PUT mu_index/doc/1&#123; &quot;year&quot;:&quot;18&quot;, # year字段自动识别为long类型 &quot;money&quot;:&quot;100.1&quot; # money字段自动识别为float类型&#125; 3.10 ES中根据自动识别的数据类型，动态生成字符类型例: 字符串类型都设为keyword类型（不分词） 以message开头的字段都设为text类型（分词） 以long_开头的字段都设为long类型 自动匹配为double的类型都设为float类型。（为了节省空间） 12345678910111213141516171819# ES根据自动识别的数据类型、字段名等动态设定字符类型PUT test_index&#123; &quot;mappings&quot;:&#123; &quot;doc&quot;:&#123; &quot;dynamic_template&quot;:[ &#123; &quot;strings&quot;:&#123; # 匹配到所有的字符串类型，全部设为keyword类型 &quot;match_mapping_type&quot;:&quot;string&quot;, &quot;mapping&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; 匹配规则的参数： match_mapping_type：匹配ES自动识别的字段类型，如boolean、long、string等 match、unmatch：匹配字段名，比如”match”:”message*” &#x3D;&#x3D;&#x3D;&gt;以message开头的数据 path_match、path_unmatch：匹配路径 3.11 自定义mapping的操作步骤 写入一条文档到ES的临时索引中，获取(复制)ES自动生成的mapping 修改获得的mapping，并在其中自定义相关配置 使用修改后的mapping创建实际所需索引。 4. ElasticSearch的Search API在ES中，为了实现对存储的数据进行查询分析，使用endpoint：**_search**。 实现对所有索引的泛查询：GET /_search 实现对一个索引的单独查询：GET /my_index/_search 实现对多个索引的指定查询：GET /my_index1,my_index2/_search 实现对符合指定要求的索引进行查询：GET /my_*/_search 在进行查询的时候，主要有两种方式：(URI Search，Request Body Search) **URI Search**：操作简单，直接通过命令行方便测试，但仅包含部分查询语法； 如：GET /my_index/_search?q=username:alfred **Request Body Search**：ES提供的完备查询语法，使用Query DSL(Domain Specific Language)进行查询 123456789# 如：Request Body Search方式进行查询GET /my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;&#125; 4.1 URI Search 通过url query参数实现搜索，常用参数有： **q**：指定查询的语句，使用query string syntax语法 **df**：q中不指定字段时默认查询的字段（在不指定的时候默认查询所有字段） **sort**：排序 **timeout**：指定超时时间，默认不超时 **from,size**：用于分页 举例： GET my_index/_search?q=alfred&amp;df=username&amp;sort=age:asc&amp;from=4&amp;size=10&amp;timeout=1s 解释：查询索引my_index中username字段中包含alfred的文档，结果按age字段升序排列，返回第5-14个文档，若超过1s未结束，则以超时结束。 query string syntax语法 前置内容：term:单词，phrase:词语。 单词与词语语法： 单词：alfred way等价于alfred OR way 词语：&quot;alfred way&quot;语句查询，要求先后顺序 泛查询：不指定字段，会在所有字段中去匹配其单词 指定字段查询：指定字段，在指定字段中匹配单词 Group分组设定，使用括号指定匹配的规则 举例：GET my_index/_search?q=username:(alfred OR way)AND lee 4.1.1 URI Search API 泛查询： 1234GET my_index/_search?q=alfred&#123; &quot;profile&quot;:true #使用profile参数，可以明确地看到ES如何执行的查询条件&#125; 指定字段查询： 123456789101112# a.查询字段username中包含alfred的文档GET my_index/_search?q=username:alfred## b.查询字段username中包含alfred或way的文档GET my_index/_search?q=username:alfred way## c.查询字段username为&quot;alfred way&quot;的文档GET my_index/_search?q=username:&quot;alfred way&quot;## d.分组后，查询字段username中包含alfred，包含way的文档GET my_index/_search?q=username:(alfred way)# 这个和b的结果一样，但是区别在于使用分组之后，不进行泛查询。 布尔操作符AND(&amp;&amp;)、OR(||)、NOT(!)、+(must)、-(must_not) 1234567891011# 查询索引my_index中username包含alfred但是不包含way的文档GET my_index/_search?q=username:(alfred NOT way)## 查询索引my_index中一定包含lee，一定不含alfred，可能有way的文档GET my_index/_search?q=username:(way +lee -alfred)# 或写成GET my_index/_search?q=username:((lee &amp;&amp; !alfred) || (way &amp;&amp; lee &amp;&amp; !alfred))## 注意：url中，+(加号)会被解析成空格，所以要用 %2B ：# 查询索引my_index中一定包含lee，一定不包含alfred，可能包含way的文档GET my_index/_search?q=username:(way %2Blee -alfred) 范围查询（支持数值和日期） 区间写法：闭区间使用[]，开区间使用&#123;&#125; age:[1 TO 10] # 1&lt;&#x3D; age &lt;&#x3D;10 age:[1 TO 10&#125; # 1&lt;&#x3D; age &lt;10 age:[1 TO ] # age &gt;&#x3D;1 age:[* TO 10] # age &lt;&#x3D;10 算数符号写法： age:&gt;=1 age:(&gt;=1 &amp;&amp; &lt;= 10) / age:(+ &gt;= 1 + &lt;= 10) 还可以对日期进行范围查询，注意：年&#x2F;月是从1月1号&#x2F;1号开始算的： 12345678# a.查询索引my_index中username字段包含alfred_或_年龄大于20的文档GET my_index/_search?q=username:alfred age&gt;20# # b.查询索引my_index中username字段包含alfred_且_年龄大于20的文档GET my_index/_search?q=username:alfred AND age&gt;20# # 查询索引my_index中birth字段在1985和1990之间的文档GET my_index/_search?q=birth:(&gt;1985 AND &lt; 1990) 通配符查询 ?代表一个字符，*代表0个或多个字符，如：name:a?lfred或name:a*d或name:alfred* 注意：通配符匹配的执行效率较低，且占用内存较多，不建议使用，如果没有特殊要求，也不要将?或者*放在最前面，因为意味着要匹配所有文档，可能会造成OOM。 正则表达式&#x2F;模糊匹配&#x2F;近似度查询 正则表达式：举例：/[a]?l.*/ 模糊匹配：fuzzy query 近似度查询：proximity search 12345# 模糊匹配。匹配与alfred差一个字符的词，比如：alfreds、alfret等GET my_index/_search?q=username:alfred~1## 近似度查询，查询字段username和&quot;alfred way&quot;差n个单词的文档GET my_index/_search?q=username:&quot;alfred way&quot; ~5 使用场景常见于用户输入词的纠错中。 4.2 Request Body SearchES自带的完备查询语句，将查询语句通过http request body发送到ES，主要参数有： query：符合Query DSL语法的查询条件 from，size timeout sort Query DSL语法： 基于JSON定义的查询语言，主要包含两个类型： 字段类查询————如：term，match，range等。只针对一个字段进行查询 复合查询————如：bool查询等。包含一个&#x2F;多个字段类查询&#x2F;符合查询语句 4.2.1 字段类查询-全文匹配针对text类型的字段进行全文检索，会对查询语句进行“先分词再查询”处理，如：match、match_phrase等 4.2.1.1 match query 对字段进行全文检索(最基本和最常用的查询类型)，举例： 12345678GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; # 关键词 &quot;username&quot;:&quot;alfred way&quot; # 字段名和查询语句 &#125; &#125;&#125; 从结果，可以返回匹配文件总数，返回文档列表，_score相关性得分等。一般的执行流程为： 1.对查询语句分词&#x3D;&#x3D;&gt;2.根据字段的倒排索引列表，进行匹配算分&#x3D;&#x3D;&gt;3.汇总得分&#x3D;&#x3D;&gt;4.根据得分排序，返回匹配文档 使用operator参数，可以控制单词间关系，有and/or： 12345678910# 使用operator参数控制单词间关系GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred way&quot;, &quot;operator&quot;:&quot;and&quot; # and，同时包含alfred和way &#125; &#125;&#125; 使用minimum_should_match参数控制需匹配的单词数 12345678910# 使用minimum_should_match参数控制需匹配的单词数GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred way&quot;, &quot;minimum_should_match&quot;:&quot;2&quot; &#125; &#125;&#125; 4.2.1.2 相关性算分，其本质就是一个排序问题 计算文档与待查询语句之间的相关度，一般有四个重要概念： Term Frequency 词频(正相关) Document Frequency 文档频率(负相关) Inverse Term Frequency 逆文本频率(正相关) Field-length Norm 文档长度(负相关) 目前ES有两个相关性算分的模型： TF/IDF模型：经典模型。 BM25模型：5.x版本后的默认模型，是对TF&#x2F;IDF的优化模型。 TF/IDF模型：在使用kibana进行查询时，使用explain参数，可以查看具体的计算方法。 12345678910# 使用explain参数，可以查看具体的相关性的得分是如何计算的GET my_index/_search&#123; &quot;explain&quot;:true, # 设置为true &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;&#125; 注意：ES计算相关性得分是根据shard进行的，即分片的分数计算相互独立，所以在使用的时候要注意分片数，可以通过设定分片数为1来避免这个问题，主要是为了观察，不代表之后所有的分片全都设为1。一般放在创建索引后，未加数据之前。 1234567# 设定shards数量为1PUT my_index&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot;:&quot;1&quot; &#125;&#125; BM25模型。5.x版本后的默认模型，是对TF&#x2F;IDF的优化模型。 best match，25指：迭代了25次才计算。BM25的使用，降低了TF&#x2F;IDF中因为TF过大导致的负面影响，在BM25中，一个单词的TF一直增长，到一定程度就趋于0变化。 4.2.1.3 match phrase query对字段做全文检索，有顺序要求。 使用match——phrase查询词语 12345678GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; # 关键词 &quot;job&quot;:&quot;java engineer&quot; &#125; &#125;&#125; 通过使用slop参数，可以控制单词间间隔： 1234567891011GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;job&quot;:&#123; &quot;query&quot;:&quot;java engineer&quot;, &quot;slop&quot;:&quot;1&quot; # 关键词，设定单词间隔 &#125; &#125; &#125;&#125; 4.2.1.4 query string query类似于URI Search中的q参数查询，举例： 使用query_string查询 1234567891011121314151617181920GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;query_string&quot;:&#123; &quot;default_field&quot;:&quot;username&quot;, &quot;query&quot;:&#123;alfred AND way&quot; &#125; &#125;&#125;##* 或 */GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;query_string&quot;:&#123; &quot;fileds&quot;:[&quot;username&quot;,&quot;job&quot;], &quot;query&quot;:&quot;alfred OR (java AND ruby)&quot; &#125; &#125;&#125; 4.2.1.5 simple query string query类似于query string，但会忽略错误的查询语法，且仅支持部分查询语句。使用+，|，-分别代替AND，OR，NOT。 使用simple query string query123456789GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;simple_query_string&quot;:&#123; &quot;fields&quot;:[username], &quot;query&quot;:&quot;alfred +way&quot; #等价于 &quot;query&quot;:&quot;alfred AND way&quot; &#125; &#125;&#125; 4.2.2 字段类查询-单词匹配4.2.2.1 term&#x2F;terms query将待查询语句作为整个单词进行查询，不做分词处理，举例： 使用term进行单查询 12345678GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;term&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;&#125; 使用terms进行多查询 12345678GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;terms&quot;:&#123; &quot;username&quot;:[&quot;alfred&quot;,&quot;way&quot;] &#125; &#125;&#125; 此时如果直接使用alfred way作为username查询条件，是不会返回任何文档的。因为在username的倒排索引列表中，存在&quot;alfred&quot;和&quot;way&quot;的索引，但是不存在&quot;alfred way&quot;的索引。 4.2.2.2 range query 范围查询，主要针对数值类型和日期类型。 gt: greater than 大于 gte: greate than or equal to 大于等于 lt: less than 小于 lte: less than or equal to 小于等于 对数值的查询 123456789101112# range query对数值的查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;range&quot;:&#123; &quot;age&quot;:&#123; &quot;gte&quot;:10, &quot;lte&quot;:20 &#125; &#125; &#125;&#125; 对日期的查询 123456789101112# range query对日期的查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;range&quot;:&#123; &quot;birth&quot;:&#123; &quot;lte&quot;:&quot;1988-01-01&quot; # 或者使用&quot;lte&quot;:&quot;now-30y&quot;,这种Date Math类型 &#125; &#125; &#125;&#125; Date Math类型：针对日期提供的一种更友好的计算方式。当前时间用now代替，具体时间的引用，需要使用||间隔。年、月、日、时、分、秒跟date一致：y、M、w、d、h、m、s。举例： 12345# 假设当前时间为2019-01-02 12:00:00now+1h =&gt; 2019-01-02 13:00:00now-1h =&gt; 2019-01-02 11:00:00now-1h/d =&gt; 2019-01-02 00:00:002019-01-01||+1M/d =&gt; 2019-02-01 00:00:00 4.2.3 复合查询包含一个&#x2F;多个字段类查询&#x2F;符合查询语句 4.2.3.1 constant_score query constant_score query: 将内部的查询结果文档得分全部设定为1或boost的值。返回的相关性得分全部为1或boost 1234567891011# 使用constant_score queryGET my_index/_Search&#123; &quot;query&quot;:&#123; &quot;constant_score&quot;:&#123; #关键词 &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125; &#125;&#125; 4.2.3.2 bool querybool query: 由一个&#x2F;多个布尔子句组成，主要包含以下四个： filter: 只过滤符合条件的文档，不计算相关性得分，返回的相关性得分全部为0； ES会对filter进行智能缓存，因此执行效率较高，在做简单匹配查询且不考虑得分的时候没推荐使用filter代替query 12345678910111213# 使用filter查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; # 关键词 &quot;filter&quot;:[ &quot;term&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; ] &#125; &#125;&#125; must: 文档必须符合must中的所有条件，影响相关性得分； 1234567891011121314151617181920# 使用must进行查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;, &#123; &quot;match&quot;:&#123; &quot;job&quot;:&quot;specialist&quot; &#125; &#125; ] &#125; &#125;&#125; must_not: 文档必须排除must_not中的所有条件； 12345678910111213141516171819202122# 使用must_not进行查询GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match&quot;:&#123; &quot;job&quot;:&quot;java&quot; &#125; &#125; ], &quot;must_not&quot;:[ &#123; &quot;match&quot;:&#123; &quot;job&quot;:&quot;ruby&quot; &#125; &#125; ] &#125; &#125;&#125; should: 文档可以符合should中的条件，影响相关性得分，分为两种情况：同时配合minimum_should_match控制满足调价你的个数&#x2F;百分比。 bool查询中只有should，不包含must的情况 bool查询中既有should，又包含must的情况，文档不必满足should中的条件，但是如果满足的话则会增加相关性得分。 123456789101112131415161718192021222324252627282930313233343536373839# bool查询中只有should的情况GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;should&quot;:[ &#123; &quot;term&quot;:&#123;&quot;job&quot;:&quot;java&quot;&#125; # 条件1 &#125;, &#123; &quot;term&quot;:&#123;&quot;job&quot;:&quot;ruby&quot;&#125; # 条件3 &#125; &#123; &quot;term&quot;:&#123;&quot;job&quot;:&quot;specialist&quot;&#125; # 条件3 &#125; ], &quot;minimum_should_match&quot;:2 # 至少需要满足两个条件 &#125; &#125;&#125;# # bool查询中同时包含should和mustGET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;should&quot;:[ # 同时包含should &#123; &quot;term&quot;:&#123;&quot;job&quot;:&quot;ruby&quot;&#125; &#125; ], &quot;must&quot;:[ # 同时包含must &#123; &quot;term&quot;:&#123;&quot;usernmae&quot;:&quot;alfred&quot;&#125; &#125; ] &#125; &#125;&#125; 当一个查询语句位于query或filter上下文的时候，ES的执行结果也不同。 - - - query 查找和查询语句最匹配的文档，并对所有文档计算相关性得分 querybool中的：must&#x2F;should filter 查找和查询语句最匹配的文档 bool中的：filter&#x2F;must_notconstant_score中的：filter 12345678910111213141516171819202122232425262728# query和filter上下文GET my_index/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ # query上下文 &#123; &quot;term&quot;:&#123;&quot;title&quot;:&quot;Search&quot;&#125; &#125;, &#123; &quot;term&quot;:&#123;&quot;content&quot;:&quot;ElasticSearch&quot;&#125; &#125; ], &quot;filter&quot;:[ # filter上下文 &#123; &quot;term&quot;:&#123;&quot;status&quot;:&quot;published&quot;&#125; &#125;, &#123; &quot;range&quot;:&#123; &quot;publish_date&quot;:&#123; &quot;gte&quot;:&quot;2015-01-01&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; 4.2.3.3 count APIcount API: 获取符合条件的文档书，使用endpoint：_count。 123456789# 使用_count获取符合条件的文档数GET my_index/_count # 关键词&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;username&quot;:&quot;alfred&quot; &#125; &#125;&#125; 4.2.3.4 Source FilteringSource Filtering: 过滤返回结果中的_source中的字段，主要由以下两种方式： GET my_index&#x2F;_search?_source&#x3D;username #url参数 使用Request Body Search： 123456789101112131415161718# 不返回_sourceGET my_index/_search&#123; &quot;_source&quot;:false&#125;# 返回_source部分字段GET my_index/_search&#123; &quot;_source&quot;:[&quot;username&quot;,&quot;age&quot;]&#125;# 通配符匹配返回_source部分字段GET my_index/_search&#123; &quot;_source&quot;:&#123; &quot;includes&quot;:&quot;*I*&quot;, &quot;encludes&quot;:&quot;birth&quot; &#125;&#125;","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://chaooo.github.io/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://chaooo.github.io/tags/Kibana/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"http://chaooo.github.io/tags/ElasticStack/"},{"name":"LogStash","slug":"LogStash","permalink":"http://chaooo.github.io/tags/LogStash/"}]},{"title":"「数据库」MongoDB学习笔记","date":"2018-10-17T10:20:16.000Z","path":"2018/10/17/database-mongodb.html","text":"1. 安装在mongodb官网下载对应自己电脑系统的安装包，地址为： http://www.mongodb.org/downloads。 1、以Windows64bit为例，下载.msi文件双击安装。2、安装过程中，点击 “Custom(自定义)” 按钮来设置安装目录(D:\\MongoDB\\bin)。3、创建数据目录(D:\\MongoDB\\data\\db),MongoDB默认数据目录\\data\\db。4、连接数据库(命令行win+r cmd,到D:\\MongoDB\\bin目录下，执行代码：mongod –dbpath D:\\MongoDB\\data\\db) 123D:cd D:\\MongoDB\\binmongod --dbpath D:\\MongoDB\\data\\db 5、启动 MongoDB JavaScript 工具(D:\\MongoDB\\bin目录下,打开mongo,会看到：) 12MongoDB shell version: 3.2.4 //mongodb版本connecting to: test //默认shell连接的是本机localhost 上面的test库 此时就可以操作数据库了。 2. 将MongoDB服务器作为Windows服务运行1、在D:\\MongoDB目录下创建mongodb.config,写入如下： 12345## 数据库文件目录dbpath=D:/MongoDB/data## 日志目录logpath=D:/MongoDB/log/mongo.logdiaglog=3 2、常规命令(cmd管理员): 123D:cd D:\\MongoDB\\binmongod --config D:\\MongoDB\\mongodb.config 3、若常规方式失败，则sc方式(cmd管理员)： 123D:cd D:\\MongoDB\\binsc create mongodb binPath= &quot;D:\\MongoDB\\bin\\mongod.exe --service --config=D:\\mongoDB\\mongodb.config&quot; 访问地址：localhost:27017测试是否启动成功 3. CRUD操作(Creat,Read,Update,Delete)3.1 MongoDB基础1、document(文档) MongoDB把所有数据存放在类似于JSON数据结构的文档内： 1&#123; &quot;item&quot;: &quot;pencil&quot;, &quot;qty&quot;: 500, &quot;type&quot;: &quot;no.2&quot; &#125; 2、collection(集合) 集合是一组相关的文档，MongoDB存储所有的文档在集合里,他们拥有一套共享的通用索引。 123&#123; &quot;item&quot;: &quot;pencil&quot;, &quot;qty&quot;: 500, &quot;type&quot;: &quot;no.1&quot; &#125;&#123; &quot;item&quot;: &quot;pencil2&quot;, &quot;qty&quot;: 550, &quot;type&quot;: &quot;no.2&quot; &#125;&#123; &quot;item&quot;: &quot;pencil3&quot;, &quot;qty&quot;: 800, &quot;type&quot;: &quot;no.3&quot; &#125; 3、database(数据库) MongoDB的默认数据库为”db”，该数据库存储在data目录中。一个mongodb中可以建立多个数据库。 3.2 数据库操作：连接及运行mongoDB“show dbs“命令可以显示所有的数据的列表“db“命令可以显示当前数据库对象或集合“use“命令可以连接到一个指定的数据库数据库也通过名字来标识。数据库名可以是满足以下条件的任意UTF-8字符串。 1.不能是空字符串（””)。 2.不得含有’ ‘（空格)、.、$、&#x2F;、\\和\\0 (空宇符)。 3.应全部小写。 4.最多64字节。 1、创建数据库：use Database_Name 1use test ##创建名为test的数据库 2、删除当前数据库： 1db.dropDatabase() 3.3 文档操作（以 Collection_Name &#x3D; col 为例）3.3.1插入：12345678910db.col.insert(Document) ##插入一条或多组数据db.col.insertOne(Document) ##插入一条数据db.col.insertMany(Document) ##插入多条数据##例如： db.col.insertOne(&#123; &quot;item&quot;: &quot;pencil&quot;, &quot;type&quot;: &quot;no.1&quot; &#125;) db.col.insertMany([ &#123; &quot;item&quot;: &quot;dog&quot;, &quot;type&quot;: &quot;no.2&quot; &#125;, &#123; &quot;item&quot;: &quot;apple&quot;, &quot;type&quot;: &quot;no.3&quot; &#125;, &#123; &quot;item&quot;: &quot;orange&quot;, &quot;type&quot;: &quot;no.4&quot; &#125; ]) 3.3.2 删除：123456789db.col.remove(&#123;&#125;) ##删除所有数据db.col.remove(query &lt;,options&gt;) # query: 查询条件(数据索引或名字) # ptions:两个可选参数 # &#123;justOne: &lt;boolean&gt;, //默认false，删除所有匹配到的。 # writeConcern: &lt;document&gt;//抛出异常的级别。 # &#125;db.col.deleteOne(query &lt;,options&gt;) ##同上，无justOne参数，只删除第一条db.col.deleteMany(query &lt;,options&gt;) ##同上，无justOne参数，只删除多条 3.3.3 更新：123456789101112131415161718db.col.update(query, update &lt;,options&gt;) # query: 查询条件(数据索引或名字) # update: 更新的内容，语法：&#123;$set:query&#125; # options:三个可选参数 # &#123;upsert: &lt;boolean&gt;, //如果不存在update的记录，是否插入新数据，默认:false。 # multi: &lt;boolean&gt;, //只更新找到的第一条记录，默认是false,如果为true,多条记录全部更新。 # writeConcern: &lt;document&gt;//#抛出异常的级别。 # &#125;##例如： db.col.update( &#123;&quot;type&quot;: &quot;no.1&quot;&#125;, &#123;$set: &#123;&quot;item&quot;: &quot;human&quot;&#125;&#125;, &#123;upsert: true, multi: true&#125; )db.col.updateOne() ##同上，无multi参数，只更新第一条db.col.updateMany() ##同上，无multi参数db.col.replaceOne() ##同updateOnedb.col.save(document &lt;,writeConcern&gt;) ##通过传入的文档整个替换 insert 与 save的区别如果插入的数据的_id相同,save将会更新该文档,而insert将会报错 update常用操作符1234567891011$set ##当文档中包含该字段的时候,更新该字段,如果该文档中没有该字段,则为本文档添加一个字段.$unset ##删除文档中的一个字段.$rename ##重命名某个列$inc ##增长某个列$setOnInsert ##当upsert为true时,并且发生了insert操作时,可以补充的字段$push ##将一个数字存入一个数组,分为三种情况,如果该字段存在,则直接将数字存入数组.如果该字段不存在,创建字段并且将数字插入该数组.如果更新的字段不是数组,会报错的.$pushAll ##将多个数值一次存入数组.上面的push只能一个一个的存入$addToSet ##与$push功能相同将一个数字存入数组,不同的是如果数组中有这个数字,将不会插入,只会插入新的数据,同样也会有三种情况,与$push相同.$pop ##删除数组最后一个元素$pull ##删除数组中的指定的元素,如果删除的字段不是数组,会报错$pullAll ##删除数组中的多个值,跟pushAll与push的关系类似. 3.4 查询12345db.col.find(&#123;&#125;) ##查询所有文档db.col.find().pretty() ##以易读的方式来读取数据db.collection.find(query, projection) # query：查询条件(数据索引或名字) # projection：可选。指定返回的字段。 3.4.1 深入查询表达式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647db.col.find()##查询所有db.col.find(&#123;filed: value&#125;) ##等值查询db.col.find(&#123;filed: &#123;$ne: value&#125;&#125;) ##不等于 $nedb.col.find(&#123;filed: &#123;$nin: [value1, value2, ...]&#125;&#125;) ##不能包含给定的值 $nindb.col.find(&#123;filed: &#123;$all: [value1, value2, ...]&#125;&#125;) ##必须包含所有给定的值 $alldb.col.find(&#123;filed: &#123;$in: [value1, value2, ...]&#125;&#125;) ##只要包含一个或多个给定的值 $indb.col.find(&#123;filed: &#123;$exists:1&#125;&#125;) ##存在filed字段的db.col.find(&#123;filed: &#123;$exists:0&#125;&#125;) ##不存在filed字段的db.col.find(&#123;filed: &#123;$mod:[3,1]&#125;&#125;) ##模三余一，$mod(取模操作)db.col.find(&#123;$or: [&#123;filed1: vulue1&#125;, &#123;filed2: vulue2&#125;]&#125;) ##或 $ordb.col.find(&#123;$nor: [&#123;filed1: vulue1&#125;, &#123;filed2: vulue2&#125;]&#125;)##排除 $nordb.col.find(&#123;filed: &#123;$size: 3&#125;&#125;) ##返回值得数组是给定的长度(3) $sizedb.col.find(&#123;$where: function()&#123;return ...&#125;&#125;) ##回调，隐式迭代，符合条件才返回db.col.find(&#123;$where: &#x27;...&#x27;&#125;&#125;) ##同上db.col.find(&#123;age: &#123;$lt: 5&#125;&#125;).limit(3) ##查询age的值小于5，限制3条 #范围查询： # $lt （小于） # $gt （大于） # $lte （小于等于） # $gte （大于等于） # limit（限制显示）db.col.find().skip(2).limit(3) ##跳过前两个文档查询后面三个 # skip(num):表示跳过前面num个文档db.col.find().sort(&#123;age: 1&#125;) ##查询后以age升序排列显示 # sort():排序，这里 1 代表升序, -1 代表降序.db.col.find(&#123;filed: /user.*/i&#125;) ##正则，查询filed以user开头不区分大小写（正则效率低）db.col.find(&#123;filed: &#123;$type: 1&#125;&#125;) ##查找filed为双精度的文档 # 根据数据类型查询 $type # |类型 |编号| # |双精度 |1 | # |字符串 |2 | # |对象 |3 | # |数组 |4 | # |二进制数据 |5 | # |对象ID |7 | # |布尔值 |8 | # |日期 |9 | # |空 |10 | # |正则表达式 |11 | # |JavaScript |13 | # |符号 |14 | # |JavaScript(带范围)|15 | # |32位整数 |16 | # |时间戳 |17 | # |64位整数 |18 | # |最小键 |255 | # |最大键 |127 | 3.4.2 group分组查询group做的聚合有些复杂。先选定分组所依据的键，此后MongoDB就会将集合依据选定键值的不同分成若干组。然后可以通过聚合每一组内的文档，产生一个结果文档。 12345group(&#123; key:&#123;字段:1&#125;, initial:&#123;变量:初始值&#125;, $reduce:function(doc,prev)&#123;函数代码&#125;&#125;) 其中key下的字段代表,需要按哪个字段分组.initial下的变量表示这一个分组中会使用的变量,并且给一个初始值.可以在后面的$reduce函数中使用.$reduce的两个参数,分别代表当前的文档和上个文档执行完函数后的结果. 栗子：如下我们按年龄分组,同级不同年龄的用户的多少: 123456789101112131415161718192021222324252627282930313233db.user.find() &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 2 &#125;db.user.group(&#123; key:&#123;age:1&#125;, initial:&#123;count:0&#125;, $reduce:function(doc,prev)&#123; prev.count++ &#125;&#125;); [ &#123;&quot;age&quot;: 0, &quot;count&quot;: 1&#125;, &#123;&quot;age&quot;: 1, &quot;count&quot;: 3&#125;, &#123;&quot;age&quot;: 2, &quot;count&quot;: 2&#125; ]db.user.group(&#123; key:&#123;age:1&#125;, initial:&#123;users:[]&#125;, reduce:function(doc,prev)&#123; prev.users.push(doc.name) &#125;&#125;); [ &#123;&quot;age&quot;: 0, &quot;users&quot;: [&quot;user0&quot;]&#125;, &#123;&quot;age&quot;: 1, &quot;users&quot;: [&quot;user1&quot;, &quot;user3&quot;, &quot;user4&quot;]&#125;, &#123;&quot;age&quot;: 2, &quot;users&quot;: [&quot;user2&quot;, &quot;user5&quot;]&#125; ] 另外本函数还有两个可选参数 condition 和 finalizecondition就是分组的条件筛选类似mysql中的having 123456789101112db.user.group(&#123; key:&#123;age:1&#125;, initial:&#123;users:[]&#125;, $reduce:function(doc,prev)&#123; prev.users.push(doc.name) &#125;, condition:&#123;age:&#123;$gt:0&#125;&#125;&#125;) ##筛选出age大于0的:[ &#123;&quot;age&quot;: 1, &quot;users&quot;: [&quot;user1&quot;, &quot;user3&quot;, &quot;user4&quot;]&#125;, &#123;&quot;age&quot;: 2, &quot;users&quot;: [&quot;user2&quot;, &quot;user5&quot;]&#125;] 3.4.3 count统计12db.goods.count() ##统计该集合总数db.goods.count(&#123;cat_id: 3&#125;) ##统计cat_id=3的总数 3.4.4 distinct排重12345678910db.user.find() &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 1 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 2 &#125; db.user.distinct(&quot;age&quot;) ## 特殊,传入的参数直接是字符串,而不是对象; [0, 1, 2] 3.4.5 子文档查询$elemMatchelemMatch投影操作符将限制查询返回的数组字段的内容只包含匹配elemMatch条件的数组元素。注意：(1)数组中元素是内嵌文档。(2)如果多个元素匹配$elemMatch条件，操作符返回数组中第一个匹配条件的元素。假设集合school有如下数据： 1234567891011121314151617181920212223242526272829303132&#123; _id: 1, zipcode: 63109, students: [ &#123; name: &quot;john&quot;, school: 102, age: 10 &#125;, &#123; name: &quot;jess&quot;, school: 102, age: 11 &#125;, &#123; name: &quot;jeff&quot;, school: 108, age: 15 &#125; ]&#125;&#123; _id: 2, zipcode: 63110, students: [ &#123; name: &quot;ajax&quot;, school: 100, age: 7 &#125;, &#123; name: &quot;achilles&quot;, school: 100, age: 8 &#125;, ]&#125;&#123; _id: 3, zipcode: 63109, students: [ &#123; name: &quot;ajax&quot;, school: 100, age: 7 &#125;, &#123; name: &quot;achilles&quot;, school: 100, age: 8 &#125;, ]&#125;&#123; _id: 4, zipcode: 63109, students: [ &#123; name: &quot;barney&quot;, school: 102, age: 7 &#125;, ]&#125; 下面的操作将查询邮政编码键值是63109的所有文档。 $elemMatch操作符将返回 students数组中的第一个匹配条件（内嵌文档的school键且值为102）的元素。 12345db.school.find(&#123;zipcode: 63109 &#125;,&#123; students: &#123; $elemMatch: &#123; school: 102 &#125; &#125; &#125; );&#123;&quot;_id&quot;: 1, &quot;students&quot;: [&#123;&quot;name&quot;:&quot;john&quot;, &quot;school&quot;:102, &quot;age&quot;:10&#125;]&#125;&#123;&quot;_id&quot;: 3&#125;&#123;&quot;_id&quot;: 4, &quot;students&quot;: [&#123;&quot;name&quot;:&quot;barney&quot;, &quot;school&quot;:102, &quot;age&quot;:7&#125;]&#125; 查询结果说明：_id为1的文档，students数组包含多个元素中存在school键且值为102的元素，$elemMatch只返回一个匹配条件的元素。_id为3的文档，因为students数组中元素无法匹配$elemMatch条件，所以查询结果不包含”students”字段。 $elemMatch可以指定多个字段的限定条件，下面的操作将查询邮政编码键值是63109的所有文档。 $elemMatch操作符将返回 students数组中的第一个匹配条件（内嵌文档的school键且值为102且age键值大于10）的元素。 12345db.school.find( &#123; zipcode: 63109 &#125;,&#123; students: &#123; $elemMatch: &#123; school: 102, age: &#123; $gt: 10&#125; &#125; &#125; &#125; ); &#123;&quot;_id&quot;: 1, &quot;students&quot;: [&#123;&quot;name&quot;:&quot;jess&quot;, &quot;school&quot;:102, &quot;age&quot;:11&#125;]&#125; &#123;&quot;_id&quot;: 3&#125; &#123;&quot;_id&quot;: 4&#125;","tags":[{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"},{"name":"mongodb","slug":"mongodb","permalink":"http://chaooo.github.io/tags/mongodb/"}]},{"title":"「数据库」嵌入式SQL语言","date":"2018-09-01T06:32:28.000Z","path":"2018/09/01/database-sql.html","text":"概述 交互式SQL语言有很多优点：记录集合操作、非过程性操作、一条语句就可实现复杂查询的结果， 然而，交互式SQL本身也有很多局限： 从使用者角度：专业人员可熟练写出SQL语句，但大部分的普通用户并非可以 从SQL本身角度：特别复杂的检索结果难以用一条交互式SQL语句完成，此时需要结合高级语言中经常出现的顺序、分支和循环结构来帮助处理 因此，高级语言+SQL语言： 既继承高级语言的过程控制性 又结合SQL语言的复杂结果集操作的非过程性 同时又为数据库操作者提供安全可靠的操作方式：通过应用程序进行操作 嵌入式SQL语言 将SQL语言嵌入到某一种高级语言中使用 这种高级语言，如C&#x2F;C++, Java, PowerBuilder等，又称宿主语言(Host Language) 嵌入在宿主语言中的SQL与前面介绍的交互式SQL有一些不同的操作方式 1. 变量声明与数据库连接 以宿主语言C语言为例，对比交互式SQL语言与嵌入式SQL语言 交互式SQL:select Sname, Sage from Student where Sname=&#39;张三&#39;; 嵌入式SQL:exec sql select Sname, Sage into :vSname, :vSage from Student where Sname=&#39;张三&#39;; 典型特点 exec sql引导SQL语句: 提供给C编译器，以便对SQL语句预编译成C编译器可识别的语句 增加一 into子句: 该子句用于指出接收SQL语句检索结果的程序变量 由冒号引导的程序变量,如: ‘:vSname’, ‘:vSage’ 1.1 变量的声明与使用 在嵌入式SQL语句中可以出现宿主语言语句所使用的变量，这些变量需要特殊的声明：1234exec sql begin declare section; char vSname[10], specName[10]=&quot;张三&quot;; int vSage;exec sql end declare section; 变量声明和赋值中，要注意： 宿主程序的字符串变量长度应比字符型字段的长度多1个。因宿主程序的字符串尾部多一个终止符为’\\0’，而程序中用双引号来描述。 宿主程序变量类型与数据库字段类型之间有些是有差异的, 有些DBMS可支持自动转换，有些不能。 声明的变量，可以在宿主程序中赋值，然后传递给SQL语句的where等子句中，以使SQL语句能够按照指定的要求(可变化的)进行检索。 嵌入式比交互式SQL语句灵活了一些：只需改一下变量值，SQL语句便可反复使用，以检索出不同结果。 示例：12345678exec sql begin declare section; char vSname[10], specName[10]=&quot;张三&quot;; int vSage;exec sql end declare section;//用户可在此处基于键盘输入给specName赋值exec sql select Sname, Sage into :vSname, :vSage from Student where Sname = :specName;//比较相应的交互式SQL语句：select Sname, Sage from Student where Sname = &#x27;张三&#x27;; 1.2 程序与数据库的连接和断开1.2.1 数据库的连接connect在嵌入式SQL程序执行之前，首先要与数据库进行连接, 不同DBMS，具体连接语句的语法略有差异 SQL标准中建议的连接语法为： exec sql connect to target-server as connect-name user user-name; 或 exec sql connect to default; Oracle中数据库连接: exec sql connect :user_name identified by :user_pwd; DB2 UDB中数据库连接: exec sql connect to mydb user :user_name using :user_pwd; 1.2.1 数据库的断开disconnect在嵌入式SQL程序执行之后，需要与数据库断开连接 SQL标准中建议的断开连接的语法为： exec sql disconnect connect-name; 或 exec sql disconnect current; Oracle中断开连接: exec sql commit release; 或 exec sql rollback release; DB2 UDB中断开连接: exec sql connect reset; exec sql disconnect current; 1.3 SQL执行的提交与撤消SQL语句在执行过程中，必须有提交和撤消语句才能确认其操作结果 SQL执行的提交： exec sql commit work; SQL执行的撤消： exec sql rollback work; 为此，很多DBMS都设计了捆绑提交&#x2F;撤消与断开连接在一起的语句,以保证在断开连接之前使用户确认提交或撤消先前的工作，例如Oracle中： exec sql commit release; 或 exec sql rollback release; 2. 事务Transaction 从应用程序员角度：事务是一个存取或改变数据库内容的程序的一次执行，或者说一条或多条SQL语句的一次执行被看作一个事务 从微观角度，或者从DBMS角度：事务是数据库管理系统提供的控制数据操作的一种手段，通过这一手段，应用程序员将一系列的数据库操作组合在一起作为一个整体进行操作和控制，以便数据库管理系统能够提供一致性状态转换的保证。 简单来说：事务是作为单个逻辑工作单元执行的一系列操作；多个操作作为一个整体向系统提交，要么都执行，要么都不执行；事务是一个不可分割的工作逻辑单元。 2.1 事务的特性: ACID 原子性Atomicity : DBMS能够保证事务的一组更新操作是原子不可分的，即对DB而言，要么都执行，要么都不执行 一致性Consistency: DBMS保证事务的操作状态是正确的，符合一致性的操作规则，它是进一步由隔离性来保证的 隔离性Isolation: DBMS保证并发执行的多个事务之间互相不受影响。例如两个事务T1和T2, 即使并发执行，也相当于或者先执行了T1,再执行T2;或者先执行了T2, 再执行T1。 持久性Durability: DBMS保证已提交事务的影响是持久的，被撤销事务的影响是可恢复的。 换句话说：具有ACID特性的若干数据库基本操作的组合体被称为事务。 3. 数据集与游标读取单行结果处理与多行结果处理的差异：Into子句与游标(Cursor) 检索单行结果，可将结果直接传送到宿主程序的变量中(Into) 示例：exec sql select Sname,Sage into :vSname,:vSage from Student where Sname = :specName; 检索多行结果，则需使用游标(Cursor) 游标是指向某检索记录集的指针 通过这个指针的移动，每次读一行，处理一行，再读一行… , 直至处理完毕 读一行操作是通过Fetch…into语句实现的：每一次Fetch, 都是先向下移动指针，然后再读取 记录集有结束标识EOF, 用来标记后面已没有记录了 游标(Cursor)的使用需要先定义、再打开(执行)、接着一条接一条处理，最后再关闭 游标可以定义一次，多次打开(多次执行)，多次关闭 3.1 游标的使用方法 Cursor的定义：declare cursor 123456789EXEC SQL DECLARE cursor_name CURSOR FOR Subquery [ORDER BY result_column [ASC | DESC][, result_column …] [FOR [ READ ONLY | UPDATE [OF columnname [, columnname…]]]];//示例:exec sql declare cur_student cursor for select Sno, Sname, Sclass from Student where Sclass= :vClass order by Sno for read only ; Cursor的打开和关闭：open cursor &#x2F;&#x2F;close cursor EXEC SQL OPEN cursor_name; EXEC SQL CLOSE cursor_name; Cursor的数据读取：Fetch 1234567891011EXEC SQL FETCH cursor_name INTO host-variable , [host-variable, …];//示例:exec sql declare cur_student cursor for select Sno, Sname, Sclass from Student where Sclass= :vClass order by Sno for read only ;exec sql open cur_student;…exec sql fetch cur_student into :vSno, :vSname, :vSage…exec sql close cur_student; 3.2 可滚动游标 ODBC支持的可滚动Cursor 标准的游标始终是自开始向结束方向移动的，每fetch一次，向结束方向移动一次；一条记录只能被访问一次；再次访问该记录只能关闭游标后重新打开 ODBC( Open DataBase Connectivity)是一种跨DBMS的DB操作平台，它在应用程序与实际的DBMS之间提供了一种通用接口 许多实际的DBMS并不支持可滚动游标，但通过ODBC可以使用该功能 可滚动游标是可使游标指针在记录集之间灵活移动、使每条记录可以反复被访问的一种游标 可滚动游标移动时需判断是否到结束位置，或到起始位置 可通过判断是否到EOF位置(最后一条记录的后面), 或BOF位置(起始记录的前面) 如果不需区分，可通过whenever not found语句设置来检测 123456789EXEC SQL DECLARE cursor_name [INSENSITIVE] [SCROLL] CURSOR[WITH HOLD] FOR Subquery[ORDER BY result_column [ASC | DESC][, result_column …][FOR READ ONLY | FOR UPDATE OF columnname [,columnname ]…];EXEC SQL FETCH[ NEXT | PRIOR | FIRST | LAST| [ABSOLUTE | RELATIVE] value_spec ]FROM cursor_name INTO host-variable [, host-variable …]; NEXT向结束方向移动一条； PRIOR向开始方向移动一条； FIRST回到第一条； LAST移动到最后一条； ABSOLUT value_spec定向检索指定位置的行, value_spec由1至当前记录集最大值； RELATIVE value_spec相对当前记录向前或向后移动，value_spec为正数向结束方向移动，为负数向开始方向移动 3.3 数据库记录的增删改 数据库记录的删除 一种是查找删除(与交互式DELETE语句相同)，一种是定位删除 1234567891011121314EXEC SQL DELETE FROM tablename [corr_name] WHERE search_condition | WHERE CURRENT OF cursor_name;//示例：查找删除exec sql delete from customers c where c.city = ‘Harbin’ and not exists ( select * from orders o where o.cid = c.cid);//示例：定位删除exec sql declare delcust cursor for select cid from customers c where c.city =‘harbin’ and not exists ( select * from orders o where o.cid = c.cid) for update of cid;exec sql open delcustWhile (TRUE) &#123; exec sql fetch delcust into :cust_id; exec sql delete from customers where current of delcust ; &#125; 数据库记录的更新 一种是查找更新(与交互式Update语句相同)，一种是定位更新 1234567891011121314EXEC SQL UPDATE tablename [corr_name] SET columnname = expr [, columnname = expr …] [ WHERE search_condition ] | WHERE CURRENT OF cursor_name;//示例：查找更新exec sql update student s set sclass = ‘035102’ where s.sclass = ‘034101’// 示例：定位更新exec sql declare stud cursor for select * from student s where s.sclass =‘034101’ for update of sclass;exec sql open studWhile (TRUE) &#123; exec sql fetch stud into :vSno, :vSname, :vSclass; exec sql update student set sclass = ‘035102’ where current of stud ; &#125; 数据库记录的插入 只有一种类型的插入语句 12345678EXEC SQL INSERT INTO tablename [ (columnname [,columnname, …] )] [ VALUES (expr [ , expr , …] ) | subqurey ] ;//示例：插入语句exec sql insert into student ( sno, sname, sclass) values (‘03510128’, ‘张三’, ‘035101’) ;//示例：插入语句exec sql insert into masterstudent ( sno, sname, sclass) select sno, sname, sclass from student; 4. 状态捕获及错误处理机制4.1 基本机制 状态，是嵌入式SQL语句的执行状态，尤其指一些出错状态；有时程序需要知道这些状态并对这些状态进行处理 嵌入式 SQL程序中，状态捕获及处理有三部分构成 设置SQL通信区: 一般在嵌入式SQL程序的开始处便设置 exec sql include sqlca; 设置状态捕获语句: 在嵌入式SQL程序的任何位置都可设置；可多次设置；但有作用域 exec sql whenever sqlerror goto report_error; 状态处理语句: 某一段程序以应对SQL操作的某种状态 report_error: exec sql rollback; SQL通信区: SQLCA SQLCA是一个已被声明过的具C语言的结构形式的内存信息区，其中的成员变量用来记录SQL语句执行的状态，便于宿主程序读取与处理 SQLCA是DBMS(执行SQL语句)与宿主程序之间交流的桥梁之一 状态捕获语句: exec sql whenever condition action; Whenever语句的作用是设置一个“条件陷阱”, 该条语句会对其后面的所有由Exec SQL语句所引起的对数据库系统的调用自动检查它是否满足条件(由condition指出). SQLERROR: 检测是否有SQL语句出错。其具体意义依赖于特定的DBMS NOT FOUND: 执行某一SQL语句后，没有相应的结果记录出现 SQLWARNING: 不是错误，但应引起注意的条件 如果满足condition, 则要采取一些动作(由action指出) CONTINUE: 忽略条件或错误，继续执行 GOTO 标号: 转移到标号所指示的语句，去进行相应的处理 STOP: 终止程序运行、撤消当前的工作、断开数据库的连接 DO函数或 CALL函数: 调用宿主程序的函数进行处理，函数返回后从引发该condition的Exec SQL语句之后的语句继续进行 状态捕获语句Whenever的作用范围是其后的所有Exec SQL语句，一直到程序中出现另一条相同条件的Whenever语句为止，后面的将覆盖前面的。 1234567891011int main() &#123; exec sql whenever sqlerror stop; … … goto s1 … … exec sql whenever sqlerror continue; s1: exec sql update agents set percent = percent + 1; … …&#125;//S1标号指示的语句受第二个Whenever语句约束。//注意：作用域是语句在程序中的位置，而不是控制流程(因是预编译程序处理条件陷阱) 状态捕获语句Whenever的使用容易引发无限循环 123456789101112int main() &#123; exec sql whenever sqlerror goto handle_error; exec sql create table customers(cid char(4) not null, cname varchar(13), … … ); … … handle_error: exec sql whenever sqlerror continue;// 控制是否无限循环：无，则可能；有，则不会 exec sql drop customers; exec sql disconnect; fprintf(stderr,”could not create customers table\\n”); return -1;&#125; 4.2 状态信息典型DBMS系统记录状态信息的三种方法 状态记录: sqlcode: 典型DBMS都提供一个sqlcode变量来记录其执行sql语句的状态，但不同DBMS定义的sqlcode值所代表的状态意义可能是不同的。 sqlcode&#x3D;&#x3D; 0, successful call; sqlcode &lt; 0, error, e.g., from connect, database does not exist , –16; sqlcode &gt; 0, warning, e.g., no rows retrieved from fetch sqlca.sqlcode: 支持SQLCA的产品一般要在SQLCA中填写sqlcode来记录上述信息; 除此而外，sqlca还有其他状态信息的记录 sqlstate: 有些DBMS提供的记录状态信息的变量是sqlstate或sqlca.sqlstate 当我们不需明确知道错误类型，而只需知道发生错误与否，则我们只要使用前述的状态捕获语句即可，而无需关心状态记录变量(隐式状态处理) 但我们程序中如要自行处理不同状态信息时，则需要知道以上信息，但也需知道正确的操作方法(显式状态处理) 4.3 程序自身进行错误信息的处理正确的显式状态处理示例: 1234567891011exec sql begin declar section; char SQLSTATE[6];exec sql end declare section;exec sql whenever sqlerror goto handle_error;… …exec sql whenever sqlerror continue;exec sql create table custs (cid char(4) not null, cname varchar(13), … … );if (strcmp(SQLSTATE, “82100”)==0) &lt;处理82100错误的程序&gt; … … 上述的if 语句是能被执行的，因为create table发生错误时是继续向下执行的。 5. 动态SQL5.1 动态SQL的概念动态SQL是相对于静态SQL而言的 静态SQL特点：SQL语句在程序中已经按要求写好，只需要把一些参数通过变量(高级语言程序语句中不带冒号) 传送给嵌入式SQL语句即可(嵌入式SQL语句中带冒号) 动态SQL特点：SQL语句可以在程序中动态构造，形成一个字符串，然后再交给DBMS执行，交给DBMS执行时仍旧可以传递变量 5.2 动态SQL的两种执行方式如SQL语句已经被构造在host-variable字符串变量中, 则： 立即执行语句: 运行时编译并执行 EXEC SQL EXECUTE IMMEDIATE :host-variable; Prepare-Execute-Using语句: PREPARE语句先编译，编译后的SQL语句允许动态参数，EXECUTE语句执行，用USING语句将动态参数值传送给编译好的SQL语句 EXEC SQL PREPARE sql_temp FROM :host-variable; EXEC SQL EXECUTE sql_temp USING :cond-variable 6. 数据字典与SQLDA6.1 数据字典的概念数据字典(Data dictionary)，又称为系统目录(System Catalogs) 是系统维护的一些表或视图的集合，这些表或视图存储了数据库中各类对象的定义信息，这些对象包括用Create语句定义的表、列、索引、视图、权限、约束等, 这些信息又称数据库的元数据–关于数据的数据。 不同DBMS术语不一样：数据字典(Data Dictionary(Oracle))、目录表(DB2 UDB)、系统目录(INFORMIX)、系统视图(X&#x2F;Open) 不同DBMS中系统目录存储方式可能是不同的, 但会有一些信息对DBA公开。这些公开的信息, DBA可以使用一些特殊的SQL命令来检索。 6.2 数据字典的内容构成数据字典通常存储的是数据库和表的元数据，即模式本身的信息： 与关系相关的信息 关系名字 每一个关系的属性名及其类型 视图的名字及其定义 完整性约束 用户与账户信息，包括密码 统计与描述性数据：如每个关系中元组的数目 物理文件组织信息： 关系是如何存储的(顺序&#x2F;无序&#x2F;散列等) 关系的物理位置 索引相关的信息 6.3 数据字典的结构 也是存储在磁盘上的关系 专为内存高效访问设计的特定的数据结构 可能的字典数据结构 Relation_metadata &#x3D; (relation_name, number_of_attributes, storage_organization, location) Attribute_metadata &#x3D; (attribute_name, relation_name, domain_type, position, length) User_metadata &#x3D; (user_name, encrypted_password, group) Index_metadata &#x3D; (index_name, relation_name, index_type, index_attributes) View_metadata &#x3D; (view_name, definition) 6.4 X&#x2F;Open标准的系统目录 X&#x2F;Open标准中有一个目录表Info_Schem.Tables, 该表中的一行是一个已经定义的表的有关信息 Table_Schem：表的模式名(通常是表所有者的用户名) Table_Name：表名 Table_Type：&#39;Base_Table&#39;或&#39;View&#39; 可以使用SQL语句来访问这个表中的信息，比如了解已经定义了哪些表，可如下进行： Select Table_Name From Tables; 模式的含义是指某一用户所设计和使用的表、索引及其他与数据库有关的对象的集合，因此表的完整名应是：模式名.表名。这样做可允许不同用户使用相同的表名，而不混淆。 一般而言，一个用户有一个模式。可以使用Create Schema语句来创建模式(用法参见相关文献)，在Create Table等语句可以使用所定义的模式名称。 6.5 Oracle的数据字典 Oracle数据字典由视图组成，分为三种不同形式，由不同的前缀标识 USER_ :用户视图，用户所拥有的对象，在用户模式中 ALL_ :扩展的用户视图，用户可访问的对象 DBA_ :DBA视图(所有用户都可访问的DBA对象的子集) Oracle数据字典中定义了三个视图USER_Tables, ALL_Tables, 和DBA_Tables供DBA和用户使用数据字典中关于表的信息 同样, Oracle数据字典中也定义了三个视图USER_TAB_Columns, ALL_TAB_Columns(Accessible_Columns), 和DBA_TAB_Columns供DBA和用户使用数据字典中关于表的列的信息 可以使用SQL语句来访问这些表中的信息： Select Column_Name From ALL_TAB_Columns Where Table_Name = ‘STUDENT’; Oracle数据字典中还定义了其他视图 TABLE_PRIVILEDGE(或ALL_TAB_GRANTS) COLUMN_PRIVILEDGE(或ALL_COL_GRANTS)可访问表的权限，列的权限 CONSTRAINT_DEFS(或ALL_CONSTRAINTS)可访问表的各种约束 可以使用下述命令获取Oracle定义的所有视图信息 Select view_name from all_views where owner = ‘SYS’ and view_name like ‘ALL_%’ or view_name like ‘USER_%’; 如果用户使用Oracle,可使用其提供的SQL*PLUS进行交互式访问 动态SQL: 表和列都已知，动态构造检索条件。 动态SQL:检索条件可动态构造，表和列也可动态构造。 6.6 SQLDA构造复杂的动态SQL需要了解数据字典及SQLDA，已获知关系模式信息 SQLDA: SQL Descriptor Area, SQL描述符区域。 SQLDA是一个内存数据结构，内可装载关系模式的定义信息，如列的数目，每一列的名字和类型等等 通过读取SQLDA信息可以进行更为复杂的动态SQL的处理 不同DBMS提供的SQLDA格式并不是一致的。 7. ODBC简介7.1 ODBC定义ODBC：Open DataBase Connection，ODBC是一种标准—不同语言的应用程序与不同数据库服务器之间通讯的标准。 一组API(应用程序接口)，支持应用程序与数据库服务器的交互 应用程序通过调用ODBC API, 实现 与数据服务器的连接 向数据库服务器发送SQL命令 一条一条的提取数据库检索结果中的元组传递给应用程序的变量 具体的DBMS提供一套驱动程序，即Driver库函数，供ODBC调用，以便实现数据库与应用程序的连接。 ODBC可以配合很多高级语言来使用，如C,C++, C#, Visual Basic, PowerBuilder等等； 7.2 通过ODBC连接数据库 ODBC应用前，需要确认具体DBMS Driver被安装到ODBC环境中 当应用程序调用ODBC API时，ODBC API会调用具体DBMS Driver库函数，DBMS Driver库函数则与数据库服务器通讯，执行相应的请求动作并返回检索结果 ODBC应用程序首先要分配一个SQL环境，再产生一个数据库连接句柄 应用程序使用SQLConnect()，打开一个数据库连接，SQLConnect()的具体参数: connection handle, 连接句柄 the server，要连接的数据库服务器 the user identifier，用户 password ，密码 SQL_NTS 类型说明前面的参数是空终止的字符串 示例12345678910111213141516int ODBCexample()&#123; RETCODE error; /* 返回状态吗 */ HENV env; /* 环境变量 */ HDBC conn; /* 连接句柄 */ SQLAllocEnv(&amp;env); SQLAllocConnect(env, &amp;conn); //分配数据库连接环境 SQLConnect(conn, &quot;aura.bell-labs.com&quot;, SQL_NTS, &quot;avi&quot;, SQL_NTS, avipasswd&quot;, SQL_NTS); //打开一个数据库连接 &#123; …. Do actual work … &#125; //与数据库通讯 SQLDisconnect(conn); SQLFreeConnect(conn); SQLFreeEnv(env); //断开连接与释放环境&#125; 7.3 通过ODBC与数据库服务器进行通讯 应用程序使用SQLExecDirect()向数据库发送SQL命令； 使用SQLFetch()获取产生的结果元组； 使用SQLBindCol()绑定C语言变量与结果中的属性 当获取一个元组时，属性值会自动地传送到相应的C语言变量中 SQLBindCol()的参数： ODBC定义的stmt变量, 查询结果中的属性位置 SQL到C的类型变换, 变量的地址. 对于类似字符数组一样的可变长度类型，应给出 •变量的最大长度 •当获取到一个元组后，实际长度的存储位置. •注: 当返回实际长度为负数，说明是一个空值。 示例123456789101112131415161718192021char branchname[80]; float balance;int lenOut1, lenOut2;HSTMT stmt;SQLAllocStmt(conn, &amp;stmt);//分配一个与指定数据库连接的新的语句句柄char * sqlquery = &quot;select branch_name, sum (balance) from account group by branch_name&quot;;error = SQLExecDirect(stmt, sqlquery, SQL_NTS);//执行查询，stmt句柄指向结果集合if (error == SQL_SUCCESS) &#123;SQLBindCol(stmt, 1, SQL_C_CHAR, branchname , 80, &amp;lenOut1);SQLBindCol(stmt, 2, SQL_C_FLOAT, &amp;balance, 0 , &amp;lenOut2);//绑定高级语言变量与stmt句柄中的属性while (SQLFetch(stmt) &gt;= SQL_SUCCESS) &#123;//提取一条记录，结果数据被存入高级语言变量中 printf (&quot; %s %g\\n&quot;, branchname, balance); &#125;&#125;SQLFreeStmt(stmt, SQL_DROP);//释放语句句柄 7.4 ODBC的其他功能 动态SQL语句的预编译-动态参数传递功能 获取元数据特性 发现数据库中的所有关系的特性 以及 发现每一个查询结果的列的名字和类型等； 默认, 每一条SQL语句都被作为一个独立的能够自动提交的事务来处理。 应用程序可以关闭一个连接的自动提交特性 SQLSetConnectOption(conn, SQL_AUTOCOMMIT, 0)&#125; 此时事务要显式地给出提交和撤销的命令 SQLTransact(conn, SQL_COMMIT) or SQLTransact(conn, SQL_ROLLBACK) 8. JDBC简介8.1 JDBC定义JDBC：Java DataBase Connection，JDBC是一组Java版的应用程序接口API，提供了Java应用程序与数据库服务器的连接和通讯能力。 JDBC API 分成两个程序包： Java.sql 核心API –J2SE(Java2标准版)的一部分。使用java.sql.DriverManager类、java.sql.Driver和java.sql.Connection接口连接到数据库 Javax.sql 可选扩展API–J2EE(Java2企业版)的一部分。包含了基于JNDI(Java Naming and Directory Interface, Java命名和目录接口)的资源，以及管理连接池、分布式事务等，使用DataSource接口连接到数据库。 8.2 JDBC的功能 java.sql.DriverManager——处理驱动的调入并且对产生新数据库连接提供支持 Java.sql.Driver——通过驱动进行数据库访问，连接到数据库的应用程序必须具备该数据库的特定驱动。 java.sql.Connection——代表对特定数据库的连接。 Try &#123;…&#125; Catch &#123;…&#125; ——异常捕获及其处理 java.sql.Statement——对特定的数据库执行SQL语句 java.sql.PreparedStatement —— 用于执行预编译的SQL语句 java.sql.CallableStatement ——用于执行对数据库内嵌过程的调用。 java.sql.ResultSet——从当前执行的SQL语句中返回结果数据。 8.3 使用JDBC API访问数据库的过程 概念性的基本过程 打开一个连接；创建“Statement”对象，并设置查询语句；使用Statement对象执行查询，发送查询给数据库服务器和返回结果给应用程序；处理错误的例外机制 具体实施过程 •传递一个Driver给DriverManager，加载数据库驱动。 Class.forName() •通过URL得到一个Connection对象, 建立数据库连接 DriverManager.getConnection(sDBUrl) DriverManager.getConnection(sDBUrl,sDBUserID,sDBPassword) •接着创建一个Statement对象(PreparedStatement或CallableStatement)，用来查询或者修改数据库。 Statement stmt=con.createStatement() •查询返回一个ResultSet。 ResultSet rs=stmt.executeQuery(sSQL) 示例： 12345678910111213141516public static void JDBCexample(String dbid, String userid, String passwd)&#123; try &#123; //错误捕获 Class.forName (&quot;oracle.jdbc.driver.OracleDriver&quot;); Connection conn = DriverManager.getConnection( &quot;jdbc:oracle:thin:@db.yale.edu:1521:univdb&quot;, userid, passwd); //加载数据库驱动，建立数据库连接 Statement stmt = conn.createStatement(); //创建一个语句对象 … Do Actual Work …. //进行SQL语句的执行与处理工作 stmt.close(); conn.close(); //关闭语句对象，关闭连接&#125; catch (SQLException sqle) &#123; System.out.println(&quot;SQLException : &quot; + sqle); &#125;&#125; 完整的示例程序 1234567891011121314151617181920212223public static void JDBCexample(String dbid, String userid, String passwd)&#123; try &#123; Class.forName (&quot;oracle.jdbc.driver.OracleDriver&quot;); Connection conn = DriverManager.getConnection( &quot;jdbc:oracle:thin:@db.yale.edu:1521:univdb&quot;, userid, passwd); Statement stmt = conn.createStatement(); try &#123; stmt.executeUpdate( &quot;insert into instructor values (‘77987&#x27;, ‘Kim&#x27;, ‘Physics’,98000)&quot;); &#125; catch (SQLException sqle) &#123; System.out.println(&quot;插入错误:&quot; + sqle); &#125; ResultSet rset = stmt.executeQuery( &quot;select dept_name, avg(salary) from instructor group by dept_name&quot;); while ( rset.next() ) &#123; System.out.println(rset.getString(“dept_name&quot;) + &quot; &quot; + rset.getFloat(2)); &#125; stmt.close(); conn.close();&#125; catch (SQLException sqle) &#123; System.out.println(&quot;SQLException:&quot; + sqle);&#125;&#125; 9. 嵌入式SQL-ODBC-JDBC三者比较执行一条SQL语句，读取执行的结果集合 嵌入式SQL的思维模式 建立数据库连接 声明一个游标 打开游标 读取一条记录(循环) 关闭游标 断开数据库连接 ODBC的思维模式 建立数据库连接 分配语句句柄 用句柄执行SQL 建立高级语言变量与句柄属性的对应 读取一条记录(循环) 释放语句句柄 断开数据库连接 JDBC的思维模式 建立数据库连接 创建语句对象 用语句对象执行SQL，并返回结果对象 从结果对象获取一条记录 提取对象的属性值传给高级语言变量(返回上一步) 释放语句对象 断开数据库连接 相同点: 都是建立数据库连接, 执行sql, 处理结果, 释放连接, 流程基本一致 不同点, 操作方式的不同: 嵌入式SQL按照语句进行操作 ODBC按照函数来进行操作 JDBC按照对象来进行操作","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"「数据库」数据库语言SQL","date":"2018-08-27T11:36:31.000Z","path":"2018/08/27/database-language.html","text":"SQL语言概述结构化查询语言(Structured Query Language)简称SQL，是一种特殊目的的编程语言，是一种数据库查询和程序设计语言，用于存取数据以及查询、更新和管理关系数据库系统。 SQL语言是集DDL、DML和DCL于一体的数据库语言 DDL语句引导词：Create(建立)，Alter(修改)，Drop(撤消) 模式的定义和删除，包括定义Database，Table，View，Index,完整性约束条件等，也包括定义对象(RowType行对象，Type列对象) DML语句引导词：Insert ，Delete, Update, Select 各种方式的更新与检索操作，如直接输入记录，从其他Table(由SubQuery建立)输入 各种复杂条件的检索，如连接查找，模糊查找，分组查找，嵌套查找等 各种聚集操作，求平均、求和、…等，分组聚集，分组过滤等 DCL语句引导词：Grant，Revoke 安全性控制：授权和撤消授权 1. 利用SQL建立数据库DDL：数据定义语言（Data Definition Language)，DDL通常由**DBA(数据库管理员)**来使用，也有经DBA授权后由应用程序员来使用 创建数据库(DB)：Create Database 数据库(Database)是若干具有相互关联关系的Table&#x2F;Relation的集合 简单语法形式：create database database 数据库名; 创建DB中的Table(定义关系模式)：Create Table Create table 表名(列名 数据类型 [Primary key|Unique] [Not null][,列名 数据类型 [Not null], …]); []表示其括起的内容可以省略，|表示其隔开的两项可取其一 Primary key: 主键约束。每个表只能创建一个主键约束 Unique: 唯一性约束(即候选键)。可以有多个唯一性约束 Not null: 非空约束。 数据类型（SQL-92标准） char(n):固定长度的字符串 varchar(n):可变长字符串 int:整数 &#x2F;&#x2F;有时不同系统也写作integer numeric(p，q):固定精度数字，小数点左边p位，右边(p-q)位 real:浮点精度数字 &#x2F;&#x2F;有时不同系统也写作float(n)，小数点后保留n位 date:日期 (如 2003-09-12) time:时间 (如 23:15:003) 注意: 现行商用DBMS的数据类型有时有些差异 2. 利用SQL简单查询DML：数据操纵语言（Data Manipulation Language)，DML通常由用户或应用程序员使用，访问经授权的数据库 向Table中添加数据(追加元组)：Insert into insert into insert into 表名[(列名[, 列名] …] values (值[,值], …); values值的排列，须与列名排列一致 若所有列名省略，则values值的排列须与该表存储中的列名排列一致 单表查询Select Select Select 列名[[,列名] …] From 表名[Where 检索条件]; 语义：从表名所给出的表中，查询出满足检索条件的元组，并按给定的列名及顺序进行投影显示。 相当于：Π[列名,...,列名](σ检索条件(表名)) Select语句中的select … , from… , where…, 等被称为子句，在以上基本形式基础上会增加许多构成要素，也会增加许多新的子句，满足不同的需求。 检索条件的书写Where 与选择运算σF(R)的条件F书写一样，只是其逻辑运算符用 and,or,not 来表示, 同时也要注意运算符的优先次序及括弧的使用。书写要点是注意对自然语言检索条件的正确理解。 Select Tname From Teacher Where Salary &gt; 2000 and D# = ’03’;&#x2F;&#x2F;检索教师表中所有工资大于2000元 并且是03系的教师姓名 排重(DISTINCT) 关系模型不允许出现重复元组。但现实DBMS，却允许出现重复元组。 在Table中要求无重复元组是通过定义Primary key或Unique来保证的; 而在检索结果中要求无重复元组, 是通过DISTINCT保留字的使用来实现的。 Select DISTINCT S# From SC Where Score &gt; 80; 排序(ORDER BY) Select语句中结果排序是通过增加order by子句实现的 order by 列名 [asc|desc] 意义为检索结果按指定列名进行排序，若后跟asc或省略，则为升序；若后跟desc, 则为降序。 模糊查询(*LIKE*) _：一个字符，%：任意长度字符。 Select Sname From Student Where Sname Like &#39;张_ _&#39;;&#x2F;&#x2F;检索名字为张某某的所有同学姓名 Select Sname From Student Where Sname Not Like &#39;张%&#39;;&#x2F;&#x2F;检索名字不姓张的所有同学姓名 3. 利用SQL多表联合查询多表联合检索可以通过连接运算来完成，而连接运算又可以通过广义笛卡尔积后再进行选择运算来实现。 检索语句: Select 列名[[,列名] …] From 表名1,表名2,… Where 检索条件; 相当于Π[列名,...,列名](σ检索条件(表名1 × 表名2 × …)) 检索条件中要包含连接条件，通过不同的连接条件可以实现等值连接、不等值连接及各种θ-连接 θ-连接之等值连接 多表连接时，如两个表的属性名相同，则需采用**表名.属性名**方式来限定该属性是属于哪一个表 Select Sname From Student, SC Where Student.S#=SC.S# and SC.C#=&#39;001&#39; Order By Score DESC;&#x2F;&#x2F;按“001”号课成绩由高到低顺序显示所有学生的姓名(二表连接) 属性重名重名处理(表别名) 连接运算涉及到重名的问题，如两个表中的属性重名，连接的两个表重名(同一表的连接)等，因此需要使用**别名**以便区分 Select 列名 as 列别名[[,列名 as 列别名] …] From 表名1 as 表别名1,表名2 as 表别名2,… Where Where 检索条件; 当定义了别名后，在检索条件中可以使用别名来限定属性 as 可以省略 θ-连接之不等值连接 Select T1.Tname as Teacher1, T2.Tname as Teacher2 From Teacher T1, Teacher T2 Where T1.Salary&gt;T2.Salary;&#x2F;&#x2F;求有薪水差额的任意两位教师 实例： Select S1.S# From SC S1, SC S2 Where S1.S# = S2.S# and S1.C#=&#39;001&#39; and S2.C#=&#39;002&#39; and S1.Score &gt; S2.Score;&#x2F;&#x2F;求“001”号课成绩比“002”号课成绩高的所有学生的学号 4. 利用SQL进行增-删-改 SQL-之更新操作 元组新增Insert：新增一个或一些元组到数据库的Table中 元组更新Update: 对某些元组中的某些属性值进行重新设定 元组删除Delete：删除某些元组 SQL-DML既能单一记录操作，也能对记录集合进行批更新操作 SQL-DML之更新操作需要利用前面介绍的子查询(Subquery)的概念，以便处理“一些”、“某些”等 SQL-之INSERT 单一元组新增命令形式：插入一条指定元组值的元组 insert into 表名 [(列名[,列名]…)] values (值 [,值]…); 批数据新增命令形式：插入子查询结果中的若干条元组。待插入的元组由子查询给出。 insert into 表名 [(列名[，列名]…)] 子查询; 示例：Insert Into St (S#,Sname) Select S#,Sname From Student Where Sname like &#39;%伟&#39;;&#x2F;&#x2F;将检索到的满足条件的同学新增到该表中 注意：当新增元组时，DBMS会检查用户定义的完整性约束条件等，如不符合完整性约束条件，则将不会执行新增动作。 SQL-之DELETE 元组删除Delete命令: 删除满足指定条件的元组 Delete From 表名 [ Where 条件表达式]; 如果Where条件省略，则删除所有的元组(清空表)。 示例：Delete From Student Where S# in ( Select S# From SC Where Score &lt; 60 Group by S# Having Count(*)&gt;= 4);&#x2F;&#x2F;删除有四门不及格课程的所有同学 SQL-之UPDATE 元组更新Update命令: 用指定要求的值更新指定表中满足指定条件的元组的指定列的值 Update 表名 Set 列名=表达式 | (子查询) [[,列名=表达式 | (子查询) ] …] [ Where 条件表达式]; 如果Where条件省略，则更新所有的元组。 示例：Update Teacher Set Salary=Salary*1.1 Where D# in (Select D# From Dept Where Dname=&#39;计算机&#39;);&#x2F;&#x2F;将所有计算机系的教师工资上调10% 5. 利用SQL语言修正与撤销数据库 修正基本表的定义 alter table tablename [add &#123;colname datatype, …&#125;] &#x2F;&#x2F;增加新列 [drop &#123;完整性约束名&#125;] &#x2F;&#x2F;删除完整性约束 [modify &#123;colname datatype, …&#125;] &#x2F;&#x2F;修改列定义 示例：Alter Table Student Drop Unique(Sname);删除学生姓名必须取唯一值的约束 示例：Alter Table Student Add Saddr char[40],PID char[18];在学生表Student上增加二列Saddr, PID SQL-DDL之撤销与修改 drop table 表名; &#x2F;&#x2F;撤消基本表 drop database 数据库名; &#x2F;&#x2F;撤消数据库 SQL-DDL之数据库指定与关闭命令 有些DBMS提供了操作多个数据库的能力，此时在进行数据库操作时需要指定待操作数据库与关闭数据库的功能。 use 数据库名; &#x2F;&#x2F;指定当前数据库 close 数据库名; &#x2F;&#x2F;关闭当前数据库 6. SQL Server介绍SQL Server 是 Microsoft提供的一款关系数据库管理系统 SQL Server 的系统数据库 Master：是SQL Server中最重要的系统数据库，存储SQL Server中的元数据。 Model：模板数据库，在创建新的数据库时，SQL Server将会复制此数据库作为新数据库的基础。 Msdb：代理服务数据库，提供一个存储空间。 Tempdb：临时数据库，为所有的临时表、临时存储过程及其他临时操作提供存储空间，断开连接时，临时表与存储过程自动被删除。 SQL Server的数据库 文件：有三种文件扩展名：.mdf、.ndf、.ldf 主数据库文件：扩展名为.mdf，是存储数据库的启动信息和部分或全部数据。一个数据库可以有多个数据库文件，但主数据库文件只有一个。 辅助数据文件：扩展名为.ndf，用于放置主数据库文件中所定义数据库的其它数据，可有多个。在数据庞大时，可以帮助存储数据。 日志文件：扩展名.ldf。每个数据库至少有一个事务日志文件。 页面：是SQL Server存储的最小单位。一页为8K或8192字节。 空间(extent)：是8个连续的页面，即64K数据，是分配数据表存储空间的一种单位 6.1 SQL Server数据库的创建-删除与维护 创建数据库 语法形式：Create Database 库名 可视化操作(查询分析器)：Database(鼠标右键) -&gt; new Database… -&gt; 填写数据库名及配置 创建数据库的过程就是为数据库设计名称、设计所占用存储空间和存 放文件位置的过程。特别是在网络数据库中，对数据库的设计显得尤为重要。如估计数据可能占用的磁盘空间有多大，日志文件及其他要占用多大空间。 创建数据库的用户自动成为数据库的拥有者。 删除数据库 语法形式：Drop Database 库名 可视化操作(查询分析器)：数据库名(鼠标右键) -&gt; Delete 对不再需要的数据库，应删除以释放空间。删除的结果将是所有数据库文件都一并被删除。 当数据库处于正在使用或正在恢复状态时，不能删除。 备份数据库 可视化操作(查询分析器)：数据库名(鼠标右键) -&gt; Tasks -&gt; Back Up… 备份就是对数据库或事务日志进行备份。SQL的备份是动态的，备份的过程还可以让用户继续改写。只有系统管理员、数据库的拥有者及数据库的备份者才有权限进行数据备份。可以通过企业管理器进行数据库备份。 完全数据库备份：完全备份数据文件和日志文件。 差异备份（增量备份）：对最近一次数据库备份以来发生的数据变化进行备份。这要在完全备份的基础上进行。特点是速度快。 事务日志备份：对数据库发生的事务进行备份。包括从上次进行事务日志备份、差异备份和数据库完全备份之后，所有已经完成的事务。能尽可能的恢复最新的数据库记录。特点是所需磁盘空间小，时间少。 数据库文件和文件组备份：用在数据库相当大的情况下。 恢复数据库 可视化操作(查询分析器)：数据库名(鼠标右键) -&gt; Tasks -&gt; Restore 数据库的恢复是指将数据库备份加载到系统中的过程。在根据数据库备份文件恢复过程中，系统将自动执行安全性检查、重建数据库结构及完成填写数据库内容。 数据库的恢复是静态的。所以在恢复前，应将需要恢复的数据库访问属性设为单用户，不要让其他用户操作。 可以通过企业管理器来完成数据库恢复。 数据库授权: 语法形式：grant 权限 on 表名 to 用户名 权限有：select,update,insert,delete,exec,dri。 对被授权的用户，要先成为该数据库的使用者，即要把用户加到数据库里,才能授权. 6.2 SQL Server数据表的创建-与增&#x2F;删&#x2F;改&#x2F;查 创建表 同一用户不能建立同一个表名的表，同一表名的表可有多个拥有者。但在使用时，需要在这些表上加上所有者的表名。 用T-SQL语句创建表，语法形式：CREATE TABLE [数据库名.所有者名.]表名 (&#123;&lt;列名 数据类型&gt;&#125; [缺省值][约束][是否为空] …) 注意：T-SQL是SQL Server软件的SQL语言，与标准版有些差异。但标准版SQL，一般情况下SQL Server软件也都支持 可视化操作(查询分析器)：数据库名 -&gt; Tables -&gt; New Table… 增加、修改表字段 语法形式：ALTER TABLE ADD | ALTER 字段名 &lt;类型&gt; 创建、删除与修改约束 约束是SQL提供自动保持数据库完整性的一种方法，共5种。 用T-SQL语句建立约束，语法形式：CONSTRAINT 约束名 约束类型 (列名) 约束名：在库中应该唯一，如不指定，系统会给出 约束类型 (5种)： primary key constraint (主键值) unique constraint (唯一性) check constraint (检查性) default constraint (默认) foreign key constraint (外部键) 列名：要约束的字段名 示例:Create Table Course ( C# char(3) , Cname char(12), Chours integer, Credit float(1), T# char(3) ) constraint pk primary key(C# )); 7. SQL语言-子查询 子查询：出现在Where子句中的Select语句被称为子查询(subquery) , 子查询返回了一个集合，可以通过与这个集合的比较来确定另一个查询集合。 三种类型的子查询：(NOT) IN-子查询；θ-Some&#x2F;θ-All子查询；(NOT) EXISTS子查询 7.1 (NOT) IN子查询 基本语法：表达式 [not] in (子查询) 语法中，表达式的最简单形式就是列名或常数。 语义：判断某一表达式的值是否在子查询的结果中。 示例： Select * From Student Where Sname in (&#39;张三&#39;, &#39;王三&#39;);&#x2F;&#x2F;列出张三、王三同学的所有信息 Select S#, Sname From Student Where S# in (Select S# From SC Where C#=&#39;001&#39;);&#x2F;&#x2F;列出选修了001号课程的学生的学号和姓名 非相关子查询：内层查询独立进行，没有涉及任何外层查询相关信息的子查询前面的子查询示例都是非相关子查询 相关子查询：内层查询需要依靠外层查询的某些参量作为限定条件才能进行的子查询 外层向内层传递的参量需要使用外层的表名或表别名来限定 示例：Select Sname From Student Stud Where S# in ( Select S# From SC Where S# = Stud.S# and C#=&#39;001&#39;);&#x2F;&#x2F;求学过001号课程的同学的姓名 注意：相关子查询只能由外层向内层传递参数，而不能反之；这也称为变量的作用域原则。 7.2 θ-Some&#x2F;θ-All子查询 基本语法：表达式 θ some (子查询) &#x2F; 表达式 θ all (子查询) 语法中，θ是比较运算符：&lt;, &gt;, &gt;=, &lt;=, =, &lt;&gt;。 语义：将表达式的值与子查询的结果进行比较： 如果表达式的值至少与子查询结果的某一个值相比较满足 关系，则表达式 θ some (子查询)的结果便为真 如果表达式的值与子查询结果的所有值相比较都满足 关系，则表达式 θ all (子查询)的结果便为真 示例： Select Tname From Teacher Where Salary &lt;= all ( Select Salary From Teacher);&#x2F;&#x2F;找出工资最低的教师姓名 Select S# From SC Where C# = “001” and Score &lt; some ( Select Score From SC Where C#=&#39;001&#39;);&#x2F;&#x2F;找出001号课成绩不是最高的所有学生的学号 在SQL标准中，也有θ-Any 谓词，但由于其语义的模糊性：any, “任一”是指所有呢？还是指某一个？不清楚，所以被θ-Some替代以求更明晰。 等价性变换需要注意 表达式 = some (子查询)和表达式 in (子查询)含义相同 表达式 &lt;&gt; some (子查询)和表达式 not in (子查询)含义不同 表达式 &lt;&gt; all (子查询)和表达式 not in (子查询)含义相同 7.3 (NOT) EXISTS子查询 基本语法：[not] Exists [not] Exists (子查询) 语义：子查询结果中有无元组存在 1234567891011121314--示例：检索选修了赵三老师主讲课程的所有同学的姓名Select DISTINCT Sname From Student Where exists ( Select * From SC, Course, Teacher Where SC.C#=Course.C# and SC. S#=Student.S# and Course.T# = Teacher.T# and Tname=&#x27;赵三&#x27;);--示例：检索学过001号教师主讲的所有课程的所有同学的姓名Select Sname From Student Where not exists //不存在 ( Select * From Course //有一门001教师主讲课程 Where Course.T# = ‘001’ and not exists //该同学没学过 ( Select * From SC Where S# = Student.S# and C# = Course.C#));--上述语句的意思：不存在有一门001号教师主讲的课程该同学没学过 8. SQL语言-结果计算与聚集计算8.1 结果计算Select-From-Where语句中，Select子句后面不仅可是列名，而且可是一些计算表达式或聚集函数，表明在投影的同时直接进行一些运算 Select Select 列名 | expr | agfunc(列名) [[, 列名 | expr | agfunc(列名) ] … ] From 表名1 [, 表名2 … ] [ Where Where 检索条件 ]; expr可以是常量、列名、或由常量、列名、特殊函数及算术运算符构成的算术运算式。特殊函数的使用需结合各自DBMS的说明书 agfunc()是一些聚集函数 1234--示例：求有差额(差额&gt;0)的任意两位教师的薪水差额Select T1.Tname as TR1, T2.Tname as TR2, T1.Salary – T2.Salary From Teacher T1, Teacher T2 Where T1.Salary &gt; T2.Salary; 8.2 聚集函数SQL提供了五个作用在简单列值集合上的内置聚集函数agfunc, 分别是：COUNT、SUM、AVG、MAX、MIN 聚合函数 支持的数据类型 描述 count() 任何类型&#x2F;* 计算结果集中的总行数 sum() Numeric 计算指定列中所有非空值的总和 avg() numeric 计算指定列中所有非空值的平均值 max() char&#x2F;numeric 返回指定列中最大值 min() char&#x2F;numeric 返回指定列中最小值 12345678--示例：求教师的工资总额Select Sum(Salary) From Teacher;--示例：求计算机系教师的工资总额Select Sum(Salary) From Teacher T, Dept Where Dept.Dname = ‘计算机’ and Dept.D# = T.D#;--示例：求数据库课程的平均成绩Select AVG(Score) From Course C, SC Where C.Cname = ‘数据库’ and C.C# = SC.C#; 9. SQL语言-分组查询与分组过滤9.1 分组查询分组：SQL可以将检索到的元组按照某一条件进行分类，具有相同条件值的元组划到一个组或一个集合中，同时处理多个组或集合的聚集运算。 分组的基本语法： 1234Select Select 列名 | expr | agfunc(列名) [[, 列名 | expr | agfunc(列名) ] … ] From 表名1 [, 表名2 … ] [ Where Where 检索条件 ] [ Group by Group by 分组条件 ] ; 分组条件可以是：列名1, 列名2, … 示例： 求每一个学生的平均成绩 Select S#, AVG(Score) From SC Group by S#; 9.2 分组过滤聚集函数是不允许用于Where子句中的：Where子句是对每一元组进行条件过滤，而不是对集合进行条件过滤 分组过滤：若要对集合(即分组)进行条件过滤，即满足条件的集合&#x2F;分组留下，不满足条件的集合&#x2F;分组剔除。 Having子句，又称分组过滤子句。需要有Group by子句支持，换句话说，没有Group by子句，便不能有Having子句。 基本语法： 1234Select Select 列名 | expr | agfunc(列名) [[, 列名 | expr | agfunc(列名) ] … ] From 表名1 [, 表名2 … ] [ Where Where 检索条件 ] [ Group by Group by 分组条件 [ Having Having 分组过滤条件] ] ; 示例：求不及格课程超过两门的同学的学号 Select S# From SC Where Score&lt;60 Group by S# Having Count(*)&gt;2; 9.3 where子句与having子句的区别 聚合函数是比较where、having 的关键。在from后面的执行顺序： where -&gt; 聚合函数(sum,min,max,avg,count) -&gt;having 列出group by来比较二者: where子句：是在分组之前使用，表示从所有数据中筛选出部分数据，以完成分组的要求，在where子句中不允许使用统计函数，没有group by子句也可以使用。 having子句：是在分组之后使用的，表示对分组统计后的数据执行再次过滤，可以使用统计函数，有group by子句之后才可以出现having子句。 注意事项 ： where 后不能跟聚合函数，因为where执行顺序大于聚合函数。 where 子句的作用是在对查询结果进行分组前，将不符合where条件的行去掉，即在分组之前过滤数据，条件中不能包含聚组函数，使用where条件显示特定的行。 having 子句的作用是筛选满足条件的组，即在分组之后过滤数据，条件中经常包含聚组函数，使用having 条件显示特定的组，也可以使用多个分组标准进行分组。 10. SQL语言实现关系代数操作SQL语言：并运算UNION, 交运算INTERSECT, 差运算EXCEPT。 基本语法形式： 子查询 &#123;Union [ALL] | Intersect [ALL] | Except [ALL] 子查询&#125; 通常情况下自动删除重复元组：不带ALL。若要保留重复的元组，则要带ALL。 假设子查询1的一个元组出现m次，子查询2的一个元组出现n次，则该元组在： 子查询1 Union ALL 子查询2 ，出现m + n次 子查询1 Intersect ALL 子查询2 ，出现min(m,n)次 子查询1 Except ALL 子查询2 ，出现max(0, m – n)次 UNION运算符是Entry-SQL92的一部分, INTERSECT、EXCEPT运算符是Full-SQL92的一部分,它们都是Core-SQL99的一部分，但有些DBMS并不支持这些运算，使用时要注意。 10.1 SQL并运算(UNION) 示例：已知两个表 Customers(Cid, Cname, City, Discnt) Agents(Aid, Aname, City, Percent) 求客户所在的或者代理商所在的城市123Select City From CustomersUNIONSelect City From Agents; 10.2 SQL交运算(INTERSECT) 示例：求既学过002号课，又学过003号课的同学学号 123Select S# From SC Where C# = ‘002’INTERSECTSelect S# From SC Where C# = ‘003’; 上述语句也可采用如下不用INTERSECT的方式来进行 Select S# From SC Where C# = ‘002’ and S# IN (Select S# From SC Where C# = ‘003’); 交运算符Intersect并没有增强SQL的表达能力，没有Intersect， SQL也可以用其他方式表达同样的查询需求。只是有了Intersect更容易表达一些，但增加了SQL语言的不唯一性。 10.3 SQL差运算(EXCEPT) 示例： 假定所有学生都有选课，求没学过002号课程的学生学号 123Select DISTINCT S# From SCEXCEPTSelect S# From SC Where C# = ‘002’; 上述语句也可采用如下不用INTERSECT的方式来进行 123Select DISTINCT S# From SC SC1 Where not exists ( Select * From SC Where C# = ‘002’ and S# = SC1.S#); 差运算符Except也没有增强SQL的表达能力，没有Except， SQL也可以用其他方式表达同样的查询需求。只是有了Except更容易表达一些，但增加了SQL语言的不唯一性。 10.4 空值的处理空值是其值不知道、不确定、不存在的值；数据库中有了空值，会影响许多方面，如影响聚集函数运算的正确性，不能参与算术、比较或逻辑运算等 在SQL标准中和许多现流行的DBMS中，空值被用一种特殊的符号Null来标记，使用特殊的空值检测函数来获得某列的值是否为空值。 空值检测： is [not ] null &#x2F;&#x2F;测试指定列的值是否为空值 示例：找出年龄值为空的学生姓名 Select Sname From Student Where Sage is null; 现行DBMS的空值处理小结 除is [not] null之外，空值不满足任何查找条件 如果null参与算术运算，则该算术表达式的值为null 如果null参与比较运算，则结果可视为false。在SQL-92中可看成unknown 如果null参与聚集运算，则除count(*)之外其它聚集函数都忽略null 10.5 内连接、外连接 标准SQL语言中连接运算通常为： Select Select 列名[[,列名]… ] From 表名1,表名2,… Where 检索条件; 即相当于采用Π[列名,…,列名](σ 检索条件(表名1 × 表名2 × …))。 SQL的高级语法中引入了内连接与外连接运算，具体形式：12345Select Select 列名 [ [, 列名] … ] From 表名1 [NATURAL] [ INNER | &#123; LEFT | RIGHT | FULL&#125; [OUTER]] JOIN 表名2 &#123; ON 连接条件 | Using (Colname &#123;, Colname …&#125;) &#125; [ Where Where 检索条件 ] … ; 由 连接类型 和 连接条件 构成连接运算。 **Natural**：出现在结果关系中的两个连接关系的元组在公共属性上取值相等，且公共属性只出现一次 Inner Join: 即关系代数中的θ-连接运算 Left Outer Join, Right Outer Join, Full Outer Join: 即关系代数中的外连接运算 **on &lt;连接条件&gt;**：出现在结果关系中的两个连接关系的元组取值满足连接条件，且公共属性出现两次 **using (Col1, Col2, …, Coln)**：Col是两个连接关系的公共属性的子集，元组在(Col1,Col2,…,Coln)上取值相等，且(Col1,Col2,…,Coln)只出现一次 示例: 1234567891011-- (Inner Join)求所有教师的任课情况并按教师号排序(没有任课的教师也需列在表中)Select Teacher.T#, Tname, Cname From Teacher Inner Join Course ON Teacher.T# = Course.T# Order by Teacher.T# ASC;--(Outer Join)求所有教师的任课情况(没有任课的教师也需列在表中)Select Teacher. T#, Tname, Cname From Teacher Left Outer Join Course ON Teacher.T# = Course.T# Order by Teacher.T# ASC ; 11. SQL语言之视图及其应用 数据库的三级模式两层映像 三级模式：数据库系统是由外模式、模式(概念模式)和内模式三级构成 应用–&gt; 外模式(多个) –&gt; 概念模式(一个) –&gt; 内模式(一个) –&gt; 数据库 两层映像：E-C映像(外模式-&gt;概念模式)、C-I映像(概念模式-&gt;内模式)。 对应概念模式的数据在SQL中被称为基本表(Table), 而对应外模式的数据称为视图(View)。视图不仅包含外模式，而且包含其E-C映像。 基本表是实际存储于存储文件中的表，基本表中的数据是需要存储的 视图在SQL中只存储其由基本表导出视图所需要的公式，即由基本表产生视图的映像信息，其数据并不存储，而是在运行过程中动态产生与维护的 对视图数据的更改最终要反映在对基本表的更改上。 11.1 视图的定义视图需要“先定义，再使用”；定义视图，有时可方便用户进行检索操作。 定义视图: create view view_name [(列名[列名] …)] as 子查询 [with check option] 如果视图的属性名缺省，则默认为子查询结果中的属性名；也可以显式指明其所拥有的列名。 with check option指明当对视图进行insert，update，delete时，要检查进行insert&#x2F;update&#x2F;delete的元组是否满足视图定义中子查询中定义的条件表达式 示例：定义一个视图 CompStud 为计算机系的学生，通过该视图可以将Student表中其他系的学生屏蔽掉1234Create View CompStud AS (Select * From Student Where D# in (Select D# From Dept Where Dname = ‘计算机’)); 11.2 视图的使用使用视图：定义好的视图，可以像Table一样，在SQL各种语句中使用 示例：检索计算机系的所有学生，我们可使用CompStud Select * From CompStud; 示例：检索计算机系的年龄小于20的所有学生，我们可使用CompStud Select * From CompStud Where Sage&lt;20; 11.3 视图的更新SQL视图更新：是比较复杂的问题，因视图不保存数据，对视图的更新最终要反映到对基本表的更新上，而有时，视图定义的映射不是可逆的。 SQL视图更新的可执行性 如果视图的select目标列包含聚集函数，则不能更新 如果视图的select子句使用了unique或distinct，则不能更新 如果视图中包括了group by子句，则不能更新 如果视图中包括经算术表达式计算出来的列，则不能更新 如果视图是由单个表的列构成，但并没有包括主键，则不能更新 对于由单一Table子集构成的视图，即如果视图是从单个基本表使用选择、投影操作导出的，并且包含了基本表的主键，则可以更新 可更新SQL视图示例： 1234567-- 定义视图create view CStud(S#, Sname, Sclass)as ( select S#, Sname, Sclass from Student where D# =&#x27;03&#x27;);-- 更新视图Insert into CStud Values (&#x27;98030104&#x27;, &#x27;张三丰&#x27;, &#x27;980301&#x27;);-- 更新视图 将转换为 更新基本表insert into Student values (&#x27;98030104&#x27;, &#x27;张三丰&#x27;, Null, Null, &#x27;03&#x27;, &#x27;980301&#x27;) 11.4 视图的撤销已经定义的视图也可以撤消 撤消视图：Drop View view_name 不仅视图可以撤消，基本表、数据库等都可以撤消 撤消基本表：Drop Table 表名 12. 数据库完整性数据库完整性(DB Integrity)是指：DBMS应保证的DB的一种特性–在任何情况下的正确性、有效性和一致性 广义完整性：语义完整性、并发控制、安全控制、DB故障恢复等 狭义完整性：专指语义完整性，DBMS通常有专门的完整性管理机制与程序来处理语义完整性问题。 12.1 基本概念关系模型中有完整性要求：实体完整性、参照完整性、用户自定义完整性 数据库完整性管理的作用 防止和避免数据库中不合理数据的出现 DBMS应尽可能地自动防止DB中语义不合理现象 如DBMS不能自动防止，则需要应用程序员和用户在进行数据库操作时处处加以小心，每写一条SQL语句都要考虑是否符合语义完整性，这种工作负担是非常沉重的，因此应尽可能多地让DBMS来承担 DBMS怎样自动保证完整性： DBMS允许用户定义一些完整性约束规则(用SQL-DDL来定义) 当有DB更新操作时，DBMS自动按照完整性约束条件进行检查，以确保更新操作符合语义完整性 完整性约束条件(或称完整性约束规则)的一般形式：Integrity Constraint::&#x3D;(O,P,A,R) O：数据集合：约束的对象(列、多列(元组)、元组集合) P：谓词条件：需要定义什么样的约束 A：触发条件：默认更新时检查 R：响应动作：默认拒绝 12.2 数据库完整性的分类 按约束对象分类: 域完整性约束条件：施加于某一列上，对给定列上所要更新的某一候选值是否可以接受进行约束条件判断，这是孤立进行的 关系完整性约束条件：施加于关系&#x2F;table上，对给定table上所要更新的某一候选元组是否可以接受进行约束条件判断，或是对一个关系中的若干元组和另一个关系中的若干元组间的联系是否可以接受进行约束条件判断 按约束来源分类: 结构约束：来自于模型的约束，例如函数依赖约束、主键约束(实体完整性)、外键约束(参照完整性)，只关心数值相等与否、是否允许空值等； 内容约束：来自于用户的约束，如用户自定义完整性，关心元组或属性的取值范围。例如Student表的Sage属性值在15岁至40岁之间等。 按约束状态分类: 静态约束：要求DB在任一时候均应满足的约束；例如Sage在任何时候都应满足大于0而小于150(假定人活最大年龄是150)。 动态约束：要求DB从一状态变为另一状态时应满足的约束；例如工资只能升，不能降：工资可以是800元，也可以是1000元；可以从800元更改为1000元，但不能从1000元更改为800元。 13. 数据库的静态完整性(约束) SQL语言支持的约束类别： 静态约束 列完整性—域完整性约束 表完整性–关系完整性约束 动态约束 触发器 Create Table有三种功能：定义关系模式、定义完整性约束 和定义物理存储特性 定义完整性约束条件：列完整性、表完整性 列约束：一种域约束类型，对单一列的值进行约束 1234567&#123; NOT NULL | //列值非空[ CONSTRAINT constraintname ] //为约束命名，便于以后撤消&#123; UNIQUE //列值是唯一| PRIMARY KEY //列为主键| CHECK (search_cond) //列值满足条件,条件只能使用列当前值| REFERENCES tablename [(colname) ][ON DELETE &#123; CASCADE | SET NULL &#125; ] &#125; &#125; 表约束：一种关系约束类型，对多列或元组的值进行约束 1234567[ CONSTRAINT constraintname ] //为约束命名，便于以后撤消&#123; UNIQUE (colname &#123;,colname…&#125;) //几列值组合在一起是唯一| PRIMARY KEY (colname &#123;,colname…&#125;) //几列联合为主键| CHECK (search_condition) //元组多列值共同满足条件 //条件中只能使用同一元组的不同列当前值| FOREIGN KEY (colname &#123;,colname…&#125;)REFERENCES tablename [(colname &#123;,colname…&#125;)]//引用另一表tablename的若干列的值作为外键 check中的条件可以是Select-From-Where内任何Where后的语句，包含子查询。 Create Table中定义的表约束或列约束可以在以后根据需要进行撤消或追加。撤消或追加约束的语句是 Alter Table(不同系统可能有差异) 示例：撤消SC表的ctscore约束(由此可见，未命名的约束是不能撤消) Alter Table SC DROP CONSTRAINT ctscore; 有些DBMS支持独立的追加约束, 注意书写格式可能有些差异 示例：Alter Table SC Add Constraint nctscore check (Score&gt;=0.0 and Score&lt;=150.0)); 现约束的方法-断言ASSERTION 一个断言就是一个谓词表达式，它表达了希望数据库总能满足的条件 表约束和列约束就是一些特殊的断言 SQL还提供了复杂条件表达的断言。其语法形式为： CREATE ASSERTION &lt;assertion-name&gt; CHECK &lt;predicate&gt; 当一个断言创建后，系统将检测其有效性，并在每一次更新中测试更新是否违反该断言。 1234567891011-- 示例: “每个分行的贷款总量必须小于该分行所有账户的余额总和”create assertion sum_constraint check (not exists (select * from branch where (select sum(amount ) from loan where loan.branch_name = branch.branch_name ) &gt;= (select sum (balance ) from account where account.branch_name = branch.branch_name )))-- 数据表：account(branch_name, account_number,…, balance) //分行，账户及其余额loan(branch_name , loan_number, amount,) //分行的每一笔贷款branch(branch_name, … ) //分行 断言测试增加了数据库维护的负担，要小心使用复杂的断言。 14. 数据库的动态完整性(触发器)实现数据库动态完整的方法—触发器Trigger 触发器Trigger Create Table中的表约束和列约束基本上都是静态的约束，也基本上都是对单一列或单一元组的约束(尽管有参照完整性)，为实现动态约束以及多个元组之间的完整性约束，就需要触发器技术Trigger Trigger是一种过程完整性约束(相比之下，Create Table中定义的都是非过程性约束),是一段程序，该程序可以在特定的时刻被自动触发执行，比如在一次更新操作之前执行，或在更新操作之后执行。 基本语法 12345678CREATE TRIGGER trigger_name BEFORE | AFTER &#123; INSERT | DELETE | UPDATE [OF colname &#123;, colname...&#125;] &#125; ON tablename [REFERENCING corr_name_def &#123;, corr_name_def...&#125; ] [FOR EACH ROW | FOR EACH STATEMENT] //对更新操作的每一条结果(前者)，或整个更新操作完成(后者) [WHEN (search_condition)] //检查条件，如满足执行下述程序 &#123; statement //单行程序直接书写，多行程序要用下行方式 | BEGIN ATOMIC statement; &#123; statement;...&#125; END &#125; 触发器Trigger意义： 当某一事件发生时(Before|After),对该事件产生的结果(或是每一元组，或是整个操作的所有元组), 检查条件search_condition,如果满足条件，则执行后面的程序段。条件或程序段中引用的变量可用corr_name_def来限定。 事件：BEFORE | AFTER { INSERT | DELETE | UPDATE …} 当一个事件(Insert, Delete, 或Update)发生之前Before或发生之后After触发 操作发生，执行触发器操作需处理两组值：更新前的值和更新后的值，这两个值由corr_name_def的使用来区分 corr_name_def的定义 12345&#123; OLD [ROW] [AS] old_row_corr_name //更新前的旧元组命别名为| NEW [ROW] [AS] new_row_corr_name //更新后的新元组命别名为| OLD TABLE [AS] old_table_corr_name //更新前的旧Table命别名为| NEW TABLE [AS] new_table_corr_name //更新后的新Table命别名为&#125; corr_name_def将在检测条件或后面的动作程序段中被引用处理 示例1: 设计一个触发器当进行Teacher表更新元组时, 使其工资只能升不能降 12345678create trigger teacher_chgsal before update of salary on teacher referencing new x, old y for each row when (x.salary &lt; y.salary)begin raise_application_error(-20003, &#x27;invalid salary on update&#x27;); //此条语句为Oracle的错误处理函数end; 示例2: 假设student(S#, Sname, SumCourse), SumCourse为该同学已学习课程的门数，初始值为0，以后每选修一门都要对其增1 。设计一个触发器自动完成上述功能。 1234567create trigger sumc after insert on sc referencing new row newi for each rowbegin update student set SumCourse = SumCourse + 1 where S# = :newi.S# ;end; 示例3：假设student(S#, Sname, SumCourse), 当删除某一同学S#时，该同学的所有选课也都要删除。设计一个触发器完成上述功能 123456create trigger delS# after delete on Student referencing old oldi for each rowbegin delete sc where S# = :oldi.S# ;end; 15. 数据库索引索引是对数据库表中一列或多列的值进行排序的一种数据结构（最常见的是B-Tree） 索引的作用 快速取数据； 保证数据记录的唯一性； 实现表与表之间的参照完整性； 在使用ORDER by、group by子句进行数据检索时，利用索引可以减少排序和分组的时间。 创建索引：CREATE INDEX 索引名称 on 表名(字段名); 删除索引：DROP INDEX 索引名称 索引注意事项： 查询时减少使用*返回全部列，不要返回不需要的列 where表达式子句包含索引的表达式置前 避免在Order by中使用表达式 索引技术是数据库自动使用，一个表格只存在一个索引就够了 缺点 索引的缺点是创建和维护索引需要耗费时间和空间 索引可以提高查询速度，会减慢写入速度 索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 15.1 索引主要种类根据数据库的功能，可以在数据库设计器中创建三种索引：唯一索引、主键索引和聚集索引。提示：尽管唯一索引有助于定位信息，但为获得最佳性能结果，建议改用主键或唯一约束。 唯一索引 唯一索引是不允许其中任何两行具有相同索引值的索引。当现有数据中存在重复的键值时，大多数数据库不允许将新创建的唯一索引与表一起保存。数据库还可能防止添加将在表中创建重复键值的新数据。例如，如果在employee表中职员的姓(lname)上创建了唯一索引，则任何两个员工都不能同姓。 主键索引 数据库表经常有一列或多列组合，其值唯一标识表中的每一行。该列称为表的主键。在数据库关系图中为表定义主键将自动创建主键索引，主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时，它还允许对数据的快速访问。 聚集索引 在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。聚集索引和非聚集索引的区别，如字典默认按字母顺序排序，读者如知道某个字的读音可根据字母顺序快速定位。因此聚集索引和表的内容是在一起的。如读者需查询某个生僻字，则需按字典前面的索引，举例按偏旁进行定位，找到该字对应的页数，再打开对应页数找到该字。这种通过两个地方而查询到某个字的方式就如非聚集索引。 索引列 可以基于数据库表中的单列或多列创建索引。多列索引可以区分其中一列可能有相同值的行。如果经常同时搜索两列或多列或按两列或多列排序时，索引也很有帮助。例如，如果经常在同一查询中为姓和名两列设置判据，那么在这两列上创建多列索引将很有意义。 16. 数据库序列序列(SEQUENCE)是序列号生成器，可以为表中的行自动生成序列号，产生一组等间隔的数值(类型为数字)。其主要的用途是生成表的主键值，可以在插入语句中引用，也可以通过查询检查当前值，或使序列增至下一个值。创建序列需要CREATE SEQUENCE系统权限。 16.1 Oracle中的序列（Sequence） 创建序列 1234567create sequence 序列名 [increment by n] --每次增加n个，默认为1 [start with n] --起始值n，默认为1 [&#123;maxvalue n | nomaxvalue&#125;] --最大值设置，递增默认10的27次方，递减默认-1 [&#123;minvalue n | nominvalue&#125;] --最小值设置，递增默认1，递减默认-10的26次方 [&#123;cycle | nocycle&#125;] --是否循环 [&#123;cache n | nocache&#125;] --是否对序列进行内存缓冲，默认为20 查询序列 NEXTVAL:返回序列中下一个有效的值，任何用户都可以引用。 CURRVAL:中存放序列的当前值,NEXTVAL 应在 CURRVAL 之前指定 ，二者应同时有效。 1234--查询下一个将要使用的序列select 序列名.nextval from dual--查询当前序列select 序列名.currval from dual Oracle将sequence的定义存储在数据字典之中。 Sequence是独立于事务的，就是说序列的增加不需要等待事务的完成，也就是说序列是异步于事务而增长的。这说明，你访问不到别的用户使用该sequence产生的值，也就是说你只能访问到你当前产生的值，即使其他用户已经增加了sequence的值；还说明如果事务回滚，sequence不会回滚，它所发生的改变是一维的。 删除序列：Drop sequence 序列名 更改序列：Alter sequence 序列名 [其余参数同创建序列] 使用序列示例： 123456789101112-- 1.直接使用insert into person (id, name, password) values (序列名.nextval, &#x27;张三&#x27;, &#x27;123&#x27;)-- 2.也可以通过建立触发器，当有数据插入表person时，使用oracle序列为其去的递增的主键值-- 2.1创建触发器create or replace trigger 触发器名 before insert on personfor each rowbegin select 序列名.nextval into :new.id from dual;end;-- 2.2插入数据insert into person ( username, age, password) values (&#x27;张三&#x27;, 20, &#x27;zhang123&#x27;) 注意点： 一个序列可以被多张别使用，不过一般建议为每个表建立单独的序列。 当使用到序列的事务发生回滚。会造成序列号不连续。在用生成的序列值作为编号做插入数据库操作时，可能遇到事务提交失败，从而导致序号不连续。 大量语句发生请求，申请序列时，为了避免序列在运用层实现序列而引起的性能瓶颈。Oracle序列允许将序列提前生成 n个先存入内存，在发生大量申请序列语句时，可直接到运行最快的内存中去得到序列。但cache个数最好不要设置过大，因为在数据库重启时，会清空内存信息，预存在内存中的序列会丢失，当数据库再次启动后，序列从上次内存中最大的序列号+1 开始存入n个。这种情况也能会在数据库关闭时也会导致序号不连续。 16.2 Mysql中的序列（AUTO_INCREMENT）MySQL中最简单使用序列的方法就是使用AUTO_INCREMENT来定义列。 orale没有类似mysql的AUTO_INCREMENT这样的自增长字段，实现插入一条记录，自动增加1.oracle是通过sequence（序列）来完成的。 首先mysql的自增长“序列”和序列是两回事，mysql本身不提供序列机制。 mysql的AUTO_INCREMENT可以设置起始值，但是不能设置步长，其步长默认就是1. mysql一个表只能有一个自增长字段。自增长只能被分配给固定表的固定的某一字段，不能被多个表共用。并且只能是数字型。 17. 数据库安全性数据库安全性是指DBMS应该保证的数据库的一种特性(机制或手段)：免受非法、非授权用户的使用、泄漏、更改或破坏 数据库安全性管理涉及许多方面 社会法律及伦理方面：私人信息受到保护，未授权人员访问私人信息会违法 公共政策&#x2F;制度方面：例如，政府或组织的信息公开或非公开制度 安全策略：政府、企业或组织所实施的安全性策略，如集中管理和分散管理，需者方知策略(也称最少特权策略) 数据的安全级别: 绝密(Top Secret), 机密(Secret),可信(Confidential)和无分类(Unclassified) 数据库系统DBS的安全级别：物理控制、网络控制、操作系统控制、DBMS控制 DBMS的安全机制 自主安全性机制：存取控制(Access Control) 通过权限在用户之间的传递，使用户自主管理数据库安全性 强制安全性机制： 通过对数据和用户强制分类，使得不同类别用户能够访问不同类别的数据 推断控制机制： 防止通过历史信息，推断出不该被其知道的信息； 防止通过公开信息(通常是一些聚集信息)推断出私密信息(个体信息)，通常在一些由个体数据构成的公共数据库中此问题尤为重要 数据加密存储机制： 通过加密、解密保护数据，密钥、加密&#x2F;解密方法与传输 DBA的责任和义务 熟悉相关的法规、政策，协助组织的决策者制定好相关的安全策略 规划好安全控制保障措施，例如，系统安全级别、不同级别上的安全控制措施，对安全遭破坏的响应， 划分好数据的安全级别以及用户的安全级别 实施安全性控制：DBMS专门提供一个DBA账户，该账户是一个超级用户或称系统用户。DBA利用该账户的特权可以进行用户账户的创建以及权限授予和撤消、安全级别控制调整等 18. 数据库自主安全性机制 通常情况下，自主安全性是通过授权机制来实现的。 用户在使用数据库前必须由DBA处获得一个账户，并由DBA授予该账户一定的权限，该账户的用户依据其所拥有的权限对数据库进行操作; 同时，该帐户用户也可将其所拥有的权利转授给其他的用户(账户)，由此实现权限在用户之间的传播和控制。 授权者：决定用户权利的人 授权：授予用户访问的权利 DBMS自动实现自主安全性： DBMS允许用户定义一些安全性控制规则(用SQL-DCL来定义) 当有DB访问操作时，DBMS自动按照安全性控制规则进行检查，检查通过则允许访问，不通过则不允许访问 DBMS将权利和用户(账户)结合在一起，形成一个访问规则表，依据该规则表可以实现对数据库的安全性控制 AccessRule ::=(S, O, t, P) S: 请求主体(用户) O: 访问对象 t: 访问权利 P: 谓词 { AccessRule｝通常存放在数据字典或称系统目录中，构成了所有用户对DB的访问权利; 用户多时，可以按用户组建立访问规则 访问对象可大可小(目标粒度Object granularity):属性&#x2F;字段、记录&#x2F;元组、关系、数据库 权利：包括创建、增、删、改、查等 谓词：拥有权利需满足的条件 示例：员工管理数据库的安全性控制示例Employee(P#,Pname,Page,Psex,Psalary,D#,HEAD) 示例要求： 员工管理人员：能访问该数据库的所有内容，便于维护员工信息 收发人员：访问该数据库以确认某员工是哪一个部门的，便于收发工作，只能访问基本信息，其他信息不允许其访问 每个员工：允许其访问关于自己的记录，以便查询自己的工资情况，但不能修改 部门领导：能够查询其所领导部门人员的所有情况 高层领导：能访问该数据库的所有内容，但只能读 两种控制示例 按名控制安全性：存储矩阵 按内容控制安全性：视图 视图是安全性控制的重要手段 通过视图可以限制用户对关系中某些数据项的存取, 例如： 视图1：Create EmpV1 as select * from Employee 视图2：Create EmpV2 as select Pname, D# from Employee 通过视图可将数据访问对象与谓词结合起来，限制用户对关系中某些元组的存取，例如： 视图1： Create EmpV3 as select * from Employee where P# &#x3D; :UserId 视图2： Create EmpV4 as select * from Employee where Head &#x3D; :UserId 用户定义视图后，视图便成为一新的数据对象，参与到存储矩阵与能力表中进行描述 18.1 SQL语言的用户与权利 SQL语言包含了DDL, DML和DCL。数据库安全性控制是属于DCL范畴 授权机制—自主安全性；视图的运用 关系级别(普通用户) &lt;– 账户级别(程序员用户) &lt;– 超级用户(DBA) (级别1)Select : 读(读DB, Table, Record, Attribute, … ) (级别2)Modify : 更新 Insert : 插入(插入新元组, … ) Update : 更新(更新元组中的某些值, …) Delete : 删除(删除元组, …) (级别3)Create : 创建(创建表空间、模式、表、索引、视图等) Create : 创建 Alter : 更新 Drop : 删除 级别高的权利自动包含级别低的权利。如某人拥有更新的权利，它也自动拥有读的权利。在有些DBMS中，将级别3的权利称为账户级别的权利，而将级别1和2称为关系级别的权利。 授权命令GRANT 1234GRANT &#123;all PRIVILEGES | privilege &#123;,privilege…&#125;&#125; ON [TABLE] tablename | viewname TO &#123;public | user-id &#123;, user-id…&#125;&#125; [WITH GRANT OPTION]; user-id ，某一个用户账户，由DBA创建的合法账户 public, 允许所有有效用户使用授予的权利 privilege是下面的权利 SELECT | INSERT | UPDATE | DELETE | ALL PRIVILEDGES WITH GRANT OPTION选项是允许被授权者传播这些权利 SQL-DCL的控制安全性-授权示例: 假定高级领导为Emp0001, 部门领导为Emp0021, 员工管理员为Emp2001,收发员为Emp5001(均为UserId, 也即员工的P#) Grant All Priviledges ON Employee TO Emp2001; Grant SELECT ON EmpV2 TO Emp5001; Grant SELECT ON EmpV3 TO public; Grant SELECT ON EmpV4 TO Emp0021; 授予视图访问的权利，并不意味着授予基本表访问的权利(两个级别：基本关系级别和视图级别) 授权者授予的权利必须是授权者已经拥有的权利 收回授权命令REVOKE 123REVOKE &#123;all privilEges | priv &#123;, priv…&#125; &#125; ON tablename | viewname FROM &#123;public | user &#123;, user…&#125; &#125;; 示例: revoke select on employee from UserB; 18.2 自主安全性的授权过程及其问题18.2.1 授权过程: 第一步：DBA创建DB, 并为每一个用户创建一个账户 假定建立了五个用户：UserA, UserB, UserC, UserD, UserE 第二步：DBA授予某用户账户级别的权利 假定授予UserA 第三步：具有账户级别的用户可以创建基本表或视图, 他也自动成为该表或该视图的属主账户，拥有该表或该视图的所有访问 权利 假定UserA创建了Employee, 则UserA就是Employee表的属主账户 第四步：拥有属主账户的用户可以将其中的一部分权利授予另外的用户，该用户也可将权利进一步授给其他的用户… 假定UserA将读权限授予UserB, 而userB又将其拥有的权限授予UserC,如此将权利不断传递下去。 注意授权的传播范围 传播范围包括两个方面：水平传播数量和垂直传播数量 水平传播数量是授权者的再授权用户数目(树的广度) 垂直传播数量是授权者传播给被授权者，再被传播给另一个被授权者, …传播的深度(树的深度) 有些系统提供了传播范围控制，有些系统并没有提供，SQL标准中也并没有限制。 当一个用户的权利被收回时，通过其传播给其他用户的权利也将被收回 如果一个用户从多个用户处获得了授权，则当其中某一个用户收回授权时，该用户可能仍保有权利。例如UserC从UserB和UserE处获得了授权，当UserB收回时，其还将保持UserE赋予其的权利。 18.2.2 强制安全性机制 强制安全性机制 强制安全性通过对数据对象进行安全性分级 绝密(Top Secret), 机密(Secret), 可信(Confidential) 和 无分类(Unclassified) 同时对用户也进行上述的安全性分级 从而强制实现不同级别用户访问不同级别数据的一种机制 强制安全性机制的实现 DBMS引入强制安全性机制, 可以通过扩展关系模式来实现 关系模式: R(A1: D1, A2: D2, …, An:Dn) 对属性和元组引入安全性分级特性或称分类特性 R(A1: D1, C1, A2: D2, C2…, An:Dn, Cn, TC)其中 C1,C2,…,Cn分别为属性D1,D2,…,Dn的安全分类特性; TC为元组的分类特性 这样, 关系中的每个元组, 都将扩展为带有安全分级的元组 强制安全性机制使得关系形成为多级关系(不同级别用户所能看到的关系的子集)，也出现多重实例、多级关系完整性等许多新的问题或新的处理技巧，在使用中需注意仔细研究。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"「数据库」数据库系统基础","date":"2018-08-22T12:20:19.000Z","path":"2018/08/22/database-base.html","text":"概述 数据库 是 电子化信息的集合 将信息规范化并使之电子化，形成电子信息’库’，以便利用计算机对这些信息进行快速有效的存储、检索、统计与管理。 表(Table)：以按行按列形式组织及展现的数据 数据库起源于规范化“表(Table)”的处理，Table中描述了一批相互有关联关系的数据–&gt;关系 数据库系统的构成（概念层次）: 数据库(DB):Database：相互之间有关联关系的数据的集合 数据库管理系统(DBMS):Database Management System 数据库应用(DBAP):Database Application 数据库管理员(DBA):Database Asministrator 计算机基本系统 1. 数据库管理系统(DBMS)1.1 从用户角度看DBMS(数据库管理系统) 数据库定义：定义数据库中的Table的表名、标题(属性以及属性值的要求)等 DBMS提供了一套数据定义语言(DDL: Data Definition Language)给用户 用户使用DDL描述其所要建立的表的格式 DBMS依照用户的定义，创建数据库及其中的表 数据库操作：向数据库的Table中增加&#x2F;删除&#x2F;更新数据及对数据进行查询、检索、统计等 DBMS提供了一套数据库操纵语言(DML: Data Manipulation Language)给用户 用户使用DML描述其所要进行的增、删、改、查等操作 DBMS依照用户的操作描述，实际执行这些操作 数据库控制：控制数据库中数据的使用(哪些用户可以使用，哪些不可以) DBMS提供了一套数据控制语言(DCL: Data Control Language)给用户 用户使用DCL描述其对数据库所要实施的控制 DBMS依照用户描述，实际ijnx控制 数据库维护：转储&#x2F;恢复&#x2F;重组&#x2F;性能监测&#x2F;分析… DBMS提供了一系列程序(实用程序&#x2F;例行程序)给用户 在这些程序中提供了对数据库维护的各种功能 用户使用这些程序进行各种数据库维护操作 (数据库维护的实用程序，一般由数据库管理员(DBA)来使用和掌握的) 1.2 数据库语言 使用者使用数据库语言，利用DBMS操纵数据库 SQL语言：结构化的数据库语言 高级语言：一条数据库语言相当于高级语言的一个或多个循环程序，数据库语言可以嵌入到高级语言(宿主语言)中使用 1.3 从系统实现角度看DBMS的功能 数据库管理系统的实现：形式 –&gt; 构造 –&gt; 自动化 DBMS为完成DB管理，在后台运行着一系列程序… 语言编译器：将数据库语言书写的内容，翻译成BDMS可执行的命令。例如：DDL编译器，DML编译器，DCL编译器等 查询优化(执行引擎)与查询实现(基本命令的不同执行算法)：提高数据库检索速度的手段。例如贯穿于数据存取各个阶段的优化程序 数据存取与索引：提供数据在磁盘&#x2F;磁带等上的搞笑存取手段。例如：存储管理器，缓冲区管理器，索引&#x2F;文件和记录管理器等 通信控制：提供网络环境下数据库操作与数据传输的手段 事务管理：提供提高可靠性并避免并发操作错误的手段 故障恢复：使用数据库自动恢复到故障发生前正确状态的手段。例如备份、运行日志操控等实用程序 安全性控制：提供合法性检验，避免非授权非法用户访问数据库的手段 完整性控制：提供数据及数据操作正确性检查的手段 数据字典管理：管理用户已经定义的信息 **应用程序接口(API)**：提供应用程序使用DBMS特定功能的二首段 数据库数据装载、重组等实用程序 数据库性能分析：统计在运行过程中数据库的各种性能数据，便于优化运行 典型的数据库管理系统(DBMS)：Oracle、DB2(IBM)、Sybase、Microsoft SQL Server、Microsoft Access、PostgreSQL 2. 数据库系统的标准结构DBMS管理数据的三个层次： External Level &#x3D; User Level（外部级别 &#x3D; 用户级别） 某一用户能够看到与处理的数据, 全局数据中的某一部分 Conceptual Level &#x3D; Logic level（概念级别 &#x3D; 逻辑级别） 从全局角度理解&#x2F;管理的数据, 含相应的关联约束 Internal Level &#x3D; Physical level（内部级别 &#x3D; 物理级别） 存储在介质上的数据，含存储路径、存储方式 、索引方式等 3. 三级模式两层映像数据库的三级模式结构是指：数据库系统是由外模式、模式(概念模式)和内模式三级构成 应用–&gt; 外模式(多个) –&gt; 模式(一个) –&gt; 内模式(一个) –&gt; 数据库 3.1 数据(视图)与模式(数据的结构) 模式(Schema):对数据库中数据所进行的一种结构性的描述，所观察到数据的结构信息 视图(View)&#x2F;数据(Data)：某一种表现形式下表现出来的数据库中的数据 3.2 三级模式(三级视图) External Schema —-(External) View 外模式：某一用户能够看到与处理的数据的结构描述 (Conceptual) Schema —- Conceptual View 模式(概念模式)：从全局角度理解&#x2F;管理的数据的结构描述, 含相应的关联约束 体现在数据之间的内在本质联系 Internal Schema —- Internal View 内模式：存储在介质上的数据的结构描述，含存储路径、存储方式 、索引方式等 3.3 两层映像 E-C Mapping：External Schema-Conceptual Schema Mapping 将外模式映射为概念模式，从而支持实现数据概念视图向外部视图的转换 便于用户观察和使用 C-I Mapping：Conceptual Schema-Internal Schema Mapping 将概念模式映射为内模式，从而支持实现数据概念视图向内部视图的转换 便于计算机进行存储和处理 3.4 标准结构的两个独立性 逻辑数据独立性 当概念模式变化时，可以不改变外部模式(只需改变E-C Mapping)，从而无需改变应用程序 物理数据独立性 当内部模式变化时，可以不改变概念模式(只需改变C-I Mapping) ，从而不改变外部模式 4. 数据模型 数据模型：模式 与 模式的结构 规定模式统一描述方式的模型，包括：数据结构、操作和约束 数据模型是对模式本身结构的抽象，模式是对数据本身结构形式的抽象 比如：关系模型：所有模式都可为抽象表(Table)的形式[数据结构]，而每一个具体的模式都是拥有不同列名的具体的表。对这种表形式的数据有哪些[操作]和[约束] 三大经典数据模型 关系模型：表的形式组织数据 层次模型：树的形式组织数据 网状模型：图的形式组织数据 5. 关系模型 形象地说，一个关系(relation)就是一个Table，关系模型就是处理Table的，它由三个部分组成： 描述DB各种数据的基本结构形式(Table&#x2F;Relation) 描述Table与Table之间所可能发生的各种操作(关系运算) 描述这些操作所应遵循的约束条件(完整性约束) 关系模型的三个要素： 基本结构：Relation&#x2F;Table 基本操作：Relation Operator 基本的:(并, UNION)、(差, DIFFERENCE)、(广义积,PRODUCT)、(选择, SELECTION)、(投影, PROJECTION)。 扩展的:(交, INTERSECTION)、(连接, JOIN)、(除, DIVISION)运算 完整性约束：实体完整性、参照完整性和用户自定义的完整性 表(Table)的基本构成要素 列&#x2F;字段&#x2F;属性&#x2F;数据项：列名，列值 行&#x2F;元组&#x2F;记录 标题&#x2F;模式 5.1 “表”的严格定义 域(Domain)：“列”的取值范围，一组值的集合，这组值具有相同的数据类型 笛卡尔积(Cartesian Product)：“元组”及所有可能组合成的元组 关系(Relation)：一组域D1,D2,…,Dn的笛卡尔积的子集，笛卡尔积中具有某一方面意义的那些元组被称作一个关系(Relation) 5.2 关系模式与关系 同一关系模式下，可有很多的关系 关系模式是关系的结构, 关系是关系模式在某一时刻的数据 关系模式是稳定的；而关系是某一时刻的值，是随时间可能变化的 5.3 关系的特性 列是同质：即每一列中的分量来自同一域，是同一类型的数据 不同的列可来自同一个域，称其中的每一列为一个属性，不同的属性要给予不同的属性名。 列位置互换性：区分哪一列是靠列名 行位置互换性：区分哪一行是靠某一或某几列的值(关键字&#x2F;键字&#x2F;码字) 关系是以内容(名字或值)来区分的，而不是属性在关系的位置来区分 理论上，关系的任意两个元组不能完全相同。(集合的要求：集合内不能有相同的两个元素)；现实应用中，表(Table)可能并不完全遵守此特性。元组相同是指两个元组的每个分量(列值)都相同。 属性不可再分特性:又被称为关系第一范式 5.4 关系的一些重要概念 候选码(Candidate Key)&#x2F;候选键 关系中的一个属性组，其值能唯一标识一个元组，若从该属性组中去掉任何一个属性，它就不具有这一性质了，这样的属性组称作候选码。 主码(Primary Key)&#x2F;主键 当有多个候选码时，可以选定一个作为主码。DBMS以主码为主要线索管理关系中的各个元组 主属性与非主属性 包含在任何一个候选码中的属性被称作主属性，而其他属性被称作非主属性 最简单的，候选码只包含一个属性； 极端的，所有属性构成这个关系的候选码，称为全码(All-Key) 外码(Foreign Key)&#x2F;外键 关系R中的一个属性组，它不是R的候选码，但它与另一个关系S的候选码相对应，则称这个属性组为R的外码或外键。 两个关系通常是靠外码连接起来的。 6. 关系模型中的完整性6.1 实体完整性 关系的主码中的属性值不能为空值； 意义：关系中的元组对应到现实世界相互之间可区分的一个个个体，这些个体是通过主码来唯一标识的；若主码为空，则出现不可标识的个体，这是不容许的。 6.2 参照完整性 如果关系R1的外码Fk与关系R2的主码Pk相对应，则R1中的每一个元组的Fk值或者等于R2 中某个元组的Pk 值，或者为空值 意义：如果关系R1的某个元组t1参照了关系R2的某个元组t2，则t2必须存在 6.3 用户自定义完整性 用户针对具体的应用环境定义的完整性约束条件 6.4 DBMS对关系完整性的支持 实体完整性和参照完整性由DBMS系统自动支持 DBMS系统通常提供了如下机制： 它使用户可以自行定义有关的完整性约束条件 当有更新操作发生时，DBMS将自动按照完整性约束条件检验更新操作的正确性，即是否符合用户自定义的完整性 7. 关系代数7.1 关系代数的特点 基于集合，提供了一系列的关系代数操作：并、差、笛卡尔积(广义积)、选择、投影和更名等基本操作 以及交、 连接和关系除等扩展操作，是一种集合思维的操作语言。 关系代数操作以一个或多个关系为输入，结果是一个新的关系。 用对关系的运算来表达查询，需要指明所用操作, 具有一定的过程性。 是一种抽象的语言，是学习其他数据库语言，如SQL等的基础 7.2 关系代数的约束某些关系代数操作，如并、差、交等，需满足”并相容性” 并相容性： 参与运算的两个关系及其相关属性之间有一定的对应性、可比性或意义关联性 定义：关系R与关系S存在相容性，当且仅当： (1) 关系R和关系S的属性数目必须相同； (2) 对于任意i，关系R的第i个属性的域必须和关系S的第i个属性的域相同 示例：关系R：STUDENT(SID char(10), Sname char(8), Age char(3)) 示例：关系S：TEACHER(TID char(10), Tname char(8), Age char(3)) 7.3 关系代数的基本操作 集合操作 并（UNIO）：R∪S 交（INTERSECTION）：R∩S 差（DIFFERENCE）：R-S 笛卡儿积（Cartesian PRODUCT）：R×S 纯关系操作 选择（SELECT）：σF(R) 投影（PROJECT）：ΠA(R) 连接（JOIN）：R⋈S 除（DIVISION）：R÷S 7.3.1 并(Union) 操作 定义：设关系R和关系S是并相容的，则关系R与关系S的并运算结果也是一个关系，记作：**R∪S**, 它由 或者出现在关系R中，或者出现在S中的元组构成。 数学描述：R∪S=&#123;t|t∈R∨t∈S&#125;，其中t是元组 并运算是将两个关系的元组合并成一个关系，在合并时去掉重复的元组。 汉语中的“或者…或者…”通常意义是并运算的要求。 R∪S 与 S∪R 运算的结果是同一个关系 7.3.2 差(Difference) 操作 定义：设关系R 和关系S是并相容的，则关系R与关系S的差运算结果也是一个关系，记作：**R-S**, 它由出现在关系R中但不出现在关系S中的元组构成。 数学描述：R－S=&#123;t|t∈R∧t∉S&#125;，其中t是元组 汉语中的“是…但不含…”通常意义是差运算的要求。 R-S 与 S-R 是不同的 7.3.3 交（Intersection Referential integrity） 操作 定义：设关系R和关系S具有相同的目n，且相应的属性取自同一个域，则关系R与关系S的交由既属于R又属于S的元组组成。其结果关系仍为n目关系。 数学描述：R∩S=&#123;t|t∈R∧t∈S&#125;，其中t是元组 7.3.4 广义笛卡尔积(Extended cartesian product) 操作 定义：关系R(&lt;a1,a2, …,an&gt;)与关系S(&lt;b1,b2, …,bm &gt;)的广义笛卡尔积(简称广义积,或 积 或笛卡尔积)运算结果也是一个关系，记作：**RxS**；两个分别为n目和m目的关系R和S的广义笛卡尔积是一个(n+m)列的元组的集合，元组的前n列是关系R的一个元组，后m列是关系S的一个元组，若R有k1个元组，S有k2个元组，则关系R和关系S的广义笛卡尔积有k1×k2个元组。 数学描述：RxS = &#123;&lt;a1,a2,…,an,b1,b2,…,bm&gt;|&lt;a1,a2,…,an&gt;∈R ∧ &lt;b1,b2,…,bm&gt;∈S&#125; RxS=SxR：RxS为R中的每一个元组都和S中的所有元组进行串接。SxR为S中的每一个元组都和R中的所有元组进行串接。结果是相同的。 两个关系R和S，它们的属性个数分别为n和m(R是n度关系，S是m度关系)则笛卡尔积R×S的属性个数&#x3D;n+m。即元组的前n个分量是R中元组的分量，后m个分量是S中元组的分量(R×S是n+m度关系). 两个关系R和S，它们的元组个数分别为x和y(关系R的基数x,S的基数y),则笛卡尔积R×S的元组个数&#x3D;x×y。(R×S的基数是x×y). 7.3.5 选择(Select) 定义：给定一个关系R, 同时给定一个选择的条件condition(简记F), 选择运算结果也是一个关系，记作**σF(R)**, 它从关系R中选择出满足给定条件condition的元组构成。 数学描述：σF(R) = &#123;t|t∈R ∧ F(t)=&#39;真&#39;&#125;,其中F表示选择条件，它是一个逻辑表达式，取逻辑值‘真’或‘假’。 选择操作从给定的关系中选出满足条件的行,条件的书写很重要，尤其是当不同运算符在一起时，要注意运算符的优先次序，优先次序自高至低为{ 括弧()；θ；¬；∧；∨ } 7.3.6 投影(Project) 定义：给定一个关系R, 投影运算结果也是一个关系，记作**A(R)**, 它从关系R中选出属性包含在A中的列构成。 数学描述：ΠA(R) = &#123;t[A] | t∈R&#125;,其中A为R中的属性列 投影操作从给定关系中选出某些列组成新的关系, 而选择操作是从给定关系中选出某些行组成新的关系 7.4 关系代数的扩展操作7.4.1 交(Intersection) 定义：假设关系R和关系S是并相容的，则关系R与关系S的交运算结果也是一个关系，记作：**R∩S**, 它由同时出现在关系R和关系S中的元组构成。 数学描述：R∩S = &#123;t|t∈R ∧ t∈S&#125;，其中t是元组 R∩S 和 S∩R 运算的结果是同一个关系 交运算可以通过差运算来实现：R∩S = R-(R-S) = S-(S-R) 汉语中的“既…又…”，“…, 并且…”通常意义是交运算的要求 7.4.2 θ-连接(θ-Join, theta-Join) 投影与选择操作只是对单个关系(表)进行操作, 而实际应用中往往涉及多个表之间的操作, 这就需要θ-连接操作 定义：给定关系R和关系S, R与S的连接运算结果也是一个关系，记作 **R⋈S[AθB]**：(括号内AθB是⋈的下标)，它由关系R和关系S的笛卡尔积中, 选取R中属性A与S中属性B之间满足 θ 条件的元组构成。 数学描述：R⋈S[AθB] = σ t[A]θs[B] (R×S)，σF(RxS)其中t是R中的元组，s是S中的元组 在实际应用中，θ-连接操作经常与投影Π、选择σ操作一起使用 特别注意：当引入θ-连接操作后，DBMS可直接进行连接操作，而不必先形成笛卡尔积。 7.4.3 等值连接(Equi-Join) 定义：给定关系R和关系S, R与S的等值连接运算结果也是一个关系，记作**R⋈S[A=B]**：(括号内A&#x3D;B是⋈的下标)，它由关系R和关系S的笛卡尔积中选取R中属性A与S中属性B上值相等的元组所构成。 数学描述：R⋈S[A=B] = σ t[A]=s[B] (R×S) 当θ-连接中运算符为“＝”时，就是等值连接，等值连接是θ-连接的一个特例； 广义积的元组组合并不是都有意义的，另广义积的元组组合数目也非常庞大，因此采用θ-连接&#x2F;等值连接运算可大幅度降低中间结果的保存量，提高速度。 7.4.4 自然连接(Natural-Join) 定义：给定关系R和关系S, R与S的自然连接运算结果也是一个关系，记作 ，它由关系R和关系S的笛卡尔积中选取相同属性组B上值相等的元组所构成。 数学描述：R⋈S = σ t[B]=s[B] (R×S) 自然连接是一种特殊的等值连接，要求关系R和关系S必须有相同的属性组B，R, S属性相同，值必须相等才能连接，要在结果中去掉重复的属性列 7.5 关系代数的基本书写思路 选出将用到的关系&#x2F;表 做”积”运算（可用连接运算替换） 做选择运算保留所需的行&#x2F;元组 做投影运算保留所需的列&#x2F;属性 基本思路： 检索是否涉及多个表，如不涉及，则可直接采用并、差、交、选择与投影，只要注意条件书写正确与否即可 如涉及多个表，则检查： 能否使用自然连接，将多个表连接起来(多数情况是这样的) 如不能，能否使用等值或不等值连接(θ-连接) 还不能，则使用广义笛卡尔积，注意相关条件的书写 连接完后，可以继续使用选择、投影等运算，即所谓数据库的“选投联”操作 7.6 关系代数之复杂扩展操作7.6.1 除(Division) 除法运算经常用于求解“查询… 全部的&#x2F;所有的…”问题 前提条件：给定关系R(A1 ,A2 , … ,An)为n度关系，关系S(B1 ,B2 , … ,Bm)为m度关系 。如果可以进行关系R与关系S的除运算，当且仅当：属性集{ B1 ,B2 , … , Bm }是属性集{ A1 ,A2 , … ,An }的真子集，即m &lt; n。 定义：关系R 和关系S的除运算结果也是一个关系，记作R÷S，分两部分来定义。 数学描述：12R÷S = &#123;t|t∈Π[R-S](R) ∧ ∀u∈S(tu∈R) &#125; = Π[R-S](R) - Π[R-S]((Π[R-S](R)×S)-R) 其中[R-S]为投影Π的下标(属性) 7.6.2 外连接(Outer-Join) 定义：两个关系R与S进行连接时，如果关系R(或S)中的元组在S(或R)中找不到相匹配的元组，则为了避免该元组信息丢失，从而将该元组与S(或R)中假定存在的全为空值的元组形成连接，放置在结果关系中，这种连接称之为外连接(Outer Join)。 外连接 &#x3D; 自然连接 (或θ连接) + 失配的元组(与全空元组形成的连接) 外连接的形式：左外连接、右外连接、全外连接 左外连接 &#x3D; 自然连接(或连接) + 左侧表中失配的元组 右外连接 &#x3D; 自然连接(或连接) + 右侧表中失配的元组 全外连接 &#x3D; 自然连接(或连接) + 两侧表中失配的元组 左外连接(Left Outer Join)记为：⋊ 右外连接(Right Outer Join)记为：⋉ 全外连接(Full Outer Join)记为：⋊⋉ 8. 关系演算关系演算是描述关系运算的另一种思维方式，它是以数理逻辑中的谓词演算为基础的，SQL语言是继承了关系代数和关系演算各自的优点所形成的 按照谓词变量的不同，可分为关系元组演算和关系域演算 关系元组演算是以元组变量作为谓词变量的基本对象 关系域演算是以域变量作为谓词变量的基本对象 8.1 关系元组演算 关系元组演算公式：{ t | P(t) } 表示：所有使谓词 P 为真的元组 t 的集合 t 是元组变量 t ∈ r 表示元组 t 在关系 r 中 t[A] 表示元组 t 的分量，即 t 在属性 A 上的值 P是与谓词逻辑相似的公式, P(t)表示以元组 t 为变量的公式 关系元组演算公式的基本形式：{ t | P(t) } P(t)可以是如下三种形式之一的原子公式： t∈R：t 是关系 R 中的一个元组，例如： { t | t∈Student} s[A] θ c：元组分量s[A]与常量 c 之间满足比较关系θ，θ:比较运算符&lt;,&lt;&#x3D;,&#x3D;,&lt;&gt;,&gt;,&gt;&#x3D; s[A] θ u[B]：s[A] 与 u[B] 为元组分量，A和B分别是某些关系的属性，他们之间满足比较关系θ， P(t)可以由公式加运算符 ∧(与)、∨(或)、¬(非)递归地构造 如果F是一个公式，则 ¬F 也是公式 如果F1、F2是公式，则 F1∧F2, F1∨F2也是公式 P(t)运算符优先次序(括弧；θ；∃；∀；¬；∧；∨)示例 构造P(t)还有两个运算符：∃(存在)、∀(任意) 如果F是一个公式，则 ∃(t∈r)(F(t)) 也是公式 如果F是一个公式，则 ∀(t∈r)(F(t)) 也是公式 运算符∃和∀，又称为量词，前者称“存在量词”，后者称“全称量词” 而被∃或∀限定的元组变量 t , 或者说，元组变量 t 前有存在量词或全称量词，则该变量被称为“约束变量”，否则被称为“自由变量”。 元组演算的等价性变换 符号&lt;=&gt;表示表示等价于,如：¬(A&gt;B) &lt;=&gt; A&lt;=B &lt;=&gt; A&lt;B∨A=B 8.2 关系域演算 关系域演算公式的基本形式：&#123;&lt;x1,x2, …,xn&gt; | P(x1,x2, …,xn)&#125;,其中 xi 代表域变量或常量, P为以xi为变量的公式。 元组演算是以元组为变量，以元组为基本处理单位，先找到元组，然后再找到元组分量，进行谓词判断； 域演算是以域变量为基本处理单位，先有域变量，然后再判断由这些域变量组成的元组是否存在或是否满足谓词判断。 公式的运算符(∧(与)、∨(或)、¬(非)、∀(全称量词)和∃(存在量词))是相同的，只是其中的变量不同。 元组演算和域演算可以等价互换。 8.2.1 基于关系域演算的QBE语言QBE: Query By Example，1975年由M. M. Zloof提出，1978年在IBM370上实现，是一种高度非过程化的查询语言，特别适合于终端用户的使用。 特点：操作独特，基于屏幕表格的查询语言，不用书写复杂的公式，只需将条件填在表格中即可 QBE操作框架由四个部分构成 关系名区：用于书写欲待查询的关系名 属性名区：用于显示对应关系名区关系的所有属性名 操作命令区：用于书写查询操作的命令 查询条件区：用于书写查询条件 QBE的操作命令 Print 或 P. —- 显示输出操作 Delete或D. —- 删除操作 Insert或I. —- 插入操作 Update或U. —- 更新操作 构造查询的几个要素 示例元素: 即域变量， 一定要加下划线 示例元素是这个域中可能的一个值， 它不必是查询结果中的元素 打印操作符P.: 指定查询结果所含属性列 查询条件: 不用加下划线 可使用比较运算符＞， ≥，＜， ≤，＝和≠ 其中＝可以省略 排序要求 升序排序(AO.)，降序排序（DO.）,多列排序，用‚AO(i).‛ 或‚DO(i).‛ 表示 ，其中i为排序的优先级， i值越小，优先级越高 8.3 安全性关系运算的安全性：不产生无限关系和无穷验证的运算被称为是安全的 关系代数是一种集合运算，是安全的 集合本身是有限的，有限元素集合的有限次运算仍旧是有限的。 关系演算不一定是安全的 如：{t|¬(R(t))}, {t R(t)∨t[2]&gt;3}可能表示无限关系 需要对关系演算施加约束条件，即任何公式都在一个集合范围内操作，而不是无限范围内操作，才能保证其安全性。 8.3.1 安全约束有限集合DOM DOM(ψ)是一个有限集合，其中的每个符号要么是ψ中明显出现的符号，要么是出现在ψ中的某个关系R的某元组的分量。 DOM主要用于约束ψ中一些谓词的计算范围，它不必是最小集合。 安全元组演算表达式，满足三个条件： 只要t满足ψ，t的每个分量就是DOM(ψ)的一个成员。 { t|ψ(t) }中t的取值只能是DOM中的值，有限的。 对于ψ中形如(∃u)(ω(u))的子表达式，若u满足ω,则u的每个分量都是DOM(ω)中的成员。 { t|ψ(t) }中的每个(∃u)(ω(u))子表达式，只需要验证DOM中的元素是否有使ω(u)为真的元素。而对于DOM以外的元素，已经明确其都不满足ω(u)，无需验证。 对于ψ中形如(∀u)(ω(u))的子表达式，若u不满足ω,则u的每个分量都是DOM(ω)中的成员。 { t|ψ(t) }中的每个(∀u)(ω(u))子表达式，只需要验证DOM中的元素是否有使ω(u)为假的元素。而对于DOM以外的元素，已经明确其都满足ω(u)，无需验证。 8.4 关于三种关系运算的一些观点 关系运算有三种：关系代数、关系元组演算和关系域演算 三种关系运算都是抽象的数学运算，体现了三种不同的思维 关系代数—以集合为对象的操作思维，由集合到集合的变换 元组演算—以元组为对象的操作思维，取出关系的每一个元组进行验证，有一个元组变量则可能需要一个循环，多个元组变量则需要多个循环 域演算—以域变量为对象的操作思维，取出域的每一个变量进行验证看其是否满足条件 三种运算之间是等价的 关系代数 与 安全的元组演算表达式 与 安全的域演算表达式 是等价的。即一种形式的表达式可以被等价地转换为另一种形式 三种关系运算都可说是非过程性的 相比之下：域演算的非过程性最好，元组演算次之，关系代数最差 三种关系运算虽是抽象的，但却是衡量数据库语言完备性的基础 一个数据库语言如果能够等价地实现这三种关系运算的操作，则说该语言是完备的 目前多数数据库语言都能够实现这三种运算的操作，在此基础上还增加了许多其他的操作，如赋值操作、聚集操作等 数据库语言可以基于这三种抽象运算来设计 用“键盘符号”来替换抽象的数学符号 用易于理解的符号组合来表达抽象的数学符号 例如：ISBL语言—基于关系代数的数据库语言 再例如：Ingres系统的QUEL语言","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"数据库","slug":"db","permalink":"http://chaooo.github.io/tags/db/"}]},{"title":"「Spring」SpringBoot MVC应用","date":"2018-06-20T09:31:30.000Z","path":"2018/06/20/spring-boot-mvc.html","text":"对Spring Web MVC封装，简化MVC结构web应用开发。 1. SpringBoot MVC开发Restful服务（前后分离）*按rest规则发送HTTP请求–&gt;Spring MVC–&gt;返回JSON结果 主要步骤： 导入spring-boot-starter-web（springmvc、rest、jackson、tomcat） 在application.properties修改tomcat端口 定义启动类RunBoot，追加@SpringBootApplication 定义Controller、Service、Dao组件 2. SpringBoot MVC开发JSP应用（PC浏览器）HTTP请求–&gt;Spring MVC–&gt;JSP–&gt;HTML响应输出结果 主要步骤： 导入spring-boot-starter-web、jasper解析器、jstl 在application.properties修改tomcat端口、viewResolver 定义启动类RunBoot，追加@SpringBootApplication 定义Controller组件，返回ModelAndView 在src&#x2F;main&#x2F;webapp下定义JSP组件 3. SpringBoot MVC开发Thymeleaf应用（PC浏览器）*HTTP请求–&gt;Spring MVC–&gt;Thymeleaf模板–&gt;HTML响应输出结果 主要步骤： 导入spring-boot-starter-web、spring-boot-starter-thymeleaf 在application.properties修改tomcat端口 定义启动类RunBoot，追加@SpringBootApplication 定义Controller组件，返回ModelAndView 在src&#x2F;main&#x2F;resources&#x2F;templates下定义模板文件1234&lt;html xmlns:th=&quot;https://www.thymeleaf.org/&quot;&gt; &lt;h1&gt;Hello&lt;/h1&gt; &lt;h2 th:text=&quot;$&#123;data&#125;&quot;&gt;&lt;/h2&gt;&lt;/html&gt; th:text表达式作用：将模型中的数据以只读文本显示到元素中 th:text表达式作用：将模型中的数据以只读文本显示到元素中。 th:if 表达式作用：if判断逻辑 th:each 表达式作用：循环逻辑 th:href 表达式作用：动态生成href链接 Thymeleaf模板和JSP区别 运行机制不同 JSP–&gt;Servlet–&gt;HTML 模板+数据–&gt;HTML输出 模板简单易用;JSP相对复杂些 JSP:9大内置对象、EL、JSTL、嵌入Java代码、框架标签 模板：模板表达式 模板效率高,比JSP性能好 模板：缓存 4. SpringBoot MVC静态资源处理静态资源包含图片、js、css等，动态资源servlet、jsp等。 SpringBoot中src&#x2F;main&#x2F;resources目录下有几个约定的静态资源存放位置 META-INF&#x2F;resources（优先级最高） resources static public（优先级最低） 自定义静态资源访问路径，编写一个配置文件 123456789101112//@Configuration@Componentpublic class MyStaticConfiguration implements WebMvcConfigurer&#123; public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(&quot;/**&quot;) .addResourceLocations( &quot;classpath:/images/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot;); &#125;&#125; 5. SpringBoot MVC异常处理 异常处理机制 SpringBoot底层提供了异常处理机制。SpringBoot提供了一个ErrorMvcAutoConfiguration自动配置组件，创建了一个BasicErrorController对象，提供两个&#x2F;error请求处理，一个返回html，另一个返回json。当MVC底层遇到异常会用转发方式发出&#x2F;error请求。 可以自定义ErrorController替代底层BasicErrorController，将错误提示转发到自定义提示界面(全局) 123456789101112131415161718192021@Controller//@RequestMapping(&quot;/error&quot;)public class MyErrorController implements ErrorController&#123; @RequestMapping(value=&quot;/error&quot;,produces= MediaType.TEXT_HTML_VALUE) public ModelAndView errorHtml() &#123; ModelAndView mav = new ModelAndView(); mav.setViewName(&quot;myerror&quot;); return mav; &#125; @RequestMapping(value=&quot;/error&quot;) @ResponseBody public Object error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(&quot;msg&quot;, &quot;程序发生了异常&quot;); return map; &#125; @Override public String getErrorPath() &#123; return &quot;/error&quot;; &#125;&#125; @ExceptionHandler异常处理（局部） ErrorController管理全局异常，@ExceptionHandler管理所在Controller组件的异常。12345678@ExceptionHandler@ResponseBodypublic Object error(Exception ex) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(&quot;msg&quot;, &quot;发生异常&quot;); map.put(&quot;type&quot;, ex.getClass()); return map;&#125; 可以将上述方法封装成一个BasicController，通过@ControllerAdvice作用到所有Controller组件上。 1234567891011@ControllerAdvice//等价于所有Controller都继承它public class BasicController &#123; @ExceptionHandler @ResponseBody public Object error(Exception ex) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(&quot;msg&quot;, &quot;发生异常&quot;); map.put(&quot;type&quot;, ex.getClass()); return map; &#125;&#125; 6. SpringBoot AOP 引入spring-boot-starter-aop 定义一个切面组件 123456789101112131415161718192021@Component//将Bean组件纳入Spring容器@Aspect//将Bean组件定义为Aspect切面public class MyAspectBean &#123; @Before(&quot;within(cn.xdl.controller.*)&quot;)//前置通知 public void before() &#123; System.out.println(&quot;----开始处理----&quot;); &#125; @After(&quot;within(cn.xdl.controller.*)&quot;)//最终通知 public void after() &#123; System.out.println(&quot;----处理完毕----&quot;); &#125; @Around(&quot;within(cn.xdl.controller.*)&quot;) public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; StopWatch watch = new StopWatch(); watch.start(); Object obj = pjp.proceed();//调用目标组件方法 watch.stop(); System.out.println(&quot;处理时间:&quot;+watch.getTotalTimeMillis()+&quot; 毫秒&quot;); return obj; &#125;&#125; 配置切面组件 @Aspect、@Before、@After、@Around、@AfterReturning、@AfterThrowing等 7. SpringBoot MVC拦截器 编写一个拦截器组件,实现HandlerInterceptor接口 1234567891011121314@Componentpublic class MyInterceptor implements HandlerInterceptor &#123; public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;执行了MyInterceptor拦截器&quot;); String user = (String)request.getSession().getAttribute(&quot;user&quot;); if(user == null) &#123; response.sendRedirect(&quot;/tologin&quot;); return false;//阻止后续流程执行 &#125; return true;//继续执行后续处理 &#125;&#125; 配置拦截器组件 12345678@Configurationpublic class MyInterceptorConfiguration implements WebMvcConfigurer&#123; @Autowired private MyInterceptor my; public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(my).addPathPatterns(&quot;/direction/list&quot;); &#125;&#125; 8. SpringBoot整合Servlet&#x2F;Filter8.1 整合Servlet首先导入spring-boot-starter-web 8.1.1 整合Servlet方式一： 编写一个Servlet组件，继承HttpServlet 在Servlet类定义前使用@WebServlet 123456789@WebServlet(name=&quot;helloservlet&quot;,urlPatterns= &#123;&quot;/hello.do&quot;&#125;,loadOnStartup=1)public class HelloServlet extends HttpServlet&#123; public void service( HttpServletRequest request, HttpServletResponse response ) throws IOException &#123; response.getWriter().println(&quot;Hello SpringBoot Servlet&quot;); &#125;&#125; 启动类前需要使用@ServletComponentScan扫描@WebServlet配置 1234567@SpringBootApplication@ServletComponentScan //扫描@WebServlet、@WebFilter、@WebListener组件public class RunBoot &#123; public static void main(String[] args) &#123; SpringApplication.run(RunBoot.class, args); &#125;&#125; 8.1.2 整合Servlet方式二： 编写一个Servlet组件，继承HttpServlet 12345678public class SomeServlet extends HttpServlet &#123; public void service( HttpServletRequest request, HttpServletResponse response ) throws IOException &#123; response.getWriter().println(&quot;Hello Spring Some Servlet&quot;); &#125;&#125; 使用ServletRegistrationBean+@Bean 1234567891011121314151617@SpringBootApplicationpublic class RunBoot &#123; public static void main(String[] args) &#123; SpringApplication.run(RunBoot.class, args); &#125; @Bean public ServletRegistrationBean&lt;Servlet&gt; someservlet()&#123; ServletRegistrationBean&lt;Servlet&gt; bean = new ServletRegistrationBean&lt;Servlet&gt;(); bean.setServlet(new SomeServlet()); bean.setLoadOnStartup(1); List&lt;String&gt; urls = new ArrayList&lt;&gt;(); urls.add(&quot;/some.do&quot;); bean.setUrlMappings(urls); return bean; &#125;&#125; 8.2 整合Filter在SpringBoot整合Servlet的基础上整合Filter 8.2.1 整合Filter方式一： 编写一个Filter组件，继承Filter 在Filter类定义前使用@WebFilter 1234567891011@WebFilter(urlPatterns=&quot;/hello.do&quot;)public class HelloFilter implements Filter&#123; @Override public void doFilter( ServletRequest request, ServletResponse response, FilterChain chain ) throws IOException, ServletException &#123; System.out.println(&quot;-----hello filter------servlet执行之前&quot;); chain.doFilter(request, response); System.out.println(&quot;-----hello filter------servlet执行之后&quot;); &#125;&#125; 启动类前需要使用@ServletComponentScan扫描@WebServlet配置 1234567@SpringBootApplication@ServletComponentScan //扫描@WebServlet、@WebFilter、@WebListener组件public class RunBoot &#123; public static void main(String[] args) &#123; SpringApplication.run(RunBoot.class, args); &#125;&#125; 8.2.2 整合Filter方式二： 编写一个Filter组件，继承Filter 12345678910public class SomeFilter implements Filter&#123; @Override public void doFilter( ServletRequest request, ServletResponse response, FilterChain chain ) throws IOException, ServletException &#123; System.out.println(&quot;-----som filter------servlet执行之前&quot;); chain.doFilter(request, response); System.out.println(&quot;-----som filter------servlet执行之后&quot;); &#125;&#125; 使用FilterRegistrationBean+@Bean 注册过滤器并设置拦截的请求地址 123456789101112131415@SpringBootApplicationpublic class RunBoot &#123; public static void main(String[] args) &#123; SpringApplication.run(RunBoot.class, args); &#125; ... @Bean public FilterRegistrationBean&lt;Filter&gt; somefilter()&#123; FilterRegistrationBean&lt;Filter&gt; bean = new FilterRegistrationBean&lt;Filter&gt;(); bean.setFilter(new SomeFilter()); // 配置要拦截的请求 bean.addUrlPatterns(&quot;/some.do&quot;); return bean; &#125;&#125; 9. SpringBoot 任务调度9.1 服务器启动后自动调用tomcat服务器启动后自动调用任务，可以使用ApplicationRunner或CommandLineRunner接口。 123456789101112131415161718@Component@Order(2)public class SomeTask1 implements ApplicationRunner &#123; @Override public void run(ApplicationArguments args) throws Exception &#123; System.out.println(&quot;----服务器启动后自动执行SomeTask1任务---&quot; + new Date()); &#125;&#125;@Component@Order(1)public class SomeTask2 implements CommandLineRunner&#123; @Override public void run(String... args) throws Exception &#123; System.out.println(&quot;----服务器启动后自动执行SomeTask2任务-----&quot;+new Date()); Thread.sleep(5000); &#125;&#125; 多个Task任务，可以通过@Order指定先后顺序，多个任务是线程同步调用。 9.2 程序运行后定时调用任务Spring提供了一个Spring Schedule模块，封装了任务调用，之前都是采用Quartz组件调用。 123456789101112131415@Component@EnableScheduling//开启Schedule模块public class SomeTask3 &#123; //在服务器启动1秒后调用任务，每隔3秒调用一次 @Scheduled(initialDelay=1000,fixedRate=3000) public void execute() &#123; System.out.println(&quot;-----周期性调用SomeTask3-----&quot;+new Date()); &#125; //在服务器启动0秒后调用任务，每隔5秒调用一次 @Scheduled(cron=&quot;0/5 * * * * ?&quot;)//秒 分 时 日 月 星期 public void execute2() &#123; System.out.println(&quot;-----周期性调用SomeTask4-----&quot;+new Date()); &#125;&#125; 使用Spring Schedule还需要指定cron表达式，表达式具体规则： 123456789101112秒 分 时 日 月 星期 年（可省略）0 0 10 1 10 ？秒： 0-59分： 0-59时： 0-23日： 1-31月： 1-12星期：1-7，1表示星期日，7表示星期六* ： 表示每一分、每一秒、每一天，任何一个可能值? ： 只用在日和星期部分，如果指定日，星期用？;如果指定星期，日用?，避免日和星期冲突 / ： 表示增量，0/1表示0\\1\\2\\3\\4递增加1；0/5表示0\\5\\10\\15；1/5表示1\\6\\11\\16\\21L ： 只用在日和星期部分，表示最后一天、周六 cron表达式案例： 123456789101112131415161718&quot;30 * * * * ?&quot; 每半分钟触发任务&quot;30 10 * * * ?&quot; 每小时的10分30秒触发任务&quot;30 10 1 * * ?&quot; 每天1点10分30秒触发任务&quot;30 10 1 20 * ?&quot; 每月20号1点10分30秒触发任务&quot;30 10 1 20 10 ? *&quot; 每年10月20号1点10分30秒触发任务&quot;30 10 1 20 10 ? 2011&quot; 2011年10月20号1点10分30秒触发任务&quot;30 10 1 ? 10 * 2011&quot; 2011年10月每天1点10分30秒触发任务&quot;30 10 1 ? 10 SUN 2011&quot; 2011年10月每周日1点10分30秒触发任务&quot;15,30,45 * * * * ?&quot; 每15秒，30秒，45秒时触发任务&quot;15-45 * * * * ?&quot; 15到45秒内，每秒都触发任务&quot;15/5 * * * * ?&quot; 每分钟的每15秒开始触发，每隔5秒触发一次&quot;15-30/5 * * * * ?&quot; 每分钟的15秒到30秒之间开始触发，每隔5秒触发一次&quot;0 0/3 * * * ?&quot; 每小时的第0分0秒开始，每三分钟触发一次&quot;0 15 10 ? * MON-FRI&quot; 星期一到星期五的10点15分0秒触发任务&quot;0 15 10 L * ?&quot; 每个月最后一天的10点15分0秒触发任务&quot;0 15 10 LW * ?&quot; 每个月最后一个工作日的10点15分0秒触发任务&quot;0 15 10 ? * 5L&quot; 每个月最后一个星期四的10点15分0秒触发任务&quot;0 15 10 ? * 5#3&quot; 每个月第三周的星期四的10点15分0秒触发任务 9.3 SpringBoot+Quartz导入spring-boot-starter-quartz, 编写Job任务组件，继承QuartzJobBean 123456public class MyTask5 extends QuartzJobBean&#123; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; System.out.println(&quot;通过Quartz工具调用定时任务&quot;+new Date()); &#125;&#125; 配置Job组件（JobDetail、Tigger） 123456789101112131415161718@Configurationpublic class QuartzConfiguration &#123; @Bean//将MyTask5任务组件封装成JobDetail public JobDetail task5() &#123; return JobBuilder.newJob(MyTask5.class) .withIdentity(&quot;task5&quot;).storeDurably().build(); &#125; @Bean//为JobDetail指定触发时间cron表达式 public Trigger task5Trigger() &#123; CronScheduleBuilder cronScheduleBuilder = CronScheduleBuilder.cronSchedule(&quot;0/5 46 10 * * ?&quot;); return TriggerBuilder.newTrigger() .forJob(task5()) .withIdentity(&quot;task5&quot;) .withSchedule(cronScheduleBuilder) .build(); &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"}]},{"title":"「Spring」SpringBoot数据库访问","date":"2018-06-14T09:29:31.000Z","path":"2018/06/14/spring-boot-database.html","text":"Springboot对于数据访问层，无论是SQL还是NOSQL，都默认采用整合Spring Data的方式进行统一处理，Springboot添加大量自动配置，屏蔽了很多设置。并引入各种Template，Repository来简化我们对数据访问层的操作。 1.SpringBoot数据库访问1.1 Spring DAO JdbcTemplate引入spring-boot-starter-jdbc后（hikari、spring-jdbc包），就可以借助DataSourceAutoConfiguration、JdbcTemplateAutoConfiguration自动配置组件创建出HikariDataSource、JdbcTemplate对象。 引入jdbc启动器、驱动包，创建连接池 根据要操作表定义entity（pojo，属性名与字段名一致） 12345678910111213141516public class Direction &#123; private int id; private String name; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 定义Dao接口 123public interface DirectionDao &#123; public List&lt;Direction&gt; findAll();&#125; 定义Dao实现类，扫描并注入JdbcTemplate使用 123456789101112@Repository//通过组件扫描加载到Spring容器public class JdbcDirectionDao implements DirectionDao &#123; @Autowired private JdbcTemplate template;//通过自动配置加载到Spring容器 @Override public List&lt;Direction&gt; findAll() &#123; String sql = &quot;select * from direction&quot;; RowMapper&lt;Direction&gt; rowMapper = new BeanPropertyRowMapper&lt;Direction&gt;(Direction.class); return template.query(sql, rowMapper); &#125;&#125; 1.2 Spring MyBatis（XML SQL版本） 引入spring-boot-starter-jdbc、驱动包、mybatis-spring-boot-starter 引入application.properties（连接池参数） 实体类(同上) SQL定义 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;cn.xdl.dao.DirectionMapper&quot;&gt; &lt;select id=&quot;selectAll&quot; resultType=&quot;cn.xdl.entity.Direction&quot;&gt; select * from direction &lt;/select&gt; &lt;select id=&quot;selectById&quot; parameterType=&quot;int&quot; resultType=&quot;cn.xdl.entity.Direction&quot;&gt; select * from direction where id=#&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; Mapper接口 1234public interface DirectionMapper &#123; public List&lt;Direction&gt; selectAll(); public Direction selectById(int id);&#125; 使用@MapperScan和mybatis.mapperLocations&#x3D;classpath:sql&#x2F;*.xml 在启动类前追加@MapperScan 12345@SpringBootApplication@MapperScan(basePackages=&quot;cn.xdl.dao&quot;)//扫描Mapper接口创建对象加载到Spring容器public class RunBoot &#123; ... ...&#125; 在application.properties追加mybatis.mapperLocations 1mybatis.mapperLocations=classpath:sql/*.xml 1.3 Spring MyBatis（注解 SQL版本） 引入spring-boot-starter-jdbc、驱动包、mybatis-spring-boot-starter 引入application.properties（连接池参数） 实体类(同上) 定义Mapper接口，使用@Select、@Update、@Insert、@Delete注解定义SQL 12345678public interface DirectionMapper &#123; @Select(&quot;select * from direction&quot;) public List&lt;Direction&gt; findAll(); @Select(&quot;select * from direction where id=#&#123;id&#125;&quot;) public Direction findById(int id); @Update(&quot;update direction set name=#&#123;name&#125; where id=#&#123;id&#125;&quot;) public int updateName(@Param(&quot;id&quot;)int id,@Param(&quot;name&quot;)String name);&#125; 使用@MapperScan（同上） 2. Spring Data JPA2.1 JpaJpa (Java Persistence API) 是 Sun 官方提出的 Java 持久化规范。中文名Java持久层API，是JDK 5.0注解或XML描述对象－关系表的映射关系，并将运行期的实体对象持久化到数据库中。 Sun引入新的JPA ORM规范主要是为了简化现有的持久化开发工作和整合 ORM 技术，结束现在 Hibernate，TopLink，JDO 等 ORM 框架各自为营的局面。 注意:Jpa 是一套规范，不是一套产品，那么像 Hibernate,TopLink,JDO 他们是一套产品，如果说这些产品实现了这个 Jpa 规范，那么我们就可以叫他们为 Jpa 的实现产品。 2.2 Spring Boot JpaSpring Boot Jpa 是 Spring 基于 ORM 框架、Jpa 规范的基础上封装的一套 Jpa 应用框架，可使开发者用极简的代码即可实现对数据的访问和操作。 Spring Boot Jpa 让我们解脱了 DAO 层的操作，基本上所有 CRUD 都可以依赖于它来实现 在Spring中使用JPA访问数据库，需要使用Spring Data模块支持。 - SpringData是对Spring框架一个扩展模块，包含对JPA、Redis、MongoDB等技术的访问支持。 Spring Boot Jpa的使用 引入spring-boot-starter-jdbc、spring-boot-starter-data-jpa、驱动包 在application.properties定义db连接池参数（同上） 定义RunBoot启动类，使用@SpringBootApplication标记（同上） 根据要操作的表定义实体类，使用@Entity、@Table、@Id、@Column定义该对象和表结构之间的映射关系 123456789101112131415161718192021222324@Entity@Table(name=&quot;direction&quot;)public class Direction &#123; @Id @Column(name=&quot;id&quot;) private int id; @Column(name=&quot;name&quot;) private String name; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 定义Dao接口，可以选择继承JpaRepository、PagingAndSortingRepository、CrudRepository等 1234public interface DirectionDao extends JpaRepository&lt;Direction, Integer&gt;&#123; //...&#125; 2.3 Dao扩展操作分页查询 1234567Pageable pageable = PageRequest.of(1, 3);//of(页数从0开始,记录条数)Page&lt;Direction&gt; page = dao.findAll(pageable);List&lt;Direction&gt; list = page.getContent();list.forEach(d-&gt;&#123;System.out.println(d.getId()+&quot; &quot;+d.getName());&#125;);System.out.println(&quot;总记录数:&quot;+page.getTotalElements()+&quot; 页数:&quot;+(page.getNumber()+1)+&quot;/&quot;+page.getTotalPages());List&lt;Direction&gt; list1 = dao.findByIdGreaterThan2(5); 按方法名规则扩展 12//where id&gt;?public List&lt;Direction&gt; findByIdGreaterThan(int id); 定义SQL语句扩展 12@Query(nativeQuery=true,value=&quot;select * from direction where id&gt;:id&quot;)public List&lt;Direction&gt; findByIdGreaterThan1(@Param(&quot;id&quot;)int id); 定义JPQL面向查询语句扩展 12@Query(&quot;from Direction where id&gt;:id&quot;) //使用类型名和属性名替代表名和字段名public List&lt;Direction&gt; findByIdGreaterThan2(@Param(&quot;id&quot;)int id); 按名称模糊查询，带分页支持 123@Query(nativeQuery=true,value=&quot;select * from direction where name like :name&quot; ,countQuery=&quot;select count(*) from direction where name like :name&quot;)public Page&lt;Direction&gt; findByNameLike1(@Param(&quot;name&quot;)String name,Pageable pageable);","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"}]},{"title":"「Spring」SpringBoot入门","date":"2018-06-06T15:06:01.000Z","path":"2018/06/06/spring-boot.html","text":"Spring Boot 是由 Pivotal 团队提供的全新框架。Spring Boot 是所有基于 Spring Framework 5.0 开发的项目的起点。Spring Boot 的设计是为了简化新 Spring 应用的初始搭建以及开发过程，并且尽可能减少你的配置文件。 从最根本上来讲，Spring Boot 就是一些库的集合，它能够被任意项目的构建系统所使用。它使用 “习惯优于配置” （项目中存在大量的配置，此外还内置一个习惯性的配置）的理念让你的项目快速运行起来。用大佬的话来理解，就是 spring boot 其实不是什么新的框架，它默认配置了很多框架的使用方式，就像 maven 整合了所有的 jar 包，spring boot 整合了所有的框架，总结一下及几点： 为所有 Spring 开发提供一个更快更广泛的入门体验。 零配置。无冗余代码生成和XML 强制配置，遵循“约定大于配置” 。 集成了大量常用的第三方库的配置， Spring Boot 应用为这些第三方库提供了几乎可以零配置的开箱即用的能力。 提供一系列大型项目常用的非功能性特征，如嵌入式服务器、安全性、度量、运行状况检查、外部化配置等。 Spring Boot 不是Spring 的替代者，Spring 框架是通过 IOC 机制来管理 Bean 的。Spring Boot 依赖 Spring 框架来管理对象的依赖。Spring Boot 并不是Spring 的精简版本，而是为使用 Spring 做好各种产品级准备。 1. 项目管理工具Maven的基本使用Maven是一个使用java编写的开源的项目管理工具，可以方便灵活的控制项目，不必浪费时间去在不同的环境中配置依赖的jar包，而专心于业务逻辑。 1.1 配置Maven的系统环境变量 下载并解压到目录，如D:\\apache-maven-3.6.1 添加新的系统环境变量MAVEN_HOME&#x3D;安装的目录：MAVEN_HOME=D:\\apache-maven-3.6.1 添加%MAVEN_HOME%\\bin到系统PATH变量. 测试Maven配置是否成功，打开命令行窗口，输入mvn -v，如果有maven 版本信息输出则证明配置成功，否则请查看自己配置路径等是否正确。 注意：安装Maven前请确保已安装JDK并成功配置其环境变量。 1.2 maven中的术语 maven插件：maven主要定义了项目对象模型的生命周期。实际上每个任务都是交由插件完成的。maven的生命周期与插件目标相互绑定，来完成每个具体的任务。 maven坐标：就是对项目的定位。groupId：组id，机构名。artifactId：构建id ，产品名或者产品的id。version ：版本号。 坐标形式：groupId + artifactId+ version maven仓库：存放maven共享构建的位置。 本地仓库：localRepository（使用conf/settings.xml设置） 私服仓库：部署在局域网中的仓库，方便整个团队的开发使用。 中央仓库：远程仓库下载地址：http://repo1.maven.org/maven2 1234&lt;!-- conf/settings.xml设置本地仓库路径 --&gt;&lt;settings ... &lt;localRepository&gt;D:/apache-maven-3.6.1/.m2/repository&lt;/localRepository&gt;... 1.3 maven构建的生命周期清除–&gt; 编译–&gt; 测试–&gt; 报告–&gt; 打包(jar\\war)–&gt; 安装–&gt; 部署 清除：mvn clean 编译：mvn compile 测试：mvn test 打包：mvn package 安装：mvn install 部署：mvn deploy 1.4 MAVEN优点 模块化项目 项目非常大时，可借助Maven将一个项目拆分成多个工程，最好是一个模块对应一个工程，利于分工协作。而且模块可以通信。 实现Jar包共享 借助Maven，可将jar包仅仅保存在“仓库”中，有需要该文件时，就引用该文件接口，不需要复制文件过来占用空间。 jar包的依赖 借助Maven可以以规范的方式下载jar包，因为所有的知名框架或第三方工具的jar包已经按照统一的规范存放到了Maven的中央仓库中。 jar包的自动导入 通过xml定义引入jar包，Maven会自动导入jar包及其依赖jar包进来。 1.5 MAVEN工具 可以命令行使用，也可以结合Eclipse和Idea使用 简化项目搭建、编译、打包、发布等工作 2. SpringBoot基础 SpringBoot是对Spring框架的封装，用于简化Spring应用搭建和开发过程。 SpringBoot是pivotal公司产品、SpringCloud也是。 2.1 SpringBoot典型特点： 去除XML配置，完全采用Java配置方式 内置tomcat服务器 利用自动配置创建很多对象（DataSource、JdbcTemplate、DispatcherServlet等） 提供一系列启动器（jar包集合） 采用properties或yml做配置文件 应用采用jar包发布 2.2 SpringBoot程序构成 创建工程，导入boot启动器（jar包） spring-boot-starter (核心、包含ioc、yml、自动配置、Log日志) spring-boot-starter-parent（包含参数设置、文件编码、jdk版本等） spring-boot-starter-jdbc（包含连接池、jdbcTemplate等） spring-boot-starter-web（包含mvc、restful、tomcat等） spring-boot-starter-test（包含junit、spring-test等） 添加配置文件application.properties或application.yml 2.3 SpringBoot配置文件application.properties 123spring.datasource.username=rootspring.datasource.password=123456server.port=8888 application.yml 123456spring: datasource: username: root password: 123456server: port: 8888 2.4 SpringBoot启动类定义启动类，通过main方法启动 123456@SpringBootApplicationpublic class Xxxx&#123; public static void main(String[] args)&#123; SpringApplication.run(Xxxx.class); &#125;&#125; 2.5 SpringBoot数据库访问在pom.xml定义spring-boot-starter-jdbc、mysql驱动包 123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 在application.properties定义数据库连接参数 1234spring.datasource.username=rootspring.datasource.password=123456spring.datasource.url=jdbc:mysql://localhost:3306/ydmaspring.datasource.driverClassName=com.mysql.jdbc.Driver 定义启动类，内部会根据自动配置机制生成DataSource和JdbcTemplate 12345678910111213141516@SpringBootApplicationpublic class RunBoot &#123; public static void main(String[] args) throws SQLException &#123; //ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); ApplicationContext ctx = SpringApplication.run(RunBoot.class, args); DataSource ds = ctx.getBean(DataSource.class); System.out.println(ds.getConnection()); JdbcTemplate template = ctx.getBean(JdbcTemplate.class); System.out.println(template); String sql = &quot;insert into paper_score (total_score,my_score,user_id) values (?,?,?)&quot;; Object[] params = &#123;100,90,1&#125;; template.update(sql,params); &#125;&#125;//提示：DataSource和JdbcTemplate都是基于自动配置机制产生，直接注入使用即可。 2.6 打包发布SpringBoot程序： 在pom.xml定义spring-boot-maven-plugin插件 12345678&lt;build&gt;&lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt;&lt;/plugins&gt;&lt;/build&gt; 点击工程右键选择run as- maven build … 执行完毕后会在项目target目录下生成一个jar包，该包就是发布包 可以采用java -jar xxxx.jar命令启动 提示：eclipse设置jdk必须指向到JDK路径，不要JRE路径。 3. SpringBoot启动过程 调用SpringApplication的静态的run方法启动 静态的run方法调用SpringApplication对象的run方法 (SpringApplication对象创建时加载spring.factories文件中Initializer和Application Listeners组件，判断程序类型servlet、reactive、default) 对象的run方法会创建Spring的ApplicationContext容器对象 获取启动Listener组件 获取environment环境参数 获取启动Logo信息Banner 根据程序类型不同创建不同类型的ApplicationContext对象 将Listener、environment、banner设置到ApplicationContext容器对象中 为ApplicationContext容器对象加载程序中各种Bean组件 开始执行启动任务ApplicationRunner、CommandLineRunner等 返回ApplicationContext容器对象 4. @SpringBootApplication作用SpringApplication.run方法在启动中，加载一个带有@SpringBootApplication标记的参数，该标记具有以下几种功能。 4.1 @SpringBootConfiguration（SpringBoot Bean定义） spring中bean定义&lt;bean id=&quot;&quot; class=&quot;&quot;&gt; SpringBoot通过@Bean、@Primary标记定义。 案例代码： 12345678910111213141516@SpringBootConfiguration//开启Bean定义功能public class BeanConfiguration &#123; @Bean//将返回的UserDao对象放入Spring容器，默认方法名为id public UserDao userdao() &#123; return new UserDao(); &#125; @Bean(&quot;dao2&quot;)//将返回的UserDao对象放入Spring容器，指定id为dao2 @Primary//默认注入该对象 public UserDao userdao1() &#123; return new UserDao(); &#125; @Bean(&quot;userService&quot;) public UserService userService() &#123; return new UserService(); &#125;&#125; @SpringBootConfiguration标记是对Spring的@Configuration封装，所以直接用@Configuration也可以。 4.2 @ComponentScan（SpringBoot组件扫描） spring中组件扫描&lt;context:component-scan base-package=&quot;&quot;/&gt; SpringBoot通过@ComponentScan 扫描指定包路径组件，带@Controller、@Service、@Repository、@Component注解标记组件 @ComponentScan(basePackages= &#123;&quot;cn.xdl.dao&quot;,&quot;cn.xdl.service&quot;&#125;) 扫描cn.xdl包及子包下的组件 @ComponentScan(basePackages=&quot;cn.xdl&quot;) 扫描当前包及子包下的组件 @ComponentScan 扫描当前包及子包组件，并且将DeptService组件纳入 @ComponentScan(includeFilters= &#123;@Filter(type=FilterType.ASSIGNABLE_TYPE,classes=DeptService.class)&#125;) 扫描当前包及子包组件，带有@Controller、@Service…、@MyComponent注解有效 @ComponentScan(includeFilters= &#123;@Filter(type=FilterType.ANNOTATION,classes=MyComponent.class)&#125;) 4.3 @EnableAutoConfiguration（SpringBoot自动配置）自动配置机制是SpringBoot框架特有功能，能在启动后自动创建一些常用对象，例如DataSource、JdbcTemplate等。 自动配置原理： 在xxx-autoconfigure.jar包中META-INF目录下有一个spring.factories文件，其中定义了大量的XxxAutoConfiguration配置组件。当开启@EnableAutoConfiguration标记时，标记内部会触发AutoConfigurationImportSelector组件调用SpringFactoriesLoader加载spring.factories文件。 自动配置组件就是采用@Configuration+@Bean+@Primary标记事先定义好的配置组件，通过Boot启动自动去spring.factories文件加载，然后在Spring容器中创建出约定对象。 1234DataSourceAutoConfiguration//创建dataSource对象JdbcTemplateAutoConfiguration//创建jdbcTemplateDispatcherServletAutoConfiguration//创建DispatcherServlet对象RedisAutoConfiguration//创建RedisTemplate对象 通过自动配置机制创建DataSource对象 引入spring-boot-starter-jdbc（hikari）、驱动包 在application.properties文件追加db参数 在启动类使用@EnableAutoConfiguration标记 DataSourceAutoConfiguration默认会创建Hikari、tomcat、dbcp2连接池对象，优先级hikari最高，依次tomcat、dbcp2. 如果通过spring.datasource.type属性指定其他类型连接池组件，SpringBoot可以按指定类型创建连接池。12spring.datasource.type=org.apache.commons.dbcp2.BasicDataSourcespring.datasource.type=com.alibaba.druid.pool.DruidDataSource 4.4 MAVEN如何排除某个jar包（扩展）在引入spring-boot-starter-jdbc启动器时，由于jar包依赖会自动引入HikariCP，可以通过&lt; exclusion&gt;标记排除依赖。 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://chaooo.github.io/tags/SpringBoot/"}]},{"title":"「Spring」SSM框架整合(Spring+SpringMVC+MyBatis)","date":"2018-05-11T14:52:55.000Z","path":"2018/05/11/spring-ssm.html","text":"SSM框架是spring MVC ，spring和mybatis框架的整合，是标准的MVC模式，将整个系统划分为表现层，controller层，service层，DAO层。 使用spring MVC负责请求的转发和视图管理 spring实现业务对象管理 mybatis作为数据对象的持久化引擎 1.搭建SSM架构步骤： 设计数据库 先写实体类entity，定义对象的属性，（参照数据库中表的字段来设置）。 编写Mapper.xml（Mybatis），定义功能，对应要对数据库进行的那些操作，比如 insert、selectAll、selectByKey、delete、update等。 编写Mapper.java(DAO接口)，将Mapper.xml中的操作按照id映射成Java函数。 配置spring和mybatis框架的整合(applicationContext.xml) 编写Service.java，为控制层提供服务，接受控制层的参数，完成相应的功能，并返回给控制层。 配置SpringMVC(web.xml) 编写Controller.java，连接页面请求和服务层，获取页面请求的参数，通过自动装配，映射不同的URL到相应的处理函数，并获取参数，对参数进行处理，之后传给服务层。 编写JSP页面调用，请求哪些参数，需要获取什么数据。 DataBase –&gt; Entity –&gt; Mapper.xml –&gt; Mapper.Java(DAO) –&gt; Service.java –&gt; Controller.java –&gt; Jsp 2.搭建SSM架构实例（管理员登录）2.1 设计数据库(以MySql为例)建立web项目，在src下新建sql脚本(admin.sql)，并在数据库中执行 1234567891011121314CREATE DATABASE exam_sys;/** 管理员表 */DROP TABLE admin;CREATE TABLE admin( id INT AUTO_INCREMENT COMMENT &#x27;管理员ID&#x27;, name VARCHAR(30) NOT NULL COMMENT &#x27;管理员账号&#x27;, password VARCHAR(30) COMMENT &#x27;管理员密码&#x27;, CONSTRAINT et_admin_id_pk PRIMARY KEY(id), CONSTRAINT et_admin_name_uk UNIQUE(NAME));/** 插入数据 */INSERT INTO admin (name, password) VALUES(&#x27;admin&#x27;, &#x27;123456&#x27;);SELECT * FROM admin;COMMIT; 2.2 先写实体类entity，定义对象的属性参照数据库中表的字段来设置 123456789101112package com.exam.entity;public class Admin &#123; private int id; private String name; private String password; /** 添加 getter/setter方法 * 添加 无参，有参构造 * 重写toString()以便于测试 */ // ...&#125; 2.3 编写AdminMapper.xml（Mybatis），定义功能对应要对数据库进行的那些操作，比如 insert、selectAll、selectByKey、delete、update等。 123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//ibatis.apache.org//DTD Mapper 3.0//EN&quot; &quot;http://ibatis.apache.org/dtd/ibatis-3-mapper.dtd&quot;&gt;&lt;!-- namespace指定和哪个Mapper映射器接口对应 --&gt;&lt;mapper namespace=&quot;com.exam.mapper.AdminDao&quot;&gt; &lt;!-- 定义SQL语句 --&gt; &lt;select id=&quot;findByNameAndPassword&quot; resultType=&quot;com.exam.entity.Admin&quot;&gt; select * from admin where name=#&#123;name, jdbcType=VARCHAR&#125; and password=#&#123;password, jdbcType=VARCHAR&#125; &lt;/select&gt;&lt;/mapper&gt; 2.4 编写AdminDao.java，将AdminMapper.xml中的操作按照id映射成Java函数。导入Mybatis相关jar包：mybatis.jar、mysql-connector-java.jar(数据库驱动)、mybatis-spring.jar(SM整合) 12345678package com.exam.mapper;import org.apache.ibatis.annotations.Param;import com.exam.entity.Admin;public interface AdminDao &#123; public Admin findByNameAndPassword(@Param(&quot;name&quot;) String name, @Param(&quot;password&quot;) String password);&#125; 2.5 配置spring和mybatis框架的整合导入Spring相关jar包：ioc&#x2F;aop&#x2F;dao&#x2F;连接池；添加Spring配置文件（applicationContext.xml）到src下。 123456789101112131415161718192021222324252627&lt;!-- 配置连接池对象 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/exam_sys&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置SqlSessionFactoryBean来创建SqlSessionFactory 属性dataSource：注入连接池对象 属性mapperLocations：指定MyBatis的映射器XML配置文件的位置 属性typeAliasesPackage：对应我们的实体类所在的包，配置此项可在Mapper映射器直接使用类名，而非包名.类名 --&gt;&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:com/exam/mapper/*.xml&quot;&gt;&lt;/property&gt; &lt;!-- &lt;property name=&quot;typeAliasesPackage&quot; value=&quot;com.exam.entity&quot;&gt;&lt;/property&gt; --&gt;&lt;/bean&gt;&lt;!-- 批量生产DAO接口实现类 ,实现类id为类名首字母小写 --&gt;&lt;bean id=&quot;mapperScanner&quot; class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;!-- &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; --&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.exam.mapper&quot;&gt;&lt;/property&gt; &lt;!-- 自定义注解可以让只让有注解的接口产生实现类，另一部分一部分不产生 --&gt; &lt;!-- &lt;property name=&quot;annotationClass&quot; value=&quot;com.annotation.MyAnnotation&quot;&gt;&lt;/property&gt; --&gt;&lt;/bean&gt;&lt;!-- 开启服务层组件扫描 --&gt;&lt;context:component-scan base-package=&quot;com.exam.service&quot;/&gt; 2.6 编写Service.java，为控制层提供服务接受控制层的参数，完成相应的功能，并返回给控制层。 1234567891011121314151617181920package com.exam.service;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import com.exam.mapper.AdminDao;@Service(&quot;adminService&quot;)public class AdminService &#123; @Autowired private AdminDao dao; public boolean Login(String name, String password) &#123; try &#123; return dao.findByNameAndPassword(name, password)!=null?true:false; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125;&#125; 2.7 配置SpringMVC导入jar包（spring-web.jar，spring-webmvc.jar）,生成web.xml并配置DispatcherServlet分发请求。 12345678910111213141516171819202122232425262728&lt;!-- 配置编码过滤器 --&gt;&lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;!-- 配置DispatcherServlet分发请求 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;!-- 在applicationContext.xml对静态资源进行放行 ：mvc:default-servlet-handler--&gt; 在applicationContext.xml中开启组件扫描(com.controller)，开启标注形式mvc，配置视图处理器 并 对静态资源进行放行。 1234567891011&lt;!-- 开启控制器组件扫描 --&gt;&lt;context:component-scan base-package=&quot;com.exam.controller&quot;/&gt;&lt;!-- 开启标注形式mvc --&gt;&lt;mvc:annotation-driven /&gt;&lt;!-- 配置视图处理器 --&gt;&lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/&quot;&gt;&lt;/property&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 对静态资源进行放行 --&gt;&lt;mvc:default-servlet-handler/&gt; 2.8 编写Controller.java，连接页面请求和服务层获取页面请求的参数，通过自动装配，映射不同的URL到相应的处理函数，并获取参数，对参数进行处理，之后传给服务层。（导入Json相关包：jackson-core.jar，jackson-databind.jar，jackson-annotations.jar） 12345678910111213141516171819202122232425262728293031323334353637package com.exam.controller;import javax.servlet.http.HttpServletRequest;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.ResponseBody;import com.exam.entity.Admin;import com.exam.service.AdminService;@Controller@RequestMapping(&quot;/admin&quot;)public class AdminController &#123; @Autowired private AdminService as; @RequestMapping(&quot;/tologin&quot;) public String toLogin() &#123; return &quot;admin/login&quot;; &#125; @RequestMapping(value=&quot;/login&quot;,method=RequestMethod.POST) @ResponseBody public boolean addUser(Admin admin, HttpServletRequest request) &#123; System.out.println(&quot;add:&quot;+admin); System.out.println(admin.getName()+&quot;---&quot;+admin.getPassword()); boolean bl = as.Login(admin.getName(), admin.getPassword()); if(bl) &#123; //登录成功的逻辑 request.getSession().setAttribute(&quot;admin&quot;, admin); return true; &#125; //登录失败的逻辑 request.setAttribute(&quot;msg&quot;, &quot;登录失败&quot;); return false; &#125;&#125; 2.9 编写JSP页面调用123456789101112131415161718192021&lt;form&gt; 管理员: &lt;input id=&quot;aName&quot; type=&quot;text&quot;&gt;&lt;br&gt; 密码:&lt;input id=&quot;aPassword&quot; type=&quot;text&quot;&gt;&lt;br&gt; &lt;input id=&quot;loginBtn&quot; type=&quot;button&quot; value=&quot;登录&quot;&gt;&lt;/form&gt;&lt;script src=&quot;js/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;$(&quot;#loginBtn&quot;).on(&quot;click&quot;, function()&#123; $.ajax(&#123; url: &quot;admin/login&quot;, type: &quot;post&quot;, data: &#123; name: $(&quot;#aName&quot;).val(), password: $(&quot;#aPassword&quot;).val() &#125;, success: function(res)&#123; alert(res); &#125; &#125;);&#125;);&lt;/script&gt;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"「Spring」持久层框架Mybatis","date":"2018-05-06T14:51:34.000Z","path":"2018/05/06/spring-mybatis.html","text":"Mybatis支持普通sql操作，存储过程的调用，它是一个高级的ORM框架(Object Relation Mapping对象关系映射–以面向对象思想访问数据库)，是一个基于Java的持久层框架。 MyBatis封装了几乎所有的JDBC操作和参数的手工设置，它会对结果集自动封装成对象，以及直接把对象存入数据库，甚至可以做到对象与对象的关系维护；诸如：建立连接、操作 Statment、ResultSet，处理 JDBC 相关异常等等都可以交给 MyBatis 去处理，我们的关注点于是可以就此集中在 SQL 语句上，关注在增删改查这些操作层面上。 1. Mybatis框架的构成 实体类 ： 封装记录信息（JavaBean） SQL定义文件 ：定义sql语句（编写SQL语句的XML） 主配置文件 ：定义连接信息、加载SQL文件 以及其他设置的XML 框架API ：用于实现数据库增删改查操作（主要通过SqlSession） 2. 使用Mybatis访问数据库以员工表Emp(id,name,salary)为例 准备数据库及创建项目（需要mybatis的jar包和数据库驱动包） 根据表建立对应的实体类：Emp(id,name,salary) 在「src」目录下创建 MyBaits 的主配置文件 mybatis-config.xml ，其主要作用是提供连接数据库用的驱动，数据名称，编码方式，账号密码等 123456789101112131415161718&lt;configuration&gt; &lt;environments default=&quot;environment&quot;&gt; &lt;environment id=&quot;environment&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot; /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=&quot;com/mapper/EmpMapper.xml&quot; /&gt; &lt;/mappers&gt;&lt;/configuration&gt; 在「src」包路径下创建配置文件（com&#x2F;mapper&#x2F;EmpMapper.xml）,然后根据需求定义sql 1234567891011&lt;mapper namespace=&quot;com.mapper.EmpMapper&quot;&gt; &lt;!-- 定义SQL语句 --&gt; &lt;select id=&quot;findById&quot; parameterType=&quot;int&quot; resultType=&quot;com.mapper.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125; &lt;/select&gt; &lt;select id=&quot;findByName&quot; parameterType=&quot;String&quot; resultType=&quot;com.mapper.Emp&quot;&gt; select * from emp32 where name = #&#123;name&#125; &lt;/select&gt;&lt;/mapper&gt; parameterType：要求输入参数的类型 resultType：输出的类型 封装工具类获取SQLSession 123456789101112131415public class SqlSessionUtil &#123; public static SqlSessionFactory ssf; static &#123; // 先构建SQLSession工厂构建器 SqlSessionFactoryBuilder ssfb = new SqlSessionFactoryBuilder(); // 构建SqlSessionFactory关联主配置文件 InputStream inputStream = SqlSessionUtil.class.getClassLoader().getResourceAsStream(&quot;mybatis-config.xml&quot;); ssf = ssfb.build(inputStream); &#125; // 获取SQLSession public static SqlSession getSqlSession() &#123; // 通过SqlSession 工厂对象 来获取SqlSession return ssf.openSession(); &#125;&#125; 编写测试类 1234567public class EmpTest &#123; public static void main(String[] args) &#123; SqlSession ss =SqlSessionUtil.getSqlSession(); Emp emp = ss.selectOne(&quot;findById&quot;, 6); System.out.println(emp); &#125;&#125; 基本原理 应用程序找 MyBatis 要数据 MyBatis 从数据库中找来数据 通过 mybatis-config.xml 定位哪个数据库 通过 EmpMapper.xml 执行对应的 sql 语句 基于 EmpMapper.xml 把返回的数据库封装在 Emp 对象中 返回一个 Emp 对象 3. Mybatis的CRUD操作以员工表Emp(id,name,salary)为例 第一步：配置EmpMapper.xml 123456789101112131415&lt;insert id=&quot;insertEmp&quot; parameterType=&quot;com.mapper.Emp&quot;&gt; insert into emp32(name, salary) values(#&#123;name&#125;, #&#123;salary&#125;)&lt;/insert&gt;&lt;delete id=&quot;deleteEmpById&quot; parameterType=&quot;int&quot;&gt; delete from emp32 where id=#&#123;id&#125;&lt;/delete&gt;&lt;update id=&quot;updateEmpById&quot; parameterType=&quot;com.mapper.Emp&quot;&gt; update emp32 set name=#&#123;name&#125; where id=#&#123;id&#125;&lt;/update&gt;&lt;select id=&quot;findById&quot; parameterType=&quot;int&quot; resultType=&quot;com.mapper.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125;&lt;/select&gt;&lt;select id=&quot;findAll&quot; resultType=&quot;com.mapper.Emp&quot;&gt; select * from emp32&lt;/select&gt; parameterType：要求输入参数的类型 resultType：输出的类型 第二步：SQLSession实现增删改查 123456789101112131415161718192021// 先构建SQLSession工厂构建器SqlSessionFactoryBuilder ssfb = new SqlSessionFactoryBuilder();// 构建SqlSessionFactory关联主配置文件InputStream inputStream = EmpTest.class.getClassLoader().getResourceAsStream(&quot;sqlmap-config.xml&quot;);SqlSessionFactory ssf = ssfb.build(inputStream);// 通过SqlSession 工厂对象 来获取SqlSessionSqlSession ss = ssf.openSession();//增加Emp emp = new Emp(0,&quot;ef2&quot;,50000);int addRows = ss.insert(&quot;insertEmp&quot;, emp);//删除int delRows = ss.delete(&quot;deleteEmpById&quot;, 12);//更新Emp emp2 = new Emp(1,&quot;hello&quot;,0);int updateRows = ss.update(&quot;updateEmpById&quot;, emp2);//查找Emp emp3 = ss.selectOne(&quot;findById&quot;, 6);List&lt;Emp&gt; empList = ss.selectList(&quot;findAll&quot;);ss.commit(); SqlSession对象的操作方法如下： insert(..) 插入操作 update(..) 更新操作 delete(..) 删除操作 selectOne(..) 单行查询操作 selectList(..) 多行查询操作 通过 session.commit() 来提交事务，也可以简单理解为更新到数据库 4. Mapper映射器使用规则： 接口的方法名和SQL定义文件中的id保持一致 接口方法的返回值类型 要和resultType 保持一致 单行：resultType 多行：List&lt;resultType&gt; 增删改返回值，推荐int，也可以是void 接口方法参数和parameterType保持 一致，如果没有parameterType则参数任意 SQL定义文件中的namespace必须包名.接口名 5. 向mapper传多个参数5.1 第一种方案：#{0}，#{1} &#x2F; #{param1} 和 #{param2}DAO层的函数方法 1public Emp findByIdAndName(int id, String name); 对应的Mapper.xml 123&lt;select id=&quot;findByIdAndName&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where id = #&#123;0&#125; and name = #&#123;1&#125;&lt;/select&gt; 其中，#{0}代表接收的是dao层中的第一个参数，#{1}代表dao层中第二参数，更多参数一致往后加即可。也可以用#{param1} 和 #{param2}实现同意效果。 5.2 第二种方案@paramDao层的函数方法 1public Emp findByIdAndName(@param(&quot;id&quot;)int id, @param(&quot;name&quot;)String name); 对应的Mapper.xml 123&lt;select id=&quot;findByIdAndName&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125; and name = #&#123;name&#125;&lt;/select&gt; 5.3 第三种方案：采用对象或Map传多参数Dao层的函数方法 12public Emp findByIdAndName(Emp emp);public Emp findByIdAndName2(Map&lt;String, Object&gt; params); 对应的Mapper.xml 1234567&lt;select id=&quot;findByIdAndName&quot; parameterType=&quot;com.bean.Emp&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125; and name = #&#123;name&#125;&lt;/select&gt;&lt;select id=&quot;findByIdAndName2&quot; parameterType=&quot;map&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where id = #&#123;id&#125; and name = #&#123;name&#125;&lt;/select&gt; 6. 结果集列名和属性名不一致的解决方法在SQL定义中，resultType属性用于指定查询数据采用哪种类型封装，规则为结果集列名和属性名一致，如果不一致将不能接收查询结果。解决方法： 使用别名，select语句使用与属性一致的别名 使用resultMap替换resultType，用resultMap指定结果集列名和属性名的对应关系 123456789101112131415161718&lt;!-- 定义resultMap将sql 结果集列名(数据库中的字段)和Emp类中的属性做一个映射关系 type:resultMap最终所映射的Java对象类型，可以使用别名 id:对resultMap的唯一标识 --&gt;&lt;resultMap type=&quot;com.bean.Emp&quot; id=&quot;empMap&quot;&gt; &lt;!-- id表示查询结果集中唯一标识 column:查询出的列名 property:type所指定的类中的属性名 --&gt; &lt;id column=&quot;e_id&quot; property=&quot;id&quot;/&gt; &lt;!-- 对普通列的映射定义 --&gt; &lt;result column=&quot;salary&quot; property=&quot;sal&quot;/&gt;&lt;/resultMap&gt;&lt;!-- 使用resultMap --&gt;&lt;select id=&quot;findEmpById&quot; parameterType=&quot;int&quot; resultMap=&quot;empMap&quot;&gt; select * from emp32 where id = #&#123;id&#125;&lt;/select&gt; 7. 类型的别名和日志输出在mybatis-config.xml中自定义类型的别名 123&lt;typeAliases&gt; &lt;typeAlias alias=&quot;emp&quot; type=&quot;com.bean.Emp&quot;/&gt;&lt;/typeAliases&gt; 在EmpMapper.xml中使用别名 resultType&#x3D;”emp” 123&lt;select id=&quot;findById&quot; parameterType=&quot;int&quot; resultType=&quot;emp&quot;&gt; select id,name,salary sal from emp32 where id = #&#123;id&#125;&lt;/select&gt; 设置MyBatis的日志输出到控制台 123456&lt;settings&gt; &lt;!--设置是否允许缓存--&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; &lt;!--设置日志输出的目标--&gt; &lt;setting name=&quot;logImpl&quot; value=&quot;STDOUT_LOGGING&quot;/&gt;&lt;/settings&gt; 8. JdbcType在执行SQL时MyBatis会自动通过对象中的属性给SQL中参数赋值，它会自动将Java类型转换成数据库的类型。而一旦传入的是null它就无法准确判断这个类型应该是什么，就有可能将类型转换错误，从而报错。 所以 MyBatis 插入空值时，需要指定JdbcType，这样相对来说是比较安全的。 一般情况下，我们没有必要按个字段去识别&#x2F;判断它是否可以为空，而是将所有的字段都当做可以为空，全部手动设置转换类型。 MyBatis包含的JdbcType类型，主要有下面这些： BIT、FLOAT、CHAR 、TIMESTAMP 、 OTHER 、UNDEFINEDTINYINT 、REAL 、VARCHAR 、BINARY 、BLOB NVARCHAR、SMALLINT 、DOUBLE 、LONGVARCHAR 、VARBINARY 、CLOB、NCHAR、INTEGER、 NUMERIC、DATE 、LONGVARBINARY 、BOOLEAN 、NCLOB、BIGINT 、DECIMAL 、TIME 、NULL、CURSOR 123&lt;select id=&quot;findByName&quot; parameterType=&quot;String&quot; resultType=&quot;com.bean.Emp&quot;&gt; select * from emp32 where name = #&#123;name, jdbcType=VARCHAR&#125;&lt;/select&gt; 9. Mabatis中#{}和${}的区别 $&#123;&#125;是字符串替换，底层使用的Statement（sql注入问题，效率低，编写sql复杂） 支持${param1}或${变量名},不支持${0}，Dao层必须使用@Param(),用到字符串时需要手动加单引号 #&#123;&#125;是预编译处理命令，底层使用PreparedStatement（可以有效防止sql注入） 不支持表名、排序方式等的占位，默认会将其当成字符串 10. 分页 在主配置文件中配置 分页拦截器（依赖于pageHelper、sqlparse相关jar） 1234&lt;!-- 配置分页拦截器 --&gt;&lt;plugins&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageHelper&quot;&gt;&lt;/plugin&gt;&lt;/plugins&gt; 查询前使用分页API 12345PageHelper.startPage(2, 2);List&lt;Emp&gt; emps = dao.orderBySalary();for(Emp emp: emps) &#123; System.out.println(emp);&#125; 11. Spring+MyBatis整合Spring与MyBatis整合需要引入一个mybatis-spring.jar文件包，该包提供了下面几个与整合相关的API: SqlSessionFactoryBean 创建SqlSessionFactory对象，为整合应用提供SqlSession对象资源 依赖于dataSource 和加载SQL定义文件 MapperFactoryBean 根据指定的某一个Mapper接口生成Bean实例 依赖于SqlSessionFactory 和 MApper接口 MapperScannerConfigurer 根据指定包批量扫描Mapper接口并生成实例 SqlSessionTemplate 类似于JdbcTemplate，便于程序员自己编写Mapper实现类 12. Spring+MyBatis完成sql操作第一步：使用Mybatis（同上） 导jar包(mybatis包&#x2F;数据库驱动包)，建立实体类，定义SQL文件，编写Mapper映射接口 第二步：配置SqlSessionFactoryBean 导入jar包（mabatis-spring&#x2F;ioc&#x2F;aop&#x2F;dao&#x2F;连接池） 配置SqlSessionFactoryBean注入dataSource和指定sql定义文件 123456789101112&lt;!-- 配置SqlSessionFactory --&gt;&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:com/mapper/*.xml&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置连接池对象 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 第三步： 方式一： 使用SqlSessionFactoryBean结合接口和SqlSessionFactory 最终产生Mapper接口的 实现类，注意这是实现类 123456789&lt;!-- 配置SqlSessionFactoryBean 产生Mapper接口的 实现类 --&gt;&lt;bean id=&quot;empDao&quot; class=&quot;org.mybatis.spring.mapper.MapperFactoryBean&quot;&gt; &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; &lt;property name=&quot;mapperInterface&quot; value=&quot;com.dao.EmpDao&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;empDao2&quot; class=&quot;org.mybatis.spring.mapper.MapperFactoryBean&quot;&gt; &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; &lt;property name=&quot;mapperInterface&quot; value=&quot;com.dao.EmpDao2&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 方式二： MapperScannerConfigurer MapperFactoryBean一次只能生产一个DAO的实现类，可以通过MapperScannerConfigurer批量生产DAO接口实现类 1234567&lt;!-- 批量生产DAO接口实现类 ,实现类id为类名首字母小写 --&gt;&lt;bean id=&quot;mapperScanner&quot; class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;!-- &lt;property name=&quot;sqlSessionFactory&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; --&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.dao&quot;&gt;&lt;/property&gt; &lt;!-- 自定义注解可以让只让有注解的接口产生实现类，另一部分一部分不产生 --&gt; &lt;property name=&quot;annotationClass&quot; value=&quot;com.annotation.MyAnnotation&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 13. 使用SqlSessionTemplate模板来完成DAO接口的实现类 使用Mybatis（同上） 配置SqlSessionFactoryBean（同上） 编写DAO接口的实现类 开启组件扫描，注入SqlSessionTemplate,依赖于SqlSessionFactory 使用SqlSessionTemplate对应API完成增删改查 123456&lt;!-- 开启组件扫描 --&gt;&lt;context:component-scan base-package=&quot;com.mapper&quot;&gt;&lt;/context:component-scan&gt;&lt;!-- 创建SqlSessionTemplate --&gt;&lt;bean id=&quot;sqlSessionTemplate&quot; class=&quot;org.mybatis.spring.SqlSessionTemplate&quot;&gt; &lt;constructor-arg index=&quot;0&quot; ref=&quot;sqlSessionFactory&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 123456789@Repository(&quot;empDao&quot;)public class EmpDaoImpl implements EmpDao &#123; @Autowired private SqlSessionTemplate sqlSessionTemplate; @Override public Emp findById(int id) &#123; return sqlSessionTemplate.selectOne(&quot;findById&quot;, id); &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"「Spring」Spring MVC框架","date":"2018-04-27T14:50:22.000Z","path":"2018/04/27/spring-mvc.html","text":"Spring MVC是Spring提供的一个强大而灵活的web框架。借助于注解，Spring MVC提供了几乎是POJO的开发模式，使得控制器的开发和测试更加简单。这些控制器一般不直接处理请求，而是将其委托给Spring上下文中的其他bean，通过Spring的依赖注入功能，这些bean被注入到控制器中。 1. Spring MVC基本概念1.1 Spring MVC 五大核心组件Spring MVC主要由DispatcherServlet、处理器映射、处理器(控制器)、视图解析器、视图组成。 DispatcherServlet：控制器，请求入口 HandlerMapping：控制器，分发请求，让请求和控制器建立一一对应关系 Controller：控制器，处理请求 ModelAndView：封装了 数据信息和视图信息 ViewResolver：视图处理器 他的两个核心是两个核心： 处理器映射：选择使用哪个控制器来处理请求 视图解析器：选择结果应该如何渲染 通过以上两点，Spring MVC保证了如何选择控制处理请求和如何选择视图展现输出之间的松耦合。 1.2 SpringMVC运行原理 Http请求：客户端请求提交到DispatcherServlet。 寻找处理器：由DispatcherServlet控制器查询一个或多个HandlerMapping，找到处理请求的Controller。 调用处理器：DispatcherServlet将请求提交到Controller。 调用业务处理和返回结果：Controller调用业务逻辑处理后，返回ModelAndView。 处理视图映射并返回模型： DispatcherServlet查询一个或多个ViewResoler视图解析器，找到ModelAndView指定的视图。 Http响应：视图负责将结果显示到客户端。 1.3 SpringMVC接口解释 DispatcherServlet接口：Spring提供的前端控制器，所有的请求都有经过它来统一分发。在DispatcherServlet将请求分发给Spring Controller之前，需要借助于Spring提供的HandlerMapping定位到具体的Controller。它是整个Spring MVC的核心。它负责接收HTTP请求组织协调Spring MVC的各个组成部分。其主要工作有以下三项： 截获符合特定格式的URL请求。 初始化DispatcherServlet上下文对应WebApplicationContext，并将其与业务层、持久化层的WebApplicationContext建立关联。 初始化Spring MVC的各个组成组件，并装配到DispatcherServlet中。 HandlerMapping接口：能够完成客户请求到Controller映射。 Controller接口： 需要为并发用户处理上述请求，因此实现Controller接口时，必须保证线程安全并且可重用。 Controller将处理用户请求，这和Struts Action扮演的角色是一致的。一旦Controller处理完用户请求，则返回ModelAndView对象给DispatcherServlet前端控制器，ModelAndView中包含了模型（Model）和视图（View）。 从宏观角度考虑，DispatcherServlet是整个Web应用的控制器；从微观考虑，Controller是单个Http请求处理过程中的控制器，而ModelAndView是Http请求过程中返回的模型（Model）和视图（View）。 ViewResolver接口：Spring提供的视图解析器（ViewResolver）在Web应用中查找View对象，从而将相应结果渲染给客户。 1.4 SpringMVC配置 在web.xml文件中进行配置applicationContext.xml路径 1234567891011121314&lt;!-- 配置DispatcherServlet --&gt;&lt;servlet&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 配置applicationContext.xml，开启注解功能、配置试图解析器 123456789101112131415&lt;!-- 配置HandlerMapping --&gt;&lt;bean id=&quot;handlerMapping&quot; class=&quot;org.springframework.web.servlet.handler.SimpleUrlHandlerMapping&quot;&gt; &lt;property name=&quot;mappings&quot;&gt; &lt;props&gt; &lt;prop key=&quot;/toHello.do&quot;&gt;helloController&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 控制器对象 --&gt;&lt;bean id=&quot;helloController&quot; class=&quot;com.controller.MyHelleController&quot;&gt;&lt;/bean&gt;&lt;!-- 配置视图处理器 --&gt;&lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/&quot;&gt;&lt;/property&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 2. Spring MVC的编写步骤 建立一个项目，导入jar包(ioc mvc) 拷贝spring配置文件到src下，同时在WEB-INF下建立jsp文件。 在web.xml中配置DisappearServlet，并通过contextConfigLocation这个初始化参数关联Spring容器对应的配置文件。 在 Spring配置文件中配置HandlerMapping的实现类SimpleUrlHandlerMapping需要通过mappings属性指定请求和控制器对应的关系。 编写一个类实现Controller接口，实现接口方法，返回ModelAndView，并且在容器创建Controller对象 在Spring配置文件中配置ViewResolver的实现类InternalResourceViewResolver，需要配置前缀prefix和后缀suffix。 3. 标注(注解)形式的MVC 建立项目，导入jar(ioc aop mvc)，拷贝spring配置文件到src下，同时在WEB-INF下建立jsp文件。 在web.xml中配置DispatcherServlet，并通过contextConfigLocation关联配置文件。 开启组件扫描 和 标注形式mvc (容器帮你创建了一个HandlerMapping对象，类型时RequestMappingHandlerMapping)。 12&lt;context:component-scan base-package=&quot;包名&quot; /&gt;&lt;mvc:annotation-driven /&gt; 编写一个Java类，不用实现Controller接口，方法返回值类型可以时String也可以是ModelAndView（方法名与参数都自由了） 使用@Controller 可以把普通Java类转换成控制器，同时在容器中创建对象 使用@RequestMapping(&quot;/路径&quot;) 设置方法上 在Spring配置文件中配置ViewResolver的实现类InternalResourceViewResolver，需要配置前缀prefix和后缀suffix。 4. mvc控制器接收页面参数 使用HttpServletRequest类型的参数来接收 123456@RequestMapping(&quot;/login.do&quot;)public String login(HttpServletRequest request) &#123; String acc_no = request.getParameter(&quot;acc_no&quot;); String acc_pwd = request.getParameter(&quot;acc_password&quot;); return &quot;main&quot;;&#125; 直接定义和页面请求参数同名的控制器参数 12345@RequestMapping(&quot;/login2.do&quot;)public ModelAndView login2(String acc_no,String acc_password, ModelAndView mav) &#123; mav.setViewName(&quot;main&quot;); return mav;&#125; 当页面参数和控制器参数名字不一致，@RequestParam(“acc_no”) 让请求参数和控制器参数对应 12345@RequestMapping(&quot;/login3.do&quot;)public ModelAndView login3(@RequestParam(&quot;acc_no&quot;) String a,String acc_password, ModelAndView mav) &#123; mav.setViewName(&quot;main&quot;); return mav;&#125; 控制器中 直接定义对象类型的参数 12345@RequestMapping(&quot;/login4.do&quot;)public ModelAndView login4(Account acc, ModelAndView mav) &#123; mav.setViewName(&quot;main&quot;); return mav;&#125; 5. mvc控制器把数据传递给页面使用EL表达式在jsp页面接收数据&lt;h1&gt;欢迎 $&#123;acc_no&#125; &lt;/h1&gt; 使用域对象 进行传输 (request session ServletContext ) 12345@RequestMapping(&quot;/login6.do&quot;)public String login6(String acc_no, HttpServletRequest req) &#123; req.setAttribute(&quot;acc_no&quot;, acc_no); return &quot;main&quot;;&#125; 使用ModelAndView进行数据传输 mav.getModel().put(&quot;acc_no&quot;, acc_no); mav.getModelMap().put(key, value); mav.getModelMap().addAttribute(&quot;acc_no&quot;, acc_no); 12345678@RequestMapping(&quot;/login7.do&quot;)public ModelAndView login7(String acc_no, ModelAndView mav) &#123; mav.setViewName(&quot;main&quot;); //mav.getModel().put(&quot;acc_no&quot;, acc_no); //mav.getModelMap().put(key, value) mav.getModelMap().addAttribute(&quot;acc_no&quot;, acc_no); return mav;&#125; 使用Model进行数据传输 12345@RequestMapping(&quot;/login8.do&quot;)public String login8(String acc_no, Model m) &#123; m.addAttribute(&quot;acc_no&quot;, acc_no); return &quot;main&quot;;&#125; 使用ModelMap进行数据传输 123456@RequestMapping(&quot;/login9.do&quot;)public String login9(String acc_no, ModelMap m) &#123; //m.addAttribute(&quot;acc_no&quot;, acc_no); m.put(&quot;acc_no&quot;, acc_no); return &quot;main&quot;;&#125; 使用自定义的对象类型默认传输（默认名类型首字母小写，可以通过@ModelAttribute(“新名”)修改） 默认名：&lt;h1&gt;欢迎 $&#123; account.acc_no &#125; &lt;/h1&gt; @ModelAttribute(“acc”)：&lt;h1&gt;欢迎 $&#123; acc.acc_no &#125; &lt;/h1&gt; 1234@RequestMapping(&quot;/login10.do&quot;)public String login10(@ModelAttribute(&quot;acc&quot;) Account acc) &#123; return &quot;main&quot;;&#125; 6. Spring MVC实现重定向 控制器方法返回String redirect:请求路径 12345678910@RequestMapping(&quot;/login11.do&quot;)public String login11(@ModelAttribute(&quot;acc&quot;) Account acc) &#123; //return &quot;forward:toMain.do&quot;; return &quot;redirect:toMain.do&quot;;&#125;@RequestMapping(&quot;/toMain.do&quot;)public String toMain() &#123; // 干其它的事情 return &quot;main&quot;;&#125; 控制器方法返回ModelAndView 使用RedirectView 完成 12345678@RequestMapping(&quot;/login12.do&quot;)public ModelAndView login12(@ModelAttribute(&quot;acc&quot;) Account acc) &#123; ModelAndView mav = new ModelAndView(); //重定向 RedirectView rv = new RedirectView(&quot;toMain.do&quot;); mav.setView(rv); return mav;&#125; 7. Spring MVC 中文参数的乱码问题tomcat8中 get 没有乱码问题，post 请求有乱码问题 参数为页面(HttpServletRequest request)与(HttpServletResponse response)时 12request.setCharacterEncoding(&quot;UTF-8&quot;);response.setContentType(&quot;application/json;charset=UTF-8&quot;); 传入参数为@RequestParam时，可以通过字符串重新编码来解决 1new String(string.getBytes(&quot;ISO-8859-1&quot;),&quot;UTF-8&quot;); 方法名前出现@RequestMapping(value=&quot;XXX&quot;)时可以在value属性后再加一个属性produces=&quot;text/html;charset=UTF-8&quot;来解决 在web.xml或者dispatcher-servlet.xml或者其他配置servlet的配置文件中添加编码过滤器 123456789101112&lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 8. Spring MVC 拦截器 拦截器和fiter的作用几乎一样，它是Spring提供的一个组件，可以用在HandlerMapping组件之后（用于身份认证，登录检查，编码设置） HandlerMapping接口 preHandle：在HandlerMapping之后控制器之前调用，返回boolean(true:继续其他拦截器和处理器，false:终止后续调用)。 postHandle：处理器执行后、视图处理前调用。 afterCompletion：整个请求处理完毕后调用。 9. Spring MVC 拦截器的使用步骤 搭建一个基于标注的mvc 编写一个类实现HandlerInterceptor接口 在Spring配置文件中配置拦截器1234567&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;mvc:exclude-mapping path=&quot;/login.do&quot;/&gt; &lt;bean class=&quot;com.xdl.interceptor.SomeInterceptor&quot;/&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 10. Spring MVC异常处理 配置spring系统提供的简单异常处理器 SimpleMappingExceptionResolver 处理所有Controller异常 12345678&lt;bean id=&quot;simpleExceptionResolver&quot; class=&quot;org.springframework.web.servlet.handler.SimpleMappingExceptionResolver&quot;&gt; &lt;property name=&quot;exceptionMappings&quot;&gt; &lt;props&gt; &lt;prop key=&quot;java.lang.RuntimeException&quot;&gt;error&lt;/prop&gt; &lt;prop key=&quot;java.lang.Exception&quot;&gt;error2&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 自定义异常处理器，实现HandlerExceptionResolver接口，处理所有Controller异常 12345678910111213@Controllerpublic class MyExceptionResolver implements HandlerExceptionResolver &#123; @Override public ModelAndView resolveException(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, Exception e) &#123; ModelAndView mav = new ModelAndView(); if(e instanceof RuntimeException) &#123; mav.setViewName(&quot;error&quot;); &#125;else if(e instanceof Exception) &#123; mav.setViewName(&quot;error2&quot;); &#125; return mav; &#125;&#125; 使用@ExceptionHandler注解实现异常处理，处理某一个Controller异常public String execute(HttpServletRequest request, Exception ex) 1234567//@Controller//public class MyController &#123;@ExceptionHandlerpublic String processException(Exception e) &#123; System.out.println(e.getMessage()); return &quot;error3&quot;;&#125; 11. Spring MVC文件上传 jsp页面（method&#x3D;”POST” enctype&#x3D;”multipart&#x2F;form-data type&#x3D;”file”）1234&lt;form action=&quot;upload.do&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 头像：&lt;input type=&quot;file&quot; name=&quot;head_img&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;&gt;&lt;br&gt;&lt;/form&gt; 控制器（MultipartFile类型来接收文件数据，需要配置文件解析器-需要依赖文件上传jar包-commons包）123&lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt;&lt;/bean&gt; 12. 文件上传与异常处理的结合12345678910111213141516171819202122232425262728293031323334@Controllerpublic class fileController &#123; @RequestMapping(&quot;/toFile.do&quot;) public String tofile() &#123; return &quot;file&quot;; &#125; @RequestMapping(&quot;/upload.do&quot;) public String upload(String acc_no, MultipartFile head_img) &#123; System.out.println(&quot;acc_no:&quot; + acc_no ); if(head_img.getSize()&gt;1024*10) &#123; throw new RuntimeException(&quot;文件过大！&quot;); &#125; // 把文件写入磁盘 String uniqueStr = UUID.randomUUID().toString(); String oriFilename = head_img.getOriginalFilename(); String suffix = oriFilename.substring(oriFilename.lastIndexOf(&quot;.&quot;)); File file = new File(&quot;F:/Eclipse/datas/&quot;+uniqueStr+suffix); try &#123; head_img.transferTo(file); &#125; catch (IllegalStateException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; System.out.println(oriFilename); System.out.println(file); return &quot;file&quot;; &#125; /** 局部异常 */ @ExceptionHandler public String processError(Exception e) &#123; return &quot;error4&quot;; &#125;&#125; 13. Spring MVC响应JSON 搭建基于标注的mvc 在控制器中，设计控制方法，控制方法返回值数据类型对应的对象转换为JSON 给方法加@RequestMapping(“&#x2F;请求路径”)、@ResponseBody，它能把Java对象转换为JSON直接返回，依赖json转换包 14. RESTREST即表述性状态传递（Representational State Transfer），使用这种软件架构风格，可以降低开发的复杂性，提高系统的可伸缩性，便于分布式应用的开发。 REST两个核心规范 url请求路径的格式，由原来的基于操作的设计改变了基于资源的设计（如:http://test/source/1234） 对http请求的方式做了规范，GET代表查询，POST增加，DELETE删除，PUT更新 restful 符合REST设计规范和风格的应用程序或设计 就是RESTful Spring MVC对REST的支持 @RequestMapping支持URI的模板，以及http请求方式设定的支持 @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;,method=RequestMethod.POST) 对URI上路径变量的处理的支持，@PathVariable @PathVariable(&quot;id&quot;) int id rest请求路径是没有后缀的，需要把url-parttern修改成/ &lt;servlet-mapping&gt;&lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt;&lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 需要对静态资源进行放行&lt;mvc:default-servlet-handler/&gt; 15. REST实例 配置web.xml与applicationContext.xml(部分配置) 12345678910&lt;!-- 修改rest请求路径 --&gt;&lt;!-- web.xml --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;!-- 对静态资源进行放行 --&gt;&lt;!-- applicationContext.xml --&gt; &lt;mvc:default-servlet-handler/&gt; 编写控制类 12345678910111213141516171819202122232425262728293031323334353637383940@Controllerpublic class AccountController &#123; @RequestMapping(&quot;/toLogin.do&quot;) public String toLogin() &#123; return &quot;login&quot;; &#125;/** 根据id查询账户 GET */ @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;, method=RequestMethod.GET) @ResponseBody public Account getAccountById(@PathVariable(&quot;id&quot;) int id) &#123; Random rm = new Random(); Account acc = new Account(id, &quot;test&quot;+rm.nextInt(100),&quot;123&quot;, rm.nextInt(999)+1000); return acc; &#125;/** 新增账户 POST */ @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;,method=RequestMethod.POST) @ResponseBody public boolean addAccount(Account acc) &#123; System.out.println(&quot;add:&quot;+acc); if(acc.getId()&gt;100) return true; return false; &#125;/** 根据id删除帐户对象 DELETE */ @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;,method=RequestMethod.DELETE) @ResponseBody public boolean deleteAccountById(@PathVariable(&quot;id&quot;) int id) &#123; System.out.println(&quot;delete:&quot;+id); if(id&gt;100) return true; return false; &#125;/** 根据id更新帐户 PUT */ @RequestMapping(value=&quot;/account/&#123;id&#125;&quot;,method=RequestMethod.PUT) @ResponseBody public boolean putAccount(@RequestBody Account acc) &#123; //@RequestBody将接收的ajax请求的json字符串写入Account对象中 System.out.println(&quot;update:&quot;+acc); if(acc.getId()&gt;100) return true; return false; &#125;&#125; 编写jsp页面 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;form&gt; &lt;p&gt;ID：&lt;input id=&quot;accountId&quot;&gt;&lt;/p&gt; &lt;p&gt;姓名：&lt;input id=&quot;accountNo&quot;&gt;&lt;/p&gt; &lt;p&gt;密码：&lt;input id=&quot;accountPassword&quot;&gt;&lt;/p&gt; &lt;p&gt;金额：&lt;input id=&quot;accountMoney&quot;&gt;&lt;/p&gt; &lt;button id=&quot;findBtn&quot; type=&quot;button&quot;&gt;查询&lt;/button&gt; &lt;button id=&quot;addBtn&quot; type=&quot;button&quot;&gt;添加&lt;/button&gt; &lt;button id=&quot;updateBtn&quot; type=&quot;button&quot;&gt;更新&lt;/button&gt; &lt;button id=&quot;delBtn&quot; type=&quot;button&quot;&gt;删除&lt;/button&gt;&lt;/form&gt;&lt;script src=&quot;js/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;$(&quot;#findBtn&quot;).on(&quot;click&quot;, function()&#123; findAccount();&#125;);$(&quot;#addBtn&quot;).on(&quot;click&quot;, function()&#123; addAccount();&#125;);$(&quot;#updateBtn&quot;).on(&quot;click&quot;, function()&#123; updateAccount();&#125;);$(&quot;#delBtn&quot;).on(&quot;click&quot;, function()&#123; delAccount();&#125;);function getDatas()&#123; var accountId = $(&quot;#accountId&quot;).val(); var accountNo = $(&quot;#accountNo&quot;).val(); var accountPassword = $(&quot;#accountPassword&quot;).val(); var accountMoney = $(&quot;#accountMoney&quot;).val(); return &#123; id: accountId, acc_no: accountNo, acc_password: accountPassword, acc_money: accountMoney &#125;;&#125;function findAccount()&#123; var datas = getDatas(); $.ajax(&#123; url: &quot;account/&quot; + datas.id, type: &quot;get&quot;, success: function(res)&#123; $(&quot;#accountNo&quot;).val(res.acc_no); $(&quot;#accountPassword&quot;).val(res.acc_password); $(&quot;#accountMoney&quot;).val(res.acc_money); &#125;, &#125;);&#125;function addAccount()&#123; var datas = getDatas(); $.ajax(&#123; url: &quot;account/&quot; + datas.id, type: &quot;post&quot;, data: datas, success: function(res)&#123; alert(res); &#125;, &#125;);&#125;function delAccount()&#123; var datas = getDatas(); $.ajax(&#123; url: &quot;account/&quot; + datas.id, type: &quot;delete&quot;, success: function(res)&#123; alert(res); &#125;, &#125;);&#125;function updateAccount()&#123; var datas = getDatas(); $.ajax(&#123; url:&quot;account/&quot;+ datas.id, type:&quot;put&quot;, data:JSON.stringify(datas), contentType:&quot;application/json&quot;,//以json字符串提交数据 success: function(res)&#123; alert(res); &#125;, &#125;);&#125;&lt;/script&gt; 注意： PUT需要以json字符串提交数据contentType:&quot;application/json&quot; @RequestBody将接收的ajax请求的json字符串写入Account对象中 JSON.stringify()：将json对象转换为json字符串 JSON.parse()：将json字符串转换为json对象","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"「Spring」面向切面编程(AOP模块)","date":"2018-04-21T14:48:36.000Z","path":"2018/04/21/spring-aop.html","text":"AOP（Aspect Oriented Programming）：面向切面编程，它是面向对象基础上发展来的技术，是面向对象更高层次的应用，它可以在不修改原有代码的情况给组件增强功能。 1. AOP涉及到的概念 Aspect：切面，用来封装共通业务逻辑；其类叫切面类，其创建的对象叫切面对象。 JoinPoint：连接点，用来封装切面所要嵌入的位置信息的对象，（主要封装了方法信息） Pointcut：切点，是一堆连接点的集合，后面会使用切点表达式来表述切点 Target：目标，要被切入共通业务逻辑的对象 Proxy：代理，被增强之后的目标对象就是代理 Advice：通知，时机，切面逻辑在目标方法执行之前调用，执行之后调用，目标方法前后，目标方法最终，目标方法出现异常 2. 编写AOP程序步骤 编写一个Sevice类，里面有登录和注册两个方法，然后使用Spring容器获取Service类对应的对象，调用登录和注册方法 在不修改登录和注册原有代码的情况下，让两个方法调用前输出****** 添加aop的jar包到lib 编写一个类，定义共同业务逻辑 配置aplicationContext.xml，创建切面对象 配置aop:config，切面–&gt;通知–&gt;切点 3. 切点表达式 Bean限定表达式 bean(&quot;容器内组件id&quot;)，支持通配符*，如：bean(&quot;*Dao&quot;)，bean(&quot;acc*&quot;) 类型限定表达式 within(&quot;包名.类型&quot;)，要求表达式最后一部分必须是类型，如：com.dao.impl.类型，com.dao.impl.*，com.dao..* 方法限定表达式 execution(&quot;表达式&quot;)，可以有 权限修饰 返回值类型 方法名(参数类型)throws 异常，必须有:返回值类型 方法名() 4. 通知的五种类型 &lt;aop:before：前置通知，目标方法执行之前调用 &lt;aop:after-returning：后置通知，目标方法执行之后调用（目标方法出异常，通知方法无法执行） &lt;aop:after-throwing：异常通知，目标方法出异常才调用 &lt;aop:after：最终通知，目标方法之后一定会执行 &lt;aop:around：环绕通知，目标方法执行前后都调用 5. 标注形式AOP步骤 建项目，添加jar包(ioc,aop)，src下添加配置文件 编写一个Sevice类，里面有登录和注册两个方法 开启组件扫描，在类上打对应标注，创建Spring容器 测试逻辑 定义一个切面类，定义切面方法，并在容器中使用标注@Component创建切面对象 开启标注形式aop：&lt;aop:aspectj-autoproxy proxy-target-class=&quot;true|false&quot; /&gt; 使用切面对应的标注以及通知对应的标注结合切点表达式完成aop： @Aspect，@Before... 6. AOP 通知对应的标注 @Before：前置通知，目标方法执行之前调用 @AfterReturning：后置通知，目标方法执行之后调用（目标方法出异常，通知方法无法执行） @AfterThrowing：异常通知，目标方法出异常才调用 @After：最终通知，目标方法之后一定会执行 @Around：环绕通知，目标方法执行前后都调用 7. @Around具体用法@Around既可以在目标方法之前织入增强动作，也可以在执行目标方法之后织入增强动作； 12345678@Around(&quot;within(com..*)&quot;)public Object showAfterDate(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println(&quot;开始时间：&quot; + new Date().getTime()); Object obj = pjp.proceed(); System.out.println(&quot;结束时间：&quot; + new Date().getTime()); System.out.println(&quot;执行时间：&quot;date2.getTime() - date.getTime()); return obj;&#125; 虽然Around功能强大，但通常需要在线程安全的环境下使用。因此，如果使用普通的Before、AfterReturing增强方法就可以解决的事情，就没有必要使用Around增强处理了。 8. 异常通知JoinPoint可以获取出异常的方法 1234@AfterThrowing(value=&quot;within(com..*)&quot;, throwing=&quot;e&quot;)public void processException(JoinPoint jp, Exception e) &#123; System.out.println(&quot;捕获到异常&quot; + jp.getSignature() + &quot;:\\n「&quot; + e +&quot;」&quot;);&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"「Spring」事务管理","date":"2018-04-15T14:47:30.000Z","path":"2018/04/15/spring-transaction.html","text":"事务的基本概念：事务指的是逻辑上的一组操作，这组操作要么全部成功，要么全部失败。 1. 事务的特性(ACID) 事务的特性：原子性、一致性、隔离性、持久性。 原子性（Atomicity）：事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency）：事务前后数据的完整性必须保持一致。 隔离性（Isolation）：多个用户并发访问数据库时，一个用户的事务不能被其他用户的事务所干扰，多个并发事务之间数据要相互隔离（数据库中相应的数据隔离级别，通过它避免事务间的冲突）。 持久性（Durability）:一个事务一旦被提交，它对数据库中数据的改变是永久性的，即使数据库发生故障也不应该对其有任何影响。 2. Spring提供事务管理的3个接口： PlatformTransactionManager：事务管理器，用来管理事务的接口，定义了事务的提交、回滚等方法。 TransactionDefinition：事务定义信息（隔离级别、传播行为、是否超时、是否只读）。 TransactionStatus：事务具体运行状态（事务是否提交，事务是否有保存点，事务是否是新事物等状态）。 Spring事务管理时，这三个接口是有联系的，Spring首先会根据事务定义信息TransactionDefinition获取信息,然后由事务管理器PlatformTransactionManager进行管理，在事务管理过程中，会产生一个事务的状态，这个状态就保存在事务具体运行状态TransactionStatus中了。 3. TransactionDefinition接口TransactionDefinition定义事务隔离级别(Isolation)、定义事务传播行为(Propagation) 如果不考虑隔离性,就会引发安全问题：脏读、不可重复读、以及虚读或者叫做幻读。 事务的传播行为：解决业务层方法之间相互调用时,使用何种事务的问题。 3.1 安全问题 脏读：一个事务读取了另一个事务改写但还未提交的数据，如果这些数据被回滚，则读到的数据是无效的。 不可重复读：同一事务中，多次读取同一数据返回的结果有所不同（读取到另一个事务已经提交的更新的数据）。 幻读：一个事务读取了几行记录后，另一个事务插入一些记录，幻读就发生了。再后来的查询中，第一个事务就会发现有些原来没有的记录。 3.2 事务的隔离级别(Isolation)： READ_UNCOMMITED(读未提交)：允许读取未提交的改变了的数据（最低级别），可能导致脏读、不可重复读、幻读等。 READ_COMMITED(读提交)：允许在并发事务提交后读取，可防止脏读，但可能导致不可重复读、幻读。 REPEATABLE_READ(可重复读)：多次读取相同字段是一致的,除非数据被事务本身改变，可防止脏读、不可重复读，但可能导致幻读。 SERIALIZABLE(序列化)：事务是串行的,完全服从ACID的级别隔离，确保不发生脏读、不可重复读、幻读等。这在所有的隔离基本中是最慢的，它是典型的通过完全锁定在事务中涉及的数据表来完成的。 DEFAULT(Spring提供)：使用数据库默认的隔离级别（Mysql默认采用REPEATABLE_READ隔离级别，Oracle默认采用READ_COMMITTED隔离级别）。 3.3 事务的传播特性(Propagation)： 第一类：运行在同一个事务 **REQUIRED**：默认，支持当前事务，如果当前没有事务，就新建一个事务。 SUPPORTS：支持当前事务，如果当前没有事务，就不使用事务(以非事务方式执行) MANDATORY：支持当前事务，如果当前没有事务，就抛出异常 第二类：运行在不同事务 **REQUIRES_NEW**：新建事务，如果当前存在事务，把当前事务挂起 NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起 NEVER：以非事务方式执行，如果当前存在事务，则抛出异常 第三类：嵌套执行–即外层事务如果失败，内层事务要么回滚到保存点要么回滚到初始状态 **NESTED**：如果当前事务存在，则嵌套事务执行 4. TransactionStatus接口平台事务管理器(PlatformTransactionManager)会根据TransactionDefinition中定义的事务信息(包括隔离级别、传播行为)来进行事务的管理,在管理的过程中事务可能产生了保存点或事务是新的事务等情况,那么这些信息都会记录在TransactionStatus的对象中。 5. PlatformTransactionManager接口（事务管理器）该接口有许多实现类例如：DataSourceTransactionManager、HibernateTransactionManager等。 5.1 Spring支持两种方式事务管理： 编程式事务管理 手动编写代码进行事务管理，通过TransactionTemlate手动管理事务（很少使用） 声明式事务管理 基于TransactionProxyFactoryBean的方式（很少使用） 基于AspectJ的xml方式，配置稍复杂,但清晰可见事务使用范围（经常使用） 基于注解的方式，配置简单,需要在使用事务管理的业务层类或方法添加@Transactional注解（经常使用） 6. 基于AspectJ的xml方式的声明式事务管理123456789101112131415161718192021222324&lt;!-- 配置事务管理器 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;jdbc连接池对象id&quot;/&gt;&lt;/bean&gt;&lt;!-- 配置事务的通知（事务的增强） --&gt;&lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;!-- propagation:事务传播行为 isolation:事务的隔离级别 read-only:只读 rollback-for:发生哪些异常回滚 no-rollback-for:发生哪些异常不回滚 timeout:过期信息 --&gt; &lt;tx:method name=&quot;transfer&quot; propagation=&quot;REQUIRED&quot; isolation=&quot;DEFAULT&quot; read-only=&quot;false&quot; rollback-for=&quot;&quot; timeout=&quot;&quot; no-rollback-for=&quot;&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 配置切面 --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;aop:pointcut id=&quot;pointcut1&quot; expression=&quot;execution(*cn.muke.spring.demo3.AccountService+.*(.))&quot;/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;pointcut1&quot;/&gt;&lt;/aop:config&gt; 7. 基于注解的声明式事务管理 配置事务管理器1234567&lt;!-- 1.创建一个事务管理器对象 --&gt;&lt;bean id=&quot;事务管理器id&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;jdbc连接池对象id&quot;/&gt;&lt;/bean&gt;&lt;!-- 2.开启声明式事务 --&gt;&lt;tx:annotation-driven transaction-manager=&quot;事务管理器id&quot; proxy-target-class=&quot;true|false&quot; /&gt; transaction-manager：指定事务管理器(由框架提供类，在容器中创建这个对象并依赖于dataSource) proxy-target-class：决定是基于接口的还是基于类的代理被创建；为true则是基于类的代理将起作用(需要cglib库)，为false(默认)则标准的JDK 基于接口的代理将起作用。 使用，在类上或者方法上标注@Transactional123456@Transactional( rollbackFor=&#123;Exception.class&#125;, readOnly=false, isolation=Isolation.DEFAULT, propagation=Propagation.REQUIRED)public void transfer()&#123;..&#125; @Transactional的属性 rollbackFor：设置检查异常也回滚 noRollbackFor：指定运行时异常不回滚 readOnly： 只读属性，当事务方法都是select语句时，可以将readOnly设置成true优化方法，提高方法执行效率。当有DML操作时这个属性必须时false。 isolation：事务的隔离级别(枚举:DEFAULT,READ_UNCOMMITTED,READ_COMMITTED,REPEATABLE_READ,SERIALIZABLE) propagation：事务的传播特性(枚举:REQUIRED,SUPPORTS,MANDATORY,REQUIRES_NEW,NOT_SUPPORTED,NEVER) Spring中事务管理器默认值针对运行时异常回滚，对检查异常不回滚。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"「Spring」JDBC详解","date":"2018-04-09T14:46:07.000Z","path":"2018/04/09/spring-jdbc.html","text":"Spring对JDBC做了简化和封装；简化了DAO实现类编写；提供了基于AOP的声明式事务管理；对JDBC中异常做了封装，把原来检查异常封装成了继承自RuntimeException的异常（DataAcessException）。 1. 数据源配置1234567891011121314151617@Configuration@ComponentScan(&quot;com.jdbc&quot;)public class MyConfiguration &#123; @Bean public DataSource mysqlDataSource() &#123; BasicDataSource dataSource = new BasicDataSource(); dataSource.setDriverClassName(&quot;com.mysql.cj.jdbc.Driver&quot;); dataSource.setUrl(&quot;jdbc:mysql://localhost:3306/test&quot;); dataSource.setUsername(&quot;root&quot;); dataSource.setPassword(&quot;123456&quot;); return dataSource; &#125; @Bean public JdbcTemplate jdbcTemplate() &#123; return new JdbcTemplate(mysqlDataSource()); &#125;&#125; 也可以使用XML配置来实现配置效果： 12345678910111213&lt;!-- 配置连接池对象 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 定义jdbcTemplate对象 --&gt;&lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;constructor-arg index=&quot;0&quot; ref=&quot;dataSource&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- 开启组件扫描 --&gt;&lt;context:component-scan base-package=&quot;com.jdbc&quot;&gt;&lt;/context:component-scan&gt; 2. JdbcTemplate的使用JdbcTemplate模板是Spring JDBC模块中主要的API，它提供了常见的数据库访问功能。JdbcTemplate类执行SQL查询、更新语句和存储过程调用，执行迭代结果集和提取返回参数值。 基本的查询： 1234567891011//DAO实现类@Repository(&quot;empDao&quot;)public class EmpDaoImpl implements EmpDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public int getCount() &#123; String sql = &quot;select count(*) from emp32&quot;; return jdbcTemplate.queryForObject(sql, Integer.class); &#125;//... 3. 通过实现RowMapper接口把查询结果映射到Java对象12345678910public class EmpRowMapper implements RowMapper&lt;Emp&gt; &#123; @Override public Emp mapRow(ResultSet rs, int n) throws SQLException &#123; return new Emp( rs.getInt(&quot;id&quot;), rs.getString(&quot;name&quot;), rs.getDouble(&quot;salary&quot;) ); &#125;&#125; 1234567891011//DAO实现类@Repository(&quot;empDao&quot;)public class EmpDaoImpl implements EmpDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public Emp getEmpById(int id) &#123; String sql = &quot;select * from emp32 where id=?&quot;; return jdbcTemplate.queryForObject(sql, new EmpRowMapper(), id); &#125;//... 4. JdbcTemplate对象的主要方法 queryForInt()： 12345//查询一个整数类型int count = jdbcTemplateObject.queryForInt(&quot;select count(*) from emp32&quot;);//一个使用绑定变量的简单查询int age = jdbcTemplateObject.queryForInt(&quot;select age from emp32 where id = ?&quot;, new Object[]&#123;10&#125;); queryForLong()： 12//查询一个 long类型long count = jdbcTemplateObject.queryForLong(&quot;select count(*) from emp32&quot;); queryForObject()： 1234567891011//查询字符串String SQL = &quot;select name from emp32 where id = ?&quot;;String name = jdbcTemplateObject.queryForObject(SQL, new Object[]&#123;10&#125;, String.class);//查询并返回一个对象：String SQL = &quot;select * from emp32 where id = ?&quot;;emp32 student = jdbcTemplateObject.queryForObject(SQL, new Object[]&#123;10&#125;, new EmpRowMapper());//查询并返回多个对象：String SQL = &quot;select * from emp32&quot;;List&lt;emp32&gt; students = jdbcTemplateObject.query(SQL, new EmpRowMapper()); update()： 1234567891011//在表中插入一行：String SQL = &quot;insert into emp32 (name, age) values (?, ?)&quot;;jdbcTemplateObject.update( SQL, new Object[]&#123;&quot;Zara&quot;, 11&#125; );//更新表中的一行：String SQL = &quot;update emp32 set name = ? where id = ?&quot;;jdbcTemplateObject.update( SQL, new Object[]&#123;&quot;Zara&quot;, 10&#125; );//从表中删除一行：String SQL = &quot;delete emp32 where id = ?&quot;;jdbcTemplateObject.update( SQL, new Object[]&#123;20&#125; ); execute()：执行DDL语句 12345678String SQL = &quot;CREATE TABLE emp32( id INT AUTO_INCREMENT, NAME VARCHAR(30), salary DOUBLE DEFAULT 5000, CONSTRAINT student_id_pk PRIMARY KEY(id), CONSTRAINT student_name_uk UNIQUE(NAME))&quot;;jdbcTemplateObject.execute( SQL ); 5. 异常转换 Spring提供了自己的开箱即用的数据异常分层——DataAccessException作为根异常，它负责转换所有的原始异常。 所以开发者无需处理底层的持久化异常，因为Spring JDBC模块已经在DataAccessException类及其子类中封装了底层的异常。 这样可以使异常处理机制独立于当前使用的具体数据库。 除了默认的SQLErrorCodeSQLExceptionTranslator类，开发者也可以提供自己的SQLExceptionTranslator实现。 例如：自定义SQLExceptionTranslator实现的简单例子，当出现完整性约束错误时自定义错误消息： 12345678910public class CustomSQLErrorCodeTranslator extends SQLErrorCodeSQLExceptionTranslator &#123; @Override protected DataAccessException customTranslate (String task, String sql, SQLException sqlException) &#123; if (sqlException.getErrorCode() == -104) &#123; return new DuplicateKeyException(&quot;完整性约束冲突&quot;, sqlException); &#125; return null; &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"「Spring」IoC注解实现","date":"2018-04-03T14:44:07.000Z","path":"2018/04/03/spring-ioc-annotation.html","text":"1. 回顾xml方式管理Java Bean 将一个Bean交由Spring创建并管理 &lt;baen id=&quot;bean&quot; class=&quot;包名.Bean&quot;&gt;&lt;/baen&gt; 获取Spring上下文 ApplicationContext app = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); 获取Bean Bean bean = app.getBean(&quot;bean&quot;, Bean.class); 2. 注解方式管理Java Bean一、创建一个class配置文件 12345678@Configurationpublic class MyConfiguration&#123; //将一个Bean交由Spring创建并管理 @Bean(name=&quot;bean1&quot;) public Bean bean()&#123; return Bean = new Bean(); &#125;&#125; 二、获取Spring上下文 12ApplicationContext context = new AnnotationConfigApplicationContext(MyConfiguration.class); 三、获取Bean 1Bean1 bean1 = context.getBean(&quot;bean1&quot;, Bean1.class); 2.1 简化注解方式的步骤1一、 开启组件扫描（去掉上述步骤1中MyConfiguration实例化Bean的方法） 123@Configuration //该注解可理解当前class等同于一个xml文件@ComponentScan(&quot;包路径&quot;) //开启组件扫描public class MyConfiguration&#123;&#125; 在applicationContext.xml中开启组件扫描方式&lt;context:component-scan base-package=&quot;包路径&quot;/&gt;。 二、 将交由Spring管理的类加上@Component注解，或（@Repository，@Controller，@Service） 1234@Component(&quot;bean1&quot;)//通过构造方法实例化Bean1public class Bean1&#123; //...&#125; @Component是通用注解，其他三个注解是这个注解的拓展，并且具有了特定的功能 @Repository注解在持久层中，具有将数据库操作抛出的原生异常翻译转化为spring的持久层异常的功能。 @Controller层是spring-mvc的注解，具有将请求进行转发，重定向的功能。 @Service层是业务逻辑层注解，这个注解只是标注该类处于业务逻辑层。 2.2 Bean别名一、 xml形式：通过name属性或alias标签 12&lt;bean id=&quot;bean1&quot; name=&quot;bean2,bean3&quot; class=&quot;com...Bean&quot;/&gt;&lt;alias name=&quot;bean1&quot; alias=&quot;bean4&quot;/&gt; 二、 注解形式 1234567@Configurationpublic class MyConfiguration&#123; @Bean(name=&#123;&quot;bean1&quot;,&quot;bean2&quot;,&quot;bean3&quot;&#125;) public Bean1 bean1()&#123; return Bean1 = new Bean1(); &#125;&#125; 注意：@Component只能指定一个名字，@Component默认值为类名首字母小写，也可以自定义，如:@Component(&quot;bean1&quot;)； 默认@scope为singleton单例，也可以进行指定 3. 注解方式Bean的注入一、 **@Value(&quot;值&quot;)**：常用于基本数据类型值注入，值可用EL表达式。 123456@Componentpublic class Player&#123; @Value(&quot;张三&quot;) private String name; //...&#125; 二、 @Autowired：常用于复杂类型值的注入 + @Autowired：可以用在成员变量，setter方法，构造方法上；优先按照类型进行匹配，匹配不上启用名字进行匹配。 + @Qualifier(&quot;名字&quot;) 根据名字匹配，配合@Autowired，不能用在构造方法上；@Qualifier指定对象必须存在，否则程序报错，可以使用@Autowired的required属性来解除这种强依赖，@Autowired(required=false):尽量去找，组件不存在也不报错。 + @Autowired的原理：在启动spring IoC时，容器自动装载了一个AutowiredAnnotationBeanPostProcessor后置处理器，当容器扫描到@Autowied、@Resource或@Inject时，就会在IoC容器自动查找需要的bean，并装配给该对象的属性 1234567891011121314151617181920212223@Componentpublic class Player&#123; @Value(&quot;张三&quot;) private String name; /** 用于成员变量 */ //@Autowired //@Qualifier(&quot;card1&quot;) private Card card; /** 用于构造方法 */ //@Autowired public Player(Card card) &#123; super(); this.card = card; &#125; /** 用于setter方法 */ @Autowired(required=false) public void setCard(Card card) &#123; this.card = card; &#125;&#125; 三、 @Resource：常用于复杂类型值的注入 + @Resource：用在成员变量和setter方法上，是JDK1.6支持的注解，优先按照名字匹配，可以通过@Resource(name=&quot;名&quot;)指定；如果没有指定name属性，用在成员变量上默认取字段名，用在setter方法上默认取属性名进行装配。名字匹配不上，会动用类型匹配。但注意：如果name属性一旦指定，就只会按照名称进行装配。 123456@Componentpublic class Player&#123; @Resource(name=&quot;card&quot;) private Card card; //...&#125; 集合类型值注入实例 123456789101112131415161718@Configuration@ComponentScan(&quot;包路径&quot;)public class MyConfiguration&#123; @Bean public List&lt;String&gt; list()&#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;aaa&quot;); list.add(&quot;bbb&quot;); return list; &#125;&#125;@Componentpublic class Player&#123; @Autowired private List&lt;String&gt; list; //...&#125; 4. 注解方式Bean的常用配置项(作用域,生命周期,懒加载等)4.1 注解方式Bean的作用域12345678910111213@Configuration@ComponentScan(&quot;包路径&quot;)public class MyConfiguration&#123; @Bean(name=&quot;bean1&quot;) @Scope(&quot;singleton&quot;) public Bean1 bean1()&#123; return Bean1 = new Bean1(); &#125;&#125;@Component@Scope(&quot;singleton&quot;)public class Bean&#123;&#125; 4.2 注解方式Bean的懒加载1234567891011121314@Configuration@ComponentScan(&quot;包路径&quot;)@Lazy //相当于xml中default-lazy-init=&quot;true&quot;public class MyConfiguration&#123; @Bean(name=&quot;bean1&quot;) @Lazy public Bean1 bean1()&#123; return Bean1 = new Bean1(); &#125;&#125;@Component@Lazypublic class Bean&#123;&#125; 4.3 Bean初始化和销毁一、实现InitializingBean和DisposableBean接口（xml和注解都支持）。 12345678910111213public class Bean implements InitializingBean&#123; @Override public void afterPropertiesSet()&#123; //执行一些初始化后的工作 &#125;&#125;public class Bean implements DisposableBean&#123; @Override public void destroy()&#123; //执行一些销毁前的工作 &#125;&#125; 二、xml形式 12345678public class Bean&#123; public void init()&#123; //执行一些初始化后的工作 &#125; public void cleanup()&#123; //执行一些销毁前的工作 &#125;&#125; 123&lt;bean id=&quot;bean&quot; class=&quot;example.Bean&quot; init-method=&quot;init&quot; destroy-method=&quot;cleanup&quot;&gt;&lt;/bean&gt; 三、注解形式1，@Bean(initMethod&#x3D;”init”, destroyMethod&#x3D;”cleanup”) 12345678910111213141516public class Bean&#123; public void init()&#123; //执行一些初始化后的工作 &#125; public void cleanup()&#123; //执行一些销毁前的工作 &#125;&#125;@Configurationpublic class MyConfiguration&#123; @Bean(initMethod=&quot;init&quot;, destroyMethod=&quot;cleanup&quot;) public Bean bean()&#123; return new Bean(); &#125;&#125; 四、注解形式2，添加@PostConstruct，@PreDestroy 1234567891011@Componentpublic class Bean&#123; @PostConstruct public void init()&#123; //执行一些初始化后的工作 &#125; @PreDestroy public void cleanup()&#123; //执行一些销毁前的工作 &#125;&#125;","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"「Spring」IoC控制反转","date":"2018-03-27T14:38:23.000Z","path":"2018/03/27/spring-ioc.html","text":"Spring是一个开源的轻量级控制反转(IOC)和面向切面(AOP)的容器框架，它主要是为了解决企业应用开发的复杂性而诞生的，但现在已不止应用于企业服务。 IOC：Inversion Of Control（控制反转），构成Spring框架的核心基础 DAO：Data Access Object（数据 访问对象），Spring对JDBC访问数据库的简化和封装 WebMVC：Spring对Web部分(jsp,servlet,ajax)以及MVC设计模式的支持 AOP：是在面向对象的基础上发展来的更高级的技术 ORM：Object Relation Mapping（对象关系映射），以面向对象的思想来访问数据库 JEE：Java的消息服务，远程调用，邮件服务等 1. IoC（控制反转）IoC：(Inversion of Control),控制反转：控制权的转移，应用程序本身不负责依赖对象的创建和维护，而是由外部容器负责创建和维护。 控制：控制对象的创建及销毁（生命周期） 反转：将对象的控制权交给IoC容器 DI：(Dependence Injection),依赖注入(注射)是IoC控制反转的一种具体实现方法，通过参数的方式从外部传入依赖，将依赖的创建由主动变为被动。 简单来说， 当 组件A 依赖 组件B 时，IoC容器通过设置A的属性，把B传入的过程叫依赖注入 IoC的好处：降低了组件的依赖程度，让组件之间变成低耦合设计。 2. Spring容器初始化任何Java类都可以在Spring容器中创建对象 并交由容器来进行管理和使用，Spring容器 实现了 IOC 和 AOP 机制，Spring容器的类型是 BeanFactory 或者 ApplicationContext BeanFactory提供配置结构和基本功能，加载并初始化Bean ApplicationContext保存了Bean对象并在Spring中被广泛使用 2.1 初始化ApplicationContext的几种方式： 本地文件12FileSystemXmlApplicationContext app = new FileSystemXmlApplicationContext(&quot;F:/workspace/appcontext.xml&quot;); Classpath12ClassPathXmlApplicationContext app = new ClassPathXmlApplicationContext(&quot;classath:applicationContext.xml&quot;); Web应用中依赖Servlet或Listener123&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; 2.2 Spring容器完成IOC的步骤 建立一个动态的Web项目，导入jar包(ioc) 拷贝Spring容器配置文件到src(Source classpath)下 在spring容器配置文件中配置文件中配置一个对象的创建 &lt;baen id=&quot;对象引用名&quot; class=&quot;包名.类名&quot;&gt;&lt;/baen&gt; 写一个测试类 创建Spring容器对象，然后从容去中获取创建的组件 applicationContext.getBean(&quot;对象引用名&quot;, 类名.class) 3. spring容器创建对象(实例化)3.1 构造器方式实例化 配置文件：&lt;baen id=&quot;对象引用名&quot; class=&quot;包名.类名&quot;&gt;&lt;/baen&gt; applicationContext.getBean(&quot;对象引用名&quot;, 类名.class)默认调用类型对应的无参构造方法1&lt;bean id=&quot;date&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt; 12ApplicationContext app = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);Date date = app.getBean(&quot;date&quot;, Date.class); 3.2 静态工厂方法实例化 使用一个类型对应的静态方法来获取这个类型的对象 &lt;bean id=&quot;对象引用名&quot; class=&quot;包名.工厂类名&quot; factory-method=&quot;静态方法名&quot;&gt;&lt;/bean&gt;1&lt;bean id=&quot;cal&quot; class=&quot;java.util.Calendar&quot; factory-method=&quot;getInstance&quot;&gt;&lt;/bean&gt; 12ApplicationContext app = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);Calendar cal = app.getBean(&quot;cal&quot;, Calendar.class); 3.3 实例工厂方法实例化 使用一个已经存在的对象，来调用对应的成员方法来获取另一个类型的对象 &lt;bean id=&quot;对象的引用名&quot; factory-bean=&quot;工厂方法的id&quot; factory-method=&quot;成员方法名&quot;&gt;&lt;/bean&gt;12&lt;bean id=&quot;cal&quot; class=&quot;java.util.Calendar&quot; factory-method=&quot;getInstance&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;time&quot; factory-bean=&quot;cal&quot; factory-method=&quot;getTime&quot;&gt;&lt;/bean&gt; 12ApplicationContext app = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);Date time = app.getBean(&quot;time&quot;, Date.class); 4. Spring DI注入的实现Spring注入是指在启动Spring容器加载bean配置的时候，完成对变量的赋值行为。Bean属性值：基本数据类型用value，复杂数据类型用ref(传入组件id)。DI的实现方法：设值注入(setter注入)、构造注入、自动化注入(自动装配) 实例：准备两个实体类Card，Player：Card有suit(花色)和point(点数)，Player有name(名字)和card(牌)。 4.1 设值注入property(属性)的name参考对象set方法 123456789&lt;bean id=&quot;card&quot; class=&quot;bean.Card&quot;&gt; &lt;property name=&quot;suit&quot; value=&quot;黑桃&quot;&gt;&lt;/property&gt; &lt;property name=&quot;point&quot; value=&quot;A&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- Player参考其setCard方法 --&gt;&lt;bean id=&quot;player&quot; class=&quot;bean.Player&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;玩家1&quot;&gt;&lt;/property&gt; &lt;property name=&quot;card&quot; ref=&quot;card&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 4.2 构造注入（Constructor arguments）构建对象时赋值，参考对应构造方法（name为构造方法参数名，也可以用index:0开始） 123456789&lt;bean id=&quot;card2&quot; class=&quot;bean.Card&quot;&gt; &lt;constructor-arg name=&quot;suit&quot; value=&quot;红桃&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;point&quot; value=&quot;K&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- Player参考其构造方法Player(name,card) --&gt;&lt;bean id=&quot;player2&quot; class=&quot;bean.Player&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;玩家2&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=&quot;card&quot; ref=&quot;card2&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 4.3 自动化注入（Autowiring mode）一般用来解决复杂值的注入，可以通过bean标记的autowrie属性(autowire&#x3D;”byName&#x2F;byType&#x2F;constructor”)指定对应的自动化的注入方式 1&lt;bean id=&quot;bean1&quot; class=&quot;example.exampleBean&quot; autowire=&quot;&quot; /&gt; 自动装配autowire属性 有五种自动装配的方式： No：默认，需要通过ref属性来连接bean。 byName： 与当前组件属性名 和 容器中其他组件的id 一致的bean，自动装配。 1234567&lt;bean id=&quot;card3&quot; class=&quot;bean.Card&quot;&gt; &lt;property name=&quot;suit&quot; value=&quot;方片&quot;&gt;&lt;/property&gt; &lt;property name=&quot;point&quot; value=&quot;J&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- Player中必须要有setCard3 方法(setter方法名要与注入组件id对应) 否则Spring会将id为card的bean通过setter方法进行自动装配(若有setCard方法)--&gt;&lt;bean id=&quot;player3&quot; class=&quot;bean.Player&quot; autowire=&quot;byName&quot;&gt;&lt;/bean&gt; byType：与当前组件属性类型 和 容器中其他组件的class 一致的bean，自动装配，如果存在多个则抛出异常。 123456&lt;bean class=&quot;bean.Card&quot;&gt; &lt;property name=&quot;suit&quot; value=&quot;方片&quot;&gt;&lt;/property&gt; &lt;property name=&quot;point&quot; value=&quot;J&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- Spring会将类型为Card的bean通过setter方法进行自动装配(setter参数类型与注入组件类型对应) --&gt;&lt;bean id=&quot;player4&quot; class=&quot;bean.Player&quot; autowire=&quot;byType&quot;&gt;&lt;/bean&gt; constructor：与当前组件 构造方法的参数 容器中其他组件的id 一致的bean，自动装配，不匹配再和 容器中其他组件的class 一致的bean，自动装配（如果存在多个则不装配），如果构造方法中第一个参数不匹配，则终止后续赋值。 123456&lt;bean id=&quot;card5&quot; class=&quot;bean.Card&quot;&gt; &lt;property name=&quot;suit&quot; value=&quot;方片&quot;&gt;&lt;/property&gt; &lt;property name=&quot;point&quot; value=&quot;J&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- Player添加构造方法Player(Card card5)，构造方法参数名与注入组件id对应，不匹配再用构造方法参数类型和注入组件class匹配，如果存在多个则不装配 --&gt;&lt;bean id=&quot;player5&quot; class=&quot;bean.Player&quot; autowire=&quot;constructor&quot;&gt;&lt;/bean&gt; autodetect：如果有默认的构造器，则通过constructor方式进行自动装配，否则使用byType方式进行自动装配。 5. DI的参数的注入Bean对象 注入类型 可以是 字符串、集合、bean对象。 5.1 注入字符串12345&lt;bean id=&quot;msg&quot; class=&quot;com.xdl.bean.OracleDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;scott&quot;/&gt; &lt;property name=&quot;password&quot;&gt;&lt;value&gt;tiger&lt;/value&gt;&lt;/property&gt; &lt;property name=&quot;msg&quot;&gt;&lt;null/&gt;&lt;/property&gt;&lt;/bean&gt; 5.2 注入集合12345678910111213141516171819202122232425262728&lt;!-- 1. 定义list集合 --&gt;&lt;property name=&quot;friends&quot;&gt; &lt;list&gt; &lt;value&gt;值1&lt;/value&gt; &lt;value&gt;值2&lt;/value&gt; &lt;/list&gt;&lt;/property&gt;&lt;!-- 2. 定义set集合 --&gt;&lt;property name=&quot;friends2&quot;&gt; &lt;set&gt; &lt;value&gt;值1&lt;/value&gt; &lt;value&gt;值2&lt;/value&gt; &lt;/set&gt;&lt;/property&gt;&lt;!-- 3. 定义map集合 --&gt;&lt;property name=&quot;phones&quot;&gt; &lt;map&gt; &lt;entry key=&quot;1594546454&quot; value=&quot;值1&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;1594546464&quot; value=&quot;值2&quot;&gt;&lt;/entry&gt; &lt;/map&gt;&lt;/property&gt;&lt;!-- 4. props集合 --&gt;&lt;property name=&quot;phones2&quot;&gt; &lt;props&gt; &lt;prop key=&quot;164545564&quot;&gt;值1&lt;/prop&gt; &lt;prop key=&quot;164546756&quot;&gt;值2&lt;/prop&gt; &lt;/props&gt;&lt;/property&gt; 5.3 集合参数的单独定义注入集合–引入：List、Set、Map、Properties集合也可以先独立定义，再注入的方式使用，这样便于重复利用。 123456789101112131415161718192021222324&lt;!-- 1. 定义list集合 --&gt;&lt;util:list id=&quot;ref_friends&quot;&gt; &lt;value&gt;值1&lt;/value&gt; &lt;value&gt;值2&lt;/value&gt;&lt;/util:list&gt;&lt;!-- 2. 定义set集合 --&gt;&lt;util:set id=&quot;ref_buddys&quot;&gt; &lt;value&gt;值&lt;/value&gt; &lt;value&gt;值2&lt;/value&gt;&lt;/util:set&gt;&lt;!-- 3. 定义map集合 --&gt;&lt;util:map id=&quot;ref_phones&quot;&gt; &lt;entry key=&quot;159454644&quot; value=&quot;值1&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;1594546454&quot; value=&quot;值2&quot;&gt;&lt;/entry&gt;&lt;/util:map&gt;&lt;!-- 4. props集合 --&gt;&lt;util:properties id=&quot;ref_phonePro&quot;&gt; &lt;prop key=&quot;164545564&quot;&gt;值1&lt;/prop&gt; &lt;prop key=&quot;16454675665564&quot;&gt;值2&lt;/prop&gt;&lt;/util:properties&gt;&lt;util:properties id=&quot;ref_db&quot; location=&quot;classpath:db.properties&quot;&gt;&lt;/util:properties&gt;&lt;!-- 使用 --&gt;&lt;property name=&quot;phones&quot; ref=&quot;ref_phones&quot;&gt;&lt;/property&gt;&lt;property name=&quot;phones2&quot; ref=&quot;ref_phonePro&quot;&gt;&lt;/property&gt; 5.3 Spring的’EL’表达式它和EL在语法上很 相似，可以读取一个bean对象&#x2F;集合中的数据。Spring EL 采用 #{Sp Expression Language} 即 #&#123;spring表达式&#125;，可在xml配置和注解中使用。 Spring EL配置连接池对象12345678&lt;!-- 引入数据库配置文件 --&gt;&lt;util:properties id=&quot;db&quot; location=&quot;classpath:db.properties&quot;/&gt;&lt;!-- 配置连接池 --&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.xdl.bean.OracleDataSource&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;#&#123;db.name&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;#&#123;db.password&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;#&#123;db.url&#125;&quot;/&gt;&lt;/bean&gt; 6. Bean的常用配置项(作用域,生命周期,懒加载等)Bean的常用配置项：Id、Class、Scope、Constructor arguments、Propertties、Autowiring mode、Lazy-initialization mode、Initialization&#x2F;destruction method 6.1 Bean作用域（Scope） Singleton作用域 单例，指一个Bean容器只存在一份 prototype作用域 每次请求(使用)创建新的实例，destroy方式不生效 Web环境作用域： request作用域：每个request请求都会创建一个单独的实例。 session作用域：每个session都会创建一个单独的实例。 application作用域：每个servletContext都会创建一个单独的实例。 websocket作用域：每个websocket连接都会创建一个单独的实例。 自定义作用域 SimpleThreadScope作用域：每个线程都会创建一个单独的实例。 6.2 Bean的生命周期（Initialization&#x2F;destruction method）Bean的生命周期：定义 –&gt; 初始化 –&gt; 使用 –&gt; 销毁 6.2.1 Bean初始化如果需要在Bean实例化之后执行一些逻辑，有两种方法： 实现InitializingBean接口(org.springframework.beans.factory.InitializingBean)，覆盖afterPropertiesSet方法，在afterPropertiesSet中执行一些初始化后的工作。 配置init-method 配置**beans**的default-init-method属性 来指定一个初始化方法，这个指定针对容器中所有的对象，由于这样影响的范围比较广，所以当对象没有对应的初始化方法程序也不会报错。 配置**bean**的init-method来指定初始化方法，这样只影响包含init-method属性所在的bean标记创建的对象，这样控制的对象比较精准，所以当类型中没有这个初始化方法则程序崩溃。1&lt;bean id=&quot;exampleId&quot; class=&quot;example.exampleBean&quot; init-method=&quot;init&quot;&gt;&lt;/bean&gt; 12345public class ExampleBean&#123; public void init()&#123; //执行一些初始化后的工作 &#125;&#125; 6.2.2 Bean销毁如果需要在Bean销毁之前执行一些逻辑，有两种方法： 实现DisposableBean接口(org.springframework.beans.factory.DisposableBean)覆盖destroy方法，，在destroy中执行一些销毁前的工作。 配置destroy-method 配置**beans**的default-destroy-method属性 来指定一个销毁方法，这个指定针对容器中所有的对象，由于这样影响的范围比较广，所以当对象没有对应的销毁方法程序也不会报错。 配置**bean**的destroy-method来指定销毁方法，这样只影响包含destroy-method属性所在的bean标记创建的对象，这样控制的对象比较精准，所以当类型中没有这个销毁方法则程序崩溃。1&lt;bean id=&quot;exampleId&quot; class=&quot;example.exampleBean&quot; destroy-method=&quot;cleanup&quot;&gt;&lt;/bean&gt; 12345public class ExampleBean&#123; public void cleanup()&#123; //执行一些销毁前的工作 &#125;&#125; 注意：销毁方法只针对单例模式的对象 6.3 Bean的懒加载（Lazy-initialization mode）Spring容器会在创建容器时提前初始化Singleton作用域的bean，可以通过bean标记lazy-init=&quot;true&quot;延迟实例化(对象被使用时才创建)。 配置lazy-init 配置**beans**的default-lazy-init=&quot;true&quot;为所有Bean设定懒加载。 配置**bean**的lazy-init=&quot;true&quot;为单独的某个Bean设定懒加载。1&lt;bean id=&quot;bean1&quot; class=&quot;example.exampleBean&quot; lazy-init=&quot;true&quot;/&gt; 适用场景：如果某个Bean在程序整个运行周期都可能不会被使用，可以考虑设定该Bean为懒加载 优点：尽可能的节约了资源 缺点：可能导致某个操作响应时间增加 6.4 Bean装配的Aware接口实现了Aware接口的bean在初始化后可以获取相应资源并进行相应的操作。 ApplicationContextAware 接口方法：setApplicationContext 作用：通常用来获取上下文对象，声明全局变量后在方法中对变量进行初始化并供其他方法调用 实现过程：创建一个类并实现ApplicationContextAware接口，重写setApplicationContext方法；在xml文件中配置该类；当spring加载该配置文件时即调用接口方法。 BeanNameAware 接口方法：setBeanName 作用：获取声明的类名，声明全局变量后在方法中对变量进行初始化并供其他方法调用 实现过程：创建一个类并实现BeanNameAware接口，重写setBeanName方法；在xml文件中配置该类；当spring加载该配置文件时即调用接口方法。 6.4 Bean装配之ResourceResources（针对于资源文件的统一接口） UrlResource：URL 对应的资源，根据一个 URL 地址即可获取 ClassPathResource：获取类路径下的资源 FileSystemResource：获取文件系统里面的资源 ServletContextResource：ServletContext 封装的资源，用于访问 ServletContext 环境下的资源 InputStreamResource：获取输入流封装的资源 ByteArrayResource：获取字节数组封装的资源 ResourceLoader: 所有的 application contexts 都实现了 ResourceLoader 接口，因此所有的 application contexts 都能通过getResource()获取Resource实例。 getResource()参数： classPath方式：”classPath:class路径下文件” file方式： “file:本地磁盘文件绝对地址” url方式： “url:URL地址下文件” 没有前缀时依赖applicationContext的配置文件路径: “文件全名” eg:applicationContext.getResource(&quot;classpath:config.txt&quot;)","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Spring","slug":"Spring","permalink":"http://chaooo.github.io/tags/Spring/"}]},{"title":"「Java教程」Web编程基础","date":"2017-06-20T12:59:40.000Z","path":"2017/06/20/java-web.html","text":"JavaWeb是用Java技术来解决相关web互联网领域的技术总和。Java提供了技术方案可以解决客户端和服务器端的实现，特别是服务器的应用，比如Servlet，JSP和第三方框架等等。 1. http协议超文本传输协议，是一种应用层的网络传输协议 http协议的特点： 简单，快速：支持多种不同的的数据提交方式，如get&#x2F;post 数据传输灵活，支持任意类型数据的传输 无连接协议：每次连接，只处理一次请求，进行一次响应，响应完毕，立即断开。 无状态协议：处理请求与响应时没有记忆能力，如果需要处理之间的信息，只能重新传递。 http协议的组成部分： 请求：浏览器连接服务器的过程 响应：服务器回复浏览器的过程 http协议的请求： 请求头：描述客户端的信息 请求体：GET没有请求体，请求体用于存储POST请求发送的数据。 请求空行：请求头与请求体之间的一行空白 请求行：描述请求方式，服务器地址，协议版本等 http协议的响应： 响应头：描述服务器的信息 响应体：响应的内容，文本，json数据等。 响应行：描述服务器协议版本，响应状态码，以及响应成功或失败的解释。 2. Servletservlet是一个运行在tomcat上的Java类，用户通过浏览器输入地址，触发这个类，这个类执行完毕，准备一个响应体，发送给浏览器。 2.1 Servlet编写步骤： 编写一个Java类，继承HttpServlet类 重新service方法 在service方法中，对用户请求进行响应。 123456789101112131415//注解：添加访问的网址@WebServlet(&quot;/hello&quot;)public class MyServlet extends HttpServlet &#123; private static final long serialVersionUID = 1L; @Override public void service(ServletRequest req, ServletResponse res) throws IOException &#123; //1.设置响应体的编码，以及内容类型 res.setContentType(&quot;text/html;charset=utf-8&quot;); //2.得到响应体输出的打印流 PrintWriter out = res.getWriter(); //3.打印文字 out.println(&quot;&lt;h1&gt;Hello Servlet!&lt;/h1&gt;&quot;); &#125;&#125; 2.2 配置ervlet类的访问网址 web3.0版本之后使用注解的方式配置ervlet类的访问网址 web3.0版本之前配置Servlet访问网址的方式： 将Servlet类，配置到web.xml中，告知tomcat，servlet的类名 配置Servlet类的别名，并给指定别名的Servlet添加映射网址。 123456789101112&lt;!-- 将servlet类，配置到web.xml中，告知tomcat，servlet的类名 --&gt;&lt;servlet&gt; &lt;!-- Servlet类别名，用于后续添加映射网址 --&gt; &lt;servlet-name&gt;demo1&lt;/servlet-name&gt; &lt;!-- Servlet类全名 --&gt; &lt;servlet-class&gt;day01_Servlet.demo1.MyServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;!-- 给指定别名的Servlet添加映射网址 --&gt; &lt;servlet-name&gt;demo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/hello&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 2.3 Servlet生命周期 实例化 –&gt; 初始化(init) –&gt; 服务(service) –&gt; 销毁(销毁之前调用destory) –&gt; 不可用 创建时机：默认情况下，当用户第一次访问Servlet的映射网址是Servlet对象被创建，后续用户再次访问，是重复利用此对象。 销毁时机：当tomcat关闭时 或 应用从tomcat卸载时。 tomcat为了便于我们进行资源的合理缓存，为生命周期事件提供了三个方法： init(); 当Servlet对象被创建时，方法执行，通常在这里进行一些可重用资源的初始化工作。 service(); 服务方法，当用户每次发起请求时，此方法用于处理请求，并进行响应，此方法每次都执行在新的线程中。 destory(); 当Servlet即将被销毁时，方法执行，释放资源的代码可写在此方法中。 2.4 get和post区别 GET请求： 没有请求体，请求时携带参数在url中，参数在url地址的?后，参数由&#x3D;连接的键值对组成，&amp;连接键值对。 只能传输字符串类型参数 浏览器url地址最大长度4kb 数据传输时，参数在url中明文显示，不安全。 POST请求： 有请求体，是一个单独的数据包，用于存储请求中的多个参数 可传输任意类型的数据，进行文件上传必须POST请求 可以传递的数据大小，理论上没有上限 数据传输时在单独的数据包，较为安全。 2.5 接收请求中的参数 根据参数的名称，接收参数的单个值 String value &#x3D; request.getParameter(String name); 根据参数的名称，接收一组参数的值 String[] values &#x3D; request.getParameterValues(String name); 1234567891011121314protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.setCharacterEncoding(&quot;UTF-8&quot;); response.setContentType(&quot;text/html;charset=utf-8&quot;); //1.接收 String username = request.getParameter(&quot;username&quot;); String[] password = request.getParameterValues(&quot;password&quot;); //2.打印 System.out.println(&quot;username:&quot; + username); System.out.println(&quot;password:&quot; + password[0]); System.out.println(&quot;password2:&quot; + password[1]); //3.浏览器输出 response.getWriter().append(&quot;&lt;div&gt;很遗憾注册失败，点击&lt;a href=\\&quot;demo1.html\\&quot;&gt;重新注册&lt;/a&gt;&lt;/div&gt;&quot;);&#125; 2.6 乱码处理2.6.1 乱码情况： 浏览器提交表单时，会对中文参数值进行自动编码。Tomcat服务器接收到的浏览器请求后，默认使用iso-8859-1去解码，当编码与解码方式不一致时，就会乱码。 tomcat8版本之前(不包含tomcat8版本), GET请求乱码 任何版本, POST请求乱码 2.6.2 请求乱码处理： 适用于所有乱码问题：(Tomcat8之后get无乱码) 指定浏览器打开页面的编码&lt;meta charset=&quot;UTF-8&quot;&gt;; 将接收到的中文乱码重新编码： 12String name = request.getParameter(&quot;userName&quot;);String userName = new String( name.getByte(&quot;ISO-8859-1&quot;),&quot;utf-8&quot;); 仅适用于POST请求： 指定浏览器打开页面的编码&lt;meta charset=&quot;UTF-8&quot;&gt;; Servlet接收之前设置解码（需在调用request.getParameter(“key”)之前设置）request.setCharacterEncoding(&quot;utf-8&quot;); 2.6.3 响应乱码的处理： 方式一：设置响应的内容类型, 以及编码格式:response.setContentType(&quot;text/html;charset=utf-8&quot;); 方式二：进设置编码格式, 不设置响应内容类型:response.setCharacterEncoding(&quot;UTF-8&quot;)(常用于客户端不是浏览器的情况, 如果在浏览器的环境下设置, 有部分浏览器无法识别, 依然会乱码); 2.7 Servlet的创建时机 通过web.xml配置Servlet, 可以修改Servlet加载的时机。 可以给Servlet节点，添加&lt;load-on-startup&gt;节点来制定servlet启动顺序。 节点中的值为数字： -1：默认-1，表示当用户第一次请求时，创建对象 &gt;=0：大于等于0，当服务器启动时，创建对象，值越小创建越早，值相同按web.xml配置顺序创建 123456789101112&lt;servlet&gt; &lt;servlet&gt; &lt;servlet-name&gt;s1&lt;/servlet-name&gt; &lt;servlet-class&gt;demo.ServletDemo&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;s1&lt;/servlet-name&gt; &lt;url-pattern&gt;/s1&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt;&lt;/servlet-mapping&gt; 3. 请求的转发与重定向3.1 请求对象request的常用操作 getMethod() : 得到请求的方式 getRequestURI() : 获取浏览器请求地址 getRemoteAddr() : 获取客户端ip地址 getRemoteHost() : 获取客户端名称 getServerName() : 获取服务器名称 getServerPort() : 获取服务器端口号 getQueryString() : 获取get请求参数字符串，其他请求返回null 3.1 请求的转发与重定向注意事项 请求转发与重定向操作，必须要有出口。 当一个请求在servlet中进行了重定向，那么这个servlet就不要再进行响应了 3.2 转发* 一个web组件，将未处理完毕的请求，通过tomcat转交给另一个web组件处理 步骤： 获取请求转发器：RequestDispather rd = request.getRequestDispacher(&quot;转发地址&quot;); 进行转发操作：rd.forward(request, response); 因为通常请求转发器获取后, 只会使用一次 , 一般不给对象起名, 简写: request.getRequestDispacher(&quot;转发地址&quot;).forward(request, response); 特点： 转发过程中，多个web组件之间共享一个请求对象request与响应对象response 在转发过程中，无论转发多少次，浏览器只发起了一次请求，所以浏览器地址不会改变 转发不能跨项目实现 比重定向效率更高 3.3 重定向* 一个web组件，处理完毕请求后，告知浏览器，将请求转向另一个地址 格式：response.sendRedirect(&quot;重定向地址&quot;)； 原理：当客户端请求服务器时，发起重定向流程： 给浏览器响应302的状态码 , 以及一个键值对, 键为: location , 值为重定向新地址. 当浏览器接收到302的状态码时, HTTP协议规定了浏览器会寻找location对象的新地址. 浏览器自动发起新的请求 , 跳转到新地址. 特点： 重定向会产生两个请求对象，多个请求对象中数据不互通 浏览器地址发生了改变 重定向可以跨域实现 比转发效率低 4. 上下文对象ServletContext 用于关联多个servlet，是servlet之间通讯的桥梁，用于多个servlet之间的信息共享 每一个项目运行时，tomcat会为这个项目创建一个servletContext，项目关闭时销毁。 获取ServletContext对象：ServletContext context = getServletContext(); 常用方法 context.setAttributes(String key, Objexct value); &#x2F;&#x2F;设置替换数据 context.getAttributes(String key); &#x2F;&#x2F;获取数据 context.removeAttributes(String key); &#x2F;&#x2F;删除数据 context.getRealPath(“&#x2F;“); &#x2F;&#x2F;获取项目运行时所在文件路径 5. 会话跟踪（状态管理） 存在两种实现： cookie: 将浏览器产生的状态存储在浏览器中 Session: 将浏览器产生的状态存储在服务器中 cookie技术原理： 服务器向客户端响应时，将数据以set-Cookie消息头（响应头）的方式发给浏览器， 浏览器接收到cookie后，会将这些数据以文本文件的方式（.txt文件）保存起来 当浏览器再次发起相同请求时，浏览器会将之前存储的cookie,添加到请求头，发给服务器 Session技术原理： 当浏览器访问服务器时，服务器可以选择为用户创建一个Session对象(类似于map集合)， 该Session对象有一个id属性，称之为SessionId，服务器会将这个SessionId以cookie方式发送给浏览器 浏览器再次访问服务器时，同时会传递SessionId的cookie给i服务器，服务器根据sessionId找到Session对象，供程序使用。 5.1 Cookie 创建Cookie：Cookie在Java中是一个类，每个cookie的对象都表示一个键值对 Cookie cookie = new Cookie(String key, String value); 注意：tomcat8.5版本之前，cookie无法出场中文 通过响应对象，将cookie添加到响应头,可添加多个 response.addCookie(Cookie cookie); 通过请求头得到cookie数组，没有则返回null Cookie[] cookies &#x3D; request.getCookies(); 取键：cookie.getName(); 取值：cookie.getValue() Cookie的存储时长： cookie.setMaxAge(int 秒)； 正数：倒计时秒数 0：表示立即删除此cookie，常用于覆盖一个存活时长较长的cookie,用于删除它 负数：默认-1，表示会话结束时自动删除（关闭浏览器） Cookie的存储路径问题 存储的cookie发送到服务器时，判断是否发送的依据是：域名相同，路径相同 为了避免路径问题，通常会将cookie设置统一路径为根路径：cookie.setPath(“&#x2F;“); 5.2 Cookie的优缺点 缺点： Cookie技术存储的数据类型，只能是字符串，且早期版本(8.5之前)不可存储中文。 数据存储在客户的计算机中，不安全，不建议存储安全敏感数据 保存数据量有限制，大约4kb左右 依赖于用户的浏览器设置，用户可以金庸cookie，可能被用户主动删除 优点： 分散服务器的压力 5.3 Session 获取Session 格式1：request.getSession();&#x2F;&#x2F;等价参数传true 格式2：request.getSession(boolean isNew); true，根据浏览器的SessionId查找一个session，若没有就新创建一个对象并返回 false，根据浏览器的SessionId查找一个session，若没有就返回null Session常用方法 session.setAttribute(String key, object value);&#x2F;&#x2F;设置&#x2F;替换值 session.getAttribute(String key);&#x2F;&#x2F;获取值 session.invalidate();&#x2F;&#x2F;销毁 设置session存活时长 默认会话时长30分钟，当浏览器最后一次访问服务器后30分钟后，若没有再次连接，则session被销毁。 可以通过修改配置文件，修改所有的session时长 修改conf/web.xml的&lt;session-config&gt;&lt;session-tiomeout&gt;数值分钟&lt;/session-tiomeout&gt;&lt;/session-config&gt; 可以通过session对象，修改单个对象的session时长 void session.setMaxInactiveInterval(int seconds) 5.4 Session的优缺点 缺点： 数据存储在服务器端，当用户量大时，对服务器造成极大的压力，很容易耗尽服务器资源 优点： 数据存储在服务器中，安全 数据类型为Object，在Java中表示可以存储所有类型的数据 session存储的数据大小，理论上无限的。 5.5 Cookie和Session的使用 Cookie和Session不是互斥的，是相辅相成的 在项目开发时： 对安全敏感的数据，存储在session中 对安全不敏感的字符串数据，可以选择存储在Cookie中 对于大的数据，应该存在数据库和文件中 注意：cookie和session是为了管理状态而非存储数据。 6.JSP6.1 JSP语法基础 Java Server Pages：java动态网页技术 JSP引擎原理：JSP引擎读取JSP文件，将文件转换为Servlet，由servlet给用户响应 注意： JSP文件的转换 发生在服务器启动时，当用户访问JSP时，其实访问的是JSP文件转换的Servlet 执行流程：浏览器请求–&gt;tomcat–&gt;JSP引擎转换为Servlet–&gt;转换的Servlet–&gt;准备响应体–&gt;响应给浏览器–&gt;浏览器解析html JSP语法结构 html代码 Java代码 Jsp特有的语法结构 Java代码声明区：指的是类的成员位置 123&lt;%! // Java代码声明区%&gt; Java代码执行区：指的是Servlet的service方法中，每次用户请求，执行区的代码都会执行起来 123&lt;% // Java代码执行区%&gt; JSP输出表达式 用于快速的将Java中的数据，输出到网页中.. 语法格式：&lt;%=数据 %&gt;，编译后被转换成out.print(数据) JSP注释： html中可以用&lt;!-- --&gt; java中可以用//，/**/，/** */ jsp注释&lt;%-- --%&gt; html和java注释会被编译，其中html注释会被编译到页面，jsp注释编译器会自动忽略 6.2 JSP三大指令 page指令 include指令 taglib指令 指令使用格式：&lt;%@ 指令名称 属性1&#x3D;值 属性2&#x3D;值 属性n&#x3D;值 %&gt;*语法上，JSP允许在单个页面出现多个相同的JSP指令 6.2.1 page指令 用于配置页面信息 12345678910111213&lt;%@ page language=&quot;java&quot;：语言 contentType=&quot;text/html;charset=utf-8&quot;：响应的内容类型，以及响应的编码格式 pageEncoding=&quot;UTF-8&quot;：文件存储的编码格式 extends=&quot;继承的父类&quot; buffer=&quot;数字/none&quot;：是否允许缓存，默认值8kb autoFlush=&quot;true/false&quot;：是否自动清除缓存，默认true session=&quot;true/false&quot;：是否提前准备session对象，默认true isThreadSafe=&quot;true/false&quot;：是否线程安全的 import=&quot;java.util.List&quot;：用于导包，多个包使用&quot;,&quot;隔开 errorPage=&quot;网址&quot;：当页面发生BUG后，显示哪个页面 isErrorPage=&quot;true/false&quot;：当前页面是否是一个错误处理页面，如果结果为true，当别的页面产生错误，跳转到此页面，会提前准备好一个对象exception，此对象封装了错误信息%&gt; 6.3 项目发生错误时，统一的处理方式 打开项目的web.xml 加入子节点&lt;error-page&gt;&lt;error-code&gt;错误码&lt;/error-code&gt;&lt;location&gt;处理网址&lt;/location&gt;&lt;/error-page&gt; 12345678&lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/error.jsp&lt;/location&gt;&lt;/error-page&gt;&lt;error-page&gt; &lt;error-code&gt;404&lt;/error-code&gt; &lt;location&gt;/404.jsp&lt;/location&gt;&lt;/error-page&gt; include指令：用于将jsp或html引入到另一个jsp中 语法格式：&lt;%@ include file=&quot;地址&quot; %&gt; include动作：用于将jsp或html引入到另一个jsp中 语法格式：&lt;jsp:include page=&quot;地址&quot;&gt; include指令 与 include动作区别： include指令：引入文件操作，是在JSP引擎的转换时发生，将多个jsp文件，生产为了一个Servlert（多个jsp &#x3D;&gt; 一个Servlet） include动作：引入文件操作，是在浏览器请求时，将引用文件的响应体添加到了请求文件的响应体中（多个jsp &#x3D;&gt; 多个Servlet） 7.内置对象(隐含对象) 在JSP中，我们的代码执行在service中，所谓内置对象，指的是在JSP引擎转换时期，在我们代码生成位置的上面，提前准备好的一些变量，对象。 内置对象通常是我们会主动创建的对象 7.1 九大内置对象 request 对象类型：java.servlet.HttpServletRequest request内置对象中包含了有关浏览器请求的信息，提供了大量get方法，用于获取cookie、header以及session内数据等。 response 对象类型：javax.servlet.HttpServletResponse response对象提供了多个方法用来处理HTTP响应，可以调用response中的方法修改ContentType中的MIME类型以及实现页面的跳转等。 config 对象类型：javax.servlet.ServletConfig 在Servlet初始化的时候，JSP引擎通过config向它传递信息。这种信息可以是属性名&#x2F;值匹配的参数，也可以是通过ServletContext对象传递的服务器的有关信息。 out 对象类型：javax.servlet.jsp.JspWriter 在JSP开发过程中使用得最为频繁的对象 page 对象类型：java.lang.Object page对象有点类似于Java编程中的this指针，就是指当前JSP页面本身。 pageContext 对象类型：pageContext pageContext对象是一个比较特殊的对象。它相当于页面中所有其他对象功能的最大集成者，即使用它可以访问到本页面中所有其他对象 session 对象类型：java.servlet.http.HttpSession session是与请求有关的会话期，用来表示和存储当前页面的请求信息。 application 对象类型：javax.servlet.ServletContext 用于实现用户之间的数据共享（多使用于网络聊天系统）。 exception 对象类型：java.lang.Throwable 作用 exception内置对象是用来处理页面出现的异常错误。 7.2 JSP四大域对象 九大内置对象中，存在四个较为特殊的对象，这四个对象用户在不同的作用域中存储数据，获取数据，删除数据 域对象的特点：每一个内置对象，都类似一个Map集合，可以存取删除数据，都具备如下三个方法： 存储数据：setAttribute(String key, Object value); 获取数据：Object value &#x3D; getAttribute(String); 删除数据： removeAttribute(String key); 四大内置对象，分别指的是： pageContext: (作用域：1个页面) 页面上下文，存储在pageContext中的数据, 作用域是最小的, pageContext在JSP代码执行时 创建, 在JSP代码执行完毕时, 销毁. request: (作用域：一次请求，如果请求被转发，可能跨越多个页面) 请求对象, 存储在请求对象中的数据, 域范围是一次请求, 请求一旦进行了响应, 就会被销毁. session: (作用域：一次会话，一次会话可能包含多个请求) 会话对象，存储在会话对象中的数据，只有在当前用户会话中可以使用，用户再次访问服务器的时间间隔超过30分钟，session就销毁了。 application: (域范围：一次服务，应用从启动到关闭application一直都在) Servlet上下文对象, 存储在application中的数据, 域范围是最大的. 在应用关闭之前 都可以使用. 7.3 EL表达式 用于将计算的结果输出到网页，也常用于快速的从域对象中取出数据，并输出到网页。 格式：$&#123;表达式&#125; EL表达式用于运算 在JSP中, 可以直接使用el表达式运算一些数据，例如: ${123+123} , 最终网页中显示的效果是: 246 用于取出域对象中的数据 取出数据直接输出：$&#123;域对象中存储的键&#125; 如果取出的数据不存在, 则不输出 (不可能显示null) 取出对象数据的属性值: 格式1： ${对象存储的键.属性名} 格式2： ${对象存储的键[“属性名”]} 格式3(动态取值)： ${对象存储的键[属性存储的键]} 取出集合中的数据 格式: ${集合存储时的key[下标]} 7.4 EL表达式取出数据的流程 四个域对象之间, 有时数据的键可能重复,优先从域范围较小的对象中, 取出数据. 步骤: 先从pageContext中, 寻找数据是否存在. 如果pageContext中数据不存在, 则去request中寻找数据是否存在 如果request 中数据不存在, 则去session中寻找数据是否存在 如果session中数据不存在, 则去application中寻找数据是否存在 如果application中数据不存在,则不输出任何数据. 8. taglib指令用于在JSP文件中，引入标签库文件。 格式： &lt;%@ taglib prefix=&quot;&quot; uri=&quot;&quot; %&gt; prefix: 是引入标签库后，标签库的名称。作用是用于区分引入的多个标签库，在使用标签库中的标签时，标签的写法：&lt;标签库名称:标签名&gt; uri: 每个标签库，都会拥有一个uri，它是用于区分标签库的，我们在引入这个库时，需要匹配uri属性 JSTL(JSP Standard Tag Library): JSP标准标签库 使用时，需要引入jar文件 if 标签，格式：&lt;库名称:if text&#x3D;”${ booble }”&gt; forEach 标签，格式：&lt;库名称:forEach items&#x3D;”${ List }” var&#x3D;”item”&gt; 自定义标签库: 编写一个Java类, 继承SimpleTagSupport类. 重写父类的doTag方法. 在doTag方法中, 通过getJspContext方法, 的到JSP页面的上下文 通过上下文对象, 得到JSP中的out对象, 通过out对象, 向网页中输出内容 编写tld文件 , 描述标签库 以及 标签. 自定义标签库案例: 1234567891011121314public class MyTag1 extends SimpleTagSupport &#123; private static ArrayList&lt;String&gt; data = new ArrayList&lt;&gt;(); static &#123; data.add(&quot;流水在碰到底处时才会释放活力。——歌德&quot;); &#125; @Override public void doTag() throws JspException, IOException &#123; JspContext context = getJspContext(); JspWriter out = context.getOut(); Random r = new Random(); int index = r.nextInt(data.size()); out.println(&quot;&lt;span&gt;&quot;+data.get(index)+&quot;&lt;/span&gt;&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728&lt;taglib xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot;xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-jsptaglibrary_2_0.xsd&quot;version=&quot;2.0&quot;&gt; &lt;!-- 描述标签库 --&gt; &lt;!-- 是对于标签库的介绍 --&gt; &lt;description&gt;我们这个标签库, 是闲的慌 , 所以写的.&lt;/description&gt; &lt;!-- 描述标签库的名称 --&gt; &lt;display-name&gt;xdl&lt;/display-name&gt; &lt;!-- 标签库的版本 --&gt; &lt;tlib-version&gt;11.88&lt;/tlib-version&gt; &lt;!-- 建议的短命名称 --&gt; &lt;short-name&gt;xdl&lt;/short-name&gt; &lt;!-- 标签库的表示, 用于引入时匹配标签库 --&gt; &lt;uri&gt;http://shuidianshuisg.com&lt;/uri&gt; &lt;!-- 开始描述标签 --&gt; &lt;tag&gt; &lt;!-- 对于标签的介绍 --&gt; &lt;description&gt;这个标签用于随机向网页中, 输出一句名言&lt;/description&gt; &lt;!-- 标签名称 --&gt; &lt;name&gt;heiheihei&lt;/name&gt; &lt;!-- 标签所对应的的Java类 --&gt; &lt;tag-class&gt;cn.xdl.tag.MyTag1&lt;/tag-class&gt; &lt;!-- 标签的内容 --&gt; &lt;body-content&gt;empty&lt;/body-content&gt; &lt;/tag&gt;&lt;/taglib&gt; 9. JavaWeb三大组件(Servlet,filter,Lister)9.1 Filter过滤器 请求的过滤器，面向切面编程思想（AOP） 使用步骤： 编写一个类，实现Filter接口 通过注解或web.xml配置过滤器规则 过滤器链： 当多个过滤器，过滤同一个请求地址时，就形成了过滤器链，所有过滤器都放行后，servlet才会处理用户请求 过滤器链执行顺序：（若同时包含注解与web.xml,优先执行web.xml） 注解方式：按照类名的自然顺序先后 web.xml配置方式：按照web.xml配置顺序，先后执行 案例： 123456789101112131415161718192021222324252627282930313233343536373839@WebFilter(&quot;/home.jsp&quot;)public class AdminFilter implements Filter &#123; /** * 当Filter即将销毁时执行 */ @Override public void destroy() &#123; &#125; /** * 有新的请求, 满足了过滤器的过滤规则, 正在过滤 * 参数1. 请求对象 * 参数2. 响应对象 * 参数3. 过滤器链对象 */ @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;过滤管理员登录的过滤器 正在执行&quot;); //1. 从请求中, 得到session HttpServletRequest req = (HttpServletRequest) request; HttpSession session = req.getSession(); //2. 判断session中是否存在username Object username = session.getAttribute(&quot;username&quot;); //3. 如果存在, 且值为admin , 则放行 if(username !=null &amp;&amp; username.equals(&quot;admin&quot;)) &#123; //放行 chain.doFilter(request, response); &#125;else &#123; //4. 否则拦截, 并响应, 提示请先以管理员身份登录 response.getWriter().append(&quot;&lt;script&gt;alert(&#x27;请先以管理员身份登录, 再访问管理页面&#x27;);window.location.href=&#x27;login.jsp&#x27;&lt;/script&gt;&quot;); &#125; &#125; /** * 当Filter初始化时 执行 */ @Override public void init(FilterConfig arg0) throws ServletException &#123; &#125;&#125; web.xml配置方式 12345678&lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;cn.xdl.demo1.EnCodingFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;/home.jsp&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 9.2 Listener监听器 监听服务器的一些状态事件，事件驱动机制。 分为两类状态事件： 服务器中组件的生命周期 一些域对象中数据变化的事件 监听服务器的启动与关闭：ServletContextListener 监听ServletContext中数据的增加,删除,以及替换：ServletContextAttributeListener 监听Session会话的开启与关闭：HttpSessionListener 监听session中数据的增加,删除,以及替换：HttpSessionAttributeListener 10. JSON在Java中的使用 JSON：JavaScript Object Notation GSON.jar，将Java中的对象转换为JSON字符串，将JSON字符串转换为Java中的对象 1234//引入jar文件Gson g = new Gson();String str = g.toJson(Java对象);//转换JSON字符串类型 对象名 = g.fromJson(Json字符串, 类型.class);//转换为Java对象 11. AJAX 一种用于网页异步请求的技术，用于与服务器进行异步交互以及对网页局部刷新操作 Ajax请求的状态（readyState） 0：正在初始化 1：请求正在发送 2：请求发送完毕 3：服务器开始响应 4：响应接收完毕，连接断开 Ajax响应的状态（status） 200：成功 404：找不到资源 500：服务器错误 11.1 GET请求AJAX12345678910var xhr = new XMLHttpRequest();xhr.open(&quot;GET&quot;, &quot;地址?参数列表&quot;);xhr.onreadystatechange = function()&#123; if(xhr.readyState === 4 &amp;&amp; xhr.status === 200)&#123; //通过xhr.responseText接收响应体 &#125;else&#123; //失败处理 &#125;&#125;xhr.send(); 11.2 POST请求AJAX123456789101112var xhr = new XMLHttpRequest();xhr.open(&quot;POST&quot;, &quot;地址&quot;);xhr.onreadystatechange = function()&#123; if(xhr.readyState === 4 &amp;&amp; xhr.status === 200)&#123; //通过xhr.responseText接收响应体 &#125;else&#123; //失败处理 &#125;&#125;//POST请求设置请求头xhr.setRequestHeader(&#x27;Content-Type&#x27;, &#x27;application/x-www-form-urlencoded&#x27;); xhr.send(参数列表); //发送请求参数 11.2 Jquery中的AJAX $.ajax(&#123;url,[settings]&#125;) 1234567891011$.ajax(&#123; url:&quot;请求的网址&quot;, type:&quot;请求方式GET/POST...&quot;, async:&quot;请求是否异步, 默认true&quot;, data:&quot;请求的参数列表, 格式与GET请求?后的格式一致&quot;, dataType:&quot;TEXT或JSON&quot;,//服务器返回的数据类型 success:function(data)&#123;//当服务器响应状态码在200-299之间时, 这里执行 //参数data:就是响应的内容, 当dataType为TEXT时, 类型为string , 当dataType为JSON时, 类型为Object &#125;, error:function()&#123;&#125; //当服务器响应状态码不再200-299之间时, 这里执行&#125;); $.get(url, [data], [callback], [type]) 123$.get(&quot;请求的网址&quot;, &#123; 请求参数键值对 &#125;,function(data)&#123; //data:响应的内容&#125;); $.post(url, [data], [callback], [type]) 123$.post(&quot;请求的网址&quot;, &#123; 请求参数键值对 &#125;,function(data)&#123; //data:响应的内容&#125;, &quot;json&quot;); $.getJSON(url, [data], [callback]) 123$.getJSON(&quot;请求的网址&quot;, &#123; 请求参数键值对 &#125;,function(data)&#123; //data:响应的内容&#125;); jquery对象.load(url, [data], [callback]) 载入远程 HTML 文件代码并插入至 DOM 中，load函数是使用jquery对象来调用.返回的结果无需解析, 直接显示到调用函数的jquery对象中。 123$(&quot;#dom&quot;).load(&quot;请求的网址&quot;, &#123; 请求参数键值对 &#125;,function()&#123; //加载成功&#125;); 11.3 Vue中的AJAX 使用vue的ajax , 除了需要引入vue.js以外, 还需要引入vue-resource.js 不创建Vue对象的情况下, 使用的ajax: Vue.http.get(&quot;请求地址&quot;,[&quot;请求的参数&quot;]).then(success,error); Vue.http.post(&quot;请求地址&quot;,[&quot;请求的参数&quot;],&#123;&quot;emulateJSON&quot;:true&#125;).then(success,error); 创建Vue实例, 使用ajax this.$http.get(&quot;请求地址&quot;,[&quot;请求的参数&quot;]).then(success,error); this.$http.post(&quot;请求地址&quot;,[&quot;请求的参数&quot;],&#123;&quot;emulateJSON&quot;:true&#125;).then(success,error); 1234567891011121314//GET请求: 传递参数列表: &#123; params:&#123; 参数名1:值1, 参数名2:值2 ... &#125; &#125;POST请求: 传递参数列表:&#123; 参数名1:值1, 参数名2:值2 ...&#125; success函数 与 error函数 格式: function(res){} &#x2F;&#x2F;res , 就是响应对象, 包含了响应的相关信息 响应对象的常用属性: url : 响应的网址 body : 响应的内容 (响应体) , 如果是JSON格式, 则返回对象, 否则返回string ok : boolean值, 响应码在200-299之间时 为 true status : 响应码, 例如: 200,302,404,500 statusText :响应码对应的文字信息, 例如: 状态码为200时, 信息为ok 响应对象的常用函数: text() : 以字符串的形式, 返回响应体 json() : 以对象的形式, 返回响应体 blob() : 以二进制的形式 , 返回响应体. 11.4 AJAX缓存问题 浏览器ajax得到响应结果后, 会缓存起来，当再次访问相同地址时, 会优先使用缓存。 缓存的原理, 是按照网址来缓存的, 我们只要让我们每次请求的网址都不一样, 就可以避免缓存出现。 在请求地址加上随机参数可以比避免缓存，如:&quot;s1.do?time=&quot;+new Date().getTime(); 11.5 AJAX跨域问题 默认编写的Servlet . 不允许其他网站的ajax跨域请求. 我们只需要给servlet的响应头中加入两个键值 , 就可以允许跨域: response.addHeader(&quot;Access-Control-allow-Origin&quot;,&quot;*&quot;); response.addHeader(&quot;Access-Control-allow-Methods&quot;,&quot;GET,POST&quot;);","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」数据结构与算法入门","date":"2017-06-05T10:12:38.000Z","path":"2017/06/05/java-algorithm.html","text":"数据结构，它是储存数据的一种结构体，在此结构中储存一些数据，而这些数据之间有一定的关系。算法（Algorithm）是对特定问题求解步骤的一种描述，它是指令的有限序列，其中每一条指令表示一个或者多个操作。 1.Java数据结构(Data Structure)1.1 数据结构数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。而各数据元素之间的相互关系，又包括三个组成成分，数据的逻辑结构，数据的存储结构和数据运算结构。而一个数据结构的设计过程分成抽象层、数据结构层和实现层。 1.2 Java数据结构 数据结构在Java的语言体系中按数据的逻辑结构可以分为两大类：线性数据结构和非线性数据结构。 线性数据结构：常见的有：一维数组，线性表，栈，队列，双队列，串。 非线性数据结构：常见的有：多维数组，集合，树，图，散列表(hash)。 按数据的存储结构分为：顺序存储结构和链式存储结构 顺序存储结构:用数据元素在存储器中的相对位置来表示数据元素之间的逻辑关系。 链式存储结构：在每一个数据元素中增加一个存放地址的指针，用此指针来表示数据元素之间的逻辑关系。 1.3 线性数据结构常见的线性数据结构有：一维数组，线性表，栈，队列，双队列，串。 1.3.1 一维数组数组是根据下标进行操作的，insert根据下标插入到具体位置，它后面的元素都往后面移动一位。 插入&#x2F;更新&#x2F;删除效率比较低，而查询效率非常高; 查询效率时间复杂度是1。 Java中: String [],int [],ArrayList,Vector,CopyOnWriteArrayList等 1.3.2 线性表线性表是有序的储存结构、链式的储存结构。链表的物理储存空间是不连续的，链表的每一个节点都知道上一个节点、或者下一个节点是谁，通常用Node表示。常见的方法有：add(index,element),addFirst(element),addLast(element)。getFirst(),getLast(),get(element)等。 插入效率比较高，插入的时候只需要改变节点的前后节点的连接即可; 而查询效率就比较低。 Java中: LinkedList，LinkedMap等(两个JDK底层也做了N多优化) 1.3.3 栈Stack栈, 最主要的是要实现先进后出，后进先出的逻辑结构。来保证一些场景对逻辑顺序的要求。常用的方法有push(element)压栈，pop()出栈。 Java中: java.util.Stack, Jvm里面的线程栈 1.3.4 队列队列是一种特殊的线性数据结构，队列只能允许在队头，队尾进行添加和查询等相关操作。队列又有单项有序队列，双向队列，阻塞队列等。基本操作方法有：add(E e)加入队列，remove(),poll()等方法。 Java中: 线程池，MQ，连接池等。 1.3.5 串串：也称字符串，是由N个字符组成的优先序列。 在Java里面就是指String, 而String里面是由chat[]来进行储存(KMP算法)。 KMP算法: 一种 字符串的查找匹配算法 算法关键点: 在字符串比对的时候，主串的比较位置不需要回退(利用之前判断过信息，通过一个next数组，保存模式串中前后最长公共子序列的长度，每次回溯时，通过next数组找到，前面匹配过的位置，省去了大量的计算时间) 1.4 非线性数据结构常见的线性数据结构有：多维数组，集合，树，图，散列表(hash)。 1.4.1 多维数组多维数组无非就是String [][],int[][]等 Java里面很少提供这样的工具类，而java里面tree和图底层的native方法用了多维数组来储存。 1.4.2 集合 由一个或多个确定的元素所构成的整体叫做集合。在Java里面可以去广义的去理解为实现了Collection接口的类都叫集合。 1.4.3 树 树的特点: 有且仅有一个根节点; 其他结点有且只有一个直接父节点; 可以有任意多个直接子节点 树的数据结构又分为： 自由树&#x2F;普通树：对子节点没有任何约束。 二叉树：每个节点最多含有两个子节点的树称为二叉树。 一般二叉树, 完全二叉树, 满二叉树 二叉搜索树&#x2F;BST：binary search tree,又称二叉排序树、二叉查找树。是有序的。 二叉平衡树，AVL树，红黑树 B-tree：又称B树、B-树, 又叫平衡(balance)多路查找树, 每个节点存储M&#x2F;2到M个关键字，所有关键字在整颗树中出现，且只出现一次，非叶子节点可以命中； B+tree：又称B+树, 在B树基础上，为叶子节点增加链表指针，所有关键字都在叶子节点中出现(有序)，叶子节点才命中； Btree：又称B树, 在B+树基础上，为非叶子节点也增加兄弟链表指针，将节点的最低利用率从1&#x2F;2提高到2&#x2F;3； 红黑树的5条性质： 每个结点要么是红的，要么是黑的。 根结点是黑的。 每个叶结点（叶结点即指树尾端NIL指针或NULL结点）是黑的。 如果一个结点是红的，那么它的俩个儿子都是黑的。 对于任一结点而言，其到叶结点树尾端NIL指针的每一条路径都包含相同数目的黑结点。 B+树的三个特点： 关键字数和子树相同 在 B+ 树中，节点的关键字代表子树的最大值，因此关键字数等于子树数。 非叶子节点仅用作索引，它的关键字和子节点有重复元素 除叶子节点外的所有节点的关键字，都在它的下一级子树中同样存在，最后所有数据都存储在叶子节点中。 根节点的最大关键字其实就表示整个 B+ 树的最大元素。 叶子节点用指针连在一起 叶子节点包含了全部的数据，并且按顺序排列，B+ 树使用一个链表将它们排列起来，这样在查询时效率更快。 1.4.4 Hash Hash，一般翻译做“散列”，也有直接音译为“哈希”的，就是把任意长度的输入（又叫做预映射， pre-image），变换成固定长度的输出，该输出就是散列值。一般通过Hash算法实现。（如：MD5,SHA1,加解密算法等） 简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。 Java中的hashCode： 默认情况就是native方法通过对象的内存的+对象的值然后通过hash散列算法计算出来个int的数字。最大的特性是：不同的对象，不同的值有可能计算出来的hashCode可能是一样的。 Hash表： Hash表综合了数组和链表两种数据结构。如：HashTable,HashMap。哈希表具有较快（常量级）的查询速度，及相对较快的增删速度，所以很适合在海量数据的环境中使用。一般实现哈希表的方法采用“拉链法”，我们可以理解为“链表的数组”。 需要注意的是，相同的内容算出来的hash一定是一样的。既：幂等性。 1.4.4.5 图 图状结构或网状结构：结构中的数据元素之间存在多对多的关系。 2. 时间复杂度与空间复杂度理解了Java数据结构，还必须要掌握一些常见的基本算法。 理解算法之前必须要先理解的几个算法的概念： 空间复杂度：一句来理解就是，此算法在规模为n的情况下额外消耗的储存空间。 时间复杂度：一句来理解就是，此算法在规模为n的情况下，一个算法中的语句执行次数称为语句频度或时间频度。 稳定性：主要是来描述算法，每次执行完，得到的结果都是一样的，但是可以不同的顺序输入，可能消耗的时间复杂度和空间复杂度不一样。 2.1 时间复杂度一个算法花费的时间与算法中语句的执行次数成正比例，哪个算法中语句执行次数多，它花费时间就多。一个算法中的语句执行次数称为语句频度或时间频度。记为T(n) 在刚才提到的时间频度中，n称为问题的规模，当n不断变化时，时间频度T(n)也会不断变化。但有时我们想知道它变化时呈现什么规律。为此，我们引入时间复杂度概念。 一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n),使得当n趋近于无穷大时，T(n)&#x2F;f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数。记作T(n)&#x3D;O(f(n)),称O(f(n)) 为算法的渐进时间复杂度，简称时间复杂度。 有时候，算法中基本操作重复执行的次数还随问题的输入数据集不同而不同，如在冒泡排序中，输入数据有序而无序，其结果是不一样的。此时，我们计算平均值。 常见的算法的时间 复杂度之间的关系为：O(1)&lt;O(logn)&lt;O(n)&lt;O(nlog n)&lt;O(n2)&lt;O(2n)&lt;O(n!)&lt;O(nn) 2.2 空间复杂度空间复杂度：算法所需存储空间的度量，记作：S(n)=O( f(n) )，其中 n 为问题的规模。 一个算法在计算机存储器上所占用的存储空间，包括存储算法本身所占用的存储空间，算法的输入输出数据所占用的存储空间和算法在运行过程中临时占用的存储空间这三个方面。如果额外空间相对于输入数据量来说是个常数，则称此算法是原地工作。 算法的输入输出数据所占用的存储空间是由要解决的问题决定的，是通过参数表由调用函数传递而来的，它不随本算法的不同而改变。存储算法本身所占用的存储空间与算法书写的长短成正比，要压缩这方面的存储空间，就必须编写出较短的算法。 3.算法的基本概念 算法: 简单来说就是解决问题的步骤。 算法的五个特征:有穷性，确定性，可行性，有输入，有输出 有穷性：对于任意一组合法输入值，在执行又穷步骤之后一定能结束，即：算法中的每个步骤都能在有限时间内完成。 确定性：在每种情况下所应执行的操作，在算法中都有确切的规定，使算法的执行者或阅读者都能明确其含义及如何执行。并且在任何条件下，算法都只有一条执行路径。 可行性：算法中的所有操作都必须足够基本，都可以通过已经实现的基本操作运算有限次实现之。 有输入：作为算法加工对象的量值，通常体现在算法当中的一组变量。有些输入量需要在算法执行的过程中输入，而有的算法表面上可以没有输入，实际上已被嵌入算法之中。 有输出：它是一组与“输入”有确定关系的量值，是算法进行信息加工后得到的结果，这种确定关系即为算法功能。 算法的设计原则：正确性，可读性，健壮性，高效率与低存储量需求 描述算法的速度必须要和数据项的个数联系起来。 算法的存储量，包括： 程序本身所占空间； 输入数据所占空间； 辅助变量所占空间； 一个算法的效率越高越好，而存储量是越低越好。 4. 常用的查找算法4.1 线性（顺序）查找算法 使用目标元素与样本数列中第一个元素起依次进行比较 若目标元素等于样本元素，则表示查找成功 若目标元素与样本元素比较完毕也不相等，则表示查找失败 4.2 二分查找算法二分查找又称折半查找，优点是比较次数少，查找速度快，平均性能好，占用系统内存较少；其缺点是要求待查表为有序表，且插入删除困难。 普通循环实现二分查找算法 12345678910111213141516171819202122232425262728public static void main(String[] args) &#123; int srcArray[] = &#123;3,5,11,17,21,23,28,30,32,50,64,78,81,95,101&#125;; System.out.println(binSearch(srcArray, 28));&#125;/** * 二分查找普通循环实现 * * @param srcArray 有序数组 * @param key 查找元素 * @return */public static int binSearch(int srcArray[], int key) &#123; int mid = srcArray.length / 2; if (key == srcArray[mid]) return mid; int start = 0; int end = srcArray.length - 1; while (start &lt;= end) &#123; mid = (end - start) / 2 + start; if (key &lt; srcArray[mid]) &#123; end = mid - 1; &#125; else if (key &gt; srcArray[mid]) &#123; start = mid + 1; &#125; else &#123; return mid; &#125; &#125; return -1;&#125; 二分查找算法如果没有用到递归方法的话，只会影响CPU。对内存模型来说影响不大。时间复杂度log2n，2的开方。空间复杂度是2。一定要牢记这个算法。应用的地方也是非常广泛，平衡树里面大量采用。 递归实现二分查找递归实现算法 123456789101112131415161718192021222324252627public static void main(String[] args) &#123; int srcArray[] = &#123;3,5,11,17,21,23,28,30,32,50,64,78,81,95,101&#125;; System.out.println(binSearch(srcArray, 0,15,28));&#125;/** * 二分查找递归实现 * * @param srcArray 有序数组 * @param start 数组低地址下标 * @param end 数组高地址下标 * @param key 查找元素 * @return 查找元素不存在返回-1 */public static int binSearch(int srcArray[], int start, int end, int key) &#123; int mid = (end - start) / 2 + start; if (srcArray[mid] == key) &#123; return mid; &#125; if (start &gt;= end) &#123; return -1; &#125; else if (key &gt; srcArray[mid]) &#123; return binSearch(srcArray, mid + 1, end, key); &#125; else if (key &lt; srcArray[mid]) &#123; return binSearch(srcArray, start, mid - 1, key); &#125; return -1;&#125; 递归不光影响的CPU。JVM里面的线程栈空间也会变大。所以当递归的调用链长的时候需要-Xss设置线程栈的大小。 4. 常用的排序算法 八大排序算法 一、直接插入排序（Insertion Sort） 二、希尔排序（Shell Sort） 三、选择排序（Selection Sort） 四、堆排序（Heap Sort） 五、冒泡排序（Bubble Sort） 六、快速排序（Quick Sort） 七、归并排序（Merging Sort） 八、基数排序（Radix Sort） 4.1 冒泡排序算法冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述： 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤①~③，直到没有任何一对数字需要比较。 代码实现： 123456789101112131415public static void bubbleSort(int[] arr)&#123; for (int i=1; i&lt;arr.length; i++)&#123; boolean flag = true;//声明标志位 for(int j=0; j&lt;arr.length-i; j++)&#123; if(arr[j] &gt; arr[j+1])&#123; int temp = arr[j+1]; arr[j] = arr[j+1]; arr[j++1] = temp; flag = false; &#125; &#125; //若此轮结束flag还是为true,则证明已经有序 if(flag) break; &#125;&#125; 冒泡排序算法复杂度: 平均时间复杂度O(n²)，最好情况O(n)，最坏情况O(n²)，空间复杂度O(1) 冒泡排序是最容易实现的排序, 最坏的情况是每次都需要交换, 共需遍历并交换将近n²&#x2F;2次, 时间复杂度为O(n²). 最佳的情况是内循环遍历一次后发现排序是对的, 因此退出循环, 时间复杂度为O(n). 平均来讲, 时间复杂度为O(n²). 由于冒泡排序中只有缓存的temp变量需要内存空间, 因此空间复杂度为常量O(1). Tips:由于冒泡排序只在相邻元素大小不符合要求时才调换他们的位置, 它并不改变相同元素之间的相对顺序, 因此它是稳定的排序算法。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」常用设计模式","date":"2017-05-30T00:34:55.000Z","path":"2017/05/30/java-design.html","text":"设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性。 1.常用的设计原则 开闭原则：对扩展开放，对修改关闭 里氏代换原则：任何父类出现的的地方，子类一定可以出现（多使用继承和多态） 依赖倒转原则：尽量多依赖于抽象类或接口而不是具体实现类，对子类具有强制性和规范性 接口隔离原则：尽量多依赖小接口而不是大接口 迪米特法则（最少知道原则）：一个实体应当少与其他实体之间发生相互作用，使系统功能模块相对独立。高内聚，低耦合。 合成复用原则：尽量多使用合成&#x2F;聚合的方式，而不是继承的方式。 2.设计模式分类2.1 基本概念 设计模式是一套被反复使用多数人知晓，经过分类编目，代码设计经验的总结。 设计模式用来解决某些特定场景下的某一类问题–&gt;通用的解决方案。 设计模式可以让代码更容易被理解，确保了复用性、可靠性、可扩展性 2.2 具体分类 创建型模式：用于对象创建的过程 单例模式、工厂方法模式、抽象工厂模式、建造者模式(生成器模式)、原型模式 结构型模式：用于把类或对象通过某种形式结合在一起，构成某种复杂或合理的结构 适配器模式、装饰者模式、代理模式、外观模式、桥接模式、组合模式、享元模式(过滤器&#x2F;标准模式) 行为型模式：用于解决类或对象之间的交互，更合理的优化类或对象之间的关系 责任链模式、命令模式、迭代子模式(迭代器模式)、观察者模式、中介者模式、解析器模式、状态模式、空对象模式、策略模式、模板模式、访问者模式、备忘录模式、 JEE 设计模式 数据访问对象模式 3.单例模式（Singleton）3.1 实现流程： 私有的构造方法 私有的静态的当前类的对象作为属性 共有的静态方法返回当前对象3.1 实现方式： 饿汉式：立即加载，对象启动时就加载 懒汉式：延迟加载，对象什么时候用到时才会加载 生命周期托管：单例对象交给别人处理 4.模板模式在模板模式中，父抽象类公开几个抽象方法供子类实现。在父抽象类中有另一个方法或几个方法使用抽象方法来实现业务逻辑。 eg: 对于使用不同的软件，我们只需要从抽象类继承并提供详细的实现,模板模式是一种行为模式。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 // 抽象类abstract class Software &#123; abstract void initialize(); abstract void start(); abstract void end(); public final void play()&#123; initialize(); start(); end(); &#125;&#125; // 不同子类以不同方法实现抽象类的的方法class Browser extends Software &#123; @Override void end() &#123; System.out.println(&quot;Browser Finished!&quot;); &#125; @Override void initialize() &#123; System.out.println(&quot;Browser Initialized!.&quot;); &#125; @Override void start() &#123; System.out.println(&quot;Browser Started.&quot;); &#125;&#125;class Editor extends Software &#123; @Override void end() &#123; System.out.println(&quot;Editor Finished!&quot;); &#125; @Override void initialize() &#123; System.out.println(&quot;Editor Initialized!&quot;); &#125; @Override void start() &#123; System.out.println(&quot;Editor Started!&quot;); &#125;&#125;// 使用public class Main &#123; public static void main(String[] args) &#123; Software s1 = new Browser(); s1.play(); s1 = new Editor(); s1.play(); &#125;&#125; 4.1 模板方法模式优缺点： 优点 模板方法模式通过把不变的行为搬移到超类，去除了子类中的重复代码。子类实现算法的某些细节，有助于算法的扩展。通过一个父类调用子类实现的操作，通过子类扩展增加新的行为，符合“开放-封闭原则”。 缺点 每个不同的实现都需要定义一个子类，这会导致类的个数的增加，设计更加抽象。 适用场景 在某些类的算法中，用了相同的方法，造成代码的重复。控制子类扩展，子类必须遵守算法规则。 5. 工厂模式 简单工厂模式：一个工厂方法，依据传入的参数，生成对应的产品对象； 工厂方法模式：将工厂提取成一个接口或抽象类，具体生产什么产品由子类决定； 抽象工厂模式：为创建一组相关或者是相互依赖的对象提供的一个接口，而不需要指定它们的具体类。 5.1 简单工厂模式的实现：1234567891011121314151617181920212223242526 // 产品接口public interface Fruit &#123; void whatIm(); &#125; // 具体类public class Apple implements Fruit &#123; @Override public void whatIm() &#123; /*苹果*/&#125;&#125;public class Pear implements Fruit &#123; @Override public void whatIm() &#123; /* 梨 */ &#125;&#125; // 工厂public class FruitFactory &#123; public Fruit createFruit(String type) &#123; if (type.equals(&quot;apple&quot;)) &#123;//生产苹果 return new Apple(); &#125; else if (type.equals(&quot;pear&quot;)) &#123;//生产梨 return new Pear(); &#125; return null; &#125;&#125; // 使用FruitFactory mFactory = new FruitFactory();Apple apple = (Apple) mFactory.createFruit(&quot;apple&quot;);//获得苹果Pear pear = (Pear) mFactory.createFruit(&quot;pear&quot;);//获得梨 简单工厂只适合于产品对象较少，且产品固定的需求 5.2 工厂方法模式实现：12345678910111213141516171819202122 // 工厂接口public interface FruitFactory &#123; Fruit createFruit();//生产水果&#125; // 具体工厂public class AppleFactory implements FruitFactory &#123; @Override public Fruit createFruit() &#123; return new Apple(); &#125;&#125;public class PearFactory implements FruitFactory &#123; @Override public Fruit createFruit() &#123; return new Pear(); &#125;&#125; // 使用AppleFactory appleFactory = new AppleFactory();PearFactory pearFactory = new PearFactory();Apple apple = (Apple) appleFactory.createFruit();//获得苹果Pear pear = (Pear) pearFactory.createFruit();//获得梨 工厂方法模式虽然遵循了开闭原则，但如果产品很多的话，需要创建非常多的工厂 5.3 抽象工厂模式实现： 抽象工厂和工厂方法的模式基本一样，区别在于，工厂方法是生产一个具体的产品，而抽象工厂可以用来生产一组相同，有相对关系的产品；重点在于一组，一批，一系列； eg：假如生产小米手机，小米手机有很多系列，小米note、红米note等；假如小米note生产需要的配件有825的处理器，6英寸屏幕，而红米只需要650的处理器和5寸的屏幕就可以了；用抽象工厂来实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 // cpu接口和实现类public interface Cpu &#123; void run(); class Cpu650 implements Cpu &#123; @Override public void run() &#123;/* 625 也厉害 */ &#125; &#125; class Cpu825 implements Cpu &#123; @Override public void run() &#123; /* 825 处理更强劲 */ &#125; &#125;&#125; // 屏幕接口和实现类public interface Screen &#123; void size(); class Screen5 implements Screen &#123; @Override public void size() &#123;/* 5寸 */&#125; &#125; class Screen6 implements Screen &#123; @Override public void size() &#123; /* 6寸 */ &#125; &#125;&#125; // 工厂接口public interface PhoneFactory &#123; Cpu getCpu();//使用的cpu Screen getScreen();//使用的屏幕&#125; // 具体工厂实现类public class XiaoMiFactory implements PhoneFactory &#123; @Override public Cpu getCpu() &#123; return new Cpu.Cpu825();//高性能处理器 &#125; @Override public Screen getScreen() &#123; return new Screen.Screen6();//6寸大屏 &#125;&#125;public class HongMiFactory implements PhoneFactory &#123; @Override public Cpu getCpu() &#123; return new Cpu.Cpu650();//高效处理器 &#125; @Override public Screen getScreen() &#123; return new Screen.Screen5();//小屏手机 &#125;&#125; 对于大批量，多系列的产品，用抽象工厂可以更好的管理和扩展； 5.4 三种工厂方式总结： 对于简单工厂和工厂方法来说，两者的使用方式实际上是一样的，如果对于产品的分类和名称是确定的，数量是相对固定的，推荐使用简单工厂模式； 抽象工厂用来解决相对复杂的问题，适用于一系列、大批量的对象生产； 6.适配器模式（Adapter） 适配器模式Adapter是结构型模式的一种，分为类适配器模式，对象适配器模式，缺省适配器模式。 类的适配器模式把适配的类的API转换成为目标类的API。使用对象继承的方式，是静态的定义方式； 对象的适配器模式把被适配的类的API转换成为目标类的API，与类的适配器模式不同的是，对象的适配器模式不是使用继承关系，而是使用委派关系。一个适配器可以把多种不同的源适配到同一个目标。 适配器模式的缺点过多的使用适配器，会让系统非常零乱，不易整体进行把握。比如，明明看到调用的是A接口，其实内部被适配成了B接口的实现，一个系统如果太多出现这种情况，无异于一场灾难。因此如果不是很有必要，可以不使用适配器，而是直接对系统进行重构。 6.1 缺省适配器模式 缺省适配(Default Adapter)模式为一个接口提供缺省实现，这样子类型可以从这个缺省实现进行扩展，而不必从原有接口进行扩展。作为适配器模式的一个特例，缺省是适配模式在JAVA语言中有着特殊的应用。 缺省适配模式是一种“平庸”化的适配器模式。(实现类不必实现接口所有方法或留空的方法，可以有选择性了) 适配器(通常是一个抽象类)添加某些具体实现(需要缺省的方法内部抛出异常)。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」网络编程基础","date":"2017-05-22T03:59:46.000Z","path":"2017/05/22/java-net.html","text":"网络编程是指编写运行在多个设备（计算机）的程序，这些设备都通过网络连接起来。java.net 包中的类和接口，它们提供低层次的通信细节。你可以直接使用这些类和接口，来专注于解决问题，而不用关注通信细节。 1. 网络编程常识1.1 七层网络模型为了保证数据传输的可靠和安全，ISO(国际标准委员会组织)将数据的传递从逻辑上划分为以下7层：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层 当发送数据时，需要按照上述七层模型从上到下层层加包再发送出去； 当接收数据时，需要按照上述七层模型从下到上层层拆包再显示出来； 1.2 IP地址 IP地址：是互联网中的唯一地址标识，也就是根据IP地址可以定位到具体某一台设备，IP地址本质上是32位二进制组成的整数叫做IPv4，当然也有128位二进制组成的整数叫做IPv6，目前主流的还是IPv4。 日常生活中采用点分十进制表示法进行IP地址的描述，也就是将每个字节的二进制转换为一个十进制整数，不同的十进制整数之间采用小数点隔开。如：192.168.1.1 1.3 端口号 根据IP地址可以定位到具体某一台设备，而该设备中启动的进程可能很多，此时可以使用端口号来定位该设备中的具体某一个进程。 网络编程需要提供：IP地址 和 端口号 端口号是16位二进制组成的整数，表示范围是：0 ~ 65535，其中0 ~ 1024之间通常被系统占用，因此网络编程需要从1025开始使用。 1.4 tcp协议与udp协议 TCP（Transmission Control Protocol，传输控制协议） 是面向连接的协议，也就是说，在收发数据前，必须和对方建立可靠的连接。一个TCP连接必须要经过三次“握手”才能建立起来。 UDP（User Data Protocol，用户数据报协议） 是一个非连接的协议，传输数据之前源端和终端不建立连接，当它想传送时就简单地去抓取来自应用程序的数据，并尽可能快地把它扔到网络上。在发送端，UDP传送数据的速度仅仅是受应用程序生成数据的速度、计算机的能力和传输带宽的限制；在接收端，UDP把每个消息段放在队列中，应用程序每次从队列中读一个消息段。 tcp协议与udp协议比较： tcp协议 udp协议 传输控制协议，面向连接 用户数据报协议，非面向连接 通信过程全程保持连接 通信过程不需要全程连接 保证了数据传输的可靠性和有序性 不保证数据传输的可靠性和有序性 全双工的字节流的通信方式 全双工的数据报的通信方式 服务器的资源消耗多，压力大，效率低 服务器资源消耗少，压力小，效率高 2. 基于tcp协议的编程模型2.1 编程模型1234567服务器端 客户端创建监听服务等待连接 &lt;----建立连接------ 连接服务器 进行通讯 &lt;----进行通讯-----&gt; 进行通讯关闭连接 关闭连接 服务器： 创建ServerSocket类型的对象并提供端口号； 等待客户端的连接请求，调用accept方法； 使用输入输出流进行通信； 关闭Socket； 客户端： 创建Socket类型的对象并提供服务器的通信地址和端口号； 使用输入输出流进行通信； 关闭Socket； 2.2 ServerSocket类和Socket类 java.net.ServerSocket类主要用于描述服务器套接字信息。 常用方法 ServerSocket(int port) 根据参数指定的端口号来构造对象 Socket accept() 监听并接收到此套接字的连接请求 void close() 用于关闭套接字 java.net.Socket类主要用于描述客户端套接字，是两台机器间通信的端点。 常用方法 Socket(String host, int port) 根据指定主机名和端口号来构造对象 InputStream getInputStream() 用于获取当前套接字的输入流 OutputStream getOutputStream() 用于获取当前套接字的输出流 void close() 用于关闭套接字 3.客户端与服务端通信演示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103//服务端线程public class ServerThread extends Thread &#123; private Socket s; public ServerThread(Socket s) &#123; this.s = s; &#125; @Override public void run() &#123; try &#123; // 3.使用输入输出流进行通信 BufferedReader br = new BufferedReader( new InputStreamReader(s.getInputStream())); PrintStream ps = new PrintStream(s.getOutputStream()); while(true) &#123; // 实现服务器接收到字符串内容后打印出来 // 当客户端没有发送数据时，服务器会在这里阻塞 String str = br.readLine(); //System.out.println(&quot;服务器接收到的数据是：&quot; + str); // 当服务器接收到&quot;bye&quot;后，则聊天结束 if(&quot;bye&quot;.equalsIgnoreCase(str)) &#123; System.out.println(&quot;客户端&quot; + s.getInetAddress() + &quot;已下线！&quot;); break; &#125; System.out.println(&quot;客户端&quot; + s.getInetAddress() + &quot;发来的消息是：&quot; + str); // 当服务器接收到客户端发来的消息后，向客户端回发消息&quot;I received!&quot; ps.println(&quot;I received!&quot;); //System.out.println(&quot;服务器发送数据成功！&quot;); &#125; // 4.关闭Socket ps.close(); br.close(); s.close(); &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;//服务端测试public class ServerStringTest &#123; public static void main(String[] args) &#123; try &#123; // 1.创建ServerSocket类型的对象并提供端口号 ServerSocket ss = new ServerSocket(8888); // 2.等待客户端的连接请求，调用accept方法 while(true) &#123; System.out.println(&quot;等待客户端的连接请求...&quot;); // 当没有客户端连接时，阻塞在accept方法的调用这里 Socket s = ss.accept(); // 获取连接成功的客户端通信地址 System.out.println(&quot;客户端&quot; + s.getInetAddress() + &quot;连接成功！&quot;); // 当有客户端连接成功后，则启动一个新的线程为之服务 new ServerThread(s).start(); &#125; //ss.close(); &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;//客户端测试public class ClientStringTest &#123; public static void main(String[] args) &#123; try &#123; // 1.创建Socket类型的对象并提供服务器的通信地址和端口号 Socket s = new Socket(&quot;XDL-20170621QCO&quot;, 8888); System.out.println(&quot;连接服务器成功！&quot;); // 2.使用输入输出流进行通信 Scanner sc = new Scanner(System.in); PrintStream ps = new PrintStream(s.getOutputStream()); BufferedReader br = new BufferedReader( new InputStreamReader(s.getInputStream())); while(true) &#123; // 希望客户端连接服务器成功后睡眠10秒再发送数据，测试服务器是否阻塞 //Thread.sleep(10000); // 练习：实现客户端向服务器发送的内容由用户从键盘输入 System.out.println(&quot;请输入要发送的内容：&quot;); //String msg = sc.next(); // 读取字符串内容时，遇到空格停止 String msg = sc.nextLine(); // 实现客户端向服务器发送字符串内容&quot;hello&quot; //ps.println(&quot;hello&quot;); ps.println(msg); System.out.println(&quot;客户端发送数据成功！&quot;); // 判断客户端发送的内容是否为&quot;bye&quot;，若是则聊天结束 if(&quot;bye&quot;.equalsIgnoreCase(msg)) &#123; System.out.println(&quot;聊天结束！&quot;); break; &#125; // 实现服务器回发消息的接收 // 当客户端没有发送数据时，服务器会在这里阻塞 String str = br.readLine(); System.out.println(&quot;客户端接收到的数据是：&quot; + str); &#125; // 3.关闭Socket br.close(); sc.close(); ps.close(); s.close(); &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 4. 基于udp协议的编程模型4.1 编程模型 主机A(接收方): 创建DatagramSocket类型的对象，并提供端口号； 创建DatagramPacket类型的对象，用于接收发来的数据； 从Socket中接收数据，调用**receive()**方法； 关闭Socket并释放有关的资源； 主机B(发送方) 创建DatagramSocket类型的对象； 创建DatagramPacket类型的对象，并提供接收方的IP地址和端口号； 通过Socket发送数据，调用**send()**方法； 关闭Socket并释放有关的资源； 4.2 DatagramSocket类 java.net.DatagramSocket类用于描述发送或接受数据报的套接字(邮局点); 常用方法 DatagramSocket() 无参的方式构造对象。 DatagramSocket(int port) 根据参数指定的端口号来构造对象。 void receive(DatagramPacket p) 用于接收数据并存放到参数指定的变量中。 void send(DatagramPacket p) 用于将参数指定的数据发送出去。 void close() 4.3 DatagramPacket类 java.net.DatagramPacket类用于描述数据报信息(信件)； 常用方法 DatagramPacket(byte[] buf, int length) 用于接收数据包并记录到参数变量中； DatagramPacket(byte[] buf, int length, InetAddress address, int port) 用于将参数指定的数据发送到参数指定的位置 InetAddress getAddress() 用于获取发送方或接收方的通信地址信息。 int getPort() 用于获取发送方或接收方的端口信息。 int getLength() 用于获取发送或接收数据的长度。 4.4 InetAddress类 java.net.InetAddress类用于描述互联网协议地址。 常用方法 static InetAddress getLocalHost() 用于获取本地主机的通信地址信息。 static InetAddress getByName(String host) 根据参数指定的主机名来获取通信地址。 String getHostName() 用于获取通信地址中的主机名信息。 String getHostAddress() 用于获取通信地址中的IP地址信息。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」反射机制","date":"2017-05-18T07:47:15.000Z","path":"2017/05/18/java-reflection.html","text":"反射(Reflection)是Java 程序开发语言的特征之一，它允许运行中的 Java 程序获取自身的信息，并且可以操作类或对象的内部属性。多数情况下反射是为了提高程序的灵活性，运行时动态加载需要加载的对象。 1. 基本概念JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为java语言的反射机制。 反射（reflect）就是把java类中的各种成分映射成一个个的Java对象；类是用来描述一组对象，反射机制可以理解为是用来描述一组类 通俗来讲，反射机制就是用于动态创建对象并且动态调用方法的机制；目前主流的框架底层都采用反射机制实现的。 1.1 相关类及描述 Class：用来描述类和接口；该类没有公共构造方法，由虚拟机和类加载器自动构造完成 Package：用来描述类所属的包 Field：用来描述类中的属性 Method：用来描述类中的方法 Constructor：用来描述类中的构造方法 Annotation：用来描述类中的注解 2. Class类java.lang.Class：用来描述类和接口；该类没有公共构造方法，由虚拟机和类加载器自动构造完成 2.1 获取Class类型对象的三种方式1234Class clazz = Class.forName(&quot;包名.类名&quot;);//用的最多，但可能抛出ClassNotFoundException异常Class clazz = 类名.class;//任何类都有一个隐含的静态成员变量classClass clazz = 对象.getClass();//Object类中的方法Class clazz = 包装类.TYPE;//获取对应基本数据类型的class对象 2.2 常用方法 static Class&lt;?&gt; forName(String className) 用于获取参数指定对应的Class对象并返回 T newInstance() 默认调用无参数构造方法创建对象，若类中不存在无参数构造方法抛出异常NoSuchMethodException Constructor getConstructor(Class&lt;?&gt;… parameterTypes) 用于获取此Class对象所表示类型中参数指定的公共构造方法。 Constructor&lt;?&gt;[] getConstructors() 用于获取此Class对象所表示类型中所有的公共构造方法 Field getDeclaredField(String name) 用于获取此Class对象所表示类中参数指定的单个成员变量信息 Field[] fs &#x3D; getDeclaredFields() 用于获取此Class对象所表示类中所有成员变量信息 Method getMethod(String name, Class&lt;?&gt;… parameterTypes) 用于获取该Class对象所表示类型中名字为name参数为parameterTypes的指定公共成员方法 Method[] getMethods() 用于获取该Class对象表示类中所有公共成员方法。 获取私有相关方法 getDeclaredConstructor(Class&lt;?&gt;… parameterTypes)；获取该类对象表示的类或接口的指定构造函数(包括私有) getDeclaredConstructors()；获取该类对象所表示的类声明的所有构造函数(包括私有) getDeclaredMethod(String name, Class&lt;?&gt;… parameterTypes) 获取一个方法(自己类 公有 私有) getDeclaredMethods(); 获取全部的方法(自己类 公有 私有) 2.3 其他方法 int result &#x3D; getModifiers(); 获取类的修饰符(权限+特征) 每一个修饰符 用一个整数来进行表示：0–默认不写，1–public，2–private，4–protected，-static， 16–final，32–synchronized，64volatile，128–transient，256–native，512–interface，1024–abstract String name &#x3D; getName(); 获取类的全名(包名.类名) String name &#x3D; getSimpleName(); 获取类简单名(只有类名 缺少包) Package p &#x3D; getPackage(); 获取当前类所属的包 p.getName(); 获取包名(Package类中的方法) Class sclazz &#x3D; getSuperClass(); 获取超类(父类)对应Class Class[] classes &#x3D; getInterface(); 获取当前类父亲接口 Class[] classes &#x3D; getClasses(); 获取类中的内部类 Object obj &#x3D; newInstance(); 默认调用无参数构造方法创建对象，若类中不存在无参数构造方法抛出异常NoSuchMethodException Field f &#x3D; getField(“属性名”); 获取类中的属性(公有的 自己类+父类) Field[] fs &#x3D; getFields(); 获取类中的全部属性(公有的 自己类+父类) getDeclaredField(“属性”); 获取当前类中的属性(公有+私有 自己类) Field[] fs &#x3D; getDeclaredFields(); 获取当前类中全部的属性(公有+私有 自己类) 3. Constructor类java.lang.reflect.Constructor类主要用于描述获取到的构造方法信息 3.1 Constructor类中的常用方法 T newInstance(Object… initargs) 使用此Constructor对象描述的构造方法来构造Class对象代表类型的新实例；该方法的参数用于给新实例中的成员变量进行初始化操作。 3.2 其他方法 con.getModifiers(); con.getName(); con.getParameterTypes(); con.getExceptionTypes(); 如何操作构造方法 执行一次,创建对象 Object &#x3D; newInstance(执行构造方法时的所有参数); con.setAccessible(true); 4. Field类java.lang.reflect.Field类主要用于描述获取到的单个成员变量信息。 4.1 Field类中的常用方法 Object get(Object obj) 调用该方法的意义就是获取参数对象obj中此Field对象所表示成员变量的数值。 Object set(Object obj, Object value) 将参数对象obj中此Field对象表示成员变量的数值修改为参数value的数值。 void setAccessible(boolean flag) 当实参传递true时，则反射的对象在使用时应该取消java语言访问检查 4.2 其他方法 int &#x3D; getModifiers(); 获取属性修饰符(权限+特征) Class &#x3D; getType(); 获取属性的类型对应的那个class String &#x3D; getName(); 获取属性的名字 操作属性: set(对象,值); Object &#x3D; get(对象); 如果是私有属性不能直接操作的，需设置一个使用权setAccessable(true);准入 5. Method类java.lang.reflect.Method类主要用于描述获取到的单个成员方法信息。 5.1 Method类中的常用方法 Object invoke(Object obj, Object… args) 使用对象obj来调用此Method对象所表示的成员方法，实参传递args。 5.2 其他方法 int mm &#x3D; m.getModifiers(); 获取方法的修饰符(权限+特征) Class mrt &#x3D; m.getReturnType(); 获取返回值数据类型 String mn &#x3D; m.getName(); 获取方法的名字 Class[] mpts &#x3D; m.getParameterTypes(); 获取方法参数列表的类型 Class[] mets &#x3D; m.getExceptionTypes(); 获取方法抛出异常的类型 如何操作方法 调用方法 让他执行一次 Object result &#x3D; invoke(对象,执行方法需要传递的所有参数…); 若方法是私有的方法 不允许操作 可以设置setAccessable(true) 设置方法使用权 准入 6. 原始方式与反射方式构造对象实例 使用原始方式来构造对象 123456789 //1.采用无参的方式构造Person对象并打印Person p = new Person();System.out.println(p); //null 0 //2.使用有参方式来构造Person对象Person p2 = new Person(&quot;zhangfei&quot;, 30);System.out.println(p2); //zhangfei 30 //3.修改与获取属性(成员变量)，调用get,set方法p2.setName(&quot;guanyu&quot;);System.out.println(&quot;修改后的姓名是：&quot; + p2.getName()); //guanyu 使用反射机制来构造对象 123456789101112131415161718192021 //1.使用获取到的Class对象来构造Person对象并打印Class c1 = Class.forName(&quot;myproject.Person&quot;);//不可省略包名System.out.println(c1.newInstance());//null 0 //2.使用有参方式来构造对象Class c2 = Class.forName(&quot;myproject.Person&quot;);Constructor ct2 = c2.getConstructor(String.class, int.class);Object obj = ct2.newInstance(&quot;zhangfei&quot;, 30);System.out.println(obj);//zhangfei 30 //3.修改与获取属性(成员变量)Field f2 = c2.getDeclaredField(&quot;name&quot;);f2.setAccessible(true);//暴力反射，设置使用权f2.set(obj, &quot;guanyu&quot;);System.out.println(&quot;修改后的姓名是：&quot; + f2.get(obj)); //guanyu //4.获取成员方法getName，使用获取到的成员方法来获取姓名并打印出来Method m1 = c2.getMethod(&quot;getName&quot;);System.out.println(&quot;获取到的姓名是：&quot; + m1.invoke(obj)); //zhangfei //5.成员方法setName，调用getMethod方法来修改姓名并打印出来Method m2 = c2.getMethod(&quot;setName&quot;, String.class);Object res = m2.invoke(obj, &quot;guanyu&quot;);System.out.println(&quot;方法调用的返回值是：&quot; + res); //nullSystem.out.println(&quot;修改后的姓名是：&quot; + m1.invoke(obj)); //guanyu 7. 注解(Annotation)7.1 注解相关概念 注释 单行注释：// 多行注释：/* */ 文档注释：/** */ 注解的写法 @XXX [(一些信息)] 注解位置 类的上面，属性上面，方法上面，构造方法上面，参数前面 注解的作用 用来充当注释的作用(仅仅是一个文字的说明)，@Deprecated 用来做代码的检测(验证)，@Override *可以携带一些信息(内容)，文件.properties&#x2F;.xml，注解 常用的注解 @Deprecated：用来说明方法是废弃的 @Override：用来做代码检测 检测此方法是否是一个重写 @SuppressWarnings(String[])：{“”}，如果数组内的元素只有一个长度，可以省略{} unused：变量定义后未被使用 serial：类实现了序列化接口 不添加序列化ID号 rawtypes：集合没有定义泛型 deprecation：方法以废弃 *unchecked：出现了泛型的问题 可以不检测 all：包含了以上所有(不推荐) 注解中可以携带信息，可以不携带；信息不能随意写，信息的类型只能是如下的类型： 基本数据类型 String类型 枚举类型enum 注解类型@ 数组类型[]，数组的内部需要是如上的四种类型 注解的分类 按运行机制分：源码注解，编译时注解，运行时注解 按照来源分：来自JDK的注解，来自第三方的注解，自定义注解 7.2 自定义注解类型的语法要求： 使用@interface关键字定义注解 成员以无参无异常方式声明 可以用default为成员指定一个默认值 成员类型是受限的，合法类型包括原始类型及String,Class,Annotation,Enumeration 如果注解只有一个成员，则成员名必须取名value(),在使用时可以忽略成员名和赋值号(&#x3D;) 注解类可以没有成员，没有成员的注解称为标识注解 需要元注解来描述说明 @Target：当前注解的放置(CONSTRUCTOR，FIELD，LOCAL_VARIABLE，METHOD，PACKAGE，PARAMETER，TYPE) @Retention：当前注解的生命周期作用域(SOURCE，CLASS，RUNTIME)，源代码文件(SOURCE)—&gt;编译—&gt;字节码文件(CLASS)—&gt;加载—&gt;内存执行(RUNTIME) @Inherited：允许子类继承 @Document：当前注解是否能被文档(javadoc)所记录 123456789@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface Description&#123; String desc(); String author(); int age() default 18;&#125; 7.3 使用自定义注解： @&lt;注解名&gt;(&lt;成员名1&gt;&#x3D;&lt;成员值1&gt;,&lt;成员名2&gt;&#x3D;&lt;成员值2&gt;,…) 1234@Description(desc=&quot;I am eyeColor&quot;, author=&quot;Chao&quot;, age=18)public String eyeColor()&#123; return &quot;red&quot;;&#125; 如果自定义注解只有一个value成员，在使用的时候就可以省略方法名，如果方法是两个以上，每一个方法必须写名字 1234567891011121314@Description(&quot;I am class annotation&quot;)public class Child implements Person&#123; @Override @Description(&quot;I am method annotation&quot;) public String name()&#123; return null; &#125; @Override public int age()&#123; return 0; &#125; @Override public void sing()&#123; &#125;&#125; 7.4 解析注解通过反射获取类、函数或成员上的运行时注解信息，从而实现动态控制程序运行的逻辑。 使用类加载器加载类 Class c=Class.forName（&quot;com.ann.test.Child&quot;) 找到类上面的注解 isAnnotationPresent（类类型）：Class对象的方法，判断当前类类型是否存在某个类类型的注解，返回类型为boolean。 拿到注解实例，需要强制类型转换。 Description d=（Description）c.getAnnotation(Description.class); 找到方法上的注解，首先，遍历所有方法，通过方法对象的isAnnotation查看是否有自定义注解。 12345678910111213141516171819202122232425public class ParseAnn&#123; public static void main(String[])&#123; try&#123;//1. 使用类加载器加载类 Class c=Class.forName（&quot;com.ann.test.Child&quot;) //2. 找到类上面的注解 boolean isExist = c.isAnnotationPresent(Description.class); if(isExist)&#123; //3. 拿到注解实例 Description d=（Description）c.getAnnotation(Description.class); System.out.println(d.value()); &#125; //4.找到方法上的注解 Method[] ms = c.getMethods(); for(Method m:ms)&#123; boolean isMExist = m.isAnnotationPresent(Description.class); if(isMExist)&#123; Description md=（Description）c.getAnnotation(Description.class); System.out.println(md.value()); &#125; &#125; &#125;catch(ClassNotFoundException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 另一种解析方法上的注解: 获取这个方法的所有注解，Annotation [] as=m.getAnnotations();然后遍历该注解，如果遍历的注解是Description类型，则把遍历的注解强转为Description类型，并进行输出value()信息。 123456789for(Method m:ms)&#123; Annotation [] as=m.getAnnotations(); for(Annotation a:as)&#123; if(a instanceof Description)&#123; Description md = (Description)a; System.out.println(md.value()); &#125; &#125;&#125; @Inherited:当自定义注解上使用了该注解，如果在父类上标识该注解，解析一个子类，子类也可以获取该注解的信息。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」多线程","date":"2017-05-12T10:07:41.000Z","path":"2017/05/12/java-thread.html","text":"多线程的存在，不是提高程序的执行速度。其实是为了提高应用程序的使用率。程序的执行其实都是在抢 CPU 的资源，CPU 的执行权。多个进程是在抢这个资源，而其中的某一个进程如果执行路径(线程)比较多，就会有更高的几率抢到 CPU 的执行权。 1. 基本概念 程序：数据结构 + 算法，主要指存放在硬盘上的可执行文件。 进程：主要指运行在内存中的程序；每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含 n 个线程；(进程是系统进行资源分配和调度的一个独立单位)。 线程：线程是进程的一个实体，同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小；(线程是 cpu 调度和分派的最小单位)。 多进程是指操作系统能同时运行多个任务（程序）。 多线程是指在同一程序(一个进程)中有多个顺序流在执行。 并行与并发： 并行：多个 cpu 实例或者多台机器同时执行一段处理逻辑，是真正的同时。 并发：通过 cpu 调度算法，让用户看上去同时执行，实际上从 cpu 操作层面不是真正的同时。并发往往在场景中有公用的资源，那么针对这个公用的资源往往产生瓶颈，我们会用 TPS 或者 QPS 来反应这个系统的处理能力。 线程和进程一样分为五个阶段：创建、就绪状态、执行状态、等待&#x2F;挂起&#x2F;阻塞、终止&#x2F;异常&#x2F;消亡。 2. 实现线程的过程java.lang.Thread 类主要用于描述线程，Java 虚拟机允许应用程序并发地运行多个执行线程。 自定义类继承 Thread 类并重写 run 方法，然后创建该类的实例调用 start 方法。 自定义类实现 Runnable 接口并重写 run 方法，然后创建该类的对象作为实参去构造 Thread 类型的对象，最后使用 Thread 类对象调用 start 方法。 2.1 实现方式一：继承 Thread 类 自己描述一个类 继承父类 Thread 重写 run 方法 new 一个线程对象，调用 start()方法，让线程进入就绪状态(需要注意的是 start 方法是 Thread 类中的) 12345678class MyThread extends Thread&#123; @Override public void run()&#123; //这里编写该线程的执行任务 &#125;&#125;MyThread mt = new MyThread();mt.start(); 2.2 实现方式二：实现 Runnable 接口 自己描述一个类 实现一个父接口 Runnable 重写 run 方法 new 一个线程对象，new 一个 Thread 并传入线程对象，调用 start()方法，让线程进入就绪状态 123456789class MyThread implements Runnable&#123; @Override public void run()&#123; //这里编写该线程的执行任务 &#125;&#125;MyThread mt = new MyThread();Thread td = new Thread(mt);td.start(); 2.3 两种方式优缺点： 使用继承 Thread 方式代码简单，但 Java 语言只支持单继承，若该类继承 Thread 类后则无法继承其他类 使用实现 Runnable 的方式代码复杂，但不影响该类继承其他类，并且支持多实现，适合多个相同程序代码的线程去处理同一个资源，增加程序健壮性，代码可以被多个线程共享，代码和数据独立。 3. 线程常用方法 3.1 相关方法的解析： Thread()：使用无参方式构造对象 Thread(String name)：根据参数指定的名称来构造对象。 Thread(Runnable target)：根据参数指定的 Runnable 引用来构造对象。 Thread(Runnable target, String name)：根据参数指定的 Runnable 引用和名称构造对象。 void run()：若使用 Runnable 对象作为参数构造的对象来调用该方法，则最终调用 Runnable 对象中的 run 方法，否则该方法啥也不做。 void start()：用于启动线程，除了主方法线程外新启动一个线程同时执行，Java 虚拟机会自动调用该线程的 run 方法。 int getPriority()：用于获取线程的优先级，优先级 1-10 void setPriority(int)：更改线程的优先级 3.2 多线程原理分析 执行 main 方法的线程叫做主线程，而执行 run 方法的线程叫做子线程。 对于 start 方法之前的代码来说，由主线程执行一次，当 start 方法调用成功之后，线程的个数由 1 个变成了 2 个，主线程继续向下执行，而新启动的线程去执行 run 方法的代码，两个线程各自独立运行。 当 run 方法执行完毕后，则子线程结束；当 main 方法执行完毕后，则主线程结束。 两个线程执行的先后次序没有明确的规定，由系统的调度算法决定。 3.3 线程的编号和名称 long getId()：用于获取调用对象所表示线程的编号 String getName()：用于获取调用对象所表示线程的名称 void setName()：用于设置线程的名称为参数指定的数值 static Thread currentThread()：获取当前正在执行线程的引用 4. 线程池 为了避免重复的创建线程，线程池的出现可以让线程进行复用。通俗点讲，当有工作来，就会向线程池拿一个线程，当工作完成后，并不是直接关闭线程，而是将这个线程归还给线程池供其他任务使用。 在线程池的编程模式下，任务是提交给整个线程池，而不是直接交给某个线程，线程池在拿到任务后，它就在内部找有无空闲线程，再把任务交给内部某个空闲线程。 一个线程同时只能执行一个任务，但可以同时向一个线程池提交多个任务 接口：Executor,CompletionService,ExecutorService，ScheduledExecutorService 抽象类：AbstractExecutorService 实现类：ExecutorCompletionService，ThreadPoolExecutor，ScheduledThreadPoolExecutor 创建线程的第三种方式是实现 Callable 接口，主要用于线程池 5. 线程的主要状态 新建状态：使用 new 关键字创建线程后进入状态，此时线程还没有开始执行 就绪状态：调用 start()进入的状态，此时线程还是没有开始执行 运行状态：使用线程调度器调用该线程后进入的状态(获得 CPU 执行权)，此时线程开始执行，当线程的时间片执行完毕后若没有完成就回到就绪状态，若任务完成进入消亡状态 消亡状态：当线程的任务执行完成之后进入的状态，此时线程已经终止 阻塞状态：当线程执行过程中发生了阻塞事件进入的状态，阻塞解除后再回到就绪状态 5.1 线程的休眠 终止线程：通常使用退出标识，使线程正常退出，也就是当 run() 方法完成后线程终止。 static void **yield()**：当线程让出处理器(离开 Running 状态)，使用当前线程进入 Runnable 状态等待。 static void **sleep(times)**：使当前线程从 Running 放弃处理器进入 Block 状态，休眠 times 毫秒，再返回到 Runnable 如果其他线程打断当前线程的 Block(sleep)，就会发生 InterruptException。 5.1 线程的等待 void **join()**：等待该线程终止，让多个线程同步执行，变成单个线程 void **join(long millis)**：表示等待参数指定的毫秒数 对象.wait() 和 **对象.notify()&#x2F;notifyAll()**可以让线程的状态来回切换 sleep()和 wait()的区别： sleep()和 wait()的区别 sleep() wait() 1.类 Thread 类 Object 类 2.调用 静态 类名. 对象. 3.理解 调用位置的线程等待 对象调用，访问对象的其他线程等待 4.唤醒 不需要唤醒 需要其他对象调用 notify 唤醒 5.锁 不会释放锁 等待后会释放锁 5.2 守护线程 boolean **isDeamon()**：用于判断是否为守护线程 void **setDeamon(boolean on)**：用于设置线程为守护线程 Java 线程有两类： 用户线程：运行在前台，执行具体任务；程序的主线程、连接网络的子线程等都是用户线程 守护线程：运行在后台，为其他前台线程服务 守护线程特点： 一旦所有线程都结束运行，守护线程会随 JVM 一起结束工作 守护线程应用： 数据库连接池中检测的线路，JVM 虚拟机启动后的监测线程；最常见的是垃圾回收线程。 设置守护线程： 可以通过调用 Thread 类的 setDeamon(true)方法来设置当前的线程为守护线程 6. 线程的同步机制 条件争用：当多个线程同时共享访问同一数据时，每个线程都尝试操作该数据，从而导致数据被破坏(corrupted)，这种现象称为争用条件。 当多个线程同时访问同一种共享资源时，可能会造成数据的覆盖等不一致性问题，此时就需要对多个线程之间进行通信和协调，该机制就叫做线程的同步机制。 Java 提供了一种内置的锁机制来支持原子性，使用synchronized关键字来保证线程执行操作的原子性，叫做对象&#x2F;同步锁机制。 特征修饰符 synchronized：表示同步，一个时间点只有一个线程访问 线程安全锁：两种形式是（锁定的永远是对象） 使用同步代码块的方式，将 synchronized 关键字放在方法体内部 123synchronized(对象)&#123; //需同步执行(锁定)的代码&#125; 使用同步方法的方式处理，直接使用 synchronized 关键字修饰整个方法，锁定的是调用方法的那个对象 1public synchronized void 方法名()&#123;&#125; 使用 synchronized 保证线程同步时应当注意： 多个需要同步的线程在访问该同步块时，看到的应该时同一个锁对象引用 在使用同步块时应当尽量减少同步范围以提高并发的执行效率 无论 synchronized 关键字加在方法上还是对象上，它取得的锁都是对象，而不是把一段代码或函数当作锁――而且同步方法很可能还会被其他线程的对象访问。 每个对象只有一个锁（lock）与之相关联。 实现同步是要很大的系统开销作为代价的，甚至可能造成死锁，所以尽量避免无谓的同步控制。 7. 线程的死锁Java 线程死锁是一个经典的多线程问题，因为不同的线程都在等待那些根本不可能被释放的锁，从而导致所有的工作都无法完成。 123456789101112131415161718/**当两个线程或多个线程之间相互锁定时就形成了死锁**///线程一：public void run() &#123; synchronized(a) &#123; //表示:持有对象锁a,等待对象锁b synchronized(b) &#123; //... &#125; &#125;&#125;//线程二：public void run() &#123; synchronized(b) &#123; //表示:持有对象锁b,等待对象锁a synchronized(a) &#123; //... &#125; &#125;&#125;// 注意：在以后的开发中尽量不要使用同步代码块的嵌套结构。 产生死锁的必要条件：a.互斥条件、b.不可抢占条件、c.占有且申请条件、d.循环等待条件。 隐性死锁：隐性死锁由于不规范的编程方式引起，但不一定每次测试运行时都会出现程序死锁的情形。由于这个原因，一些隐性死锁可能要到应用正式发布之后才会被发现，因此它的危害性比普通死锁更大。 两种导致隐性死锁的情况：加锁次序和占有并等待。 加锁次序：当多个并发的线程分别试图同时占有两个锁时，会出现加锁次序冲突的情形。如果一个线程占有了另一个线程必需的锁，就有可能出现死锁。 占有并等待：如果一个线程获得了一个锁之后还要等待来自另一个线程的通知，可能出现另一种隐性死锁。 7.1 死锁的避免 避免死锁的原则：顺序上锁，反向解锁，不要回头 静态策略：使产生死锁的四个必要条件不能同时具备，从而对进程申请资源的活动加以限制，以保证死锁不会发生。 动态策略：不限制进程有关申请资源的命令，而是对进程所发出的每一个申请资源命令加以动态地检查，并根据检查结果决定是否进行资源分配。具体策略有：安全序列和银行家算法。 8.内存可见性8.1 基本概念 可见性：一个线程对共享变量值的修改，能够及时的被其他线程看到 共享变量：如果一个变量在多个线程的工作内存中都存在副本，那么这个变量就是这几个线程的共享变量 Java 内存模型(JMM)： Java Memory Model 描述了 Java 程序中各种变量(线程共享变量)的访问规则，以及在 JVM 中将变量存储到内存中和从内存中读取变量这样的底层细节。 所有的变量都存储在主内存中 每个线程都有自己的独立的工作内存，里面保存该线程使用到的变量的副本(来自主内存的拷贝) Java 内存模型规定： 线程对共享变量的所有操作都必须在自己的工作内存中进行，不能直接从主内存中读写。 不同线程之间无法直接访问其他线程工作内存中的变量，线程间变量值的传递需要通过主内存来完成。 要实现共享变量的可见性，必须保证两点： 线程修改后的共享变量值能够及时从工作内存中刷新到主内存中 其他线程能够及时把共享变量的最新值从主内存更新到自己的工作内存中。 Java语言层面支持的可见性实现方式：Synchronized，volatile 8.2 Synchronized 实现可见性 Synchronized 能够实现：原子性(同步)、可见性 JMM 关于 synchronized 的两条规定： 线程解锁前，必须把共享变量的最新值刷新到主内存中 线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从内存中重新读取最新的值（注意：加锁与解锁需要是同一把锁） 线程执行互斥代码的过程： 获得互斥锁 清空工作内存 从主内存拷贝变量的最新副本到工作内存 执行代码 将更改后的共享变量的值刷新到主内存 释放互斥锁 重排序：代码的书写顺序与实际的执行顺序不同，指令重排序是编译器或处理器为了性能而做的优化 编译器优化重排序（编译器处理） 指令级并行重排序（处理器优化） 内存系统的重排序（处理器读写缓存的优化） as-is-serial:无论如何重排序，程序执行的结果应该与代码的顺序执行结果一致 单线程中重排序不会带来内存可见性问题 多线程中程序交错执行时，重排序可能造成内存可见性问题 不可见的原因 syschronized 解决方案 1.线程的交叉执行 原子性 2.重排序结合线程交叉执行 原子性 3.共享变量未及时更新 可见性 8.3 volatile 实现可见性 深入来说：通过加入内存屏障和禁止重排序优化来实现的。 对 volatile 变量执行写操作时，会在写操作后加入一条 store 屏蔽指令 对 volatile 变量执行读操作时，会在读操作前加入一条 load 屏蔽指令 通俗地讲：volatile 变量在每次被线程访问时，都强迫从主内存中重读该变量的值，而当该变量发生变化时，又会强迫线程将最新的值刷新到主内存。这样任何时刻，不同的线程总能看到该变量的最新值。 线程写volatile 变量的过程： 改变线程工作内存中 volatile 变量副本的值 将改变后的副本的值从工作内存刷新到主内存 线程读volatile 变量的过程： 从主内存中读取 volatile 变量的最新值到线程的工作内存中 从工作内存中读取 volatile 变量的副本 volatile 不能保证 volatile 变量复合操作的原子性 volatile 适用场景： 对变量的写操作不依赖其当前值 该变量没有包含在具有其他变量的不变式中 8.4 Synchronized 和 volatile 比较 volatile 不需要加锁，比 synchronized 更轻量级，不会阻塞线程； 从内存可见性角度讲，volatile 读相当于加锁，volatile 写相当于解锁 synchronized 既能保证可见性，又能保证原子性，而 volatile 只能保证可见性，无法保证原子性 volatile 没有 synchronized 使用广泛。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」IO机制","date":"2017-05-05T09:31:22.000Z","path":"2017/05/05/java-io.html","text":"输入输出（I&#x2F;O）是指程序与外部设备或其他计算机进行交互的操作。几乎所有的程序都具有输入与输出操作，Java把这些输入与输出操作用流来实现，通过统一的接口来表示，从而使程序设计更为简单。 1. File类 File与真实硬盘中的文件或文件夹 不是一个东西 File是在内存中的一个对象&lt;—映射—&gt;硬盘上的文件或文件夹 java.io.File类用于文件或目录信息(名称、大小等)的抽象表示方式，不能对文件内容进行访问。 File类中的常用的方法 canRead()，canWrite()，isHidden()，isFile()，isDirectory() length()，获取文件中字节的个数 lastModified()，获取文件最后的修改时间—&gt;毫秒值 *String path &#x3D; getAbsolutePath()，获取文件的绝对路径 D:&#x2F;&#x2F;test&#x2F;&#x2F;Test.txt 绝对路径&lt;—-&gt;相对路径 绝对路径可以通过完整的字符串，定位盘符，文件夹，文件 相对路径没有盘符的写法，当前工程(项目)所在的位置找寻 String name &#x3D; getName()，获取文件的名字 Test.txt *boolean &#x3D; **createNewFile()**，创建新的文件 *boolean &#x3D; mkdir ，创建新的文件夹 外层没有 不能创建 *boolean &#x3D; mkdirs，创建新的文件夹 外层没有 可以自动创建 String pname &#x3D; getParent()，获取当前file的父亲file名字 *File file &#x3D; getParentFile()，获取当前file的父亲file对象 String[] names &#x3D; list()，获取当前file的所有儿子名字 *File[] files &#x3D; listFiles()，获取当前file的所有儿子对象 *boolean &#x3D; delete()，删除文件或空的文件夹 不能删除带元素的文件夹 文件夹的路径(找父目录) 1234567//查找当前file的所有父目录File file = new File(&quot;D:\\\\test\\\\bbb\\\\inner\\\\InnerTest.txt&quot;);File pfile = file.getParentFile();while(pfile!=null)&#123; System.out.println(pfile.getAbsolutePath()); pfile = pfile.getParentFile();//再找一遍&#125; 文件夹的遍历—-需要一个递归 123456789101112131415//设计一个方法 用来展示(遍历)文件夹,参数--&gt;file(代表文件或文件夹)public void showFile(File file)&#123; //获取file的子元素 //files==null是个文件 //files!=null是个文件夹 //files.length!=0是一个带元素的文件夹 File[] files = file.listFiles();//test文件夹所有子元素 if(files!=null &amp;&amp; files.length!=0)&#123; for(File f:files)&#123; this.showFile(f); &#125; &#125; //做自己的显示(file是文件或file是一个空的文件夹) System.out.println(file.getAbsolutePath());&#125; 文件夹的删除—-需要一个递归 123456789101112//设计一个方法 删除文件夹,参数 filepublic void deleteFile(File file)&#123; //判断file不是空文件夹 File[] files = file.listFiles(); if(files!=null &amp;&amp; files.length!=0)&#123; for(File f:files)&#123; this.deleteFile(f); &#125; &#125; //删除file (file是个文件或file是一个空文件夹) file.delete();&#125; 2. IO流 流的本质是数据传输，根据数据传输特性将流抽象为各种类，方便更直观的进行数据操作。 流的分类: 根据处理数据类型的不同分为：字符流和字节流 根据数据流向不同分为：输入流in(读取)和输出流out(写入) 操作的目标来区分: 文件流，数组流，字符串流，数据流，对象流，网络流… IO流的框架结构 123456789101112131415161718|——IO流 |————字节流 |————InputStream |————FileInputStream |————DataInputStream |————ObjectInputStream |————OutputStream |————FileOutputStream |————DataOutputStream |————ObjectOutputStream |————PrintStream |————字符流 |————Reader |————BufferedReader |————InputStreamReader |————Writer |————BufferedWriter |————OutputStreamWriter 3. 文件流读取文件中的信息in，将信息写入文件中out；文件流按照读取或写入的单位(字节数)大小来区分 字节型文件流(1字节)：FileInputStream&#x2F;FileOutputStream 字符型文件流(2字节–1字符)：FileReader&#x2F;FileWriter 字节流和字符流的区别： 读写单位不同：字节流以字节（8bit）为单位，字符流以字符为单位，根据码表映射字符，一次可能读多个字节。 处理对象不同：字节流能处理所有类型的数据（如图片、avi等），而字符流只能处理字符类型的数据。 结论：只要是处理纯文本数据，就优先考虑使用字符流。 除此之外都使用字节流。 输入流和输出流 对输入流只能进行读操作，对输出流只能进行写操作。 4. 字节型文件流4.1 字节型文件输入流FileInputStream(读) FileInputStream类在java.io包，继承自InputStream类(字节型输入流的父类)。 创建对象 调用一个带File类型的构造方法 调用一个带String类型的构造方法 常用方法 int code &#x3D; read(); 每次从流管道中读取一个字节，返回字节的code码 *int count &#x3D; read(byte[] ) 每次从流管道中读取若干个字节，存入数组内 返回有效元素个数 int count &#x3D; available(); 返回流管道中还有多少缓存的字节数 skip(long n);跳过几个字节 读取 多线程—&gt;利用几个线程同时读取文件 *close() 将流管道关闭—必须要做,最好放在finally里 注意代码的健壮性，判断严谨（eg:非空判断） 4.2 字节型文件输出流FileOutputStream(写) FileOutputStream类在java.io包，继承自OutputStream类(所有字节型输出流的父类)。 创建对象 调用一个带File参数，还有File boolean重载 调用一个带String参数，还有String boolean重载 eg: new FileOutputStream(“D:&#x2F;&#x2F;test&#x2F;&#x2F;bbb.txt”, true)&#x2F;&#x2F;第二个参控制每次写入追加还是重载 常用方法 write(int code); 将给定code对应的字符写入文件 ‘&#x3D;’ write(byte[]); 将数组中的全部字节写入文件 getByte() write(byte[] b, int off, int len); flush(); 将管道内的字节推入(刷新)文件 close(); 注意在finally中关闭 创建的是文件输入流，若文件路径有问题，则抛出异常 FileNotFoundException 创建的是文件输出流，若文件路径有问题，则直接帮我们创建一个新的文件 设计一个文件复制的方法 12345678910111213141516171819202122232425262728293031323334public void copyFile(File file, String path) &#123; FileInputStream fis = null; FileOutputStream fos = null; try &#123; //创建输入流读取信息 fis = new FileInputStream(file); //创建一个新的File对象 File newFile = new File(path +&quot;\\\\&quot;+ file.getName());//&quot;E:\\\\test\\\\test.txt&quot; //创建一个输出流 fos = new FileOutputStream(newFile); byte[] b = new byte[1024];//通常1kb-8kb之间 int count = fis.read(b); while(count != -1) &#123; fos.write(b, 0, count);//将读取到的有效字节写入 fos.flush(); count = fis.read(b); &#125; System.out.println(&quot;复制完毕！&quot;); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭 if(fis!=null) &#123; try &#123; fis.close(); &#125; catch (IOException e) &#123;e.printStackTrace();&#125; &#125; if(fos!=null) &#123; try &#123;fos.close();&#125; catch (IOException e) &#123;e.printStackTrace();&#125; &#125; &#125;&#125; 5. 字符型文件流FileReader&#x2F;FileWriter：只能操作纯文本的文件 .txt &#x2F; .properties 5.1 字符型文件输入流FileReader(读) FileReader类在java.io包，继承自InputStreamReader，Reader 创建对象 调用一个带File类型的构造方法 调用一个带String类型的构造方法 常用方法 read() read(char[]) close() 12345678910111213141516File file = new File(&quot;F://test//Test.txt&quot;);try &#123; FileReader fr = new FileReader(file); // int code = fr.read(); // System.out.println(code); char[] c = new char[1024]; int count = fr.read(c); while(count!=-1) &#123; System.out.println(new String(c, 0, count)); count = fr.read(c); &#125;&#125; catch (FileNotFoundException e) &#123; e.printStackTrace();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 5.2 字符型文件输出流FileWriter(写) FileWriter类在java.io包，继承自OutputStreamWriter，Writer 构造方法 带file参数，带file,boolean参数 带String参数，带String,boolean参数 常用方法 write(int) write(char[]) write(string) flush()，close() 6. *缓冲流 缓冲流,也叫高效流，是对4个基本的File…流的增强，所以也是4个流，按照数据类型分类： 字节缓冲流：BufferedInputStream，BufferedOutputStream 字符缓冲流：BufferedReader，BufferedWriter 缓冲流的基本原理，是在创建流对象时，会创建一个内置的默认大小的缓冲区数组，通过缓冲区读写，减少系统IO次数，从而提高读写的效率。 缓冲流读写方法与基本的流是一致 6.1 字节缓冲流 BufferedInputStream，BufferedOutputStream 构造方法 public BufferedInputStream(InputStream in) ：创建一个 新的缓冲输入流。 public BufferedOutputStream(OutputStream out)： 创建一个新的缓冲输出流。 1234// 创建字节缓冲输入流BufferedInputStream bis = new BufferedInputStream(new FileInputStream(&quot;bis.txt&quot;));// 创建字节缓冲输出流BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(&quot;bos.txt&quot;)); 6.2 字符缓冲流 BufferedReader，BufferedWriter 构造方法 public BufferedReader(Reader in) ：创建一个 新的缓冲输入流。 public BufferedWriter(Writer out)： 创建一个新的缓冲输出流。 1234// 创建字符缓冲输入流BufferedReader br = new BufferedReader(new FileReader(&quot;br.txt&quot;));// 创建字符缓冲输出流BufferedWriter bw = new BufferedWriter(new FileWriter(&quot;bw.txt&quot;)); 字符缓冲流的基本方法与普通字符流调用方式一致，不再阐述，我们来看它们具备的特有方法。 特有方法: BufferedReader：public String readLine(): 读一行文字。 BufferedWriter：public void newLine(): 写一行行分隔符,由系统属性定义符号。 12345678910111213141516171819202122//设计一个方法，用来用户登录认证public String login(String username, String password) &#123; try &#123; BufferedReader br = new BufferedReader(new FileReader(&quot;F://test//User.txt&quot;)); //User.txt每行存储格式：张三-123 String user = br.readLine();//user表示一行记录，记录账号密码 while(user!=null) &#123; //将user信息拆分，分别与参数比较 String[] value = user.split(&quot;-&quot;);//value[0]账号，value[1]密码 System.out.println(value[0]); if(value[0].equals(username)) &#123; if(value[1].equals(password)) &#123; return &quot;登录成功&quot;; &#125; &#125; user = br.readLine(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return &quot;账号或密码错误！&quot;;&#125; readLine方法演示: 12345678try &#123; BufferedWriter bw = new BufferedWriter(new FileWriter(&quot;F://test//User.txt&quot;, true)); bw.newLine(); bw.write(&quot;java-888&quot;); bw.flush();&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 7. 转换流7.1 字符编码 字符编码Character Encoding : 就是一套自然语言的字符与二进制数之间的对应规则。 字符集 Charset：也叫编码表。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等。 常见字符集: ASCII字符集 ： ASCII（American Standard Code for Information Interchange，美国信息交换标准代码） ISO-8859-1字符集： 拉丁码表，别名Latin-1，用于显示欧洲使用的语言；ISO-8859-1使用单字节编码，兼容ASCII编码。 GBxxx字符集： GB就是国标的意思，是为了显示中文而设计的一套字符集。 GB2312（简体中文码表），GBK（最常用的中文码表），GB18030（最新的中文码表） Unicode字符集 ： Unicode编码系统为表达任意语言的任意字符而设计，是业界的一种标准，也称为统一码、标准万国码。 UTF-8、UTF-16和UTF-32；最为常用的UTF-8编码。 编码引出的问题 在IDEA中，使用FileReader 读取项目中的文本文件。由于IDEA的设置，都是默认的UTF-8编码，所以没有任何问题。但是，当读取Windows系统中创建的文本文件时，由于Windows系统的默认是GBK编码，就会出现乱码。 7.2 InputStreamReader类转换流java.io.InputStreamReader，是Reader的子类，是从字节流到字符流的桥梁。它读取字节，并使用指定的字符集将其解码为字符。它的字符集可以由名称指定，也可以接受平台的默认字符集。 构造方法 InputStreamReader(InputStream in): 创建一个使用默认字符集的字符流。 InputStreamReader(InputStream in, String charsetName): 创建一个指定字符集的字符流。 12InputStreamReader isr = new InputStreamReader(new FileInputStream(&quot;in.txt&quot;));InputStreamReader isr2 = new InputStreamReader(new FileInputStream(&quot;in.txt&quot;) , &quot;GBK&quot;); 指定编码读取: 12345678910111213141516171819202122public class ReaderDemo2 &#123; public static void main(String[] args) throws IOException &#123; // 定义文件路径,文件为gbk编码 String FileName = &quot;E:\\\\file_gbk.txt&quot;; // 创建流对象,默认UTF8编码 InputStreamReader isr = new InputStreamReader(new FileInputStream(FileName)); // 创建流对象,指定GBK编码 InputStreamReader isr2 = new InputStreamReader(new FileInputStream(FileName) , &quot;GBK&quot;);// 定义变量,保存字符 int read; // 使用默认编码字符流读取,乱码 while ((read = isr.read()) != -1) &#123; System.out.print((char)read); // ��Һ� &#125; isr.close(); // 使用指定编码字符流读取,正常解析 while ((read = isr2.read()) != -1) &#123; System.out.print((char)read);// 大家好 &#125; isr2.close(); &#125;&#125; 7.3 OutputStreamWriter类转换流java.io.OutputStreamWriter ，是Writer的子类，是从字符流到字节流的桥梁。使用指定的字符集将字符编码为字节。它的字符集可以由名称指定，也可以接受平台的默认字符集。 构造方法 OutputStreamWriter(OutputStream in): 创建一个使用默认字符集的字符流。 OutputStreamWriter(OutputStream in, String charsetName): 创建一个指定字符集的字符流。 12OutputStreamWriter isr = new OutputStreamWriter(new FileOutputStream(&quot;out.txt&quot;));OutputStreamWriter isr2 = new OutputStreamWriter(new FileOutputStream(&quot;out.txt&quot;) , &quot;GBK&quot;); 指定编码写出 123456789101112131415161718public class OutputDemo &#123; public static void main(String[] args) throws IOException &#123; // 定义文件路径 String FileName = &quot;E:\\\\out.txt&quot;; // 创建流对象,默认UTF8编码 OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(FileName)); // 写出数据 osw.write(&quot;你好&quot;); // 保存为6个字节 osw.close();// 定义文件路径String FileName2 = &quot;E:\\\\out2.txt&quot;; // 创建流对象,指定GBK编码 OutputStreamWriter osw2 = new OutputStreamWriter(new FileOutputStream(FileName2),&quot;GBK&quot;); // 写出数据 osw2.write(&quot;你好&quot;);// 保存为4个字节 osw2.close(); &#125;&#125; 8. 对象流 对象序列化和反序列化 Java 提供了一种对象序列化的机制。用一个字节序列可以表示一个对象，该字节序列包含该对象的数据、对象的类型和对象中存储的属性等信息。字节序列写出到文件之后，相当于文件中持久保存了一个对象的信息。 反之，该字节序列还可以从文件中读取回来，重构对象，对它进行反序列化。对象的数据、对象的类型和对象中存储的数据信息，都可以用来在内存中创建对象 简单来讲 对象的序列化指的是：将一个完整的对象 拆分成字节碎片 记录在文件中 对象的反序列化指的是：将文件中记录的对象随便 反过来组合成一个完整的对象 如果想要将对象序列化到文件中：需要让对象实现Serializable接口，是一个示意性接口； 如果想要将对象反序列化：需要给对象提供一个序列化的版本号，private long serialVersionUID = 任意L; 8.1 ObjectOutputStream类 java.io.ObjectOutputStream 类，将Java对象的原始数据类型写出到文件,实现对象的持久存储。 构造方法 public ObjectOutputStream(OutputStream out)： 创建一个指定OutputStream的ObjectOutputStream。 12FileOutputStream fileOut = new FileOutputStream(&quot;employee.txt&quot;);ObjectOutputStream out = new ObjectOutputStream(fileOut); 序列化操作 一个对象要想序列化，必须满足两个条件: 该类必须实现java.io.Serializable 接口，Serializable 是一个标记接口，不实现此接口的类将不会使任何状态序列化或反序列化，会抛出NotSerializableException 。 该类的所有属性必须是可序列化的。如果有一个属性不需要可序列化的，则该属性必须注明是瞬态的，使用transient 关键字修饰。 写出对象方法 public final void writeObject (Object obj) : 将指定的对象写出。1234567891011121314151617181920212223242526272829303132//满足两个条件public class Employee implements java.io.Serializable &#123; public String name; public String address; public transient int age; // transient瞬态修饰成员,不会被序列化 public void addressCheck() &#123; System.out.println(&quot;Address check : &quot; + name + &quot; -- &quot; + address); &#125;&#125;//写出对象方法public class SerializeDemo&#123; public static void main(String [] args) &#123; Employee e = new Employee(); e.name = &quot;zhangsan&quot;; e.address = &quot;beiqinglu&quot;; e.age = 20; try &#123; // 创建序列化流对象 ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(&quot;employee.txt&quot;)); // 写出对象 out.writeObject(e); // 释放资源 out.close(); fileOut.close(); System.out.println(&quot;Serialized data is saved&quot;); // 姓名，地址被序列化，年龄没有被序列化。 &#125; catch(IOException i) &#123; i.printStackTrace(); &#125; &#125;&#125;//输出结果：//Serialized data is saved 8.2 ObjectInputStream类 ObjectInputStream反序列化流，将之前使用ObjectOutputStream序列化的原始数据恢复为对象。 构造方法 public ObjectInputStream(InputStream in)： 创建一个指定InputStream的ObjectInputStream。 反序列化操作1 如果能找到一个对象的class文件，我们可以进行反序列化操作，调用ObjectInputStream读取对象的方法。 对于JVM可以反序列化对象，它必须是能够找到class文件的类。如果找不到该类的class文件，则抛出一个 ClassNotFoundException 异常。 public final Object readObject () : 读取一个对象。 12345678910111213141516171819202122232425262728public class DeserializeDemo &#123; public static void main(String [] args) &#123; Employee e = null; try &#123; // 创建反序列化流 FileInputStream fileIn = new FileInputStream(&quot;employee.txt&quot;); ObjectInputStream in = new ObjectInputStream(fileIn); // 读取一个对象 e = (Employee) in.readObject(); // 释放资源 in.close(); fileIn.close(); &#125;catch(IOException i) &#123; // 捕获其他异常 i.printStackTrace(); return; &#125;catch(ClassNotFoundException c) &#123; // 捕获类找不到异常 System.out.println(&quot;Employee class not found&quot;); c.printStackTrace(); return; &#125; // 无异常,直接打印输出 System.out.println(&quot;Name: &quot; + e.name);// zhangsan System.out.println(&quot;Address: &quot; + e.address); // beiqinglu System.out.println(&quot;age: &quot; + e.age); // 0 &#125;&#125; 反序列化操作2 另外，当JVM反序列化对象时，能找到class文件，但是class文件在序列化对象之后发生了修改，那么反序列化操作也会失败，抛出一个InvalidClassException异常。发生这个异常的原因如下： 该类的序列版本号与从流中读取的类描述符的版本号不匹配 该类包含未知数据类型 该类没有可访问的无参数构造方法 Serializable 接口给需要序列化的类，提供了一个序列版本号。serialVersionUID 该版本号的目的在于验证序列化的对象和对应类是否版本匹配。 123456789101112public class Employee implements java.io.Serializable &#123; // 加入序列版本号 private static final long serialVersionUID = 1L; public String name; public String address; // 添加新的属性 ,重新编译, 可以反序列化,该属性赋为默认值. public int eid; public void addressCheck() &#123; System.out.println(&quot;Address check : &quot; + name + &quot; -- &quot; + address); &#125;&#125; 9. 打印流(PrintStream类) 平时我们在控制台打印输出，是调用print方法和println方法完成的，这两个方法都来自于java.io.PrintStream类，该类能够方便地打印各种数据类型的值，是一种便捷的输出方式。 构造方法 public PrintStream(String fileName); 使用指定的文件名创建一个新的打印流。 1PrintStream ps = new PrintStream(&quot;ps.txt&quot;)； 改变打印流向 System.out就是PrintStream类型的，只不过它的流向是系统规定的，打印在控制台上。不过，我们可以改变它的流向。 123456789101112public class PrintDemo &#123; public static void main(String[] args) throws IOException &#123; // 调用系统的打印流,控制台直接输出97 System.out.println(97); // 创建打印流,指定文件的名称 PrintStream ps = new PrintStream(&quot;ps.txt&quot;); // 设置系统的打印流流向,输出到ps.txt System.setOut(ps); // 调用系统的打印流,ps.txt中输出97 System.out.println(97); &#125;&#125; 10. Properties类的使用 Java.util.Properties，主要用于读取Java的配置文件。 Properties类继承自Hashtable 配置文件：在Java中，其配置文件常为.properties文件，格式为文本文件，文件的内容的格式是“键&#x3D;值”的格式，文本注释信息可以用”#”来注释。 Properties类的主要方法： getProperty ( String key)，用指定的键在此属性列表中搜索属性。也就是通过参数 key ，得到 key 所对应的 value。 load ( InputStream inStream)，从输入流中读取属性列表（键和元素对）。通过对指定的文件（比如说上面的 test.properties 文件）进行装载来获取该文件中的所有键 - 值对。以供 getProperty ( String key) 来搜索。 setProperty ( String key, String value) ，调用 Hashtable 的方法 put 。他通过调用基类的put方法来设置 键 - 值对。 store ( OutputStream out, String comments)，以适合使用 load 方法加载到 Properties 表中的格式，将此 Properties 表中的属性列表（键和元素对）写入输出流。与 load 方法相反，该方法将键 - 值对写入到指定的文件中去。 clear ()，清除所有装载的 键 - 值对。该方法在基类中提供。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」异常处理机制","date":"2017-04-25T00:34:31.000Z","path":"2017/04/25/java-exception.html","text":"Java语言提供了完善的异常处理机制。正确运用这套机制，有助于提高程序的健壮性。 1. 基本概念 异常用于在Java语言中描述运行阶段发生的错误。 在Java中有一个定义好的规则Throwable（可以抛出的） java.lang.Throwable类是所有错误(Error)和异常(Exception)的超类。 Error类主要用于描述比较严重无法编码解决的错误，如：JVM内存资源耗尽等。 Exception类主要用于描述比较轻微可以编码解决的错误，如：文件损坏、非法输入等。 java.lang.Exception类是所有异常的超类，主要分为两大类： RuntimeException - 运行时异常，也叫非检测性异常 IOException和其他异常 - 其他异常也叫做检测性异常 注意：当程序运行过程中发生异常而又没有手动处理时，则由java虚拟机采用默认方式处理，即打印异常名称、原因、发生位置并终止程序。在开发中尽量使用条件判断避免异常的发生。 12345678910Throwable类 |————Exception类 |————RuntimeException异常 |————ArithmeticException类 |————ArrayIndexOutOfBoundsException类 |————NullPointerException类 |————ClassCastException类 |————NumberFormatException类 |————IOException和其他异常 |————Error类 2. 异常的分支结构2.1 运行时异常（非检查异常） Error和RuntimeException都算作运行时异常 javac编译的时候，不会提示和发现的， 在程序编写时不要求必须做处理，如果我们愿意可以添加处理手段(try throws) 要求大家出现这样异常的时候 知道怎么产生及如何修改 InputMisMatchException 输入不匹配 int value &#x3D; input.nextInt();&#x2F;&#x2F; abc *NumberFormatException 数字格式化 int value &#x3D; Integer.parseInt(“123.45”); NegativeArraySizeException 数组长度负数 int[] array &#x3D; new int[-2]; *ArrayIndexOutOfBoundsException 数组索引越界 int[] array &#x3D; {1,2,3}; array[5]; *5NullPointerException 空指针异常 int[][] array &#x3D; new int[3][]; array[0][0] &#x3D;10; Person p &#x3D; null; p.getName(); ArithmeticException 数字异常 10&#x2F;0 整数不允许除以0 Infinity小数除以0会产生无穷 *ClassCastException 造型异常 Person p &#x3D; new Teacher(); Student s &#x3D; (Student)p; *StringIndexOutOfBoundsException 字符串越界 String str &#x3D; “abc”; str.charAt(5); *IndexOutOfBoundsException 集合越界 List家族 ArrayList list &#x3D; new ArrayList(); list.add(); list.add(); list.add(); list.get(5); IllegalArgumentException 非法参数异常 ArrayList list &#x3D; new ArrayList(-1); 2.2 编译时异常(检查异常) 除了Error和RuntimeException以外其他的异常 javac编译的时候，强制要求我们必须为这样的异常做处理(try或throws) 因为这样的异常在程序运行过程中极有可能产生问题的 异常产生后后续的所有执行就停止 123456//eg: InterruptExceptiontry&#123; Thread.sleep(5000);&#125;catch(Exception e)&#123; //...&#125; 3. 添加处理异常的手段 处理异常不是 异常消失了 处理异常指的是：处理掉异常之后，后续的代码不会因为此异常而终止执行 两种手段： 异常的捕获：try{}catch(){}[ finally{} ] throws抛出 final，finally，finalize区别 final：特征修饰符，修饰变量，属性，方法，类 修饰变量：基本类型:值不能改变；引用类型:地址不能改变(如果变量没有初值,给一次机会赋值) 修饰属性：特点与修饰变量类似(要求必须给属性赋初始值,否则编译报错) 修饰方法：不能被子类重写 修饰类：不能被其他的子类继承 finally：处理异常手段的一部分 try{}catch(){}后面的一个部分 这个部分可有可无，如果有只能含有一份，且必须执行 finalize：是Object类中的一个protected方法 对象没有任何引用指向的时候 – 会被GC回收 当对象回收的时候 默认调用finalize方法 若想要看到对象回收的效果，可以重写 public void finalize(){} 4. 异常的捕获12345678try&#123; 可能发生异常的代码;&#125;catch(异常类型 引用变量)&#123; 针对该异常的处理代码;&#125;catch ...finally&#123; 无论是否发生异常都要执行的代码;&#125; 处理异常放在方法内部 可能会出现的小问题 如果在方法内部含有返回值，不管返回值return关键字在哪里，finally一定会执行完毕，返回值的具体结果得看情况。 1234567891011public String test() &#123; try &#123; //...可能产生异常的的代码 return &quot;值1&quot;;//事先约定好 返回值 &#125;catch(Exception e)&#123; e.printStackTrace();//打印输出异常的名字 &#125;finally &#123; System.out.println(&quot;finally块执行啦&quot;); &#125; return &quot;值2&quot;;&#125; 上述执行结果：若try中代码块产生异常return返回 值2，若try中无异常则return返回 值1，无论return在哪finally都会执行。 异常捕获的注意事项： 当需要多分catch分子时，切记小类型应该放在大类型的前面； 懒人写法：catch(Exception e){…} finally通常用于善后处理，如：关闭已经打开的文件等。 5. 异常的抛出 当程序中发生异常又不方便直接处理时，可以将异常转移给方法调用者进行处理，这个过程叫做异常的抛出。 语法格式：访问权限 返回值类型 方法名(形参列表) throws 异常类型1,异常类型2,…{} ，如：public void show() throw Exception &#123;&#125; 重写方法的抛出规则： 不抛出异常 抛出父类异常中的子类异常 抛出和父类一样的异常 不能抛出同级不一样的异常 不能抛出更大的异常 6. 自定义异常 可以根据需要自定义异常类。 自定义异常的方式： 继承Exception或者异常的子类。 提供两个构造，无参构造和String做参数的构造。 异常的手段 如果继承是RuntimeException—-&gt;运行时异常(不需要必须添加处理手段) 如果继承是Exception—–&gt;编译时异常(必须添加处理手段) 类中可以写带String参数的构造方法，可以做细致的说明 通过throw关键字，new一个异常的对象 主动产生异常：throw new 异常类型(); 7. 总结 1.在开发中尽量使用条件判断避免异常的发生; 2.若实在避免不了，则进行异常捕获； 3.若实在捕获不了，则进行异常抛出； 4.若需要使用针对性异常，则自定义异常。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」集合框架","date":"2017-04-17T14:10:57.000Z","path":"2017/04/17/java-collection.html","text":"为了方便对多个对象的操作，对对象进行存储，集合就是存储对象最常用的一种方式。 1. Collection集合 Collection集合框架，字面意思容器；与数组类似，集合的长度存储之后还能改变，集合框架中包含了一系列不同数据结构的实现类。 数组与集合的比较 数组的特点： 数组本质上就是一段连续的存储单元，用于存放多个类型相同的数据类容； 支持下标访问，实现随机访问非常方便； 增删操作不方便，可能会移动大量元素； 数组一旦声明长度固定无法更改； 数组支持基本数据类型，也支持引用数据类型； 集合的特点： 集合的存储单元可以不连续，数据类容可以不相同； 集合部分支持下标访问，部分不支持； 集合中增删元素可以不移动大量元素； 集合大小可以随时动态调整； 集合中的元素必须是引用数据类型（基本数据类型可用包装类）； 1234567891011121314-Collection接口 |————List接口 |————ArrayList类 |————LinkedList类 |————Stack类 |————Vector类 |————Queue接口 |————LinkedList类 |————Set接口 |————HashSet类 |————TreeSet类-Map接口 |————HashMap类 |————TreeMap类 Collection存储的都是value,其中List有序可重复，Set无序无重复 Map存储的是以key-value形式,key无序无重复 value无序可重复 序 : 顺序–添加进去的元素，取得元素的顺序一致；注意指的不是集合自己的顺序 Collection集合的常用方法 boolean add(E e); 向集合中添加对象 boolean contains(Object o); 判断是否包含指定对象 boolean remove(Object o); 从集合中删除对象 void clear(); 清空集合 int size(); 返回包含对象的个数 boolean isEmpty(); 判断是否为空 1234567Collection c2 = new ArrayList(); //多态boolean b1 = c2.add(new String(&quot;one&quot;)); //trueboolean b2 = c2.add(new Integer(2)); //trueSystem.out.println(&quot;c2 = &quot; + c2); //[one, 2]boolean b3 = c2.contains(new Integer(2));//true//contains方法工作原理：(o==null ? e==null : o.equals(e)); 2. List集合 java.util.List集合是Collection集合的子集合。 List集合中元素有先后放入次序并且元素可以重复；实现类有：ArrayList类、LinkedList类、Stack类以及Vector类。 ArrayList类的底层使用数组进行数据管理，访问元素方便，增删不方便。 LinkedList类的底层使用链表进行数据管理，访问不方便，增删方便。 Stark类的底层使用数组进行数据管理，该类主要描述具有后进先出的特征的数据结构，叫做栈。 Vector类的底层使用数组进行数据管理，与ArrayList类似，与之比线程安全的类，因此效率低。 List类除了继承Collection定义的方法外，还根据线性表的数据结构定义了一系列方法，其中最常用的是基于下标的get()，set()方法。 List类常用方法 void add(int index, E element) 向集合指定位置添加元素 boolean addAll(int index, Collection&lt;?extends E&gt; c) 向集合中添加所有元素 E get(int index) 从集合中获取指定位置的元素 E set(int index, E element) 修改指定位置的元素 E remove(int index) 删除指定位置的元素 int indexOf(Object o) 在集合中检索某个对象，判断逻辑(o&#x3D;&#x3D;null?get(i)&#x3D;&#x3D;null:o.equals(get(i))) T[] toArray(T[] a) 将集合中的对象序列化以对象数组的形式返回。 List subList(int fromIndex, int toIndex) 获取List从fromIndex(包括)和 toIndex(不包括)之间的部分视图 3. 泛型机制 集合可以存放不同的对象，本质上都看作Object类型放入，此时从集合中取出也是Object类型，为了表达该元素真实类型需要强制类型转换，而强制类型转换可能发生类型转换异常。 从jdk1.5开始推出泛型机制，在集合名称后面使用&lt;数据类型&gt;的方式明确要求该集合中可以存放的数据类型。如：List&lt;String&gt; lt = new LinkedList&lt;String&gt;();。 从jdk1.7开始可省略后面&lt;&gt;的数据类型，叫做菱形特性，如：List&lt;String&gt; lt = new ArrayList&lt;&gt;();。 泛型本质就是参数化类型，让数据类型作为参数传递，public interface List&lt;E&gt;&#123;&#125;其中E是占位形参，由于实参可以支持各种广泛的类型，因此得名泛型。 泛型可以用在哪里： 泛型类：类定义的时候描述某种数据类型，集合的使用就是这样 泛型接口：与泛型类的使用基本一致，子类实现接口时必须添加泛型 泛型方法：方法调用时传参数，方法的泛型与类无关，带有泛型的方法可以不放在带有泛型的类中 方法参数泛型限制，高级泛型，规范边界，extends，super 4. Queue集合 java.util.Queue集合是Collection集合的子集合。 Queue集合主要描述具有先进先出特性的数据结构，叫做队列(FIFO:First Input First Output)。 Queue集合主要实现类是LinkedList类，因为该类在增删方面有一定优势。 Queue接口中主要方法 boolean offer(E e) 将一个对象添加至队尾，若添加成功则返回true E poll() 从队首删除并返回一个元素 E peek() 返回队首的元素（但并不删除） 12345Queue&lt;Integer&gt; q1 = new LinkedList&lt;Integer&gt;();//将数据11、22、33、44、55依次入队for(int i=1; i&lt;=5; i++) &#123; q1.offer(i*11);&#125; 5. *ArrayList类 底层是利用(动态)数组形式实现，jdk1.5，所属的包 java.util ArrayList特点适合遍历轮询，不适合插入删除 如何构建一个ArrayList对象 无参数构造方法，带默认容量构造方法，带collection参数的构造方法 ArrayList中常用的方法 增删改查：add(E e)，remove(index)，set(index value)，get(index)，size() 类中其他常用的方法 addAll并集，removeAll差集，ratainAll交集; indexOf()，lastIndexOf()，contains()，List&#x3D;subList(); isEmpty()，clear()，ensureCapacity()，iterator();迭代器 toArray(T[] x)，trimToSize(); 6. Vector类 是ArrayList集合的早期版本，所属的包 java.util Vector底层也是利用(动态)数组的形式存储 Vector是线程同步的(synchronized)，安全性高，效率较低 扩容方式与ArrayList不同 默认是扩容2倍，可以通过构造方法创建对象时修改这一机制 构造方法和常用方法与ArrayList类似 7. Stack类 Stack类，栈，java.util包 构造方法只有一个无参数 除了继承自Vacton类的方法外还有特殊的方法 push(E e)将某一个元素压入栈顶(add()) E &#x3D; pop()将某一个元素从栈顶取出并删掉(E &#x3D; remove()) E &#x3D; peek()查看栈顶的一个元素 不删除(get()) boolean &#x3D; empty()判断栈内元素是否为空(isEmpty()) int &#x3D; search()查找给定的元素在占中的位置(indexOf()) 应用场景 中国象棋，悔棋 栈中存储每一次操作的步骤 撤销功能 8. *LinkedList类 LinkedList类，java.util包 底层使用双向链表的数据结构形式来存储 适合于插入或删除 不适合遍历轮询 构建对象 无参数构造方法，带参数的构造方法(collection) 常用的方法 增删改查：add()，remove()，set()，get()，size()，offer，poll，peek 手册中提供的其他常用方法：addAll，addFist，addLast()，clear()，contains()，element()，getFirst()，getLast()，indexOf()，lastIndex() 插入删除的特性是否像想的那样 对比ArrayList Linked 9. Set集合 java.util.Set集合是Collection集合的子集合。 Set集合没有先后放入次序，并且不允许有重复关系，实现类有HashSet类和TreeSet类。 其中HashSet类底层是采用哈希表进行数据管理的。 其中TreeSet类的底层是采用二叉树进行数据管理的。 1234//方法和Collection集合基本一样Set&lt;String&gt; set1 = new HashSet&lt;String&gt;();set1.add(&quot;one&quot;);System.out.println(&quot;s1=&quot;+s1); set集合的无重复特性 HashSet，无重复原则有两个方法同时起作用 equals hashCode 默认比较的是两个对象的地址 若第二个对象地址与之前的一致 不再存入 如果想要改变其比较的规则 可以重写上述两个方法 TreeSet，无重复原则有一个方法起作用 compareTo 上述这个方法不是每一个对象都有的 若想要将某一个对象存入TreeSet集合中，需要让对象所属的类实现接口Comparable 实现接口后将compareTo方法重写，返回值int，负数靠前排布，整数排列靠后 9.1 Set集合的遍历 所有Collection的实现类都实现了其iterator方法，该方法返回Iterator接口类型对象，用于实现对集合元素的迭代遍历。 迭代器Iterator&lt;E&gt; iterator()，主要方法有 boolean hasNext() 判断集合中是否有可以迭代&#x2F;访问的元素 E next() 用于取出一个元素并指向下一个元素 void remove() 用于删除访问到的最后一个元素 12345678Iterator&lt;String&gt; it = set1.iterator();//获取当前集合的迭代器对象while(it.hasNext()) &#123;//判断是否有可以访问的元素 String temp = it.next();//取出一个并指向下一个 System.out.println( temp ); if(&quot;two&quot;.equals(temp))&#123; it.remove();//删除set1中该元素 &#125;&#125; 增强for循环(for each结构) 语法格式：for(元素类型 变量名:集合/数组)&#123; 循环体; &#125;。 执行流程：不断从集合&#x2F;数组中取出一个元素赋值给变量名后执行循环体，直到取出所有元素。 123456789//遍历集合for(String ts : s1) &#123; System.out.println(ts);&#125;//遍历数组int[] arr = &#123;11,22,33,44,55&#125;;for(int ti : arr) &#123; System.out.println(ti);&#125; 10. HashSet类 HashSet集合底层采用HashMap（数组+链表–&gt;散列表），java.util包。 它不保证set 的迭代顺序；特别是它不保证该顺序恒久不变。此类允许使用null元素。 创建对象：无参数，有参数 集合容器的基本使用 增删改查：boolean &#x3D; add(value)，addAll(collection c)，retainAll，removeAll，boolean &#x3D; remove(Object) 没有修改方法 iterator() 获取一个迭代器对象 size() 无重复的原则 在HashSet中，元素都存到HashMap键值对的Key上面，而Value时有一个统一的值private static final Object PRESENT &#x3D; new Object();，(定义一个虚拟的Object对象作为HashMap的value，将此对象定义为static final。) 11. TreeSet类 TreeSet类，无序无重复，java.util包。(底层TreeMap 二叉树 利用Node(left item right)) 创建对象： 无参数构造方法 ，带Collection构造方法 基本常用方法：add(E e)，iterator()，remove(E e)，没有修改，size() 二叉树主要指每个节点最多只有两个子节点的树形结构。 满足以下三个特征的二叉树叫做有序二叉树： 左子树中的任意节点元素都小于根节点元素； 右子树中的任意节点元素都大于根节点元素； 左子树和右子树内部也遵守上述规则； 无序无重复：treeSet集合本身有顺序，我们指的无序存入的和取出来的不一致。 元素放入TreeSet集合过程：由于TreeSet集合底层采用有序二叉树进行数据的管理，当有新元素插入到TreeSet集合时，需要使用新元素与集合中已有的元素依次比较来确定存放合理位置，而比较元素大小规则有两种方式： 使用元素的自然排序规则进行比较并排序，让元素类型实现java.lang.Comparable接口； 使用比较器规则进行比较并排序，构造TreeSet集合时传入java.util.Comparable接口； 注意： 1. 自然排序的规则比较单一，而比较强的规则比较多元化，而且比较器优先于自然排序； 2. 可以使用Collections工具类对集合中的元素进行操作； 12. Map集合 java.util.Map&lt;K, V&gt;集合存取元素的基本单位是：单对元素（键值对key-value）。 Map：映射，通过某一个key可以直接定位到一个value值 key无序无重复 value无序可重复 key无序还是一样，指的是存入顺序与取得顺序不一致，key无重复当然指的是，元素不能一致 主要有两个实现类：HashMap类和TreeMap类。 Map基本使用：HashMap，TreeMap，Properties Map集合常用方法： 增改：put(key,value)，删：remove(key)，查：get(key),containsKey(key),containsValue(value) Map集合的遍历方式：a.迭代Key，b.迭代Entry Map集合的性能调优： 加载因子较小时散列查找性能会提高，同时也浪费了散列桶空间容量。0.75是性能和空间相对平衡的结果，在常见散列表时指定合理容量，减少rehash提高性能。（Capacity:容量，Initial capacity:初始容量，Size:数据大小，Load factor:加载因子(size&#x2F;capacity),默认0.75） 13. HashMap类 包:java.util，底层散列表的形式（数组+链表） 构造方法创建对象 无参数 带默认容量的 带map参数的构造方法 特点:(数组+链表)底层散列表形式存储，key无序无重复,value无序可重复 找寻某一个唯一元素的时候建议使用map，更适合于查找唯一元素，Map$Entry 基本方法： 增 put(key,value)，存放一组映射关系key-value key存储的顺序与取得顺序不同 不同的key可以存储相同的value key若有相同的 则将 原有的value覆盖而不是拒绝存入(跟set刚好相反) 删 E &#x3D; remove(key); 改 replace(key,newValue)，put(key,value2) 查 E &#x3D; get(key)； Set &#x3D; keySet()获取全部的key Set &#x3D; entrySet(); size(); 12345678Set&lt;Entry&lt;Integer,String&gt;&gt; entrys = map.entrySet();//获取集合中全部的entry对象Iterator&lt;Entry&lt;Integer,String&gt;&gt; it = entrys.iterator();while(it.hasNext())&#123; Entry&lt;Integer,String&gt; entry = it.next();//entry key value Integer key = entry.getKey(); String value = entry.getValue(); System.out.println(key+&quot;--&quot;+value);&#125; 除了上述几个常用的方法外 其他API中提供的方法 clear，containsKey(key)，containsValue(value) getOrDefault(key,defaultValue);如果key存在就返回对应的value 若没有找到则返回默认值 isEmpty() putAll(map) putIfAbsent(key,value);&#x2F;&#x2F;如果key不存在才向集合内添加 如果key存在就不添加啦 map集合在什么情形下用? 想要存储一组元素 数组 or 集合，如果存储的元素以后长度不变 用数组，如果长度以后不确定 用集合 如果发现长度以后不确定—&gt;集合 list Set Map List家族有序的 Set家族无重复 Map家族k-v 存储有顺序用这个 存储元素希望自动去掉重复元素用这个 通过唯一的k快速找寻v用这个 ArrayList:更适合遍历轮询 HashSet:性能更高 HashMap:性能更高 LinkedList:更适合插入和删除 TreeSet:希望存进去的元素自动去重复,同时还能自动排序 Tree:希望存进去的元素key自动排序 Stack:LIFO - - 14. TreeMap类 java.util包 构造方法：无参数，带map参数 常用方法：put， get，remove，replace，size 底层数据结构的存储：红黑二叉树（层级多余2层可能会左旋或右旋） 自然有序，按照Unicode编码自然有序 ap集合中的key需要可比较的 key的对象需要实现Comparable接口 15. Lambda表达式 java8支持的新的语法格式，Lambda允许把函数作为一个方法的参数(函数作为参数传递进方法中)，使用lambda表达式可以使代码变得更加简洁紧凑。 函数式编程：一种抽象程度很高的编程范式。函数也可以跟变量、对象一样使用，可以作为参数，也可以作为返回值，大大简化了代码的开发。 lambda表达式语法由参数列表、箭头函数-&gt;和函数体组成，函数体即可以是一个表达式，也可以是一个语句块。 123(int a, int b) -&gt; a+b() -&gt; 42(String s) -&gt; &#123;System.out.println(s);&#125; 函数式接口：指仅仅只包含一个抽象方法的接口，每一个该类型的lambda表达式大都会被匹配到这个抽象方法。 jdk1.8提供了一个@FunctionalInterface注解来定义函数式接口，如果我们定义的接口不符合函数式的规范便会报错。 15.1 Lambda表达式-方法引用 方法引用：只需要使用方法的名字，而具体调用交给函数式接口，需要和Lambda表达式配合使用。 方法引用和lambda表达式拥有相同的特性，我们并不需要为方法引用提供方法体，我们可以直接通过方法名称引用已有的方法。 16. Stream API Stream(流)借助lambda表达式来进行集合数据处理,分为中间操作和最终操作两种；最终操作返回一特定类型的计算结果，而中间操作返回Stream本身，这样就可以将多个操作依次串起。 虽然大部分情况下stream是容器调用Collection.stream()方法得到的，但stream和collections有以下不同： 无存储。stream不是一种数据结构，它只是某种数据源的一个视图，数据源可以是一个数组，Java容器或I&#x2F;O channel等。 为函数式编程而生。对stream的任何修改都不会修改背后的数据源，比如对stream执行过滤操作并不会删除被过滤的元素，而是会产生一个不包含被过滤元素的新stream。 惰式执行。stream上的操作并不会立即执行，只有等到用户真正需要结果的时候才会执行。 可消费性。stream只能被“消费”一次，一旦遍历过就会失效，就像容器的迭代器那样，想要再次遍历必须重新生成。 对stream的操作分为为两类，中间操作和结束操作，二者特点是： 中间操作总是会惰式执行，调用中间操作只会生成一个标记了该操作的新stream，仅此而已。 结束操作会触发实际计算，计算发生时会把所有中间操作积攒的操作以pipeline的方式执行，这样可以减少迭代次数。计算完成之后stream就会失效。 16.1 stream方法使用 stream跟函数接口关系非常紧密，没有函数接口stream就无法工作（通常函数接口出现的地方都可以使用Lambda表达式，所以不必记忆函数接口的名字)。 12345// 找出最长的单词Stream&lt;String&gt; stream = Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);Optional&lt;String&gt; longest = stream.reduce((s1, s2) -&gt; s1.length()&gt;=s2.length() ? s1 : s2);//Optional&lt;String&gt; longest = stream.max((s1, s2) -&gt; s1.length()-s2.length());System.out.println(longest.get());","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」核心工具类","date":"2017-04-12T08:42:47.000Z","path":"2017/04/12/java-api.html","text":"API (Application Programming Interface) 应用程序编程接口，Java中的API，就是JDK提供的各种功能的Java类。 常用的包 java.lang包：是Java最核心的包，JVM(Java虚拟机)启动时自动加载lang包的所有类和接口，无需import。如：System类、String类、Object类、Class类… java.util包：是Java工具包，包括很多工具类和集合。如：Scanner类、Random类… java.io包：是输入输出包，包括读写各种设备。 java.net包：是网络编程的包，包括各种网络编程。 java.sql包：是操作数据库的所有类和接口。 1. Object类与其常用方法1.1 Object类 java.lang.Object类在Java类继承结构中位于顶端(根类)，任何类都是该类的直接或间接子类。 Object定义了“对象”的基本行为，被子类默认继承。 1.2 equals() 和 hashCode() boolean equals()方法用于非空对象的“相等”逻辑，默认比较两个对象的地址，返回布尔值。 equals()方法要求：自反性&#x2F;对称性&#x2F;传递性&#x2F;一致性&#x2F;非空性。 Java类可以根据需要重写继承自Object的equals()方法。 注意：当equals()方法被重写时，必须重写hashCode方法，以维护hashCode方法的常规协定，该协定声明相等对象必须具有相等的哈希码。 int hashCode():返回对象的哈希码值，对应一个内存。 hashCode规范要求： 一致性，同一对象，若没有改变属性值，多次调用其hashCode应该时一致的 如果两个对象判定相等，它们的hashCode应该时同一个值 如果两个对象不相等，它们的hashCode可以相同，但最好不相同而可以提高哈希表的性能。 hashCode()方法和equals()方法的判断条件必须保持一致，如果重写一个，另一个也必须重写。 1.3 toString() String toString()：用于获取调用对象的字符串形式，返回”包名.类名@hashCode值的16进制”。 Java类可以根据需要重写toString方法返回更有意义的信息。 Java在使用System.out.println()打印对象时或者+连接字符串时，默认调用toString()方法。 2. 包装类2.1 包装类 由于某些特殊场合(集合)中要求所有数据内容都必须是对象，而对于基本数据类型的变量来说不满足该要求，为了使得该变量也能够使用就需要对变量打包处理变成对象，此时就需要借助包装类。 Java语言8种基本类型分别对应了8中“包装类”，每一种包装类都封装了一个对应的基本类型成员变量，还提供了一些针对该数据类型的实用方法。 基本类型 对应包装类 byte java.lang.Byte short java.lang.Short int java.lang.Integer long java.lang.Long float java.lang.Float double java.lang.Double boolean java.lang.Boolean char java.lang.Character 八个包装类都在同一个包下（java.lang包），不需要import导包直接使用 八个包装类中有六个是与数字相关，都默认继承父类Number 八个包装类都实现了Serializable, Comparable 八个包装类都有带自己对应类型参数的构造方法，其中有七个(除了Character)还有构造方法重载，带String类型 八个包装类都提供了各自对应的拆包方法，如intValue,floatValue,将包装类对象拆成基本类型 2.2 Integer类 java.lang.Integer类是int类型的包装类，该类型对象中包含一个int类型的成员变量。该类由final关键字修饰表示不能被继承。 Integer类重写了**equals()**方法（重写后比较的是数值）、hashCode()以及toString()方法。 Integer类的常用方法 Integer(int i) 根据参数指定整数来构造对象 Integer(String s) 根据参数指定的字符串来构造对象 int intValue() 获取调用对象中整数值并返回 static Integer valueOf(int i) 根据参数指定整数值得到Integer类型对象 static int parseInt(String s) 将字符串类型转换为int类型并返回 2.3 装箱和拆箱123456int i = 100;Integer it = Integer.valueOf(i); //实现了int类型到Integer类型的转换，这个过程叫做装箱int ia = it.intValue();//实现了Integer类型到int类型的转换，这个过程叫做拆箱//jdk5增加了自动拆箱和装箱功能（编译器预处理）:Integer i = 100;//自动装箱int ia = i;//自动拆箱 笔试考点： 在Integer类部提供了自动装箱池技术，将**-128~127间的整数已经装箱完毕**，当使用该范围整数时直接取池中的对象即可，从而提高效率。 Integer类加载的时候，自己有一个静态的空间立即加载Integer类型的数组，存储256个Integer对象（-128 ~ 127），当使用该范围整数时，直接取静态区中找对应的对象；如果我们用的对象范围会帮我们创建一个新的Integer对象。 1234567891011121314151617Integer it1 = 128;Integer it2 = 128;Integer it3 = new Integer(128);Integer it4 = new Integer(128);System.out.println(it1.equals(it2));//比较内容 trueSystem.out.println(it1 == it2);//比较地址 falseSystem.out.println(it3.equals(it4));//比较内容 trueSystem.out.println(it3 == it4);//比较地址 falseInteger it5 = 127;Integer it6 = 127;Integer it7 = new Integer(127);Integer it8 = new Integer(127);System.out.println(it5.equals(it6));//比较内容 trueSystem.out.println(it5 == it6);//比较地址 true, 自动装箱池范围-128~127。System.out.println(it7.equals(it8));//比较内容 trueSystem.out.println(it7 == it8);//比较地址 false 3. 数学处理类 java.lang.Math构造方法是私有的，我们不能直接调用创建对象；由于Math中提供的属性及方法都是static 不需要创建对象。 常用的方法 返回值类型 Math.abs() 返回给定数字的绝对值(参数 int long float double) Math.ceil() double 向上取整 Math.floor() double 向下取整 Math.rint() double 临近的整数 如果两边距离一样 则返回偶数 Math.round() int 四舍五入的整数 Math.max(a,b)&#x2F;min(a,b) (参数int long float double) Math.pow(a,b) double a的b次方 (参数double 返回值double) Math.sqrt(double a) 获取给定参数的平方根 Math.random() double 随机产生一个[0.0–1.0) 0-9之间的随机整数：int value &#x3D; (int)**(Math.random()10*); Math.random()计算小数的时候精确程度可能有些损失 3.1 Random类 java.util.Random，在java.util包中的类，需要import导入，没有任何继承关系 默认继承Object类 常用的方法 Random r &#x3D; new Random(); r.nextInt(); 随机产生 int取值范围的整数 有正有负(-2^31~2^31-1即正负21亿之间) r.nextInt(int bound); 随机产生一个[0–bound)整数；注意bound必须为正数，否则会出现如下的运行时异常：IllegalArgumentException r.nextFloat() 随机产生一个 [0.0—1.0) r.nextBoolean() 随机产生一个boolean值 true false 3.2 UUID类 java.util.UUID，在java.util包中的类，需要import导入，没有任何继承关系 默认继承Object类 只有有参构造方法，我们通常不会创建对象 UUID uuid &#x3D; UUID.randomUUID();&#x2F;&#x2F;通常用于数据库表格主键 primary key 产生一个32位的随机元素 每一个位置是一个16进制的数字 3.3 BigDecimal java.math.BigDecimal类处理大浮点数，需要import导入，继承自Number Java浮点数据类型(float和double)在运算时会有舍入误差，如果希望得到精确运算结果，可以使用java.math.BigDecimal类。 提供的构造方法全部都是带参数的 通常利用带String参数的构造方法创建这个类的对象：BigDecimal bi &#x3D; new BigDecimal(“1.23”); BigDecimal类的常用方法 BigDecimal(String val) 根据参数指定的字符串来构造对象 BigDecimal setScale(int newScale, RoundingMode roundingMode) 两个参数前面是保留小数点之后的位数，后面参数是设置的模式(向上取整或向下等) BigDecimal add(BigDecimal augend) 用于实现加法运算 BigDecimal subtract(BigDecimal subtrahend) 用于实现减法运算 BigDecimal multiply(BigDecimal multiplicand) 用于实现乘法运算 BigDecimal divide(BigDecimal divisor) 用于实现除法运算，也可传入更多参数设置保留小数点位数和取值模式 123456BigDecimal d3 = new BigDecimal(&quot;3.0&quot;);BigDecimal d4 = new BigDecimal(&quot;2.9&quot;);System.out.println(d3.add(d4));//加：5.9System.out.println(d3.subtract(d4));//减：0.1System.out.println(d3.multiply(d4));//乘：8.70System.out.println(d3.divide(d4, 8, BigDecimal.ROUND_HALF_UP));//除：1.03448276 对于divide方法，通常需要制定精度和舍入模式，否则当遇到无限小数时，除法会一直进行下去直至抛出异常。 3.4 BigInteger java.math.BigInteger类处理大整数，需要import导入，继承自Number java提供的整数类型(int\\long)的存储范围有限，当需要进行很大整数运算时可以使用java.math.BigInteger类，理论上其储值范围只受内存容量限制。 如何创建对象，提供的构造方法全部都是带参数的 通常利用带String参数的构造方法创建这个类的对象：BigInteger bi &#x3D; new BigInteger(“123”); 和BigDecimal类似，BigInteger也提供add()、substract()、multiply()、divide()等方法。 3.5 DecimalFormat类 所属的包 java.text，import导入才能使用 通过带String参数的构造方法创建一个格式化对象(0:未满会补齐，#：未满不补） 123456789101112 //调用format方法将一个小数格式化成一个字符串DecimalFormat df = new DecimalFormat(&quot;000.000&quot;);System.out.println(df.format(12.45)); //012.450System.out.println(df.format(12345.6789)); //12345.679DecimalFormat df2 = new DecimalFormat(&quot;###.###&quot;);System.out.println(df2.format(12.45)); //12.45System.out.println(df2.format(12345.6789)); //12345.679DecimalFormat df3 = new DecimalFormat(&quot;000.###&quot;);System.out.println(df3.format(12.45)); //012.45System.out.println(df3.format(12345.6789)); //12345.679 4. Scanner类和System类4.1 Scanner类 所属的包java.util包 需要import导包 通过一个带输入流的构造方法创建对象 常用方法 nextInt() nextFloat() next() nextLine() 4.1 System类 所属的包java.lang包 不需要导入 不需要创建对象 通过类名就可以访问 有三个属性及若干的方法 三个属性out in err 方法：gc() exit(0); currentTimeMillis()获取系统当前时间毫秒; 5. 日期类5.1 Date类 java.util.Date类表示特定的瞬间，精确到毫秒。 通常使用无参数的构造方法，或者带long构造方法 Date类中常用的方法 before(); after(); setTime() getTime();—–&gt;long compareTo(); &#x2F;&#x2F;-1 1 0 Date类大多数用于进行时间分量计算的方法已经被Calender取代。 1234Date date = new Date();//当前日期信息 //Date类重写了toString方法，输出格式如：Sun Jan 06 11:52:55 CST 2019long time = date.getTime();//1970年1月1日距今毫秒数。date.setTime(time + 24\\*60\\*60\\*1000);//通过毫秒数设置时间 5.2 SimpleDateFormat类 java.text.SimpleDateFormat类主要用于实现日期和文本类型之间的转换。是DateFormat(抽象类)的子类 其构造方法 SimpleDateFormat(String pattern) 12345678Date date = new Date();SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);String dateStr = sdf.format(date);// format用于将日期按指定格式转换为字符串String str = &quot;2013-01-06&quot;;SimpleDateFormat sdf2 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);Date date2 = sdf2.parse(str);//如果字符串格式不匹配将抛出异常 常用格式字符串 含义 示例 y 年 yyyy年——2013年；yy——13年 M 月 MM月——01月；M月——1月 d 日 dd日——01日；d日——1日 H 小时(24) HH:mm:ss—12:46:33 h 小时(12) hh(a):mm:ss—12(下午):47:48 m 分钟 – s 秒 – 5.3 Calendar类 java.util.Calendar类是一个抽象类,主要用于取代Date类中过时的方法来描述年月日时分秒信息。 有构造方法，用protected修饰的，通常访问不到，通常会调用默认的getInstance(); 通常使用Calendar的静态方法getInstance获得Calendar对象；getInstance方法将根据系统地域信息返回不同的Calendar类的实现 123Calendar c1 = Calendar.getInstance();c1.set(2008,9-1,20,8,8,8);System.out.println(c1.getTime()); 常用方法 after() before() setTime() getTime()—-&gt;Date getTimeInMillis()—-time getTimeZone()—TimeZone Calendar里面包含一个date属性 可以操作date的某一个局部信息 set get calendar.set(Calendar.YEAR,2015); *int year &#x3D; calendar.get(Calendar.YEAR); TimeZone java.util包 可以通过calendar对象.getTimeZone()获取 或 TimeZone.getDefault(); 常用方法 tz.getID() —-&gt; Asia&#x2F;Shanghai tz.getDisplayName() —-&gt; 中国标准时间 6. String类6.1 基本概念 String类 —&gt; 引用类型 —&gt; java.lang包 没有任何继承关系，实现三个接口Serializable, CharSequence, Comparable java.lang.String类用于描述字符串数据，java程序中所有的字符串字面值都可以使用String类的实例(对象)加以描述，如”abc”等，任何一个字符对应2字节定长编码。 String类由final关键字修饰表示该类不能被继承，该类描述的字符串内容是常量，一旦创建无法更改，因此可以被共享。对字符串重新赋值不是改变其内容，而是改变引用的指向。 12345678910//如何构建对象String str1 = &quot;abc&quot;; //直接将字符串常量赋值给str (字符串常量池)String str2 = new String();//无参数构造方法创建空的对象String str3 = new String(&quot;abc&quot;);//带string参数的构造方法创建对象byte[] bArr = &#123;97, 98, 99, 100, 101&#125;;//a:97，b:98，c:99，d:100String str4 = new String(bArr);//将数组中的每一个元素转化成对应的char 组合成Stringchar[] cArr = &#123;&#x27;h&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;l&#x27;, &#x27;o&#x27;&#125;;String str5 = new String(cArr);//将数组中的每一个char元素拼接成最终的StringString str6 = String(char[], index, count);//使用char数组中下标从index位置开始的count个字符来构造对象String str7 = String(byte[], index, length);//使用byte数组下标从index位置开始length个字节来构造对象 6.2 字符串常量池 由于String类型对象描述的字符串内容是个常量，若多个相同的内容单独存储会造成时间和空间的浪费。 出于性能考虑，Java虚拟机(JVM)将字符串字面量对象缓存在常量池中；对于重复出现的字符串直接量，JVM会首先在缓存池中查找，如果存在即返回该对象。 12345678910String str1 = &quot;Hello&quot;;String str2 = &quot;Hello&quot;;String str3 = new String(&quot;Hello&quot;);System.out.println(str1.equals(str2));//比较内容 trueSystem.out.println(str1==str2);//比较地址 true，不会重新创建System.out.println(str1.equals(str3));//比较内容 trueSystem.out.println(str1==str3);//比较地址 false，使用new会重新创建新的String对象 //1.下面的代码中创建了几个对象并分别存放在什么位置？String s1 = &quot;hello&quot;; //1个对象，常量池。String s2 = new String(&quot;world&quot;); //2个对象，1个在常量池，1个new后在堆区(内容为常量池里的副本) 6.3 String类常用方法 第一梯队(重写): equals hashCode compareTo toString 第二梯队(常用):charAt()，codePointAt()，indexOf()，lastIndexOf()，substring()，split()，replace()，length()，concat()，contains()， trim()，getBytes()， toCharArray()，matches()。 第三梯队(一般):toUpperCase()，toLowerCase()，startsWith()，endsWith()，isEmpty()。 重写了equals(obj)，hashCode()，toString()方法，compareTo(str)方法实现自Comparable接口 boolean &#x3D; equals(Object obj); 继承自Object类中的方法，重写后改变了规则，比较字符串中的字面值（&#x3D;&#x3D;与equals()区别）; int &#x3D; hashCode(); 继承自Object类中的方法，重写了：31*h+和… int &#x3D; compareTo(); 实现自Comparable接口，实现方法：结果按照字典排布(unicode编码)顺序，按照两个字符串的长度较小的那个(次数)来进行循环，若每次的字符不一致 则直接返回code之差，若比较之后都一致 则直接返回长度之差 String &#x3D; toString() Object类中返回类名@hashCode(16进制形式) String类重写后返回的是String对象的字面值 忽略大小写比较：equalsIgnoreCase(), compareToIgnoreCase(); String类的成员方法 char charAt(int index) 返回字符串指定位置 int codePointAt(int index) “abc”0–&gt;97，返回给定index对应位置的那个char所对应的code码 String concat(String) 将给定的字符串拼接在当前字符串之后 int length() 返回字符串序列的长度 注意：区别数组的length是属性，String的length()是方法，集合是size()方法 123456789101112131415String str6 = new String(&quot;hello&quot;);System.out.println(&quot;下标为0的字符是：&quot;+str6.charAt(0));// hSystem.out.println(&quot;字符串长度是：&quot;+str6.length());// 5 //将字符串&quot;12345&quot;转换为整数类型String str = new String(&quot;123456&quot;); //方式一：Integer类中的pareseInt方法int ia = Integer.parseInt(str);System.out.println(&quot;转换出来结果是：&quot;+ ia);//123456 //方式二：利用ASCII数值进行转换&#x27;1&#x27;-&#x27;0&#x27;=1，&#x27;2&#x27;-&#x27;0&#x27;=2，...int res = 0;for(int i=0; i&lt;str.length(); i++)&#123; res = res*10 + (str.charAt(i)-&#x27;0&#x27;);&#125;System.out.println(&quot;转换出来结果是：&quot;+ res);//123456 String类的常用基本方法 boolean contains(CharSequence s) 判断当前字符串是否包含参数指定的内容 String toLowerCase() 返回小写形式 String toUpperCase() 返回大写形式 String trim() 返回去掉前后空格的字符串 boolean startsWith(String prefix) 判断是否以参数字符开头 boolean endsWith(String suffix) 判断是否以参数字符结尾 boolean equals(Object anObject) 比较字符串内容是否相等，String类已重写 boolean equalsIgnoreCase(String anotherString) 同上，并且忽略大小写 int indexOf(String str) 返回第一次出现str位置，找不到返回-1 int indexOf(String str, int fromIndex) 同上，从fromIndex开始检索 String substring(int beginIndex, int endIndex) 截取字符串，beginIndex开始，endIndex结束 String substring(int beginIndex) 截取字符串，beginIndex开始到结尾 6.4 正则相关方法 正则表达式本质就是一个字符串，用于对用户输入数据的格式进行验证。 正则相关方法 boolean matches(String regex) 用于判断是否匹配正则表达式规则。 String[] split(String regx) 以正则为分割符，将字符串拆分成字符串数组 String replaceAll(String regex, String replacement) 正则替换 7. StringBuilder类&#x2F;StringBuffer类7.1 基本概念 java.lang.StringBuilder类和java.lang.StringBuffer类描述的字符串内容是个可以改变的字符串序列。 StringBuffer和StringBuilder继承AbstractStringBuilder间接继承 Object，实现接口Serializable,CharSequence,Appendable StringBuffer&#x2F;StringBuilder没有compareTo方法 StringBuffer&#x2F;StringBuilder含有一个String没有的方法 append();拼接 7.2 特性可变字符串，char[] value; 动态扩容 7.3 对象的构建123456 //无参数构造方法 构建一个默认长度16个空间的对象 char[]StringBuilder builder = new StringBuilder(); //利用给定的参数 构建一个自定义长度空间的对象 char[]StringBuilder builder = new StringBuilder(20); //利用带String参数的构造方法 默认数组长度字符串长度+16个StringBuilder builder = new StringBuilder(&quot;abc&quot;); 7.4 StringBuilder中常用的方法 最主要的方法 append() 频繁的拼接字符串的时候使用此方法 提高性能 ensureCapacity(int minimumCapacity) 确保底层数组容量够用 capacity();&#x2F;&#x2F;字符串底层char[]的容量 length();&#x2F;&#x2F;字符串有效元素个数(长度) setLength();&#x2F;&#x2F;设置字符串的有效元素个数 char &#x3D; charAt(int index); int &#x3D; codePointAt(int index); String &#x3D; substring(int start [,int end]);&#x2F;&#x2F;注意需要接受返回值 看见截取出来的新字符串效果 StringBuilder &#x3D; delete(int start [,int end]);&#x2F;&#x2F;StringBuilder类中独有的方法String类没有，将start到end之间的字符串删掉 不用接受返回值就看到效果啦 StringBuilder &#x3D; deleteCharAt(int index);&#x2F;&#x2F;String类中没有的方法，将给定index位置的某一个字符删除掉啦 int &#x3D; indexOf(String str [,int fromIndex]); int &#x3D; lastIndexOf(String str [,int fromIndex]);&#x2F;&#x2F;找寻给定的str在字符串中第一次出现的索引位置 带重载 则从某一个位置开始找 insert(int index,value);&#x2F;&#x2F;将给定的value插入在index位置之上 replace(int start,int end,String str);&#x2F;&#x2F;将start和end之间的部分替换成str, builder.replace(2,5,”zzt”); setCharAt(int index,char value);&#x2F;&#x2F;将index位置的字符改成给定的value toString();&#x2F;&#x2F;将StringBuilder对象 构建成一个string对象 返回 trimToSize();&#x2F;&#x2F;将数组中无用的容量去掉 变成length长度的数组 7.5 总结 StringBuilder类不一定需要，是为了避免String频繁拼接修改字符串信息的时候才用的，底层数组是可变的，提高了性能； 常用方法 与String类不同的独有方法：append()，insert()，delete()，deleteCharAt()，reverse()； 与String类相同的方法：length()，charAt()，codePointAt()，indexOf()，lastIndexOf()，substring()，replace()；名字相同 用法不一致 不是很常用的方法：ensureCapacity()，capacity()，setLength()，trimToSize()，setCharAt(); String家族笔试中经常容易考察的知识点 String所属的包 继承关系 实现接口 java.lang 继承Object 接口Serializable,CharSequence,Comparable String构建方式 常量 构造方法 String对象内存结构 字符串常量区 new堆内存对象 &#x3D;&#x3D; equals()区别 “a”+”b”+”c” String不可变特性 长度及内容 String中的常用方法 concat(); toUpperCase(); String和StringBuilder区别 | String和StringBuffer区别 String不可变字符串 JDK1.0 有一个接口Comparable 不可变体现在长度及内容 有一些方法StringBuilder没有 concat compareTo toUpperCase StringBuilder可变字符串 JDK1.5 有一个接口Appendable 可变字符串 没有final修饰 底层可以进行数组扩容 有一些方法String没有 append() insert() delete() reverse() StringBuffer和StringBuilder的不同 它们方法基本相同 StringBuffer早期版本1.0，早期版本，线程同步，安全性比较高，执行效率相对较低 StringBuilder后来的版本1.5，后期版本，线程非同步，安全性比较低，执行效率相对较高 8. Optional类 可能包含或不包含非空值的容器对象。 如果一个值存在， isPresent()将返回true和get()将返回值。 获取字符串长度： 方式1：if(null&#x3D;&#x3D;str){return 0;}else{return str.length();} 方式2：return Optional.ofNullable(str).map(String::length).orElse(0); 1234567891011121314// 获取两个字符串长度和String str1 = &quot;zhangsan&quot;;String str2 = null;int str1Length = Optional.ofNullable(str1).map(String::length).orElse(0);int str2Length = Optional.ofNullable(str2).map(String::length).orElse(0);System.out.println(str1Length + str2Length);//8，8+0//步骤分解://构建Optional对象Optional&lt;String&gt; op1 = Optional.ofNullable(str1);//将str1的长度的结果构建成Optional对象Optional&lt;Integer&gt; op2 = op1.map(String::length);//如果长度不为空，则获取长度值，否则返回默认值int len = op2.orElse(0);System.out.println(len);//8","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」面向对象","date":"2017-04-10T10:50:47.000Z","path":"2017/04/10/java-object.html","text":"面向对象是相对于面向过程而言，过程其实就是函数，对象是将函数和属性进行了封装。Java中的面向对象（Object Oriented）是一种新兴的程序设计方法，或者是一种新的程序设计规范(paradigm)，其基本思想是使用对象、类、继承、封装、多态等基本概念来进行程序设计。 1. 类与对象1.1 类的定义1class 类名 &#123;类体&#125; 类名由多个单词组成时，要求每个单词首字母大写 1.2 成员变量的定义1class 类名 &#123; 数据类型 成员变量名=初始值; ... &#125; 成员变量名由多个单词组成时，要求第二个起每个单词首字母大写 1.3 对象的创建1new 类名(); 当一个类定义完毕后使用new关键字创建&#x2F;构造该类的对象的过程叫做类的实例化。 1.4 引用12类名 引用变量名;Person p = new Person(); //声明person类型的引用p指向Person类型对象 12引用变量名.成员变量名;p.name = &#x27;zhangsan&#x27;; 在JAVA中，使用引用数据类型声明的变量叫做引用变量，简称‘引用’。 使用引用可以记录对象在堆区中存放的内存地址信息，便于下次访问。 除八种基本类型之外，用类名（接口，数组）声明的变量称为引用类型变量，引用类型变量存的某个对象的地址信息，引用的功能在于访问对象。 1.5 成员方法123class 类名 &#123; 返回值类型 成员方法名(形参列表)&#123;方法体;&#125;&#125; 返回值类型：可以是基本数据类型，也可以是引用，当方法不需要返回数据用void 形参列表：数据类型 形参1, 数据类型 形参2, … 2. 构造方法和方法重载2.1 构造方法1class 类名 &#123; 构造方法名(形参列表)&#123;构造方法体;&#125; &#125; 构造方法名与类名相同且没有返回值 当使用new关键字构造对象时，会自动调用构造方法，实现成员变量的初始化工作。 2.2 默认构造方法 当一个类中没有没有自定义任何构造方法时，编译器会提供一个无参的空构造方法，叫做默认&#x2F;缺省构造方法。 若类中出现自定义构造方法，则编译器不再提供构造方法。 2.3 方法重载（overload）在Java中，方法名相同，参数列表不同的方法构成重载关系。 体现形式：参数个数，参数顺序，参数类型。（与形参变量名和返回值无关，但最好返回值类型相同） 实际意义：调用者只需要记住一个方法名就可以不同的版本，从而实现不同的效果。 2.4 this关键字在构造方法中出现this时,this代表当前正在构造的对象；在成员方法中出现this,this代表当前正在调用的对象。 使用方式： 当形参变量和成员变量同名时，在方法体中优先使用形参变量，若希望使用成员变量，则需要加上this，即this.变量名 在构造方法的的第一行，可以调用本类中的其他构造方法。 2.5 封装面向对象的三大特征：封装，继承，多态。 封装基本概念：封装就是对成员变量的数值进行密封包装处理以及合理性判断 封装基本流程： 私有化成员变量(private) 提供公有的get、set方法，并在set方法体中进行合理性判断 在构方法中调用set方法进行合理值的判断 2.6 static关键字基本概念：通常情况下成员变量隶属于对象层级，也就是每创建一个对象就会申请一块独立的内存空间来存储就会造成内存空间的浪费。 为了解决上诉问题，Java中使用static关键字修饰该成员变量表达静态的含义，此时成员变量提升到类层级，所有对象共享，随着类的加载准备就绪，与对象创建再无关。 static可以修饰：修饰属性 修饰方法 修饰块 修饰类(内部类) 特点 静态元素在类加载时就初始化，此时还没创建对象，可以通过类名直接访问 静态元素存储在静态元素区，每个类有一个自己的区域，与别的类不冲突 静态元素只加载一次，全部类对象及类本身共享 静态元素区Carbage Collection无法管理，可以粗暴理解为常驻内存 非静态成员和静态成员都可以访问静态成员 静态成员不可以访问非静态成员 静态元素中不可出现this或super关键字，静态元素属于类的 3. 方法的传递和递归3.1 传参 基本数据类型变量作为参数传递时，型参数值改变不会影响实参变量的数值。 引用类型变量作为参数传递时，形参指向内容的改变会影响实参变量指向的内容。 引用数据类型变量作为参数传递时，形参改变指向后再改变指向内容不会影响实参指向的内容。 3.2 递归的调用 递归是指方法体内部调用自身 必须有递归的规律和退出条件 使用递归必须使得问题简单化而不是复杂化 若递归影响到程序的执行性能时，则用递推取代之 4. 单例设计模式基本概念：当一个类有且只能对外提供一个对象时，这样的类就叫作单例类，而设计单例类的思想和模式，叫做单例设计模式。 12345678910/** * 编程实现Singleton类的封装 */public class Singleton&#123; private static Singleton sin = new Singleton();//2.提供本类的引用指向本类的对象 private Singleton()&#123;&#125; //1.私有化构造方法 public static Singleton getInstance()&#123;//3.提供公有的get方法将上述成员变量的数值返回出去 return sin; &#125;&#125; 实现流程： 私有化构造方法（private） 提供本类类型的引用指向本类类型对象（private static） 提供公有的get方法将上述对象return出去（public static） 实现方式：饿汉式和懒汉式，开发中推荐饿汉式。 5. 继承（extends） 继承就是子类复用父类的代码，关键字extends表示类和类的继承关系 使用继承可以提高代码复用性、扩展性、以及可维护性。 子类不能继承父类的构造方法和私有方法，私有成员变量可以继承但不能直接使用。 无论使用何种方式构造方式构造子类的对象都会自动调用父类的无参构造方法来初始化从父类中继承下来的成员变量，相当于在构造方法的第一行增加super()的效果。 使用继承必须满足逻辑关系：子类 is a 父类，不能滥用继承。 在Java中只能支持单继承，也就是一个一个子类只能有一个父类，但一个父类可以有多个子类。 1234567891011class Cricle extends Shape&#123; int r; Cricle()&#123;&#125; //编译器会加入无参的调用 super()。 Cricle(int x, int y, int r)&#123; super(x, y); //通过super关键字调用父类的构造方法。 setR(r); &#125; public void setR(int r)&#123; this.r = r; &#125;&#125; 6. 方法的重写（Override）概念：从父类继承下来的方法不满足子类的需求时，就需要子类中重新写一个和父类一样的方法，覆盖从父类中继承下来的版本，该方法就叫方法的重写。原则： 要求方法名相同，参数列表相同，返回值类型相同；jdk1.5开始返回子类类型。 要求访问权限不能变小，可以相同或变大 重写的方法不能抛出更大的异常 7. 访问控制 public修饰的内容可以在任意位置使用，private修饰的内容只能在本类中使用， 通常情况下，成员变量都使用private修饰，成员方法都使用pubic修饰 访问控制符 访问权限 本类内部 本类中的包 子类 其他包 public 共有的 Y Y Y Y protected 保护的 Y Y Y N 不写 默认的 Y Y N N private 私有的 Y N N N 8. 包（Package）为了解决命名冲突问题，便于文件的管理 1234package 包名；package 包名1.包名2.包名3...包名n;/* 指定包名时应按照一定的规范，eg: 公司域名反写.项目名称.模块名称.类名 */org.apache.commons.lang.StringUtil; 9. final关键字 final关键字修饰类体现该类不能被继承（防止滥用继承）。 final关键字修饰方法体现在该方法不能被重新，但可以被继承（防止不经意间造成的方法重写）。 final关键字修饰成员变量体现在改成员变量必须初始化且不能更改（防止不经意间造成的数据更改）。 扩展：在开发中很少单独使用static或者final单独修饰成员变量，而是使用**public static final**共同修饰成员变量来表达常量的含义，而常量的命名规范是：所有字母大写，不同单词之间下划线连接。 10. 对象的创建过程 单个对象的创建过程 main方法是程序的入口，若创建对象时没有指定初始值则采用默认初始化方式处理； 若声明成员变量时进行了显示初始化操作，则最终采用显示初始化的初始值处理； 执行构造块中的代码可以对成员变量进行赋值； 执行构造方法体中的代码可以对成员变量进行再次赋值； 此时对象构造完毕，继续向下执行后续的代码； 子类对象的创建过程 main方法是程序的入口，先加载父类的的代码再加载子类的代码； 先执行父类静态代码块，再执行子类的静态代码块； 先执行父类的构造块，再执行父类的构造方法体，此时包含的父类对象构造完毕； 先执行子类的构造块，再执行子类的构造方法体，此时子类对象构造完毕，继续向下执行后续代码。 11. 多态 语法：父类的引用指向子类的对象 123父类类型 引用变量名 = new 子类类型();Person pw = new Worker();pw.show();//再编译阶段调用Person的show()方法，在运行阶段调用Worker的show()方法。 多态的效果： 父类的引用可以直接调用父类独有的方法。 父类的引用不可以直接调用子类独有的方法。 对于父类子类都有的非静态方法来说，编译阶段调用父类的，运行阶段调用子类重写后的。 对于父类子类都有的静态方法来说，只调用父类的。 多态的实际意义：屏蔽不同子类的差异性实现通用的编程，从而带来不同的结果。 多态的表现形式 多态的前提要有继承的关系 使用父类引用指向子类对象 Person p &#x3D; new Teacher();&#x2F;&#x2F;向上转型 该引用只能调用父类中定义的属性&#x2F;方法 执行结果，如果调用属性:执行父类的，如果调用方法:看子类是否重写 若想要调用子类独有的成员，将身份还原回去(向下转型&#x2F;造型)，若需要转换的类型与真实对象类型不匹配，会产生一个运行时异常ClassCastException 引用数据类型之间的转换 转换必须发生在父子类之间，否则编译报错。 自动类型转换：小到大，子类型向父类型的转换，eg:Person pw = new Worker();。 强制类型转换：大到小，父类型向子类型转换，eg:((Worker) pw).getSalary();//将父类引用强制转换子类型调用子类方法。 为了避免类型转换异常，对象进行强制类型转换时应该用instanceof判断引用变量真正指向的对象是否是要转换的目标类型。 123456/*语法格式：*/ 对象 instanceof 类型 //返回布尔值if(pw instanceof Teacher)&#123; Teacher t = (Teacher) pw;&#125;else&#123; System.out.println(&quot;转换会有异常&quot;);&#125; 多态的使用场合： 1234567// 通过方法的参数传递形成多态。public static void draw(Shape s)&#123;&#125;TestShape.draw(new Rect(1,2,3,4));// 在方法体中直接使用多态的语法格式。TestAbstrat ta = new SubTestAbstract();ta.show(); 12. 抽象类 基本概念 用abstract关键字修饰的类称为抽象类。 抽象类不能实例化，抽象类的意义在于被继承。 抽象类为其子类“抽象”出了公共部分，通常也定义了子类所必须具体实现的抽象方法。 抽象方法：指不能具体实现的方法，没有方法体并使用abstract修饰。 12345public abstract class Shape&#123; //一个类若定义了抽象方法，则必须以abstract关键字声明为抽象类 private int x; private int y; public abstract boolean contains(int x, int y);//用abstract修饰的方法，称之为抽象方法，没有方法体&#125; 注意： 抽象类中可以有成员变量，成员方法，以及构造方法。 抽象类中可以没有抽象方法，也可以有抽象方法。 具有抽象方法的类必须是抽象类，因此其真正意义的抽象类应该是有抽象方法，并且使用abstract修饰。 子类必须实现抽象方法（不同子类可能有不同实现），否则改子类也变抽象。 抽象类对子类具有强制性和规范性，因此叫做模板设计模式。 推荐使用多态的语法格式实现抽象类，若需要更换子类时，该方法中只需要将new关键字后面的类型名称修改而其他位置无需改变就可以立即生效，从而提高了代码的维护性和扩展性。 多态实现抽象类的缺点：若希望调用子类独有的方法时，则需要强制类型转换。 13. 接口 基本概念：接口可以看成是特殊的抽象类。即只包含抽象方法的抽象类。通过interface关键字定义。 1234interface Runner &#123; //-通过interface关键字定义接口 public static final int SEF_SPEED=100;//-接口中不能定义成员变量，只能定义常量 public void run();//-接口中只可以定义没有实现的方法（可以省略public abstract）&#125; 一个类可以通过implements关键字实现接口，一个类可以实现多个接口，并且该类需要实现这些接口中定义的所有方法。 12345678910class American implements Runner,... &#123; //与继承不同，可以实现多个接口 @Override public void run()&#123;//该类需要实现接口中定义的所有方法 System.out.println(&quot;run...&quot;); &#125; public static void main(String[] args) &#123; Runner ra = new American();//接口作为一种类型声明，并且声明的变量可以引用实现类的对象 ra.run();//通过该变量可以调用该接口定义的方法 &#125;&#125; 一个接口可以通过extends关键字继承另一个接口，子接口继承了父接口所有的方法。 1interface Hunter extends Runner&#123;...&#125; 类与接口的关系 类和类使用extends继承，仅支持单继承。 接口和接口使用extends继承，支持多继承。 类使用implements实现接口，支持多实现。 抽象类与接口的关系（笔试题） 定义抽象类:abstract class，而定义接口:interface； 类继承抽象类:extends单继承，而类实现接口:implements多实现； 抽象类可以有构造方法，而接口不能有构造方法； 抽象类可以有成员变量，而接口只能有常量； 抽象类可以有成员方法，而接口只能有抽象方法； 抽象类中增加方法子类可以不用重写，而接口中增加方法子类必须重写； 从jdk1.8开始允许接口中有非抽象方法，但需要default关键字修饰。 14. 内部类 内部类指的是在Java中可以将一个类定义在另一个类定义在另一个类的内部 内部类定义在 类的内部 ，与类成员层次一致 内部类定义在 方法&#x2F;块内部（与类成员相差一个层次，方法的局部变量一个层次） 成员内部类：将一个类直接定义在类的里面，作为成员，与属性或方法层次一致 局部内部类：将一个类定义在方法&#x2F;块里面，作为成员的内部结构，与临时的局部变量一个层次 匿名内部类：成员匿名内部类，局部匿名内部类 静态内部类：成员静态内部类 14.1 *成员内部类 将一个类直接定义在类的里面，作为成员，与属性或方法层次一致 成员内部类可以与正常类一样 使用不同的修饰符来修饰 好处1.省略了一个.java文件 好处2.成员内部类中可以访问外部类的所有成员 包括私有的 若想要在内部类中通过对象.调用外部类成员 外部类.this.外部类成员; 内部类存在后 源代码进行编译 产生一个字节码 Demo$InnerDemo.class 14.2 局部内部类 将一个类定义在方法&#x2F;块里面，作为成员的内部结构，与临时的局部变量一个层次 局部内部类像是一个局部的变量一样，不能用public protected private及static 只能用abstract或final 局部内部类命名规则Demo$1InnerTestMethod Demo$2InnerTestMethod 局部内部类使用的变量只能是final修饰 14.3 *匿名内部类将类直接定义在类中 或者类成员中 成员匿名内部类 局部匿名内部类匿名内部类没有类的所有结构(名字 修饰符) 只有类体通常会在抽象类或接口创建的后面使用，当然具体的类也可以有匿名子类匿名类内部没有构造方法，也不能用任何修饰符来修饰 当接口类型的引用作为方法的形参时，实参的传递方式有两种： 自定义类实现接口并重写抽象方法，然后创建该类的对象作为实参传递。 直接使用匿名内部类的语法格式得到接口类型的引用，再作为实参传递。 12345678910111213141516171819202122232425262728293031public interface A &#123; public abstract void show();&#125;//-方式1：自定义类实现接口并重写抽象方法，然后创建该类的对象作为实参传递public class SubA implements A &#123; @Override public void show() &#123; System.out.println(&quot;这里自定义类实现接口并重写抽象方法！&quot;); &#125;&#125;//测试类public class ATest &#123; public static void test(A a) &#123; a.show(); &#125; public static void main(String[] args) &#123; //ATest.test(new A());//报错，A是接口，不能new对象 //-方式1：接口实现类的对象作为实参传递 ATest.test(new ASub());//接口类型引用指向实现类的对象，形成了多态。 //-方式2：匿名内部类 // 接口/父类类型 引用变量名 = new 接口/父类类型() &#123;方法的重写&#125;; A ta = new A() &#123; @Override public void show() &#123; System.out.println(&quot;这里是匿名内部类&quot;); &#125; &#125;; ATest.test(ta);//得到接口类型的引用，再作为实参传递 &#125;&#125; 匿名内部类定义：如果在一段程序需要创建一个类的对象（通常这个类需要实现某个接口或继承某个类），而且对象创建后这个类的价值就不存在了，这个类不必命名，称之为匿名内部类。 语法格式：接口/父类类型 引用变量名 = new 接口/父类类型() &#123;匿名类类体，这里重写方法&#125;;。 1SuperType obj = new SuperType(...)&#123; ... &#125;; 14.4 静态内部类 成员静态内部类 不需要外部类对象，通过正常的方式直接创建内部类 静态元素不能访问非静态成员(自己类和外部类) 15. 回调模式回调模式是指：如果一个方法的参数是接口类型，则在调用该方法时，需要创建并传递一个实现此接口的对象；而该方法在运行时会调用到参数对象中所实现的方法 123456789101112131415interface Action&#123; public void doSth();&#125;//repeat方法需要一个Action接口类型参数，让其doSth方法重复执行n次public static void repeat(int n, Action ac)&#123; for(int i=0; i&lt;n; i++)&#123; ac.doSth();&#125;&#125;//此处的语义可解释为：通过接口回调传递了一个方法给repeat,让repeat将其执行5次。public static void main(String[] args)&#123; repeat(5, new Action()&#123;//通过匿名内部类传递参数 public void doSth()&#123; System.out.println(&quot;Hello&quot;) &#125; &#125;);&#125; 16. 抽象方法的笔试考点 abstract与哪些关键字不能共存： final关键字；因为final关键字修饰的类不能被继承，方法不能被重写，而abstract关键字修饰的类继承后，该类的方法需要重写，相互冲突。 static关键字；因为static能被实例化可直接调用，而abstract不能被实例化，相互冲突。 private关键字；因为private修饰的私有方法不能被继承，就不能重写，而abstract方法需要重写。 17. 枚举类（enum） 一个类中的对象 认为个数是有限且固定的 可以将每一个对象一一列举出来 创建枚举类型要使用 enum 关键字，隐含了所创建的类型都是 java.lang.Enum 类的子类（java.lang.Enum 是一个抽象类）。枚举类型符合通用模式 Class Enum&lt;E extends Enum&gt;，而 E 表示枚举类型的名称。枚举类型的每一个值都将映射到 protected Enum(String name, int ordinal) 构造函数中，在这里，每个值的名称都被转换成一个字符串，并且序数设置表示了此设置被创建的顺序。 我们自己定义的每一个enum类型 都会默认继承Enum 间接继承Object Enum类型，有两个属性 name—–&gt;枚举对象的名字，name()获取name属性 ordinal—&gt;枚举对象在类中罗列的顺序 类似index 也从0开始 ordinal()获取序号 一些常用的方法 valueOf() 通过给定的name获取对应的枚举对象 values() 获取全部的枚举对象 —&gt; 返回一个数组 Day[] compareTo() 可以比较两个枚举对象 int toString() 由于这个方法没有final修饰 可以覆盖(重写) switch内部判断枚举的应用 我们也可以在enum中描述自己的一些属性或方法 必须在enum类中第一行 描述一下枚举的样子 最后需要分号结束; 可以定义自己的属性 类创建的过程中 帮我们创建枚举类型的对象 需要给枚举类型提供对应样子的构造方法 构造方法只能private修饰 可以重载 1234567891011121314151617181920public enum Day&#123; //描述了七个当前类的对象 monday(&quot;星期一&quot;,1),tuesday(&quot;星期二&quot;,2),wednesday,thursday,friday,saturday,sunday; private String name; private int index; private Day()&#123;&#125; private Day(String name,int index)&#123; this.name=name; this.index=index; &#125; public String getName()&#123; return this.name; &#125; public void setName(String name)&#123; this.name=name; &#125;&#125; 18. 内存机制问题 类创建在哪儿 对象创建在哪里 继承关系 静态成员 方法执行 栈内存—&gt;Person p &#x3D; new Person();—-&gt;堆内存 方法区—类模板 栈内存—-变量空间,方法临时执行空间（从创建开始执行完毕,立即回收） 堆内存—-new申请对象空间（垃圾回收器GC,对象空间没有任何引用指向视为垃圾） 方法区—-常量 类模板 静态成员（有且只有一份,不回收） Runtime类(是单例模式)之中提供了几个管理内存的方法 maxMemory totalMemory freeMemory 栈内存溢出错误StackOverflowError 堆内存溢出错误OutOfMemoryError Object类中有一个finalize方法 如果重写也能看见对象回收的效果 GC系统提供的一个线程 回收算法","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"「Java教程」语法基础","date":"2017-04-07T09:33:54.000Z","path":"2017/04/07/java-grammar.html","text":"这是Java教程的第一篇，梳理Java基础知识是学习其他专业知识的第一步阶梯；要想精通编程必须通过大量的编程训练，在实践中掌握编程知识，培养编程能力，并逐步理解和掌握程序设计的思想和方法。 1. 数据类型1.1 两大类： 基本数据类型：byte, short, int, long, float, double, boolean, char。 引用类型：数组，类（抽象类）class，接口interface，枚举enum，注解@interface。 注：单个字节表示8位二进制位，最左边表示符号位（0：正，1：负）。 1.2 整数类型（byte, short, int, long） byte： 1字节，范围-2^7 ~ 2^7-1，即-128-127。 short: 2字节，范围-2^15 ~ 2^15-1，即-32768 ~ 32767。 int: 4个字节，范围-2^31 ~ 2^31-1，即正负21亿之间。 long： 8个字节，范围-2^63 ~ 2^63-1，比int更大。如果要表示long类型数值，需要L或l结尾。 1.3 浮点类型（float，double） float：4个字节，单精度浮点数，取到小数点后7位有效数字。如果表示float类型数值，需要F或f结尾。 double：8个字节，双精度浮点数，取到小数点后15位有效数字。如果表示double类型数值，需要D或d结尾。 扩展：浮点运算有时会有误差，为了实现精确运算可使用java.math.BigDecimal类型加以描述。 1.5 字符类型（char） char：2个字节，表示单个字符的数据类型。事实是一个16位无符号整数，值是对应字符的编码，如：&#39;a&#39;,&#39;1&#39;,&#39;中&#39; 等。 开发中很少用到char类型，而使用String类型描述的多个字符组成的字符串，使用双引号&quot;&quot;引起来。 需记住的ASII码字符：&#39;a&#39;:97，&#39;A&#39;:65，&#39;0&#39;:48，空格:32，换行符:10 常用转义符(逃逸字符)：\\t:制表符，\\n:换行，\\&quot;，\\&#39;，\\\\:反斜杠本身，\\b:回退一格，\\r:回车 字符类型计算 一个字符加一个数字，得到Unicode码表中那个数之后的字符 两个字符相减得到它们在表中的距离 char也可以和int之间相互赋值 2. 变量与常量2.1 常量 常量是一个值，在程序运行的过程中不能再次发生改变 基本类型的值都可以认为是常量：4,3.4,&#39;a&#39;,true；引用类型String类的值&quot;abc&quot;视为常量。 常量存储在常量缓冲区(常量池)中，有且只有一份,常量池中的值默认空间大小int(32bit),double(64bit)。 2.2 变量 变量是在栈内存中开辟的一块内存空间，程序执行过程中可以改变； 变量空间在创建的时候（声明变量），必须指定数据类型，变量空间的名字 变量空间，里面只能存储一个内容（值&#x2F;引用） 空间内的类容的类型与定义时一致，内容可以改变 内存结构与执行过程：类的定义，编译，加载空间各个区，变量赋值 2.3 标识符（变量）命名规则 必须字母，数字，下划线以及美元$等组成，且首位非数字。 不能使用Java语言中的关键字，如class，static，void，int等。 区分大小写，长度无限制，但不能过长，尽量见名知意。 2.3 命名的规约 类名字：首字母大写，如果两个以上的单词，所有首字母都大写 变量名：首字母小写，如果两个以上的单词，之后的首字母大写 遵循驼峰式命名规约，所有名字都需要见名知义，为了增强程序的可读性 3. 数据类型之间的转换3.1 基本数据类型之间转换 自动类型转换：从小类型到大类型自动转换 12byte --&gt; short --&gt; int --&gt; long --&gt; float --&gt; double char -----^ 强制类型转换：需在被转换数据前加上类型，会造成精度损失或者溢出 12long big = 1024L\\*1024\\*1024;int i = (int)big; 3.2其他数据类型之间转换 同种大类型之间才能发生转换 基本–基本 可以直接转换(自动 强制) 引用–引用 可以直接转化(自动 强制 – 上转型 下转型) 基本–引用 不可以直接进行转化(间接-桥梁-包装类) 小数据类型一致:整型–&gt;整型 &#x2F; 浮点–&gt;浮点 比较内存空间的大小 大数据类型空间可以直接接受小数据类型的值(自动转换) 小数据类型空间不可以直接接受大数据类型的值(强制类型转换) 强制类型转换,写法好用,但是需要注意取值范围的问题,丢失一部分数据 1234byte a = 1; int b = a;//自动直接转化就可以int a = 1; byte b = (byte)a;//需要强制类型转换float x = 3.4F; double y = x;//自动直接转化double x = 3.4; float y = (float)x;//强制转换 小数据类型不一致:整型–&gt;浮点 比较精确程度 浮点型的精确程度更高 任何一个浮点型空间都可以直接接受一个整型的值 反之需要进行强制类型转换(强行将小数点之后的部分去掉,只保留整数) 12int a = 1; float b = a;//自动直接转化float a =1.0F; int b = (int)a;//强制类型转换 整型–&gt;字符 每一个字符都对应这一个Unicode码 a–97 12char x = &#x27;a&#x27;; int y = x;//自动转化 y--97int x = 97; char y = (char)x;//强制的转化 布尔类型很特殊 不能与其他基本类型之间发生转化 4. 运算符 算术运算符： +，-，*，/，% 关系运算符： &gt;，&lt;，&gt;=，&lt;=，==，!= 自增减运算符： ++，-- 逻辑运算符： &amp;&amp;，||，！（短路特性：逻辑与&amp;&amp;运算，若第一个条件为假,跳过第二个条件；逻辑或||运算，若第一个条件为真，跳过第二个条件） 三目运算符： 条件 ? 表达式1 ： 表达式2 赋值运算符：=，+=，-=，*=，= 5. 循环 while循环和for循环完全可以互换。 while循环主要用于明确循环条件，但不明确循环次数的场合 for循环主要用于明确次数或范围的场合 while(true) 等价于 for(;;)，表示无限循环。 6. 数组6.1 一维数组 数组类型[] 数组名 = new 数据类型[长度] ：动态方式 12数组类型[] 数组名 = new 数据类型[长度] //动态方式int[] arr = new int[5]; 123数组类型[] 数组名 = &#123;初始值1, 初始值1, 初始值1, ...&#125; //静态方式int[] arr = &#123;10, 20, 30, 40&#125;;/*特殊方式：*/ int[] arr = new int[]&#123;10, 20, 30&#125;; 6.2 二维数组12数组类型[][] 数组名 = new 数据类型[行数][列数]int[][] arr = new int[5][6]; 12数组类型[][] 数组名 = &#123;&#123;初始值1, 初始值1, 初始值1&#125;,&#123;值2,...&#125; ...&#125;int[] arr = &#123;&#123;10, 20, 30&#125;, &#123;01, 5, 3&#125;, &#123;8, 20, 6&#125;&#125;; 二维数组arr.length表示行数，arr[0].length表示此行的长度 7. 函数 函数是一块代码，接收零个或多个参数，做一件事情，并返回零个或一个值。 123456789public static int sum(int a, int b)&#123; int i; int sum=0; for(i=a; i&lt;=b; i++)&#123; sum += i; &#125; System.out.println(a +&quot;到&quot;+ b +&quot;的和是&quot;+ sum); return sum;&#125; 函数的调用：函数名(参数值); 即使没有参数也需要(),()起到了调用函数的作用，如果有参数，则需要给出正确的数量和顺序 函数的返回：return停止函数的执行，并返回一个值，可以再赋值给变量，传递给另一个函数，甚至可以丢弃，有时候要的是副作用。","tags":[{"name":"后端开发","slug":"back-end","permalink":"http://chaooo.github.io/tags/back-end/"},{"name":"Java","slug":"Java","permalink":"http://chaooo.github.io/tags/Java/"}]},{"title":"BlueLake博客主题的详细配置","date":"2016-12-29T03:25:33.000Z","path":"2016/12/29/bluelake.html","text":"2021年2月3日更新：BlueLake主题主题写了有些年头了，随着Hexo升级到5.X，BlueLake很多配置已经过时，在使用中难免出现问题。 所以，我利用工作之余抽了些时间进行了大改版，主题模板引擎由Jade(Pug)换成了EJS，以landscape为原型进行二次开发；保留BlueLake主题老版本的其他功能，如原生JS实现站内搜索功能，本地分享等；新添加了一些新的功能，如打赏模块，集成新的三方评论等等。 说了这么多不如你亲自体验来的直接，BlueLake主题地址：https://github.com/chaooo/hexo-theme-BlueLake。 在阅读本文之前，假定您已经成功安装了Hexo，并使用 Hexo 提供的命令创建了一个静态博客。Hexo是一个快速、简洁且高效的博客框架。Hexo基于Node.js ，使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 需要特别注意的是Hexo有两个_config.yml配置文件，一份位于站点根目录下，主要包含 Hexo 站点本身的配置，下文中会称为**根_config.yml；另一份位于主题目录下（themes&#x2F;主题名&#x2F;_config.yml），这份配置由主题作者提供，主要用于配置主题相关的选项,下文中会称为主题_config.yml**。 1. 安装您可以直接到BlueLake发布页下载，然后解压拷贝到themes目录下，修改配置即可。不过我还是推荐使用GIT来checkout代码，之后也可以通过git pull来快速更新。 1.1 安装主题在根目录下打开终端窗口： git bash1git clone https://github.com/chaooo/hexo-theme-BlueLake.git themes/bluelake 1.2 安装主题插件在根目录下打开终端窗口： git bash1234npm install hexo-renderer-jade --savenpm install hexo-renderer-stylus --savenpm install hexo-generator-feed --savenpm install hexo-generator-sitemap --save 1.3 启用主题打开根_config.yml配置文件，找到theme字段，将其值改为bluelake(先确认主题文件夹名称是否为bluelake)。 根_config.yml_config.yml1theme: bluelake 1.4 验证首先启动 Hexo 本地站点，并开启调试模式： git bash1hexo s --debug 在服务启动的过程，注意观察命令行输出是否有任何异常信息，如果你碰到问题，这些信息将帮助他人更好的定位错误。 当命令行输出中提示出：INFO Hexo is running at http://0.0.0.0:4000/. Press Ctrl+C to stop.此时即可使用浏览器访问 http://localhost:4000，检查站点是否正确运行。 1.5 更新主题今后若主题添加了新功能正是您需要的，您可以直接git pull来更新主题。 git bash12cd themes/bluelakegit pull 2. 配置2.1 配置网站头部显示文字打开根_config.yml，找到： 根_config.yml_config.yml12345title: # 标题，如：秋过冬漫长subtitle: # 副标题，如：没有比脚更长的路，走过去，前面是个天！description: # 网站关键字，如：key, key1, key2, key3keywords:author: title和subtitle分别是网站主标题和副标题，会显示在网站头部； description在网站界面不会显示，内容会加入网站源码的meta标签中，主要用于SEO； keywords在网站界面不会显示，内容会加入网站源码的meta标签中，主要用于SEO； author就填写网站所有者的名字，会在网站底部的Copyright处有所显示。 2.2 设置语言该主题目前有七种语言：简体中文（zh-CN），繁体中文（zh-TW），英语（en），法语（fr-FR），德语（de-DE），韩语 （ko）,西班牙语（es-ES）；例如选用简体中文，在根_config.yml配置如下： 根_config.yml_config.yml1language: zh-CN 2.3 设置菜单打开主题_config.yml，找到： 主题_config.ymlthemes/bluelake/_config.yml12345678910111213menu: - page: home directory: . icon: fa-home - page: archive directory: archives/ icon: fa-archive # - page: about # directory: about/ # icon: fa-user - page: rss directory: atom.xml icon: fa-rss 主题默认是展示四个菜单，即主页home，归档archive，关于about，订阅RSS；about需要手动添加，RSS需要安装插件，若您并不需要，可以直接注释掉。 2.4 添加about页此主题默认page页面是关于我页面的布局，在根目录下打开命令行窗口，生成一个关于我页面： git bash1hexo new page &#x27;about&#x27; 打开主题_config.yml，补全about页面的详细信息： 主题_config.ymlthemes/bluelake/_config.yml123456789101112about: photo_url: # 头像的链接地址 e.g. http://cdn.itdn.top/blog/themeauthor.jpg items: - label: email # 个人邮箱 url: ## Your email with mailto: e.g. mailto:zhenggchaoo@gmail.com title: ## Your email e.g. zhenggchaoo@gmail.com - label: github # github url: ## Your github&#x27;url e.g. https://github.com/chaooo title: ## Your github&#x27;name e.g. chaooo - label: weibo # 微博 url: ## Your weibo&#x27;s url e.g. http://weibo.com/zhengchaooo title: ## Your weibo&#x27;s name e.g. 秋过冬漫长 当然您也可以自定义重新布局about页面，只需要修改layout/page.jade模板就好。 2.5 添加本地搜索默认本地搜索是用原生JS写的，但还需要HEXO插件创建的JSON数据文件配合。安装插件hexo-generator-json-content来创建JSON数据文件： git bash1npm install hexo-generator-json-content@2.2.0 --save 然后在根_config.yml添加配置： 根_config.yml_config.yml123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 最后在主题_config.yml添加配置： 主题_config.ymlthemes/bluelake/_config.yml1local_search: true 2.6 首页添加文章置顶在根目录下打开命令行窗口安装： git bash12npm uninstall hexo-generator-index --savenpm install hexo-generator-index-pin-top --save 然后在需要置顶的文章的Front-matter中加上top: true即可。 123456---title: BlueLake博客主题的详细配置tags: [hexo,BlueLake]categories: Hexo博客top: true--- 3.集成第三方服务3.1 添加评论目前主题集成多种第三方评论，比如基于Github Issue的GITALK），Valine评论，畅言评论，Disqus评论、来必力评论、友言评论、网易云跟帖评论等。 配置主题_config.yml： 主题_config.ymlthemes/bluelake/_config.yml123456789101112131415161718192021222324252627282930313233343536# Gitalk commentgitalk: enable: false owner: ## Your GitHub ID, e.g. username repo: ## The repository to store your comments, make sure you&#x27;re the repo&#x27;s owner, e.g. gitalk.github.io client_id: ## GitHub client ID, e.g. 75752dafe7907a897619 client_secret: ## GitHub client secret, e.g. ec2fb9054972c891289640354993b662f4cccc50 admin: ## Github repo owner and collaborators, only these guys can initialize github issues. language: &#x27;zh-CN&#x27; ## Language pagerDirection: last # Comment sorting direction, available values are last and first.# Valine comment. https://valine.js.orgvaline: enable: false # if you want use valine,please set this value is true appId: # leancloud application app id appKey: # leancloud application app key notify: false # valine mail notify (true/false) https://github.com/xCss/Valine/wiki verify: false # valine verify code (true/false) pageSize: 10 # comment list page size avatar: mm # gravatar style https://valine.js.org/#/avatar lang: zh-cn # i18n: zh-cn/en placeholder: Just go go # valine comment input placeholder(like: Please leave your footprints ) guest_info: nick,mail,link #valine comment header info# Changyan comment. 畅言changyan: enable: false appid: ## changyan appid appkey: ## changyan appkey# Other commentscomment: disqus: ## disqus_shortname livere: ## 来必力(data-uid) uyan: ## 友言(uid) cloudTie: ## 网易云跟帖(productKey) 3.2 站点统计主题_config.ymlthemes/bluelake/_config.yml1234busuanzi: true # 卜算子阅读次数统计baidu_analytics: # 百度统计google_analytics: # 谷歌统计gauges_analytics: # Gauges 3.2.1 卜算子阅读次数统计若busuanzi设置为true将计算文章的阅读量，及网站的访问量与访客数，并显示在文章标题下和网站底部。 3.2.2 百度统计登录百度统计，定位到站点的代码获取页面。复制//hm.baidu.com/hm.js?后面那串统计脚本id(假设为：8006843039519956000) 主题_config.ymlthemes/bluelake/_config.yml1baidu_analytics: 8006843039519956000 注意： baidu_analytics不是你的百度id或者百度统计id，如若使用其他统计，配置方法与百度统计类似。","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chaooo.github.io/tags/hexo/"},{"name":"BlueLake","slug":"BlueLake","permalink":"http://chaooo.github.io/tags/BlueLake/"}]},{"title":"自定义HEXO站内搜索Javascript+json","date":"2016-11-09T01:24:56.000Z","path":"2016/11/09/json-search.html","text":"开始之前目前很多Hexo博客都用的Swiftype和Algolia等第三方搜索服务。其实针对无数据库的情况下，Hexo本身也提供了两个插件来生成数据文件作为数据源： hexo-generator-search生成xml格式的数据文件。 hexo-generator-json-content 生成json格式的数据文件。今天的主角是hexo-generator-json-content，对于 Javascript语言来说还是解析 json 更方便，如果需要用 xml 做数据文件也可以使用已有的atom.xml。 1.安装1$ npm install hexo-generator-json-content@2.2.0 --save 然后执行hexo generate时会自动生成content.json文件，若使用默认设置，生成的数据结构如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445meta: &#123; title: hexo.config.title, subtitle: hexo.config.subtitle, description: hexo.config.description, author: hexo.config.author, url: hexo.config.url&#125;,pages: [&#123; //-&gt; all pages title: page.title, slug: page.slug, date: page.date, updated: page.updated, comments: page.comments, permalink: page.permalink, path: page.path, excerpt: page.excerpt, //-&gt; only text ;) keywords: null //-&gt; it needs settings text: page.content, //-&gt; only text minified ;) raw: page.raw, //-&gt; original MD content content: page.content //-&gt; final HTML content&#125;],posts: [&#123; //-&gt; only published posts title: post.title, slug: post.slug, date: post.date, updated: post.updated, comments: post.comments, permalink: post.permalink, path: post.path, excerpt: post.excerpt, //-&gt; only text ;) keywords: null //-&gt; it needs settings text: post.content, //-&gt; only text minified ;) raw: post.raw, //-&gt; original MD content content: post.content, //-&gt; final HTML content categories: [&#123; name: category.name, slug: category.slug, permalink: category.permalink &#125;], tags: [&#123; name: tag.name, slug: tag.slug, permalink: tag.permalink &#125;]&#125;] 2.配置hexo-generator-json-content默认生成的json数据内容非常全，默认配置如下： 123456789101112131415161718192021222324252627282930313233jsonContent: meta: true keywords: false # (english, spanish, polish, german, french, italian, dutch, russian, portuguese, swedish) pages: title: true slug: true date: true updated: true comments: true path: true link: true permalink: true excerpt: true keywords: true # but only if root keywords option language was set text: true raw: false content: false posts: title: true slug: true date: true updated: true comments: true path: true link: true permalink: true excerpt: true keywords: true # but only if root keywords option language was set text: true raw: false content: false categories: true tags: true 因为默认生成了很多我们不需要的数据，所以我们要对其进行配置让它只生成我们想要的内容,在hexo/_config.yml中加入： 123456789101112131415161718jsonContent: meta: false pages: false posts: title: true #文章标题 date: true #发表日期 path: true #路径 text: true #文本字段 raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true #标签 这样，就只生成每篇文章的标题，日期，路径，标签和文本字段，同时也减小了文件的大小。例如： 1234567891011&#123; &quot;title&quot;: &quot;自定义HEXO站内搜索Javascript+json&quot;, &quot;date&quot;: &quot;2016-11-09T01:24:56.000Z&quot;, &quot;path&quot;: &quot;2016/11/09/自定义HEXO站内搜索Javascript-json.html&quot;, &quot;text&quot;: &quot;目前很多Hexo博客都用的Swiftype和Algolia等第三......#这里显示整篇文章的内容&quot;, &quot;tags&quot;: [&#123; &quot;name&quot;: &quot;javascript,hexo&quot;, &quot;slug&quot;: &quot;javascript-hexo&quot;, &quot;permalink&quot;: &quot;http://chaoo.oschina.io/tags/javascript-hexo/&quot; &#125;]&#125; 3.JavaScript实现代码接下来就是用JS实现查询方法并把结果渲染到页面。 3.1 xhr加载数据12345678910111213141516171819202122var searchData;function loadData(success) &#123; if (!searchData) &#123; var xhr = new XMLHttpRequest(); xhr.open(&#x27;GET&#x27;, &#x27;/content.json&#x27;, true); xhr.onload = function() &#123; if (this.status &gt;= 200 &amp;&amp; this.status &lt; 300) &#123; var res = JSON.parse(this.response || this.responseText); searchData = res instanceof Array ? res : res.posts; success(searchData); &#125; else &#123; console.error(this.statusText); &#125; &#125;; xhr.onerror = function() &#123; console.error(this.statusText); &#125;; xhr.send(); &#125; else &#123; success(searchData); &#125;&#125; 3.2 匹配文章内容返回结果12345678910function matcher(post, regExp) &#123; // 匹配优先级：title &gt; tags &gt; text return regtest(post.title, regExp) || post.tags.some(function(tag) &#123; return regtest(tag.name, regExp); &#125;) || regtest(post.text, regExp);&#125;function regtest(raw, regExp) &#123; regExp.lastIndex = 0; return regExp.test(raw);&#125; 3.3 结果渲染到页面123456789101112131415function render(data) &#123; var html = &#x27;&#x27;; if (data.length) &#123; html = data.map(function(post) &#123; return tpl(searchTpl, &#123; title: post.title, path: post.path, date: new Date(post.date).toLocaleDateString(), tags: post.tags.map(function(tag) &#123; return &#x27;&lt;span&gt;&#x27; + tag.name + &#x27;&lt;/span&gt;&#x27;; &#125;).join(&#x27;&#x27;) &#125;); &#125;).join(&#x27;&#x27;); &#125; &#125; 3.3 查询匹配1234567891011function search(key) &#123; // 关键字 =&gt; 正则，空格隔开的看作多个关键字 // a b c =&gt; /a|b|c/gmi var regExp = new RegExp(key.replace(/[ ]/g, &#x27;|&#x27;), &#x27;gmi&#x27;); loadData(function(data) &#123; var result = data.filter(function(post) &#123; return matcher(post, regExp); &#125;); render(result); &#125;);&#125;","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chaooo.github.io/tags/hexo/"},{"name":"BlueLake","slug":"BlueLake","permalink":"http://chaooo.github.io/tags/BlueLake/"},{"name":"Javascript","slug":"Javascript","permalink":"http://chaooo.github.io/tags/Javascript/"}]},{"title":"好用的Web包管理器-Bower","date":"2016-08-12T07:32:41.000Z","path":"2016/08/12/bower.html","text":"Bower是twitter推出的客户端包管理工具，用于命令行操作包的搜索、下载、更新、卸载(如jQuery、Bootstrap、JavaScript、HTML、CSS之类的网络资源)。Bower对包结构没有强制规范，可以很方便获取各种Web模块文件，但bower本身不存储模块文件和模块版本信息，模块发布者通过register方式将模块可访问的公开的git地址记录在bower的数据库中，而所有版本都是通过代码库的tag来决定的。 开始之前在安装bower之前，必须确认你已经安装了Node.js和Git。 1.安装Bower使用npm，打开终端，键入： 1npm install -g bower #全局安装bower 移步这里查看不同平台上安装的问题。 2.使用Bower使用help命令查看帮助。 12345678910111213141516171819202122232425262728293031323334bower helpUsage: bower &lt;command&gt; [&lt;args&gt;] [&lt;options&gt;]Commands: cache Manage bower cache help Display help information about Bower home Opens a package homepage into your favorite browser info Info of a particular package init Interactively create a bower.json file install Install a package locally link Symlink a package folder list List local packages - and possible updates login Authenticate with GitHub and store credentials lookup Look up a package URL by name prune Removes local extraneous packages register Register a package search Search for a package by name update Update a local package uninstall Remove a local package unregister Remove a package from the registry version Bump a package versionOptions: -f, --force Makes various commands more forceful -j, --json Output consumable JSON -l, --loglevel What level of logs to report -o, --offline Do not hit the network -q, --quiet Only output important information -s, --silent Do not output anything, besides errors -V, --verbose Makes output more verbose --allow-root Allows running commands as root -v, --version Output Bower version --no-color Disable colorsSee &#x27;bower help &lt;command&gt;&#x27; for more information on a specific command. 3.安装包到本地通过命令bower install安装软件包默认到bower_components&#x2F;目录。 1bower install &lt;package&gt; #package为包名 想要下载的包可以是GitHub上的短链接（如jquery&#x2F;jquery）、.git 、一个URL或者其它. 12345bower install # 通过 bower.json 文件安装bower install jquery # 通过在github上注册的包名安装bower install desandro/masonry # GitHub短链接bower install git://github.com/user/package.git # Github上的 .gitbower install http://example.com/script.js # URL 安装选项 12345-F, --force-latest: Force latest version on conflict-p, --production: Do not install project devDependencies-S, --save: Save installed packages into the project’s bower.json dependencies-D, --save-dev: Save installed packages into the project’s bower.json devDependencies-E, --save-exact: Configure installed packages with an exact version rather than semver 4.用bower.json文件来管理依赖发布项目的时候没有必要把所有依赖的库发布上去，只需在根目录生成一个bower.json文件即可，别人使用时在根目录执行bower install就可根据bower.json来安装依赖的包。在项目中执行 1bower init 会提示你输入一些基本信息，根据提示按回车或者空格即可，然后会生成一个bower.json文件，用来保存该项目的配置.如果想保存依赖信息(dependencies)到你的bower.json文件，安装包时，命令后面跟上--save即可。 5.使用下载好的包对于已经下载下来的包，默认在当前目录的bower_components文件夹。你可以直接在项目里引用。例如： 12&lt;link rel=&quot;stylesheet&quot; href=&quot;bower_components/bootstrap/dist/css/bootstrap.min.css&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;bower_components/jquery/dist/jquery.min.js&quot;&gt;&lt;/script&gt; 6.更新包若下载的包升级了，只需执行update命令即可更新，例如： 1bower update jquery 这样就可以自动升级到最新版的jquery了。更新选项 1234-F, --force-latest: Force latest version on conflict-p, --production: Do not install project devDependencies-S, --save: Update dependencies in bower.json-D, --save-dev: Update devDependencies in bower.json 7.搜索包12bower search #搜索所有包bower search &lt;packageName&gt; #搜索指定名称的包 或者可以在这里:https://bower.io/search/搜索喜欢的包. 8.卸载包1bower uninstall &lt;name&gt; [&lt;name&gt; ..] [&lt;options&gt;] 卸载选项 12-S, --save: Remove uninstalled packages from the project’s bower.json dependencies-D, --save-dev: Remove uninstalled packages from the project’s bower.json devDependencies","tags":[{"name":"bower","slug":"bower","permalink":"http://chaooo.github.io/tags/bower/"},{"name":"环境配置","slug":"env","permalink":"http://chaooo.github.io/tags/env/"}]},{"title":"基于Hexo+github搭建静态博客","date":"2016-05-23T03:16:51.000Z","path":"2016/05/23/hexo-github.html","text":"开始之前在安装hexo之前，必须确认你已经安装了Node.js和Git。 1.创建GitHub仓库注册GitHub账号，创建一个以”用户名.github.io”命名的仓库，如我的用户名为chaooo,那我的仓库名为：chaooo.github.io，仓库默有master分支，用于托管生成的静态文件，再新建一个develop(名字自定)分支，用于托管后台文件，方便以后换电脑时后台文件不会丢失。 2.配置Git设置Git的用户名和邮件地址（邮箱就是你注册Github时候的邮箱），打开Git Bash,键入： 12$ git config --global user.name &quot;username&quot;$ git config --global user.email &quot;email@example.com&quot; 3.本地Git与GitHub建立联系这里介绍SSH的配置，先检查电脑是否已经有SSH 1$ ls -al ~/.ssh 如果不存在就没有关系，如果存在的话，直接删除.ssh文件夹里面所有文件。输入以下指令后，一路回车就好： 1$ ssh-keygen -t rsa -C &quot;emailt@example.com&quot; 然后键入以下指令： 12$ ssh-agent -s$ ssh-add ~/.ssh/id_rsa 如果出现这个错误:Could not open a connection to your authentication agent，则先执行如下命令即可： 1$ ssh-agent bash 再重新输入指令： 1$ ssh-add ~/.ssh/id_rsa 到了这一步，就可以添加SSH key到你的Github账户了。键入以下指令，拷贝Key（先拷贝了，等一下可以直接粘贴）： 1$ clip &lt; ~/.ssh/id_rsa.pub 在github上点击你的头像–&gt;Your profile–&gt;Edit profile–&gt;SSH and GPG keys–&gt;New SSH keyTitle自己随便取，然后这个Key就是刚刚拷贝的，你直接粘贴就好（也可以文本打开id_rsa.pub复制其内容），最后Add SSH key。最后还是测试一下吧，键入以下命令： 1$ ssh -T git@github.com 你可能会看到有警告，没事，输入“yes”就好。 4.初始化hexo文件夹到GitHub的chaooo.github.io仓库下，点击Clone or download,复制里面的HTTPS地址。在E盘或是你喜爱的文件夹下，右键Git Bash Here: 键入git clone -b develop &lt;刚复制的地址&gt; 12$ git clone -b develop https://github.com/chaooo/chaooo.github.io.git$ mkdir Hexo-admin Hexo安装配置1.Hexo初始化进入Hexo-admin文件夹 1$ cd Hexo-admin 接下来只需要使用 npm 即可完成 Hexo 的安装: 1$ npm install -g hexo-cli 安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件: 12$ hexo init$ npm install 接下来也可以本地预览博客，执行下列命令,然后到浏览器输入localhost:4000看看。 12$ hexo generate$ hexo server 输入Ctrl+C停止服务。 2.Hexo配置用编辑器打开 Hexo-admin&#x2F; 下的配置文件_config.yml找到： 12345# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: repository: 到GitHub的chaooo.github.io仓库下，点击Clone or download,复制里面的HTTPS地址到repository:，添加branch: master。 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/chaooo/chaooo.github.io.git branch: master 3.完成部署最后一步，快要成功了，键入指令： 123$ npm install hexo-deployer-git --save$ hexo generate$ hexo deploy 输入弹出框的用户名与密码(首次使用git会弹出)。OK，我们的博客就已经完全搭建起来了，在浏览器输入（当然，是你的Repository名），例如我的：chaooo.github.io&#x2F;每次修改本地文件后，需要键入hexo generate才能保存，再键入hexo deploy上传文件。成功之后命令行最后两句大概是这样： 123To https://github.com/chaooo/chaooo.github.io.git 7f3b50a..128a10d HEAD -&gt; masterINFO Deploy done: git 当然，不要忘了回退到父文件夹提交网站相关的文件以备今后迁移，依次执行git add .、git commit -m “…”、git push origin develop。 日常操作1.写文章执行new命令，生成指定名称的文章至 Admin-blog\\source_posts\\文章标题.md 。 1$ hexo new [layout] &quot;文章标题&quot; #新建文章 然后用编辑器打开“文章标题.md”按照Markdown语法书写文章。 其中layout是可选参数，默认值为post。到 scaffolds 目录下查看现有的layout。当然你可以添加自己的layout， 同时你也可以编辑现有的layout，比如post的layout默认是 hexo\\scaffolds\\post.md 1234title: &#123; &#123; title &#125; &#125;date: &#123; &#123; date &#125; &#125;tags:--- 我想添加categories，以免每次手工输入，只需要修改这个文件添加一行，如下： 12345title: &#123; &#123; title &#125; &#125;date: &#123; &#123; date &#125; &#125;categories:tags:--- 文件标题也是md文件的名字，同时也出现在你文章的URL中，postName如果包含空格，必须用”将其包围。请注意，大括号与大括号之间我多加了个空格，否则会被转义，不能正常显示；所有文件&quot;：&quot;后面都必须有个空格，不然会报错。 2.提交每次在本地对博客进行修改后，先执行下列命令提交网站相关的文件。 123$ git add .$ git commit -m &quot;...&quot;$ git push origin develop 然后才执行hexo generate -d发布网站到master分支上。 1$ hexo generate -d 3.本地仓库丢失当你想在其他电脑工作，或电脑重装系统后，安装Git与Node.js后，可以使用下列步骤： 3.1拷贝仓库1$ git clone -b develop https://github.com/chaooo/chaooo.github.io.git 3.2配置Hexo在本地新拷贝的chaooo.github.io文件夹下通过Git bash依次执行下列指令: 1234$ npm install -g hexo-cli$ npm install hexo$ npm install$ npm install hexo-deployer-git --save 小Tips:hexo 命令1234567891011121314hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&#x27;ctrl + c&#x27;关闭server）hexo deploy #将.deploy目录部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本hexo deploy -g #生成加部署hexo server -g #生成加预览#命令的简写hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy","tags":[{"name":"hexo","slug":"hexo","permalink":"http://chaooo.github.io/tags/hexo/"}]},{"title":"Hello World","date":"2016-05-06T12:59:59.000Z","path":"2016/05/06/hello-world.html","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]